# 参考

- https://dev.classmethod.jp/articles/yoshim_2017ad_tfidf_1-2/
- https://ja.wikipedia.org/wiki/Tf-idf

# tf-idfとは

## TF(Term Frequency)とは

- "単語の出現頻度" "単語頻度"と呼ばれる。
- 各文書においてその単語がどのくらい出現したのか」を意味する
- (**よく出現する単語は、その文書の特徴を判別するのに有用！**)

$$
tf = \frac{\text{文書Aにおける単語Xの出現頻度}}{\text{文書Aにおける全単語の出現頻度の和}}
$$

もう少し数学的に定義式を書いてみる。

$$
tf(t, d) = \frac{f_{t,d}}{\sum_{t'\in d} f_{t', d}}
$$

ここで、$f_{t,d}$は文書dに含まれるある単語tの出現頻度(raw count)である。単純には、分母には文書dに含まれる総単語数を用いる。

### TFの定義の様々な種類(=tf-idfにおけるtf側の重み付け方法)

$tf(t, d)$の定義は他にも様々なものがある。

- binary
- raw count
- term frequency
- log normalization
- double normaliation 0.5
- double normalization K

## IDF(Inverse Document Frequency)とは

- "逆文書頻度"と呼ばれる。
- idfは、「**その単語がどれくらい情報を提供するか**」という指標。
  - すなわち、ある単語が、すべての文書の中で普遍的なのか珍しいのかということ
  - **単語が「レア」なら高い値**を、「**色々な文書によく出現する単語」なら低い値**を示すもの。
  - （**レアな単語は、その文書の特徴を判別するのに有用**！）
- (logを使っているのは、文書数の規模に応じた変動影響を緩和するため）
- (右辺に１を足す場合もある。これはidfが0にならないようにするため。)

$$
idf = \log(\frac{\text{全文書数}}{\text{単語Xを含む文書数}})
$$

もう少し数学的に定義式を書いてみる。

$$
idf(t, D) = \log \frac{N}{|{d \in D : t \in d}|}
$$

ここで、

- $N$はコーパスに含まれる文書の総数$|D|$。
- $|{d \in D : t \in d}|$は単語tが含まれる文書の数。
  - すなわち、全ての文書$d \in D$の少なくとも１つ以上の文書dにおいて、$tf(t,d) \neq 0$である必要がある。分母がゼロになってしまうので。
  - **ゼロ除算を防ぐため**に、分母を$1 + |{d \in D : t \in d}|$と調整するのが一般的...!!

### idfの定義の様々な種類(=tf-idfにおけるidf側の重み付け方法)

- idfを使用しない
- 標準的なidf
- +1をしてスムージングを行うidf
- 最大値を取るidf
- 確率論的idf

## TFとIDFを踏まえて、TF-IDFとは??

ここまでをまとめますと「tf」、「idf」の意味については下記のようだと解釈できる

- TF:ある文書において、どの単語がどれくらい出現したのか?
- IDF:全文書において、各単語がどれくらいレアか??

TF-IDFは上記の２つの概念(指標)を組み合わせた指標=>２つの指標の積!

tfidf(t, d, D): コーパスDにおける文書dの単語tのtfidf

$$
tfidf(t, d, D) = tf(t, d) \cdot idf(t, D)
$$

- **tf-idfの重みが高くなる**のは、（与えられた文書内で）**その単語の単語頻度(term frequency, tf)が高く**、かつ、**文書集合全体においてその単語の文書頻度(document frequency, df)が低い**場合である。
  - それゆえに、重みは普遍的な語をフィルタする傾向がある。
- **idfの対数内の値(分数)は常に1以上**なので、idf及びtf-idfの値は**常に0以上**になる。
  - 単語がより多くの文書に現れる場合、対数の中の分数は1に近づき、それゆえにidfとtf-idfは0に近づく。

