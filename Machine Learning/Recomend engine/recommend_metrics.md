# レコメンドの評価とは？

レコメンドの評価とは、与えられたケースにおける**良いレコメンドとは何か**を決定する枠組みである。
これには、何が正解かという明確な答えはなく、**状況や目的に応じた様々な評価方法や指標**があり、指標も日々新たに考案されアップデートされている感じらしい...。

評価指標が多く存在する為、**どれを優先させて見るか**というのは、**最終的にレコメンドに求めるビジネス面のゴールや KPI によって決定**される。

# 評価方法

レコメンド評価には、大きく分けてオフライン評価とオンライン評価の 2 つがある。

## オフライン評価

- ユーザの商品に対するレーティング（評価値）や購買など、**利用履歴を用いた評価**を意味する。
  - まず、利用履歴を学習データとテストデータに分割し、学習データのみでレコメンドモデルを構築する。
  - 次に構築したモデルをテストデータに適用して、ユーザのレーティングや購買を予測し、どの程度正確に言い当てられるかを評価。
  - 学習時にテストデータを隠しておく（リーブアウトする）ことで、ユーザの未知の行動を予測できるかシミュレートしている感じ。
  - **データを分割してモデルを評価するのは一般的な機械学習での評価方法と同じ**。分割は、ホールドアウトや K-fold 交差検証を用いて行うのが一般的。
- 長所：
  - オフライン評価は現行システムにロジックを実装する必要もなく、**ユーザの利用履歴さえあればモデルを評価することが可能**なため、手軽に行える。
    - ＝＞実務でも、実サービスで実験する前にオフライン評価でアルゴリズムの有効性を検証した上でサービス実装することが一般的。
  - ユーザとインタラクションする必要がないため制約も少なく、論文でのモデルの精度評価やコンペティションの精度比較はオフラインがメインとなる。
- 短所：
  - **実サービスにてレコメンドを行う場合は、オフラインとは異なる状況となる**。
    - 例えば、**実際におすすめを提示することで購入されるアイテム**は、オフラインでは評価できない。
    - 言い換えると、そもそも**オフライン評価は、私達がおすすめしようとするまいとユーザが購入したものを予測している**ので、それを正確に予測できることにどれだけ意味があるかは議論の余地がある。
    - 実際に知りたい(評価したい)のは、「**私達がオススメした時にユーザが購入するかどうか？**」
    - ＝＞オフライン評価と実サービスでの評価が必ずしも一致しない...!＝オフライン評価の限界。

## オンライン評価

- **実サービスにレコメンドを実装**してモデルを評価する。
  - **実際に一部のユーザに、予測結果に基づいたレコメンド内容を提示し、良い評価を受けるかを確認**する。 -　**A/B テスト**と呼ばれる評価方法はこのオンライン評価。
  - 対象ユーザをランダムに振り分け、一方のユーザには既存のロジックのレコメンドを提示（コントロール）、もう一方は比較したい新しいロジックを提示して（トリートメント）、両群の結果を比較して評価する。
- 長所：
  - 実際のサービスでの最終的なレコメンドの評価となりうるので、オンライン評価は重要視される。
- 短所：
  - ユーザに実験内容を提示するため、手間やリスクも大きくオフライン評価ほど手軽ではない。

# レコメンド精度の評価指標の種類

チートシートは以下
![レコメンド評価指標のcheet sheet](images/cheetSheet_recommendMetrics.png)

# 回帰の評価指標

Explicit データに対する?

# 分類の評価指標

Implicit データに対する?
評価の大まかなコンセプトはｙ

1. Precision@K
   - レコメンドした K 個のアイテムにおいて、実際にユーザが嗜好したアイテムの割合。
   - $$ \frac{N(rec(K) \cap actual)}{K}$$
2. Recall@K
   - 実際にユーザが嗜好したアイテムにおいて、レコメンドした K 個のアイテムが含まれる割合。
   - $$ \frac{N(rec(K) \cap actual)}{N(actual)}$$
3. f1-score@K
   - Precision@K と Recall@K の両方を考慮した指標。
   - $$ \frac{2\cdot Precision@K \cdot Recall@K}{Precision@K + Recall@K}$$
4. MAP@K
   - ユーザ毎に Precision＠K の K についての平均(AP@K)を算出し、その結果についてユーザ全体の平均を取る。
   - $$ AP@K = \sum_{i=1}^k{\frac{Precision@i}{min(k, N(actual))}} $$
   - $$ MAP@K = \sum\_{u \in U}{\frac{AP@K}{N(U)}}$$
   -

## Precision@K

aa

## Recall@K

## f1-score@K

## MAP@K

- Mean Average Precision の略。
- MAP の前に、AP(Average Precision)について。
  - Precision は閾値によって変化する。
  - AP は適合アイテムが出現した時点をそれぞれ閾値として、閾値ごとの Precision を算出し、Precision の平均をとったものである。
  - なお**AP は PR 曲線の AUC と同じ値**となる。
  - ユーザuのAPは以下の様に表される。
  - 

# 参考

- https://yolo-kiyoshi.com/2019/12/03/post-1557/
- https://blog.brainpad.co.jp/entry/2017/08/25/140000
