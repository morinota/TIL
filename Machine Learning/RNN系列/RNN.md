# RNN(Recurrent NN、再起的ニューラルネットワーク)の意味合いを解釈！

「長いものを理解する」NN

## RNNの数式

$$
入力:x^{(0)}, x^{(1)}, \cdots, x^{(T)} \\
(y^{(t)}, h^{(t)} = f(x^{(t)}, y^{(t-1)}, h^{(t-1)}))\\
出力: y^{(0)}, y^{(1)}, \cdots, y^{(T)}
$$

この数式(特に真ん中)が、RNNがどういう思想で作られたものを表している。

## 一行目

$x^{(0)}, x^{(1)}, \cdots, x^{(T)} $という、順番のあるデータ＝**系列データ**という。

- 時系列データ
- 動画、音声
- 自然言語

## 二行目

$(y^{(t)}, h^{(t)} = f(x^{(t)}, y^{(t-1)}, h^{(t-1)}))$の中の、関数fがRNNを意味する。
$h^{(t)}$で情報が流れていく。
![](https://camo.qiitausercontent.com/b1423e0dc6a49df2e446eaa7366d256d5dba4837/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3335383139322f63326530343034342d313564392d346666382d303733632d3732316165346366386564662e706e67)

上図ではオレンジ色の部分がRNN。

## 上図は何を為ているのか

=>「**系列データ処理**の模倣」これがRNNがやりたいこと。

代表格＝＞機械翻訳。日本語の文章を英語の文章に飜訳するタスクを例に考えていく。
「私はアイシアです。」
「I am AIcia.」
この飜訳を、まず人間の頭がどうやっていくか考える。

- まず上の文章を理解する。
  - 一単語目から読み始めるぞ～！
  - 私についての文！
  - 私が何かを喋ろうとしてる！
  - 私はアイシアの何かなんだな！
  - 私＝アイシアなんだな！
  - 「私＝アイシア」で文終了！
- 訳すときの脳内は...
  - 「私＝アイシア」を書こう！
  - 私＝＞I
  - 後は「＝アイシア」でOK！am
  - AIcia
  - 全部書いたから.を打つ。

系列データを入力して、系列データを出力してる！
人間は「心の中の状態」が上記の各ステップを経る毎にどんどん変わってる！
つまり、**系列データ処理**では**心の状態**を更新している！

- 上の段：
  - 入力$x_t$に応じて、文の理解$h_t$を更新。
  - つまり、$h_t = f(x_t, h_{t-1})$
- 下の段：更に2種類行われてる。
  - 残タスク$h'$に応じて、次の単語$y_t$を出力。
    - すなわち、$y*t = f(h'*{t-1}) $
  - 出力$y$に応じて、残タスク$h$を更新
    - すなわち、$h'_{t} = f(y_t, h'_{t-1})$

tの位置とかプライム'とか色々あるが、この3つを全部くっつけるとRNNの数式：

$$y^{(t)}, h^{(t)} = f(x^{(t)}, y^{(t-1)}, h^{(t-1)})$$

解釈：「**出力と次の状態は、今の入力と前の出力と前の状態から決まる！**」

「情報hを更新しながら流していく」事がRNNで一番大事なこと！本来のhの意味はhidden layerだけど、heartのhだと思えばOK！

# RNN の3通りの使い方 - RNN の混乱ポイントを倒す！

RNNで扱われるタスクには三種類ある。

![](https://camo.qiitausercontent.com/b1423e0dc6a49df2e446eaa7366d256d5dba4837/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3335383139322f63326530343034342d313564392d346666382d303733632d3732316165346366386564662e706e67)

## まず、RNNの三種類の"InputとOutputの組み合わせ"

RNNの3つの役割を理解するには、InputとOutputが何かを理解する。
上図でいえば、矢印の根っこについてるのがInput。矢印の先についてるのがOutput。
気づいたかもしれないが、それぞれ2種類ずつある。

- Input:
  - 系列データの入力$x_0, x_1, \cdots, x_t$
  - or まだ一文字も読んでない時の心の状態$h_{-1}$
- Output:
  - 系列データを出力として見るパターン(yを全部使うパターン$y_0, y_1, \cdots, y_t$, またはhを全部使うパターン$h_0, h_1, \cdots, h_t$)
  - 最後のyとhだけを出力として見るパターン$y_{T}$や$h_{T}$

InputとOutputの組み合わせによって、RNNの役割が多様になる。
RNNには大まかには3つのパターンがある。

- 入力=系列データの入力, 出力=系列データの出力
- 入力=系列データの入力, 出力=最後のyとhだけ
- 入力=最初のhだけ, 出力=系列データの出力

### 1. 系列＝＞系列

このパターンの役割「系列の構造を保って変換」。
＝＞重ねて利用できる。RNNを重ねるって意味?

### 2. 系列＝＞非系列

可変長の入力を固定次元のベクトルへ。
RNNの特徴＝**入力の次元がバラバラ**でも、このモデルを挟んだら、毎回**同じ次元のベクトルにする事**ができる。＝＞以降の処理を簡単にできる？
なので、RNNの特徴として「可変長の入力に対応できる」事がアピールされる。

このパターンのRNNの役割は「**系列＝＞意味の符号化**」。
$h_T$は、全ての入力に対して理解できた、本質的な意味・心の状態を意味する。
この役割の事をEncoderという。

### 3. 非系列＝＞系列

このパターンのRNNの働きは「意味＝＞系列の変換」。
この役割の事をDecoderという。

## 三種類のタスク

### 1. seg2vec (系列＝＞非系列)(しーぐ to ベック)

例えば、以下の例が有名。

- テキスト＝＞トピック分類
- 時系列データ＝＞次の値予測

### 2. vec2seg (非系列＝＞系列)

例えば、

- 画像＝＞キャプション生成
  - ex)画像「猫がくつろいでる写真」＝＞テキスト「This cat is relaxing.」

実際には、画像データ=CNNで意味抽出=> $h_{-1}$(良い感じのベクトル) =RNN=> this cat is relaxing.
(これも後述する、**Encoder-Decoder model**)

### 3. seg2seg (系列＝＞系列)

例えば、

- 音声 =RNN=> テキスト
  - (聞いた瞬間から逐次的(順を追って一歩一歩進むような)にメモし始める感じ)
  - 入出力の系列に、直接対応があるパターン。
- 日本語 =RNN=> $h_T => h_{-1}$ =RNN=>英語
  - 2段階の変換をかませる必要がある。
  - 「系列＝＞意味＝＞系列」こういったモデルの事を、**Encoder-Decoder model**という。


