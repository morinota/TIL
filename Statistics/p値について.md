# 参考

- https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4877414/

# Abstract

# Introduction

# Statistical tests, P values, and confidence intervals: a caustic primer

## Statistical models, hypotheses, and tests

## Uncertainty, probability, and statistical significance

## Moving from tests to estimates

# What P values, confidence intervals, and power calculations don’t tell us

## Common misinterpretations of single P values

- ThePvalue is the probability that the test hypothesis is true; for example, if a test of the null hypothesis gaveP = 0.01, the null hypothesis has only a 1 % chance of being true; if instead it gaveP = 0.40, the null hypothesis has a 40 % chance of being true.

- ThePvalue for the null hypothesis is the probability that chance alone produced the observed association; for example, if thePvalue for the null hypothesis is 0.08, there is an 8 % probability that chance alone produced the association.

- A significant test result (P ≤ 0.05) means that the test hypothesis is false or should be rejected.

- A nonsignificant test result (P > 0.05) means that the test hypothesis is true or should be accepted.

- A largePvalue is evidence in favor of the test hypothesis.

- A null-hypothesisPvalue greater than 0.05 means that no effect was observed, or that absence of an effect was shown or demonstrated.

- Statistical significance indicates a scientifically or substantively important relation has been detected.

- Lack of statistical significance indicates that the effect size is small.

- ThePvalue is the chance of our data occurring if the test hypothesis is true; for example,P = 0.05 means that the observed association would occur only 5 % of the time under the test hypothesis.

- If you reject the test hypothesis becauseP ≤ 0.05, the chance you are in error (the chance your “significant finding” is a false positive) is 5 %.

- P = 0.05 andP ≤ 0.05 mean the same thing.

- Pvalues are properly reported as inequalities (e.g., report “P < 0.02” whenP = 0.015 or report “P > 0.05” whenP = 0.06 orP = 0.70).

- Statistical significance is a property of the phenomenon being studied, and thus statistical tests detect significance.

- One should always use two-sidedPvalues.

## Common misinterpretations of P value comparisons and predictions

- When the same hypothesis is tested in different studies and none or a minority of the tests are statistically significant (allP > 0.05), the overall evidence supports the hypothesis.

- When the same hypothesis is tested in two different populations and the resultingPvalues are on opposite sides of 0.05, the results are conflicting.

- When the same hypothesis is tested in two different populations and the samePvalues are obtained, the results are in agreement.

- If one observes a smallPvalue, there is a good chance that the next study will produce aPvalue at least as small for the same hypothesis.

- The specific 95 % confidence interval presented by a study has a 95 % chance of containing the true effect size.

- An effect size outside the 95 % confidence interval has been refuted (or excluded) by the data.

- If two confidence intervals overlap, the difference between two estimates or studies is not significant.
-
- An observed 95 % confidence interval predicts that 95 % of the estimates from future studies will fall inside the observed interval.
- If one 95 % confidence interval includes the null value and another excludes that value, the interval excluding the null is the more precise one.

## Common misinterpretations of power

- If you accept the null hypothesis because the nullPvalue exceeds 0.05 and the power of your test is 90 %, the chance you are in error (the chance that your finding is a false negative) is 10 %.
- If the nullPvalue exceeds 0.05 and the power of this test is 90 % at an alternative, the results support the null over the alternative.

# A statistical model is much more than an equation with Greek letters

# Conclusions
