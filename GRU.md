# GRU - RNNに記憶を持たせる試みその1

RNNに使われる層の一種＝Gated Recurrent Unitは、RNNに対して長期的な記憶力を与えるという試みの中で生まれたもの。

## Simple RNNの限界

$$
Input:x^{(0)}, x^{(1)}, \cdots, x^{(T)} , h^{(-1)}\\
y^{(t)}, h^{(t)} = f(x^{(t)}, y^{(t-1)}, h^{(t-1)})\\
Output: y^{(0)}, y^{(1)}, \cdots, y^{(T)}, h^{(T)}
$$

Simple RNN (in Keras)の場合、

$$
y^{(t)} = h^{(t)} = \tanh(x^{(t)} \times W_xh + h^{(t-1)} \times W_hh + b_h)
$$

また、上式は活性化関数がtanhのDense Layerに他ならないので以下のように表せる。(GRUの説明で使う記法)

$$
y^{(t)} = h^{(t)} = f^{\tanh}(x_t, h_{t-1})
$$

なお$x^{(t)}$と$h^{(t-1)}$は横ベクトルとして実装されている。b_hは定数項の横ベクトル。W_xhとW_hhは行列。
よってtanhの中身は、隠れ層$h^{(t)}$と同じ次元の横ベクトル。
これに、成分毎にtanhをひっかけたものが$y^{(t)} = h^{(t)} $
(今回はsigmoidやtanhがどばどば出てくる。正規化する関数)

＝＞この式では、hの値(や向き)がころころ変わる。(xが足されるから！)
＝＞昔の情報がどんどん薄れていってしまう！！

## 困るのはどんな時？？

単純なタスクでは問題にならない。
一方で何万字ある文章を要約するようなタスクだと、昔の情報も覚えてなきゃいけない。
よってSimple RNNでは、重厚なタスクは解けない！
この問題を解決するのがGRU！

## GRUの場合の数式

f()の形がSimple RNNと変わる！
fは以下。

$$
z^{(t)} = f_{z}^{\sigma}(x^{(t)}, h^{(t-1)}
$$

$$
r^{(t)} = f_{r}^{\sigma}(x^{(t)}, h^{(t-1)}
$$

$$
\tilde{h}^{(t)} = f_{h}^{\tanh}(x^{(t)}, r^{(t)} \circ h^{(t-1)})
$$

$$
h^{(t)} = (1 - z_t) \circ h_{t-1} + z_t \circ \tilde{h}_t
$$

まさにゲートが暴れてて、記憶力の解決に向かってる！

添え字の$\sigma$はシグモイド関数を意味する。
$\circ$はアダマール積(要素毎の積)を意味する。

## 数式の意味合い

ポイントは、左辺のそれぞれの登場人物。その**定義と使われ方**を押さえればOK！！

- まずz。
  - 定義は、シグモイド関数を使った全結合層で、x_tとh_t-1から作られてる。
  - 使われ方＝右辺にzがいつ登場するか？
    - 4つ目の式。使われ方にzの意味がほぼ込められてる。
    - 前のHidden layerの値h_t-1と、新しく作られたHidden layerの値$\tilde{h}_t$を、(1-z)：zの比でブレンドしてh_tを作ってください！
    - ＝＞zは、前のhと新しいhの配合比率を意味する！
    - もしzがほぼ0なら。
      - h_t はほぼh_t-1。あんまり変わらない。文脈があんまり変わらない場面では、前と同じHidden Layerを次に渡す。
    - 逆に段落の変わり目などでは、z=1に近づく。
      - ＝＞前のHidden Layerの影響がほぼなくなる。
    - 実際には、**隠れ層ベクトルの成分毎**にzを指定できる(アダマール積だから！)
  - 意味：**hをどの程度更新するかを決める**＝＞「**Update gate vector**」と呼ばれる！
- r。
  - 定義は、シグモイド関数を使った全結合層で、x_tとh_t-1から作られてる。
  - 使われ方
    - 3つ目の式で、h_t-1と成分毎に掛け合わせてる。＝＞$\tilde{h}\_t $の定義に使われてる？
    -
  - 意味：
    - rが極端な値の場合をまず考えるのがオススメ。
    - r = 0
      - 隠れ層の値を更新する為のh_tilde_tの生成の際、h_t-1の影響がほとんど無視される。
      - 昔の文脈・情報を忘れる・リセットしてる感じ？？
    - r = 1
      - x_tとh_t-1の情報を平等に使って、h_tilde_tを生成する。
      - 文脈と新しい単語入力の両方を良い感じに見ましょうって感じ！
  - ＝＞rの使われ方は、「h_tildeを決める前に、h_t-1をr倍する」
  - ＝＞従って意味は、「前までの情報をどれだけ弱めるか」＝＞「Reset Gate Vector」と呼ばれる！
- h_tilde：
  - tanh関数を使った全結合層で、x_tとr倍されたh_t-1から作られている。
  - h_tを作る時に、h_t-1と混ぜられて使われる。
  - 意味：GRUの心の中hを、xを加味して更新する。

## まとめ

このように、zとrがGRUの本質的な部分。
rによって前までの情報をどれくらい忘れるか、zによって心の中をどれくらい更新するか。
このように「前のものをどの程度忘れるか」を制御することで、RNNに長期記憶力を持たせる。これがGRU。

## プラスアルファ：DNN勉強してたら絶対出てくる図の書き方と意味

![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/Gated_Recurrent_Unit%2C_base_type.svg/780px-Gated_Recurrent_Unit%2C_base_type.svg.png)

まず外側の枠：RNNの基本要素のRNN部分、を意味する。下図のオレンジ色の部分とイコール。

![](https://camo.qiitausercontent.com/b1423e0dc6a49df2e446eaa7366d256d5dba4837/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3335383139322f63326530343034342d313564392d346666382d303733632d3732316165346366386564662e706e67)

上図ではオレンジ色の中身を記述していく！
○印は、何らかの演算を意味する。○の中に、演算記号を入れる。
ここでの×はアダマール積。
4行の数式と意味は同じになる。