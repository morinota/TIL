---
format:
    revealjs:
        # incremental: false
        # theme: [default, custom_lab.scss]
        theme: [default, ../custom_lab.scss]
        logo: https://s3-ap-northeast-1.amazonaws.com/qiita-image-store/0/1697279/dfa905d1c1e242b4e39be182ae21a2b6ac72c0ad/large.png?1655951919
        footer: "â‡’ https://qiita.com/morinota"
from: markdown+emoji

fig-cap-location: bottom

title: Misunderstanding Causal Effect by Including "Intermediate Variable" \n ä¸­é–“å¤‰æ•°ã‚’åŠ ãˆã‚‹äº‹ã«ã‚ˆã‚‹å› æœåŠ¹æœã®èª¤è§£
subtitle: Things to keep in mind when you want to discuss "Causal Relation" through Ovservational Data:part 3 \n è¦³å¯Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å› æœé–¢ä¿‚ã‚’è­°è«–ã—ãŸã„æ™‚ã«æ³¨æ„ã™ã¹ãã“ã¨:part 3
date: 2022/11/15
author: Masato MORITA
title-slide-attributes: 
  data-background-image: https://i.imgur.com/nTazczH.png
  data-background-size: contain
  data-background-opacity: "0.5"
---
# Today's My Topic

I want to share one of the idea of **Statistical Causal Inference (çµ±è¨ˆçš„å› æœæ¨è«–)**:

- One of the application fields of statistics and machine learning.
- The aim of Statistical Causal Inference (çµ±è¨ˆçš„å› æœæ¨è«–) is analysing causal relation using **observational dataset**(not experimental data).

# First of all, **Why I chose this topic**??

1. I have the impression that more and more members are using regression analysis in their research activities these days.

2. I recently read a book on statistical causal inference and learned how to analyze causality using regression analysis, so I want to share some information a little bit...!

3. You all want to discuss causal relation in your own analysis & research..??


# Today's My Objective

- (Dear all)
    - I want to share **the patterns that may mislead us** when discussing causal relation based on observational data.

- (Dear user of regression analysis)
    - I want to share **what we should careful when interpreting the value of regression coefficient** from viewpoint of causal inference.

## Outline

1. Review:Correlation vs Causation, Confounding, Collider bias
    - (å¾©ç¿’)ç›¸é–¢é–¢ä¿‚vså› æœé–¢ä¿‚ã€äº¤çµ¡, åˆæµç‚¹ãƒã‚¤ã‚¢ã‚¹
2. When **Correlation $\neq$ Causal relation** happens (i.e. regression coefficient $\neq$ causal effect)? part 3: Bias by intermediate variable
    - ã©ã®å ´åˆã«å›å¸°ä¿‚æ•°ã®å€¤ã¨å› æœåŠ¹æœã®å€¤ãŒã‚ºãƒ¬ã‚‹ï¼Ÿpart 3: ä¸­é–“å¤‰æ•°ã‚’åŠ ãˆã‚‹äº‹ã«ã‚ˆã‚‹ãƒã‚¤ã‚¢ã‚¹

# 1. Review:Correlation vs Causation(Causal relation), Confounding, Collider bias

(å¾©ç¿’)ç›¸é–¢é–¢ä¿‚vså› æœé–¢ä¿‚ã€äº¤çµ¡ã€åˆæµç‚¹ãƒã‚¤ã‚¢ã‚¹

## What is Causal Relation(å› æœé–¢ä¿‚)?
- The definition of causal relation is ...
    - In case that **"when factor X is changed (intervention), factor Y also changes,"** we can say "there is a **causal relationship of factor X â†’ factor Y**
- The difference with correlation is ...
![](https://i.imgur.com/DrJbhL9.png)
- Therefore, the insight from Observational Data are basically Correlation.

## What is Causal Effect(å› æœåŠ¹æœ)?
- The definition of causal relation is ...
    - In case that **"when factor X is changed (intervention), factor Y also changes**," we can say "there is a **causal relationship of factor X â†’ factor Y**

- In above case,
    - X: "cause"(åŸå› å¤‰æ•°), Y: "outcome"(çµæœå¤‰æ•°)
    - "**causal effect(intervention effect)** of $X \Rightarrow Y$" :Average change of Y when X is changed by one unit

In Causal Inference, one motivation is **to quantify of causal effect** from observational data.

## Regression Analysis å›å¸°åˆ†æ

:::: {.columns}
:::{.column width="60%"}
â€œRegression Analysisâ€ å›å¸°åˆ†æ is assuming relation (ğ’™ and ğ’š) as a mathematical formula likeâ€¦

$$
y = f(x, \theta)
$$

and estimating appropriate parameters (ğœ½) from dataset( = combination of x and y).

Example of $y = f(x, \theta)$:

- Linear Regression model:
    - $y = f(x) = a_0 + a_1 x + \epsilon = \mathbf{x}^T \mathbf{a} + \epsilon$
    - $\epsilon \sim \text{Norm}(\mu=0, \sigma^2)$
- Supervised machine learning models
    - $y = f(x) = f_1(f_2(f_3(\cdots f_N(x) \cdots)))$
:::
:::{.column width="40%"}

![](https://i.imgur.com/CKfZps4.png)
![](https://i.imgur.com/KHtM4OF.png)

:::
::::
## What is Linear Regression Analysis?

It's a little bit mathematical.

$$
\mathbf{y} = f(\mathbf{x_1}, \cdots, \mathbf{x}_k) + \mathbf{\epsilon}\\
= a_0 + a_1 \mathbf{x}_1 + \cdots + a_k \mathbf{x}_k + \mathbf{\epsilon}
= \mathbf{a}X + \mathbf{\epsilon}
$$

where is ...

- $\mathbf{y}$ : (in Causal Inference, it should be "outcome")
- $X$ : (in Causal Inference, it should include "cause")
- $\mathbf{a}$ : the regression coefficent of $X$.
- $\mathbf{\epsilon}$ : Probabilistic component of y (i.e. random term)

In the most popular method, collecting the pair of $X$ and $y$ as observational data, and estimate $\mathbf{a}$ by minimizing the sum of $\mathbf{\epsilon}$.

## the usage of Regression analysis

the usage of Regession analysis is separated mainly three objectives:

![](https://i.imgur.com/fz7Mzgz.png)

from viewpoint of "Causal Inference", regression analysis is utilized for "**Description**" and "**Control/Intervention**".

## Example of "Correlation = Causal relation": height=>weight

:::: {.columns}

::: {.column width="50%"}

![Height & weight of 100 randomly obtained Nagoya citizens](https://i.imgur.com/d2sdIxu.png)

:::

::: {.column width="50%"}

Now, let's imagine that we have obtained the height & weight data of 100 randomly selected people in Nagoya City as Observational Data.

$$
\text{weight} = a_0 + a_1 \times \text{height} + \epsilon \\
\Rightarrow a_1 = 0.34 (p = 0.00)
$$

In this case, **$a_1$ can be interpret 'causal effect' of 'height' $\Rightarrow$ 'weight'.**

However, the value of the regression coefficient **does not necessarily fit** with the causal effect.

:::

::::

# 2. When is **regression coefficients** $\neq$ **causal effects** happend?

there are mainly 4 cases :

1. Direction of causation is opposite å› æœã®å‘ããŒé€†

2. **Confounding** äº¤çµ¡

3. Collider Bias åˆæµç‚¹ãƒã‚¤ã‚¢ã‚¹

4. Bias by Intermediate variables

## What is â€Confounding(äº¤çµ¡)"?


When we discuss the relation of x & y, **is it enough to focus on only x & y??**
Let's imagine the example of â€œgrip strengthâ€ and â€œmath abilityâ€ in elementary school.

::::{.columns}
:::{.column width="50%"}
:::{.r-stack}
![](https://i.imgur.com/VPVYdPU.png){.fragment}

![](https://i.imgur.com/7BSKG4V.png){.fragment}
:::

:::
:::{.column width="50%"}

![](https://i.imgur.com/PmE2t50.png)

:::
::::

Originally, **x and y donâ€™t have causation**. However, **ğ¶ makes Correlation between X and Y**.
=> Correlation $\neq$ Causation is happened! it's called â€œ**Confounding (äº¤çµ¡)**â€.

So the essence of this phenomena is â€¦ the observed relation will be changed **depending on whether we consider C or not**!

## Solution of Confounding:

:::: {.columns}

::: {.column width="50%"}

![Causal Diagram with confounding factor ](https://i.imgur.com/zGtNK3m.png){width=100%}

:::

::: {.column width="50%"}

- So, the way "to estimate causal effect in **Confounding**" is **Cut the indirect upstream connection** between X and Y.
    - => One way is **adding confounding factors to explanatory variables**.
- In order to do that ...
    1. imagine and draw structure of relations (it's called '**causal diagram**')
    2. consider **what will be confaunding factor** of X and Y
    3. observe the candidates of confounding factor similar to X and Y.
:::

::::

## Summary of my previous previous short lecture

- Correlation $\neq$ Causation(Causal relation)
- When discussing causal relation from observational data, we should consider **confounding factors**(i.e. **indirect upstream connection** from X to Y).
- When estimating causal effects from regression coefficients through regression analysis, we should **check confounding factors** and **add them to the explanatory variables**.
<!-- TODO: å…¨å“¡ãŒå›å¸°åˆ†æçŸ¥ã£ã¦ã‚‹è¨³ã§ã‚‚ç„¡ã„ã®ã§ã€ä»¥ä¸‹ã®ã‚ˆã†ã«æ›¸ãæ›ãˆãŸæ–¹ãŒè‰¯ã„ã‹ã‚‚ã€‚ -->
<!-- è¦³å¯Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å› æœåŠ¹æœã‚’å®šé‡åŒ–ã™ã‚‹éš›ã¯ã€äº¤çµ¡å› å­ã®å½±éŸ¿ã‚’ã‚«ãƒƒãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚å›å¸°åˆ†æã®å ´åˆã¯èª¬æ˜å¤‰æ•°ã«åŠ ãˆã‚‹ã®ãŒä¸€ã¤ã®æ‰‹ã€‚ -->

# However...

So, according to my last short lecture...

- we should add **factors that seem to be related to X(cause) & Y(outcome)** to the model as explanatory variables"...??

However...in actual... I am afraid that...**this is also not necessarily true**!

- Sometimes, **due to adding** factors that seems to be related to the X(cause) & Y(outcome), "regression coefficients $\neq$ causal effects"(i.e. Correlation $\neq$ Causation) is happened.

=> In my previous short lecture, I talked about **Collider Bias**.

# 2. When is **regression coefficients** $\neq$ **causal effects** happend?

there are mainly 4 cases :

1. Direction of causation is opposite å› æœã®å‘ããŒé€†

2. Confounding äº¤çµ¡

3. **Collider Bias åˆæµç‚¹ãƒã‚¤ã‚¢ã‚¹**

4. Intermediate variables are included

## What is "collider bias" (åˆæµç‚¹ãƒã‚¤ã‚¢ã‚¹)??
:::: {.columns}

::: {.column width="50%"}
"collider" is ...

- here, in these causal diagrams, factor B is called "collider(åˆæµç‚¹)" of X and Y:

"collider bias(åˆæµç‚¹ãƒã‚¤ã‚¢ã‚¹)" is ...

- the bias(åã‚Š) caused by **selecting(é¸æŠ)/ stratifing(å±¤åˆ¥åŒ–)/ adjusting(èª¿æ•´) data at the "collider"** of cause(X) and effect(Y) in the data collection process or analytics process.

:::

::: {.column width="50%"}

![Causal Diagram having Collider](https://i.imgur.com/obXAU5R.png){width=100%}

:::

::::

## Example of Collider Bias: enviromnental subsidy

:::: {.columns}
::: {.column width="50%"}

![](https://i.imgur.com/zqf4iWo.png){width="60%"}

:::{.r-stack}

![](https://i.imgur.com/2zNOSUC.png){.fragment width="80%"}

![](https://i.imgur.com/oyrnC2l.png){.fragment width="80%"}

:::

:::

::: {.column width="50%"}

- Please imagine **a environmental subsidy(ç’°å¢ƒè£œåŠ©é‡‘) for automobile makers** to promote "reduce their environmental impact" of automobile.
- **"Whether receiving subsidy or not"** depends on two indicators: **"CO2 emissions"** and **"TMR(Total Material Requirement)"** in the one automobile's life cycle.
- by **selecting(é¸æŠ)/ stratifing(å±¤åˆ¥åŒ–)/ adjusting(èª¿æ•´) data at the "collider"** of cause(X) and outcome(Y), correlation $\neq$ causation is happened! = it's **collider bias**

:::

::::

## Summary of my previous two short lecture: 

- Correlation $\neq$ Causation(Causal relation)
- When estimating causal effects from regression coefficients in regression analysis,
    - In case of '**confounding factor**'(i.e. **indirect upstream connection**), we **should add them** to the explanatory variables for avoid "confounding(äº¤çµ¡)".
    - In case of '**collider**'(i.e. **indirect downstream connection**), we **should not add them** to the explanatory variables for avoid "collider bias(åˆæµç‚¹ãƒã‚¤ã‚¢ã‚¹)".

So today's short lecture, I will introduce **the last case** of happening Correlation $\neq$ Causation(Causal relation): **Bias by intermediate variable**

# 2. When is **regression coefficients** $\neq$ **causal effects** happend?

there are mainly 4 cases :

1. Direction of causation is opposite å› æœã®å‘ããŒé€†

2. Confounding äº¤çµ¡

3. Collider Bias åˆæµç‚¹ãƒã‚¤ã‚¢ã‚¹

4. **Intermediate variables are included** ä¸­é–“å¤‰æ•°ã‚’åŠ ãˆã‚‹äº‹ã«ã‚ˆã‚‹ãƒã‚¹ã‚¯

## What is Intermediate variables?

::::{.columns}

::: {.column width="50%"}

- In case that there is two causal relations: $x_1 => x_2$ and $x_2 => y$, we can say that $x_2$ is **the intermediate variable** from $cause(x_1)$ to $outcome(y)$.
- Intermediate variable is useful for understanding mechanism of the relation from $cause(x_1)$ to $outcome(y)$, etc.
- However, by **just including to the analysis**, it makes **the bias of estimated causal effect** from $cause(x_1)$ to $outcome(y)$.

:::
::: {.column width="50%"}
![Causal diagram with intermediate variable](https://i.imgur.com/nTazczH.png){width="100%"}
:::
::::

## Example: Salt Insake(å¡©åˆ†æ‘‚å–é‡) => mortality risk(æ­»äº¡ãƒªã‚¹ã‚¯) & Blood Pressure(è¡€åœ§)

Let's think about causation from "Salt Insake(cause)" to "mortality risk(outcome)".

::::{.columns}

::: {.column width="40%"}

![Causal Diagram](https://i.imgur.com/vj7qFXz.png){width="100%"}

:::

::: {.column width="60%"}

- The causal diagram is assumed as left figure.(Maybe it's not correct medically, but for the ease of explanation...)
- This causal diagram mean that ...
    - Increasing "salt intake" contribute to "mortality risk" increases through increasing "blood pressure".

:::
::::

From viewpoint that "salt intake = **cause**" & "mortality risk = **outcome**", "blood pressure" is **Intermediate Variable**.

So, in order to estimate **the causal effect** from "salt Intake($x_1$)" to "mortality risk($y$)", *should we inclode "blood pressure($x_2$)" to regression analysis...??*

## Generating sample dataset & check the answer

the setting for generating data is bellow.

1. "salt intake($x_1$)" is depend on $Norm(\mu=0, \sigma^2 = 1)$ (Please imagine the values is already standardized with $mean=0$ and $variance = 1$.)

2. "blood pressure($x_2$)" is depend on $x_1$, the average, and random term($\varepsilon_{x2}$)

$$
x_2 = 5 \times x_1 + \varepsilon_{x2} + 110 (\text{Average of blood pressure})
$$

3. "mortality risk($y$)" is depend on $x_2$ and random term($\varepsilon_{y}$)
$$
y = 0.5 \times x_2 + \varepsilon_{y}
$$

So, in this setting, the **causal effect from ""salt intake($x_1$)"" to "mortality risk($y$)"** is **$5 \times 0.5 = 2.5$**. (this is the answer.)

We try to estimate this "2.5" by using regression analysis and observational data.

## Generated dataset is below.

![](https://i.imgur.com/ZSo38Ps.png){width="100%"}


You can see that there are **positive correlations** between each of the three values.(It's not surprising because I just generated these data with such a setup... :smile:)

## If you wanna try this experiment, you can also generate the dataset by using the code below.

```python
@dataclass
class SampleDataForIntermediateVariableMask:
    CAUSAL_EFFECT_SALT_TO_BLOOD = 5
    CAUSAL_EFFECT_BLOOD_TO_RISK = 0.5
    CAUSAL_EFFECT_SALT_TO_RISK = CAUSAL_EFFECT_SALT_TO_BLOOD * CAUSAL_EFFECT_BLOOD_TO_RISK

    SALT_INTAKE_MEAN = 10
    SALT_INTAKE_SIGMA = 5
    SALT_INTAKE_MEAN_STANDARDIZED = 0.0
    SALT_INTAKE_SIGMA_STANDARDIZED = 1.0

    BLOOD_PRESSURE_MEAN = 110

    salt_intake_array: np.ndarray
    blood_pressure_array: np.ndarray
    mortarity_risk_array: np.ndarray
    dataframe: pd.DataFrame

    @classmethod
    def generate(cls, sample_num: int = 5000) -> "SampleDataForIntermediateVariableMask":
        """generate sample dataset for experiment"""
        salt_intake_array = cls._generate_salt_intake_array(sample_num)
        blood_pressure_array = cls._generate_blood_pressure_array(sample_num, salt_intake_array)
        mortarity_risk_array = cls._generate_mortarity_risk_array(sample_num, blood_pressure_array)

        df = cls._summarize_to_dataframe(salt_intake_array, blood_pressure_array, mortarity_risk_array)

        return SampleDataForIntermediateVariableMask(
            salt_intake_array,
            blood_pressure_array,
            mortarity_risk_array,
            df,
        )

    @classmethod
    def _generate_salt_intake_array(cls, sample_num: int) -> np.ndarray:
        """æ­£è¦åˆ†å¸ƒã‚’ä½¿ã£ã¦å¡©åˆ†æ‘‚å–é‡ã®ä»®æƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹ã€‚
        å˜ä½ã¯[g/day]ã ãŒã€ä»Šå›ã¯å¹³å‡0ã€åˆ†æ•£1ã¨ãªã‚‹ã‚ˆã†ã«æ¨™æº–åŒ–ã•ã‚Œã¦ã„ã‚‹ã¨ã™ã‚‹ã€‚
        """
        return np.random.normal(
            loc=cls.SALT_INTAKE_MEAN_STANDARDIZED,
            scale=cls.SALT_INTAKE_SIGMA_STANDARDIZED,
            size=sample_num,
        )

    @classmethod
    def _generate_blood_pressure_array(cls, sample_num: int, salt_intake_array: np.ndarray) -> np.ndarray:
        """ã€Œè¡€åœ§ï¼5Ã—å¡©åˆ† ï¼‹èª¤å·®ï¼‹å¹³å‡å€¤ã€ã®é–¢ä¿‚å¼ã«ã‚ˆã‚Šè¡€åœ§ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
        error_blood_pressure = np.random.normal(loc=0, scale=1.0, size=sample_num)
        return cls.CAUSAL_EFFECT_SALT_TO_BLOOD * salt_intake_array + error_blood_pressure + cls.BLOOD_PRESSURE_MEAN

    @classmethod
    def _generate_mortarity_risk_array(cls, sample_num: int, blood_pressure_array: np.ndarray) -> np.ndarray:
        error_mortarity_risk = np.random.normal(loc=0, scale=1.0, size=sample_num)
        return cls.CAUSAL_EFFECT_BLOOD_TO_RISK * blood_pressure_array + error_mortarity_risk

    @classmethod
    def _summarize_to_dataframe(
        cls, salt_intake_array: np.ndarray, blood_pressure_array: np.ndarray, mortarity_risk_array: np.ndarray
    ) -> pd.DataFrame:
        return pd.DataFrame(
            data={
                "salt_intake": salt_intake_array,
                "blood_pressure": blood_pressure_array,
                "mortarity_risk": mortarity_risk_array,
            }
        )
def main() -> None:
    sample_data = SampleDataForIntermediateVariableMask.generate(sample_num=5000)
    print(sample_data.dataframe.head())

if __name__ == "__main__":
    main()
```

## Let's Regression Analysis:)

Our objective is estimating **"the causal effect of $X_1$ =>$t_{dual}$"**. So, firstly, I tried single linear regression:

- Explanatory variables = [$x_1$] : salt intake(cause)
- Explained variable = $y$ : mortatity risk(outcome)

$$
y \sim Normal(\mu = a_0 +  a_1 \times x_1, \sigma^2)
$$

using this assumed formula and datasets(5000 samples), we estimated $a_0$, $a_1$, $\sigma^2$.

After estimation, let's check **whether $\hat{a_1}$ is close to $2.5$(=actual causal effect)** or not.

## The result of single linear regression analysis is ...

::::{.columns}
:::{.column width="50%"}

$$
y \sim Normal(\mu = a_0 +  a_1 \times x_1, \sigma^2)
$$

$\hat{a_1} = 2.52$. This value is **close to $2.5$(=actula causal effect)**

($\hat{a_0} = 55.02, \hat{\sigma^2}=1.28$)

:::

:::{.column width="50%"}

![](https://i.imgur.com/vj7qFXz.png){width="100%"}

:::
::::

=> So here, the causal effect of "$x_1$ -> $y$" **is properly estimated** by a single regression with "salt intake($x_1$)" as the only explanatory variable.

In the case of **intermediate variable**, same with collider, **this simple regression is "necessary and sufficient"!**

## So then, what happens if we add intermediate variable to explanatory variables here?

- Explanatory variables = [$x_1$, $x_2$] : 
    - $x_1$ is salt intake(cause), $x_2$ is blood pressure(intermediate variable)
- Explained variable = $y$ : mortatity risk(outcome)

$$
y \sim Normal(\mu = a_0 +  a_1 \times x_1 + a_2 \times x_2, \sigma^2)
$$

Same with previous model, using this formula and datasets(5000 samples), we estimated $a_0$, $a_1$, $a_2$, $\sigma^2$.

After estimation, let's check **whether $\hat{a_1}$ is close to $2.5$(=actual causal effect)** or not.

## The result is...

::::{.columns}
:::{.column width="60%"}

$$
y \sim Normal(\mu = a_0 +  a_1 \times x_1 + a_2 \times x_2, \sigma^2)
$$

$\hat{a_1} = -0.13$. 

($\hat{a_0} = -3.20, \hat{a_2} = 0.52, \hat{\sigma^2}=1.00$)

By adding "blood pressure($x_2$)" to the formula, **$\hat{a_1} \neq$ causal effect** (i.e. **Correlation $\neq$ Causation**) is happened!

:::

:::{.column width="40%"}

![](https://i.imgur.com/vj7qFXz.png){width="100%"}

:::

::::

In fact, in this causal diagram, by adding the intermediate variable "blood pressure($x_2$)", **Causal effects from $x_1$ to $y$ are apparently masked**.

<!-- ã¯ãŸã—ã¦ã€ã“ã®é‡å›å¸°ã®çµæœã‹ã‚‰ã€Œå¡©åˆ†æ‘‚å–é‡â†’è¶…éæ­»äº¡ãƒªã‚¹ã‚¯ã®å› æœé–¢ä¿‚ã¯ãªã„ã€ã¨çµè«–ã¥ã‘ã¦ã—ã¾ã£ã¦è‰¯ã„ã®ã‹?? -->
From the results of this multiple regression, can we really conclude that **there is no causal relation from "salt intake" to "mortality risk"...?**

## Additional contents: "AIC" vs "estimate causal effect"(dear user of statistics, regression analysis, exc.)

this table shows the result of each regression analysis.

| explanatory variables       |$\hat{a_1}$ |$abs(2.5 - \hat{a_1})$ |   AIC | 
|---------------------------------|------------|-------------|----------------| 
|                   [salt_intake] |   2.516027 |                  0.016027 |   15454.237607 | 
|   [salt_intake, blood_pressure] |  -0.131639 |                  2.631639 |   14205.506504 | 

[salt_intake] model can estimate causal effect more properly, but AIC is higher than [salt_intake, blood_pressure] model!

This example shows that...

- "**Good model evaluated by AIC**" and "**$\hat{a_1}$ is representing the causal effect more properly**" is **essentially different**.:satisfied:

## summary

::::{.columns}

:::{.column width="60%"}

- **Correlation $\neq$ Causation** is often happend in observational data.
- "**confounding factor**":  
    - =>we **should add them** to the explanatory variables for avoid "confounding(äº¤çµ¡)".
- "**collider**"ï¼š
    - =>we **should not add them** to the explanatory variables for avoid "collider bias(åˆæµç‚¹ãƒã‚¤ã‚¢ã‚¹)".
- "**intermediate variable**": 
    - =>we **should not add them** to the explanatory variables for avoid "Bias by intermediate variable"

:::
:::{.column width="40%"}

![Causal Diagram with Confounding factor, collider, and intermediate variable](https://i.imgur.com/ELYvjiq.png){width="100%"}

:::
::::



# Finally...

<!-- Observational Dataã‹ã‚‰å› æœé–¢ä¿‚ã‚’è©•ä¾¡ã™ã‚‹ã®ã¯å‡„ãé›£ã—ã„ã‚ˆï¼

ãªãœãªã‚‰åˆ†æã—ãŸã„"cause(X)"ã¨"outcome(Y)"ä»¥å¤–ã«ã€äº¤çµ¡å› å­ã‚„åˆæµç‚¹ã€ä¸­é–“å¤‰æ•°ã®å½±éŸ¿ã«ã‚ˆã‚Šæ“¬ä¼¼ç›¸é–¢ãŒç™ºç”Ÿã—ã¦ã—å¾—ã‚‹ã‹ã‚‰ã ã‚ˆï¼

(ãªã®ã§ç†æƒ³çš„ã«ã¯ã€Experimental Dataã‚’ä½¿ã£ãŸæ–¹ãŒè‰²ã€…ã¨å®‰å¿ƒã—ã¦å› æœé–¢ä¿‚ã‚’è©•ä¾¡ã§ãã‚‹ã‚ˆï¼)

ã§ã‚‚ã¨ã«ã‹ãä¸€ç•ªå¤§äº‹ãªã®ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã‚‹ä¸Šã§ã€ä»Šæ—¥è©±ã—ãŸæ§˜ãªãƒã‚¤ã‚¢ã‚¹ã¨ã‹æ“¬ä¼¼ç›¸é–¢ãŒå­˜åœ¨ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹äº‹ã‚’èªè­˜ã—ã¦ãŠãã“ã¨ã ã‚ˆï¼

"Supirious Correlation", "Confounding", "Collider Bias", ç­‰ã®è¨€è‘‰ã‚’keep in your mind ã—ã¦ãã‚Œã‚‹ã¨å¬‰ã—ã„ã‚ˆï¼ -->
That is all for my series of short lectures about causal inference. :smile:

You may feel **the difficulty** to evaluate causal relation from **observational data**! :scream:

(So, ideally, it is better to use **experimental data** to evaluate causal relation with more confidence!)

But anyway, the most important thing is **remembering the potential of these kinds of bias!**

I hope you all keep in your mind the phrases: "Confounding", "Collider Bias", "Bias by intermediate variable", and "Correlation $\neq$ Causation" ~ :relaxed:

# Thank you for your kind attention:)

let's enjoy your analytics life ~ :wave:

References

- Multiple Regression Viewed from Causal Inference Perspective (çµ±è¨ˆçš„å› æœæ¨è«–ã®è¦–ç‚¹ã«ã‚ˆã‚‹é‡å›å¸°åˆ†æ), Mahabu Iwasaki
- å²©æ³¢ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ vol.3 å› æœæ¨è«–-å®Ÿä¸–ç•Œã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å› æœã‚’èª­ã‚€.
- [Causal Inference, Backdoor Criterion](https://medium.data4sci.com/causal-inference-part-xi-backdoor-criterion-e29627a1da0e)


