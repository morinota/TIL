## LayerXさんの事例

- <https://tech.layerx.co.jp/entry/2024/07/25/173317>

### 概要

- Overview
  - バクラク経費精算に新しく追加された「内訳推薦」機能の目的や、その実装アーキテクチャについて解説してるよ。
- Context
  - Issue：何が課題か
    - 従業員が経費精算時に多数の内訳から適切なものを選ぶのが大変で、手間がかかってた。
  - Why：なぜその課題を解決する必要があるのか
    - 内訳選択の手間を減らして、経理業務の効率化を図るため。
  - Who：誰を対象としたものか
    - バクラク経費精算を利用する全ての従業員と経理担当者。
  - Values：得られる価値は何か
    - 内訳選択の自動化で、従業員の負担軽減と経理処理の効率化が期待できる。
- Scope
- Background
- Goals / Non-goals
- Solution / Technical Architecture
  - 内訳推薦機能の設計と実装に焦点を当てて、特にリアルタイム性や各テナントごとのカスタマイズ性に対応する部分を詳しく説明してるよ。
- System Context Diagrams
- Alternative Solution
- Milestones
- Operations

### 気になったことメモ

「内訳は各テナント様ごとに自由に決めることができる」問題に対してですが、推薦問題として落とし込むことで解決したとのこと。

- 「新しい内訳が日々追加され得る」&「内訳はテナント様ごとに異なる」という要件に対応するために、**分類や予測ではなくランキング問題として捉える**ことで解きやすくしてるっぽい...!:thinking: (多分コンテキストとして内訳の特徴量を渡して、スコアリングして並び替える感じ...!!これだったら新しい内訳が追加された時は、特徴量パイプラインに流し込めばいいはずだし。)

「新しい内訳が日々追加され得る」->「ニアリアルタイム性が必要」をどう対応したか??

- 1. **推薦候補となる内訳データは、推論サーバがプロダクトのDBから直接リアルタイムに取得する**ようにした!!
  - 新しい内訳が追加された場合に、すぐに推薦候補に反映されるようになる。
  - AmazonのRead replica(=DBの参照専用の複製らしい...!:thinking:)を使うことなども候補に入ったが、**リクエスト量的に十分耐えれる範囲だと判断し、SageMaker内部から直接参照することに**
  - プロダクトからのリクエストパラメータに推薦候補(内訳)を含める案もあったが、**リクエストに含めるには大きすぎるかな**と判断した。(テナント様によっては1000件以上の内訳を設定されているケースもあるため)
- 2. 特徴量のDaily更新
  - 特徴量は、dailyでS3にアップロードを行う特徴量作成パイプラインを動かしている。
    - なるべく早く特徴量に反映できるように、**GO株式会社様の事例を元にバックグラウンドでデータ更新スレッドを動かす**ようにした。
    - アップロード方法: bucketのlatest pathと日付ごとのpathにそれぞれ同じ特徴量をアップロードしている。
      - **2つのpathにアップロードしてる理由は、もし問題が起きた際にすぐにfallbackできるようにするため**。
  - 特徴量（のオンラインアクセス方法！）は、Sagemaker Endpoint内のインメモリで持たせるようにしている。
    - DynamoDBやFeatureStoreなどの活用も考えたが、特徴量のカーディナリティやデータサイズが小さく、実装も手軽なことからインメモリで持たせる意思決定をした。
    - Endpoint起動時にS3からダウンロードしてくるようにした。
    - また、**Sagemaker Endpointの中では、非同期に定期的にS3のlatest pathを見に行くスレッドを動かしており**、もし特徴量が更新されている場合は、コンテナ内にある特徴量を更新するようにしている。
- 3. モデルのWeekly更新
  - 特徴量を新鮮に保っていたとしても、カテゴリカル特徴量のOrdinalEncoderなども更新しないと新しいカテゴリ値は未知のカテゴリとして見なされてしまう。
  - しかし、カテゴリカル特徴量のEncoderを更新する場合はカテゴリ値の意味が変わってしまうため、モデルも同時に更新する必要がある。
  - そのため、**モデルもEncoderも同じタイミングで更新し、S3にアップロードするようにした**。
  - **アップロード方法とEndpoint内部での更新の仕方は、特徴量の場合と同じ**。
  - ちなみに、Weeklyなのは、コスト的にもDailyで更新するメリットがほぼないと判断したため。
  
この例の他にも、**バクラクではドメイン知識が必要な場面に対しての意思決定に、推薦というアプローチがハマる場所が多々ある**、とのこと。

## ちなみに、LayerXさんの事例で紹介されてた、GO株式会社様のデータ更新スレッドの話

参考: <https://speakerdeck.com/t24kc/202305-gotechtalk?slide=20>

内容は、「**ニアリアルタイム（30分ごと）に更新されるデータ（特徴量）を用いて、機械学習モデルを更新する仕組みがない**」という課題を、どのような意思決定の流れでどのように解決したか、という話。
(要するに、**ニアリアルタイムで更新される特徴量を、推論サーバにいい感じに反映させる仕組みをどう実現するか**、みたいな話かな...!!:thinking:)

従来の推論APIのシステム構成

- 従来は、REST APIのPodが起動する際に、各プロセスのグローバル変数にデータをロードしていた。
  - この従来APIシステム構成で、**30分ごとに新しい特徴量データをロードするには、Podの再デプロイが必要**なため現実的ではない...!
    - 各プロセスごとにメモリが割り当てられるため、あるプロセスのグローバル変数を更新しても他プロセスには反映されない。
    - 30分単位でデータ更新したいが、Podを再デプロイしないとグローバル変数が再読込されない。

解決案の候補(<https://speakerdeck.com/t24kc/202305-gotechtalk?slide=14>)

- 案1: Vertex AI Feature Store ✖️ オンラインサービング
  - 実装方式: Vertex AI Feature Storeの利用
  - サービング方式(i.e. 特徴量の取得方法:thinking:): オンラインサービング(少量の最新データを取得)
  - メリット:
    - 低レイテンシー/低メモリ
    - 複数データソース(BigQuery,GCS)に対して、統一したI/F(=Interface)で取得可能。
  - デメリット:
    - コンピュートコスト大
    - バッチ処理と比較して高い。
- 案2: Vertex AI Feature Store ✖️ バッチ（オフライン）サービング
  - 実装方式: Vertex AI Feature Storeの利用
  - サービング方式: バッチサービング(大量の定期更新データを取得)
  - メリット:
    - 案1と同様に、複数データソースに対して統一したI/Fで取得可能。
    - サーバキャッシュに乗せることで低レイテンシー。
  - デメリット:
    - 高メモリ(大量の特徴量をメモリに乗っけておくので...!:thinking:)
    - リアルタイムデータの参照ができない。
- 案3: 独自実装 ✖️ オンラインサービング
  - 実装方式: 独自実装
  - サービング方式: オンラインサービング(少量の最新データを取得。Redis開発想定)
    - Redis(インメモリデータストア)に特徴量をキャッシュしておいて、推論時に必要な分だけRedisから取得するってことか...!!
  - メリット:
    - 低レイテンシー/低メモリ
  - デメリット:
    - コンピュートコスト大(確かに、Redis高いイメージある...!:thinking:)
- 案4: 独自実装 ✖️ バッチ（オフライン）サービング
  - 実装方式: 独自実装
  - サービング方式: バッチサービング(大量の定期更新データを取得。データ取得プロセスを開発する想定)
  - メリット:
    - サーバキャッシュに乗せることで低レイテンシー
    - 使用メモリ次第で低コスト
    - 現状の実装ベース
  - デメリット:
    - 高メモリ
    - リアルタイムデータの参照ができない

これらの4つの解決案について、それぞれ実験して、レイテンシ、使用メモリ、コンピュートコストの観点で比較したとのこと...!!(意思決定に迷った時に、実験して判断するの、工学的な感じがする...!:thinking:)

- 案1: Vertex AI Feature Store ✖️ オンラインサービング
  - レイテンシ: 100-200 msec
  - 使用メモリ: 数KB
  - コンピュートコスト: 1ノードあたり $700/month
- 案2: Vertex AI Feature Store ✖️ バッチ（オフライン）サービング
  - レイテンシ: 1-3 msec (サーバーキャッシュ利用時の値。通常参照時は数千msec。)
  - 使用メモリ: 数10MB
  - コンピュートコスト: 軽微なストレージ料金(feature storeのオフラインストア分か...!:thinking:)
- 案3: 独自実装 ✖️ オンラインサービング
  - レイテンシ: 5-10 msec
  - 使用メモリ: 数KB
  - コンピュートコスト: M1(4GB) Standardの場合、$200/month (Redisインスタンスを動かすための仮想マシンの運用コスト!)
- 案4: 独自実装 ✖️ バッチ（オフライン）サービング
  - レイテンシ: 1-3 msec (サーバーキャッシュ利用時の値。通常参照時は100-200msec。)
  - 使用メモリ: 数10MB
  - コンピュートコスト: Podに割り当てられたリソースの余剰部分で賄える(=コストなし！:thinking:)

実験結果をもとに、案4: 独自実装 ✖️ バッチ（オフライン）サービングを採用。
選定理由は以下。

- すでに特徴量はBigQueryで集約管理しているため、**I/F共通化の恩恵が小さい**こと。
- **特徴量データサイズが小さく、サーバーキャッシュに乗り切る**こと。
  - サーバーキャッシュに乗れば、通信オーバーヘッドがない分、オンラインサービングよりも高速に動作すること。
  - (埋め込み表現の場合はどうなんだろう...? 大きいのか? 小さいのか? 判断がパッとできないな...:thinking:)
- **利用する特徴量は30分単位で更新できればよく、バッチサービングで要件を満たせる**こと。
  - (当たり前だけど、要件を満たせるか否かって大事だ...!:thinking:)
- 現在の実装ベースのまま開発できること。

サービングプロセスの新構成として、**データ更新スレッドを導入**したとのこと...!!

multiprocessing.Managerを利用すると、複数プロセス間でデータを共有できる!!

```python
from multiprocessing import Manager

def load_realtime_feature():
    retunn pd.read_csv('features/1.2.0/realtime.csv.gz')

def set_realtime_feature():
    global realtime_feature
    for index, row in load_realtime_feature().iterrows():
        realtime_feature[row.key] = {"value1": row.value1, "value2": row.value2}

# multiprocessing.Managerを利用すると、複数プロセス間でデータを共有できる!!
realtime_feature = Manager().dict() 
set_realtime_feature()

@app.post('/predict')
async def predict(data: DataModel): # asyncなので非同期関数(複数リクエストを同時に処理できる)
    # 特徴量作成
    features = model.get_feature(data, realtime_feature)
    # 推論
    return model.prediction(features)
```

データ更新用のバックグラウンドスレッドの実装例

```python
def load_realtime_feature():
    return pd.read_csv('features/1.2.0/realtime.csv.gz')

def set_realtime_feature():
    global realtime_feature
    for index, row in load_realtime_feature().iterrows():
        realtime_feature[row.key] = {"value1": row.value1, "value2": row.value2}

def __realtime_feature_thread():
    while True:
        set_realtime_feature()
        time.sleep(300) # 5分ごとにVolumeを再読み込み

realtime_feature = Manager().dict() # マルチプロセス間で共有できる辞書として定義
set_realtime_feature()

# 定期的なデータ再読み込みのバックグラウンドスレッドの追加
t = threading.Thread(target=__realtime_feature_thread)
t.daemon = True # (メインプロセスが終了すると自動的にデーモンスレッドも終了する)
t.start()
```

運用上の考慮点: 特徴量のバージョン管理について

```linux
features/
|-- 1.2.0/
|   |-- realtime.csv.gz
|-- 1.3.0/
     |-- realtime.csv.gz
```

- データの後方互換性がなくなるタイミングで**データファイルのバージョンを変更**し、モデルではデータバージョンを指定して処理することで、新旧両方のデータを扱える
  - 例: `features/1.2.0/realtime.csv.gz` -> `features/1.3.0/realtime.csv.gz`
  - モデルによって、違うバージョンの特徴量を利用することが可能。
  - これにより、**後方互換性がない更新が入っても、既存のパイプラインはエラーにならない**(=うんうん、古いバージョンのデータを使い続けるので...!!:thinking:)
- なお、特徴量のバージョン更新時のデプロイ手順には注意
  - 1. 新しい特徴量データのデプロイ (i.e. 新しい特徴量パイプラインのデプロイ)
  - 2. APIで利用する特徴量バージョンの更新。
  - この手順を踏むことで、データフォーマット変更時のエラーを回避。

オンラインサービングやFeature Storeを利用するメリットが小さかったため見送り
