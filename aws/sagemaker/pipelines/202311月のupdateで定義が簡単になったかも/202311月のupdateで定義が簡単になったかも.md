## refs

- 元記事:https://aws.amazon.com/jp/about-aws/whats-new/2023/11/amazon-sagemaker-pipelines-developer-ai-ml/?nc1=h_ls
- Sagemaker Pipelinesの公式ドキュメント:https://aws.amazon.com/jp/sagemaker/pipelines/
- classmethodさんの記事: https://dev.classmethod.jp/articles/update-reinvent2023-sagemaker-summary/#:~:text=Machine%20Learning%20Blog-,SageMaker%20Pipelines%E3%81%8C%E6%94%B9%E8%89%AF%E3%81%95%E3%82%8C%E3%81%9FPython%20SDK%E3%82%92%E6%8F%90%E4%BE%9B,-Python%20SDK(SageMaker
- stepデコレータの話: [Lift-and-shift Python code with the @step decorator](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-step-decorator.html)
- AWS communiryの記事(わかりやすかった!): [Sagemaker Pipelines using step decorators](https://community.aws/content/2bFfwOMvMaWfOuwUy30HMF1qgGb/sagemaker)
- [Quick Start - Using @step Decorated Step with Classic TrainingStep](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-pipelines/step-decorator/quick-start/notebooks/using_step_decorator_with_classic_training_step.html)
- [サンプルのnotebook](https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/step-decorator/batch-examples/basic-pipeline-batch-inference.ipynb)
- [step decoratorの公式ドキュメント](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#step-decorator)

## 概要

- 投稿日は2023年11月29日。
- 改善されたPython SDKによって、使い慣れたPython構文を使用して、MLワークフローを素早く定義できるようになった。
- 改善されたSDKの主な機能は以下:
  - **Pythonデコレータ(@step)**で簡単にpipeline内の各stepを定義できる。
    - @stepデコレータはかなり興味深いアップデートであり、pipelineに組み込みたい関数にデコレータを付与するだけで、**従来の記述よりもかなり簡素化した表現でpipelineを定義できそう**...??
  - ワークフロースケジューラー

## @stepデコレータについてメモ

- Python関数をpipelineのstepに変換するデコレータ。
  - 紐づけられた関数内のコードを、`DelayedReturn`オブジェクトにwrapする。

(注意点):**各引数の値が設定されていない場合の@step decoratorの挙動**:
  - 1. まずSagemakerのconfigファイルから値を検索しようとする。
    - (`config.yaml`のやつ!)
    - @step decorator がconfig.yamlを利用できるように、yamlが存在するディレクトリを`SAGEMAKER_USER_CONFIG_OVERRIDE`環境変数に保存する必要がある。
  - 2. config.yamlに値が見つからない場合、Sagemaker SDKのデフォルト値を使用する。
    - SDKのデフォルト値の詳細は[link](https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk)

@remoteデコレータもあるらしい。

decorator的な使い方もできるし、functionを引数にとって新しいfunctionを返すような使い方(＝高階関数的な使い方...!)もできるっぽい。
  - (decoratorってそもそも全て高階関数的な使い方ができる??:thinking_face:)

step decoratorをつけた関数の返り値 

- step decoratorをつけた関数の返り値は、`DelayedReturn`オブジェクトになる。
  - DelayedReturnの意味: 関数は即座に実行はされず、パイプライン実行時にstepが走るタイミングに実行される。
- 各stepの`DelayedReturn`オブジェクトはそのstepの出力を表し、他のstepの入力として使うことができる。
  - -> このように使うことで、**pipelineの各step間の依存関係を表現でき**、pipelineの定義時にstepの配列を渡せる...!
  - decorateする関数の返り値は、単数型でも、tuple型やlist型やdict型などのコレクション型でもOK...!
    - コレクション型の場合は以下のように参照できる。
      - `a_member = a_delayed_return[2]`
      - `a_member = a_delayed_return['a_key']`
      - `a_member = a_delayed_return[2]['a_key']`

step decoratorの場合、基本的にTrainingJobとして実行されるっぽい。

- でも特に各TrainingJob間の接続は開発者的には意識する必要がなくなる。
  - **ここがstep decoratorの特徴?? step間の接続方法が抽象化・情報隠蔽されてる感じ...!**
- 実行imageや、パッケージ依存関係はどう指定できるのか気になる。
  - stepごとに切り替えられる? もしくはpipeline内では共通になる?? :thinking_face:
- TrainingJob名がある程度コントロールできるのか気になる...!
  - デフォルトでは意味を解釈できないIDを含んだ名前になってしまう感があるので。
    - せめて{pipeline名}-{timestamp}-{step名} みたいな感じにしたい。


### step decoratorのparameters

- `name`: stepの名前を指定する。
  - デフォルトでは関数名と、重複を避けるためにuuid4 IDを使って作られる。
- `display_name`: stepの表示名を指定する。
  - デフォルトでは関数名
- `description`: stepの説明を指定する。
  - デフォルトでは関数のdocstringが使われる。
- retry_policies (List[RetryPolicy]): 再試行ポリシーのリスト。デフォルトはNone。
- `dependencies` (str): 依存関係ファイルのパス。デフォルトはNone。
  - requirements.txt または environment.yml ファイルのパスを指定可能。
- `pre_execution_commands` (List[str]): 実行前のコマンドリスト。デフォルトはNone。pre_execution_scriptとは併用不可。
- `pre_execution_script` (str): 実行前のスクリプトファイルのパス。デフォルトはNone。pre_execution_commandsとは併用不可。
- `environment_variables` (dict[str, str] or dict[str, PipelineVariable]): ステップ内で使用する環境変数。デフォルトはNone。
- `image_uri` (str, PipelineVariable): DockerイメージのURI。デフォルトはNone。
  - SageMaker Studioノートブックの場合、カーネルイメージが使用される。
  - お、これはいじりそう！
- `instance_count` (int, PipelineVariable): 使用するインスタンスの数。デフォルトは1。Sparkジョブ以外では1以上はサポートされない。
- `instance_type` (str, PipelineVariable): インスタンスタイプ。例：ml.c4.xlarge。指定されていない場合、エラー。
- `job_conda_env (str, PipelineVariable): ジョブの実行時にアクティブにするConda環境の名前。デフォルトはNone。
- `job_name_prefix` (str): SageMakerジョブ名のプレフィックス。
  - お！これはいじりそう！
  - (じゃあこれを指定すると、job_nameが`{job_name_prefix}-{timestamp}`みたいな感じになるのかな...?)
- `keep_alive_period_in_seconds` (int, PipelineVariable): ジョブ完了後にインフラを保持する期間。デフォルトは0秒。
  - これは0で良さそう!
- `max_retry_attempts` (int, PipelineVariable): InternalServerFailureエラー後の最大再試行回数。デフォルトは1回
- `max_runtime_in_seconds` (int, PipelineVariable): トレーニングの上限時間（秒）。デフォルトは86400秒（1日）
- `role` (str): SageMakerトレーニングジョブを実行するためのIAMロール。デフォルトはNone。
  - これは結構ワナで、指定しないと、デフォルトでadmin-roleなどが指定されてしまうので注意したい...!
- `security_group_ids` (List[str, PipelineVariable]): セキュリティグループIDのリスト。デフォルトはNone。
- `subnets` (List[str, PipelineVariable]): サブネットIDのリスト。デフォルトはNone。
- `tags` (Optional[Tags]): ジョブに付与するタグ。デフォルトはNone。
- `volume_kms_key` (str, PipelineVariable): EBSボリュームを暗号化するためのKMSキー。デフォルトはNone。
- `volume_size` (int, PipelineVariable): トレーニング中にデータを保存するストレージボリュームのサイズ（GB）。デフォルトは30GB。
- `encrypt_inter_container_traffic` (bool, PipelineVariable): コンテナ間トラフィックの暗号化を指定。デフォルトはFalse。
- `spark_config` (SparkConfig): Sparkアプリケーションの構成。デフォルトはNone。指定されると、Sparkイメージが使用される。
- `use_spot_instances` (bool, PipelineVariable): SageMakerのスポットインスタンスを使用するかどうか。デフォルトはFalse。使用する場合、max_wait_time_in_secondsも設定する必要がある。
  - お、これはTrainingJobでspot instanceを使うかどうかの設定か...!必要に応じていじりそう!
- `max_wait_time_in_seconds` (int, PipelineVariable): スポットトレーニングジョブの待機タイムアウト（秒）。デフォルトはNone。

### step decoratorを使って定義すると、step間のI/O的な接続を気にせず実装できそう。

最初のstepで、input_data_pathとしてs3uriを受け取り、受け取ったs3uriをファイルストレージ内のpathのように扱って`pl.read_parquet(input_data_path)`を実行できるみたい。

```python
input_data_path = 's3://bucket/path/to/data.parquet'

@step(name='preprocess', image_uri='hoge', instance_type='ml.m5.xlarge', role_arn="fuga")
def preprocess(input_data_path: str) -> pl.DataFrame:
    # json serializationするために、import文はたぶん関数内で書く必要があるっぽい。
    import polars as pl
    # input_data_pathをファイルストレージ内のpathのように扱って、データを読み込める
    return pl.read_parquet(input_data_path)
```

S3からデータを入力するのはめちゃ簡単になった。
でもS3にデータをuploadする場合は、`s3fs`とかを使って、uploadコードを書く必要はあるみたい。(参考: [stepデコレータのサンプルnotebook](https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/step-decorator/batch-examples/basic-pipeline-batch-inference.ipynb))

```python
@step()
def evaluation(data: pl.DataFrame) -> None:
    output_s3_path = 's3://bucket/path/to/output.parquet' 
    # upload data to s3
    s3_fs = s3fs.S3FileSystem()
    with s3_fs.open(output_s3_path, 'wb') as f:
        f.write(data.to_parquet())
```

- ちなみに、s3fsって?
  - s3fsは、S3をファイルシステムとしてマウントするためのPythonライブラリ。
