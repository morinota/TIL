<!-- (番外編)MLOps勉強会で推薦システム関連の発表を聞いて知らない用語を調べる!:MLOps Maturity Assessment -->

# MLOps Maturity Assessment

published date: 18 April 2023,
authors: Vechtomova Maria
url(paper): https://mlops.community/mlops-maturity-assessment/
(勉強会発表者: morinota)

---

(論文じゃないですが、いつもと同じフォーマットっぽく書いてみます。)

## どんなもの?

- 先週のMLOps勉強会の発表にて、引用されていた。
- あるprojectのMLOpsの成熟度を評価するのに役立つアンケート。
- MLOpsの幅広い側面をカバーしていて、MLOpsの実践を改善したいと考えてるチームにとって有効な資料??
- 7つのセクションに分かれており、セクション1~4の全ての問にYesと答える事ができれば、プロジェクトはMLOpsの観点で成熟しているとみなせる。
  - (i.e. セクション1~4が、MLプロジェクトを本番環境で確実にデプロイする為の必要条件)
  - セクション5~7は、MLOps成熟度の必要条件を超えた項目。
- 「No」と答えた観点が、プロジェクトのMLOps成熟度を向上させる為に組織が取り組むべき事になる。

## 先行研究と比べて何がすごい？

- 機械学習をプロダクトに導入する上で、MLOpsのベストプラクティスを採用することが重要。
- しかし、MLOpsのベストプラクティスが実際にどのようなものなのか、また、開発組織のMLOpsをより成熟させるにはどのようにすればよいのか、について構造化された情報を見つけるのは難しい。
- 既存のMicrosoftの [MLOps maturity model](hoge) やGoogleの [MLOps levels](hoge) もあるが、MLOps未成熟な状態からいかに100%の成熟度を達成する方法について、必ずしも有効な洞察を与えてくれるわけではない。(これらは、強く大きなテック組織を前提としてる感がある??:thinking:)
- MLOps Maturity Assessmentは、hogehoge。

## 技術や手法の肝は？

- 本アンケートは、以下の7つのセクションに分かれている:
  - Documentation
  - Traceability & Reproducibility
  - Code quality
  - Monitoring & Support
  - Data transformation pipelines & Feature store
  - Model explainability
  - A/B testing & Feedback loop
- セクション1~4の全ての問にYesと答える事ができれば、プロジェクトはMLOpsの観点で成熟しているとみなせる。
  - (i.e. セクション1~4が、MLプロジェクトを本番環境で確実にデプロイする為の必要条件)
- セクション5~7は、MLOps成熟度の必要条件を超えた項目。
- 本アンケートでは、高度なMLOps(=section 5~7)の実践に移る前に、基本条件(=section 1~4)を優先する事を推奨している。
- 本アンケートにて「No」と答えた観点が、プロジェクトのMLOps成熟度を向上させる為に組織が取り組むべき事になる。

### セクション1: Documentation

- 1.1. Project Documentation(MLプロジェクトに関する文書化)
  - MLプロジェクトのビジネスゴールとKPIは文書化され、常に最新の状態に保たれているか??
  - MLプロジェクトに関与する全てのチームメンバーの概要(各自の責任も含む)を作成し、常に最新の状態に保たれているか??
  - MLモデルのリスク評価を作成し、文書化し、最新の状態に保たれているか??
    - (MLモデルのリスク評価=MLモデルの推論結果が誤ってる場合のビジネス損失...??:thinking:)
- 1.2. ML model documentation
  - データ収集、分析、クリーニングのステップは、各ステップの動機を含めて文書化されているか??
    - (特徴量エンジニアリングとか前処理の話??:thinking:)
  - データ定義(MLモデルでどのような特徴量が使われ、その特徴量が何を意味するのか)が文書化されているか??
  - MLモデルの選択が文書化され、その正当性が証明されているか??
- 1.3. Technical documentation
  - リアルタイム推論のusecaseのために、API(リクエストとレスポンスの構造と定義、データ型)が文書化されているか??
  - ソフトウェア・アーキテクチャ設計は文書化され、常に最新の状態に保たれているか??

### セクション2: Traceability(追跡可能性?) and reproducibility(再現性)

(Traceability: 開発に関する情報や成果物の履歴、所在を追跡できること?:thinking:) ([引用](https://www.qbook.jp/column/1670.html#:~:text=%E3%82%BD%E3%83%95%E3%83%88%E3%82%A6%E3%82%A7%E3%82%A2%E9%96%8B%E7%99%BA%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8B%E3%83%88%E3%83%AC%E3%83%BC%E3%82%B5%E3%83%93%E3%83%AA%E3%83%86%E3%82%A3%E3%81%A8%E3%81%AF%E3%80%81%E3%80%8C%E9%96%8B%E7%99%BA%E3%81%AB%E9%96%A2%E3%81%99%E3%82%8B%E6%83%85%E5%A0%B1%E3%82%84%E6%88%90%E6%9E%9C%E7%89%A9%E3%81%AE%E5%B1%A5%E6%AD%B4%E3%80%81%E6%89%80%E5%9C%A8%E3%82%92%E8%BF%BD%E8%B7%A1%E3%81%A7%E3%81%8D%E3%82%8B%E3%81%93%E3%81%A8%E3%80%8D%E3%81%A7%E3%81%99%E3%80%82))

#### 2.1. Infrastructure traceability and reproducibility

- インフラはコード(i.e. IaC)として定義されているか??
- IaCはバージョン管理システムに保存されているか??
- IaCに変更を加える為に、プルリクエストプロセスが使われているか??
- プルリクがマージされると、CD(continuous delivery)パイプラインを通じて、本番環境に変更が自動的に適用されるか??
- 変更をデプロイするのはCDパイプラインだけであるか??個々の開発者はインフラをデプロイする権限を持っていないか?? (ふむふむ...!:thinking:)
- MLプロジェクトは、少なくとも2つの環境(preproductionとproduction)を持っているか??
  - (pre-production = dev環境と見做して良いんだろうか??:thinking:)
- MLプロジェクトに関連するすべての環境は、本番データ(データはどの時点でも同じであるべき)へのアクセス権(read and/or write)を持っているか??
  - (dev環境でも本番のデータを使って学習できたら良いけど、write権限を与えるのは怖い気がする...!:thinking:)

#### 2.2. ML code traceability and reproducibility

- データ収集、分析、クリーニングするためのコードは、バージョン管理システムに保存されているか??
- MLモデルコードは、データ準備コードやAPIコードとともに、バージョン管理システムに保存されているか??
- MLプロジェクトに関連するコードを変更するために、PRが使われるか??
- PRがマージされると、CDパイプラインを通じて変更が本番環境に自動的に適用されるか??
- 環境はコードとして定義され、再現可能であるか?? (環境って、インフラっぽい話??:thinking:)
- MLモデルのコードは、異なる環境で実行するために変更を必要とすべきではない。 MLモデルを各環境に展開するプロセスは同じであるか??
- どのような環境でも、与えられた機械学習モデルの実行/デプロイについて、**一義的に調べることが可能であるか**?? 具体的には、1. 対応するコード/git上のコミット、2. 学習や推論に使われるインフラ、3. 学習や推論に使用される環境、4. MLモデルの成果物、5. どのようなデータがモデルの学習に使われたのか、など。
- MLモデルの再トレーニング戦略が存在し(=その戦略を取れる仕組みがある)、その動機付け(=その戦略の必要性を認識してるかってこと?:thinking:)があるか??
  - (再トレーニング戦略=新しいデータ等を使って、オンライン学習や0から学習させるってことかな??:thinking:)
- roll-back戦略により、以前のMLモデルのバージョンに戻すことができるか??

### セクション3: Code Quality(コード品質)

#### 3.1. インフラのコード品質

- PR作成時に、設定ファイルの検証や自動テストの実行を含むCIパイプラインが実行されるか??
- PRをマージする前に、他のチームメンバー（開発者やセキュリティスペシャリストなど）が変更をレビューし、承認する必要がある仕組みになってるか??

#### 3.2. MLモデルのコード品質

- **Pre-commit hook**(=commitが実行される前にコードの品質チェックを行う仕組み、らしい。linterとかの話??:thinking:)は、コードがバージョン管理システムにプッシュされる前に、コード品質ガイドラインに従っていることを確認するために実装されているか??
- MLモデルのコードには、MLプロセスのすべてのステップ（データ処理、MLモデルの学習、APIのデプロイ）のテストが含まれているか??
  - (MLモデルの学習はどんなテストを用意すべきなんだろう...:thinking:)
- PR作成時に、設定ファイルの検証や自動テストの実行を含むCIパイプラインが実行されるか??
- PRをマージする前に、他のチームメンバー（開発者やセキュリティスペシャリストなど）が変更をレビューし、承認する必要がある仕組みになってるか??
- リアルタイム推論のusecaseでは、APIの負荷テストとストレステストを定期的に実施し、**APIがレイテンシー要件を満たしていることを確認する戦略**をとれるか??
- リアルタイム推論のusecaseでは、認証とネットワークはセキュリティガイドラインに従っているか??
- MLモデルのコードは文書化されているか??(コードとして文書化)
- MLモデルコードの新しいリリースがあるたびに、リリースノートを作成しているか??(なるほど...!:thinking:)

### セクション4: Monitoring & Support

#### 4.1. インフラのモニタリング

#### 4.2. アプリケーションのモニタリング

#### 4.3. KPIとMLモデル性能のモニタリング

#### 4.4. data driftとoutlierのモニタリング ()

## 次に読むべき論文は？
