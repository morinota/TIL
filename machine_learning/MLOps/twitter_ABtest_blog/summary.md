# Bayesian Personalized Ranking from Implicit Feedback

published date: hogehoge September 2022,
authors: Wondo Rhee, Sung Min Cho, Bongwon Suh
url(paper): https://arxiv.org/ftp/arxiv/papers/1205/1205.2618.pdf
(勉強会発表者: morinota)

---

n週連続推薦システム系論文読んだシリーズ 36 週目の記事になります。
ちなみに35週目は [タイトル](url) でした!

## どんなもの?

- 読んだきっかけ:
  - Work In ProgressさんのPodcast配信 -> NetflixさんのABテストに関するブログ連載 -> X(旧Twitter)さんのABテストのサンプルサイズ設計に関するブログを読んで、もう少しABテストの設計周りの雰囲気を掴みたいと思い、参考文献に記載されてた本論文を読むに至りました:)
- 概要:
  - Webサービスにおける制御実験のpracticeやguidelineについてまとめた論文。
- 本記事では、以下の3つの資料を参考にしながら理解したことをまとめつつ、特に推薦システムのABテストに関して思いを馳せる。

## ABテストってどんなものだっけ?

- hoge

## コントロール実験に関する用語の定義

### Overall Evaluation Criterion (OEC, 総合評価基準):

- 実験を経て改善させたい目的となる定量的な指標 (i.e. netflixさんの資料における primary decision metric?:thinking:)
- 実験によっては複数の目的がある場合もある。その場合は、複数の指標を重み付けして組み合わせた単一のOECを用意することが推奨される。

### Factor

- OECに影響を与えると考えられる、制御可能な実験変数 ()
- factorには値(levelやversionとも呼ばれる)が割り当てられる。
  - (例えばpush通知のパーソナライスするか否かのABテストの場合、factor = is_personalized, value = true or false、みたいな?:thinking:)
- 単純なA/Bテストでは、2つの値(=AとB)を持つ単一のfactorを持つ。

### Variant

- factorにlevel(=値)を割り当てることで、ユーザ体験がテストされる。
  - 既存のバージョンを指定する特別なvariantであるcontrolと、新しいversionを試すvariantであるtreatmentがある。
  - ex) 施策にバグがあった場合、実験は中止され、すべてのユーザがコントロールの variant を割り当てられる。

### Experimental unit (実験単位)

### primary decision metricとかの話は書きたい。

## 意思決定のためのツールとしての統計的仮説検定の前提知識

(統計的仮説検定の前提知識の理解を以下にまとめてます。すでに認識済みって方は読む必要なさそうです...!)

- 誰もが、ABテストを経てなるべく正しい意思決定をしたいよね...!
- でもまず認識すべきこと: **意思決定に対するどんなアプローチも、不確実性や間違いを犯す可能性を完全に排除することはできない**。
  - -> 統計的仮説検定のアプローチは、結果の不確実性や意思決定のエラーを犯す確率を定量化・理解するために有効。
- ABテスト結果に基づく意思決定のエラーには、2種類ある:
  - **false positive(type 1 error)**: 実際には施策に効果がないのに、効果があると判断してしまう。
    - (写真を猫か猫以外かで判定する機械学習モデルで例えると、猫以外の写真を猫だと判定してしまうこと!)
    - (統計的仮説検定も、「施策に効果があると言える」と「効果があると言える十分な証拠がない」を判定する2値分類モデルとして捉えることができるな...:thinking:)
  - **false negative(type 2 error)**: 実際には施策に効果があるのに、効果がないと判断してしまう。
    - (写真を猫か猫以外かで判定する機械学習モデルで例えると、猫の写真を猫ではないと判定してしまうこと!)
- 残念ながら、これらのエラーの両方を完全に排除することはできない。
  - 基本的に両者の確率はトレードオフの関係にあるから。
  - false positiveされやすさが極めて小さくなるように実験をデザインすると、必然的にfalse negativeされやすさが大きくなる。
- -> **我々が可能 & 目指すべきことは、この2種類のエラーの原因を定量化し、理解し、コントロールすること**。

### false positive(type 1 error)に関する概念の整理

- 優れた仮説(施策)とprimary decision metricへの明確な理解を得たあとは、ABテストを設計するための統計的な側面について考えていく。
- このプロセスは通常、**acceptable(許容可能な) false positive rateを固定することから始まる**。
  - 慣例では、**許容可能なfalse positive rateは5%**に設定されることが多い。
    - i.e. 「実際には施策に効果がない場合において、1回/20回くらいは誤って効果があると判断すること」を許容すること。
    - (i.e. 我々は、猫以外の写真の5%くらいを猫であると誤ってラベル付けしてしまうことを許容するよ!:thinking:)
    - 「acceptable false positive rateが5%」は「significance level(有意水準)が5%」とも言い換えられる。
- false positive rateとp-value(p値)との関連:
  - false positive rateは、観測されたtreatmentとcontrolのmetric値の **statistical significance(統計的有意性)** と密接に関連する。そして statistical significanceは、p-valueを用いて計算される。
  - p値の解釈: 実際にはtreatmentとcontrolに差がない場合に、今回の観測結果と同じような結果が得られる確率。

#### statistical significanceやp-valueの概念を直感的に理解するためのコインの例

- コインの例、あまりに単純なシナリオすぎない?? -> 多くの企業はbinary metricに関心あるからそんなことない!
  - -> コインが表か否か = binary metric。
  - netflixを含む多くの企業では、施策の適用によって**binary user activityの割合に違いが生じるか(ex. クリックするか否か、継続するか否か)**をABテストで検証・理解したい...!
- コインが不公平かどうかを判断したいケースを想定する。
  - コインを100回ひっくり返して、表が出た割合を計算する実験。
  - 仮にコインが完全に公平(=コインが従う確率分布の期待値が0.5)だったとしても、正確に50回表が出ることを期待できない。ランダム性(i.e. noise)があるから。(確率的試行なので...!)
  - じゃあ「コインが公平である」という主張を棄却するのに十分な証拠は何か?
    - 60回表が出た場合、不公平だと結論づけられる? 70回? 80回? 90回?
  - 判断方法を揃え、関連するfalse positive rateを理解する方法が必要...!
    - (ここでfalse positiveは、公平なコインなのに誤って不公平だと判断すること:thinking:)
- 思考の練習:
  - 1. まずコインは公平である(i.e. パラメータp=0.5のベルヌーイ分布に従う...!:thinking:)と仮定する = Null Hypothesis (帰無仮説) (帰無仮説は常に現状維持や公平であることを表明する)
  - 2. 何が帰無仮説に対する有力な証拠となるかを判断するために、帰無仮説が真であると仮定して、あらゆる可能性のある観測結果の確率を計算する。(i.e. p=0.5, n=100として、ベルヌーイ分布の"標本平均の"確率質量関数を計算する...!:thinking:)
    - 計算した結果が図3(黒&青、色は無視):
    - (ここのpは、確率分布の形を決めるパラメータの意味。p値の意味ではない点に注意:thinking:)
  - 3. **コインが公平であるという仮定の下での結果の確率分布(図3)を、実験で収集されたデータと比較**できる。
    - 実験で収集された観測結果は、100回のうち55回表だったとする。
    - **「コインが公平であるという主張を棄却する上で、この観測結果がどの程度有力な証拠か??」の度合いを定量化したい**...!
    - -> 観測結果よりも発生しづらいすべての結果に関連する確率を足し合わせる(i.e. 累積質量)
      - (単峰性の確率分布を前提にしてるのかな。まあ統計的仮説検定で使う分布って単峰性か...:thinking:)
      - この例では、表と裏のどちらがより起こりやすいかという仮定をしてない(i.e. 両側検定)ので、55%以上が表になる確率と55%以上が裏になる確率を累積する。
    - この累積した確率質量が、**p-value**と呼ばれるもの。
      - この例では0.32となる。
      - 解釈: 公平なコインで100回ひっくり返す実験を何度も繰り返した場合、そのうちの32%の実験で少なくとも55回以上表が出るもしくは55回以上裏が出るような結果になるはず...!

![figure3]()

- コインが不公平であるという有意な証拠があるかどうか、**p値をどう使えばいい**??
  - -> **最初に定義したacceptable false positive rate=5%に基づく**。p値が0.05%未満の場合、統計的に有意な証拠があると判断し、帰無仮説を棄却する。
    - (公正なコインを仮定した場合、その観測結果が十分に起こりにくいのであれば、コインは公正ではないと判断 i.e. 意思決定して良さそうだよね...!正しい意思決定っぽい...!:thinking:)

#### 関連する概念1: Rejection Region(棄却域)

- 統計的仮説検定の判定ルールのもう一つの見方が、rejection region(棄却域)を定義すること。
  - **rejection regionとp値には等価性があり、どちらも同じ判定(意思決定)を導く**: 観測値がrejection regionに含まれる場合、その時のp値は0.05%未満である。
    - (=うんうん、やってることは同じ...!:thinking:)
  - コインが不公平であると結論づける観測値の集合を用意しておくという観点。
- rejection regionの定義も、**acceptable false positive rate=5%**に基づく。
  - コインが公平である(i.e. 帰無仮説が真である)と仮定し、**確率質量の累積値が0.05%を超えないような最も発生しづらい観測結果の集合**として、rejection regionを定義する。
    - この集合には、帰無仮説が正しい場合に最も極端な結果、つまり帰無仮説(の棄却?)に対する証拠が最も強い観測結果たちが含まれる。
  - もし収集された観測結果がrejection regionに含まれていたら、帰無仮説をrejectする。
    - コイン実験の例では、表40回未満と表60回以上の観測結果がrejection regionに含まれる。(図3の青色の観測結果の集合)
- rejection regionの境界値(コインの例では40回と60回)を、**critical value(臨界値?)**と呼ぶ。

#### 関連する概念2: Confidence Intervals(信頼区間)

- rejection regionと同様にp値と関連する概念が、Confidence Interval(信頼区間)。
  - **信頼区間とp値には等価性があり、どちらも同じ判定(意思決定)を導く**。
  - -> p値が0.05%未満である = null valueが95% confidence intervalの外側にある。
- confidence intervalも、p値やrejection regionと同様にacceptable false positive rate=5%に基づく。(まあ等価性があるので当然か...!:thinking:)
  - p値やrejection regionの概念はまず帰無仮説が真であるという仮定から導かれる概念だが、**confidence Intervalの場合は観測結果から導かれる**。
- コインの例での思考練習:
  - **ある観測結果が与えられた時、どのnull value(帰無値)であれば(i.e. どの帰無仮説であれば??:thinking:)棄却されないという判定(意思決定)を導くだろうか?**
    - (null value = 帰無仮説が真の場合の検定統計量の期待値? 例えば公平なコインを帰無仮説とした場合は検定統計量の期待値は50%?:thinking:)
  - コインの例では、55%表が出たという観測結果が得られると仮定した場合に、公平なコインである(i.e. 表が出る確率が50%である)という帰無仮説は棄却されなかった。また同様に、表が出る確率が47.5％、50％、60％であるという帰無仮説も棄却されない。
  - -> **55%表という観測結果に対して、帰無仮説が棄却されない値には、約45％から65％表が出るというnull valueが含まれる**。
    - (この境界値は、rejection regionの概念における critical valueと同じ値...!でも意味合いは異なるよね:thinking:)
- このnull valueの値域が、confidence interval(信頼区間): (ある観測結果が与えられた条件で)**帰無仮説が棄却されないnull valueの値の集合**。
  - ちなみに、acceptable false positive rate=5% (i.e. significance level=5%)に設定している場合は、95% confidence intervalと呼ばれる。
    - 解釈: 実験を繰り返し行うと、95% confidence intervalはtrue value(=コインの例では、真の表が出る確率。真の分布の期待値?)を、実験回数における95%の割合でカバーする。
    - (自分はこの解釈をまだ理解できてない...!true valueの話が急に出てくる? confidence intervalは仮説検定というよりはtrue valueの推定の概念なのかな??:thinking:)
    - (仮説検定って、真の分布が帰無分布と言えるか否かのbinaryの判定を試みる方法論だと思うから、この2値分類を色んな帰無分布に対して適用していくと、何となく真の分布がこの辺りにあるって推定できる、みたいな??:thinking:)
    - (まあp-valueによる意思決定 = rejection regionによる意思決定 = confidence intervalによる意思決定ならば、必ずしもこの解釈を理解していることはABテストを運用する上では必須ではなさそう??:thinking:)

![figure4]()

### false negative(type 2 error)に関する概念の整理

- false negative(type 2 error): 実際には施策に効果があるのに、効果がないと判断してしまうこと。
  - (写真を猫か猫以外かで判定する機械学習モデルで例えると、実際は猫の写真なのに猫ではないと判定してしまうこと!)
- false negativeは**検出力(power)**と密接に関連する。
  - 検出力(power): **実験設計(acceptable false positive rateやサンプルサイズ)と、ABテストで検出したいtrue effectのサイズを条件づけた時**の true positiveの確率を意味する統計的概念。
  - -> 実際、検出力の値は、単に1からfalse negative rateを引いたもの。
    - (つまりfalse negative rateも、powerと同様の変数に依存して値が決まるもの:thinking:)

#### 検出力(power)の概念を直感的に理解するためのコインの例

- 前セクションのp-valueなどと同様に、コインの例を用いて理解を試みる。
  - 100回コインを反転させて表が出る割合を計算する実験を使って、コインが不公平か否かを判断することが目的。

![]()

- まずコインが公平であるという帰無仮説のもとでの観測結果の確率分布を算出する(上図の黒線の分布)
  - **null distribution(帰無分布)**と呼ぶらしい。
  - (具体的には、p=0.5, n=100の二項分布の確率質量関数を算出すればOK? いや違うか。p=0.5のベルヌーイ分布の"標本平均 $\bar{x}$ の"確率質量関数を算出すればいいのか...!:thinking:)
    - (nが十分に大きければ標本平均 $\bar{x}$ の分布は、中心極限定理によっては $\mu = p$、$\sigma^2 = p(1-p)/n$ の正規分布に近似できるから...! ググった感じではnの大きさは30以上が目安っぽい。:thinking:)
  - (=観測結果は確率変数なので...!:thinking:)
- 続いて、**「おそらくコインがこの程度不公平である」という仮説(=特定の対立仮説)**を指定し、null distributionと同様に確率分布を算出する。
  - 今回は、「おそらく表の期待値が64%くらいなのでは?」という仮説を指定する。(=これが検出したいeffect sizeになる...!:thinking:) (上図の赤線の分布)
  - この分布を、**alternative distribution(対立分布)**と呼ぶらしい。
  - (具体的には、p=0.64, n=100として、ベルヌーイ分布の"標本平均 $\bar{x}$ の"確率質量関数を算出すればいいのか:thinking:)
- 上図を見ると、視覚的には...
  - 検出力の大きさ = **alternative distributionのうち、帰無仮説のcritical valueよりも外側にある(i.e. rejection regionに含まれる)確率質量の割合**。
    - = (薄い赤色のエリアの確率質量の累積値)/ (赤線のエリアの確率質量の累積値)
  - (ちなみに、null distribution と alternative distribution の峰の差が検出したいeffect sizeかな:thinking:)
- この例では、「おそらく表の期待値が64%くらいなのでは?」という対立仮説を指定する場合は、このテストの検出力は80%になる。
  - (なるほど...!**alternative distributionを仮定しないと、すなわち検出したいeffect sizeを仮定しないと、検出力は計算できないのか**...! p値とはは仮定しなくても計算できるけど...! 逆にp値の計算には観測結果が必要だけど、検出力の計算には不要:thinking:)
- 解釈:
  - 真の表が出る確率が64%のコインを使って、100回コインを反転させる実験を、accepetable false positive rate=5%で繰り返した場合、80%の確率でコインが公平であるという帰無仮説を正しくrejectできる...!
  - (言い換えると、20%の確率でfalse negativeが発生する。つまり真の表が出る確率が64%だったとしても、コインが公平であるという帰無仮説をrejectできない:thinking:)
- 仮に検出力100%を目指すためには、alternative distributionのほぼ全てのエリアがrejection regionに含まれるようにする必要がある。そのためには...
  - acceptable false positive rateを5%よりも大きくして、帰無仮説のrejection regionを広げる??
  - 検出したいeffect sizeを大きくして、alternative distributionをもっと左側に寄せる??
  - サンプルサイズnを大きくして、null distributionとalternative distributionの分布の幅を狭める??(i.e. 確率分布の分散を小さくする??)

#### 検出力を上げるためには...

- ABテストの設計において、まずsignificance level(有意水準, i.e. acceptable false positive rate, 慣例は5%)を指定した上で、false negative rateを制御するために実験を設計(=実験における変数を制御)する。
- **検出力を高めるために我々が制御可能な変数は、以下の3つ**(=ABテストにおけるハイパーパラメータみたいな?:thinking:):
  - 1. effect size(効果量):
    - 効果が大きい施策を打てば、controlとtreatmentのmetric値の差が大きく観測されやすいよね。
    - コインの実験例で言えば、55%表が出る不公平コインよりも、75%表が出る不公平コインの方が、直感的にも公平ではないという実験結果が得られやすいはずだよね...!
    - (netflixさんのブログの文脈では、**ここでのeffect sizeの意味は、施策の真のeffect sizeのことっぽい。これは一応、効果量を計算する上で必要な「検出したい効果量」とは異なる概念なのかな**:thinking:)
      - (でも結局、true effect sizeが大きければ「検出したい効果量」を大きく設定できて、alternative distributionを右側に寄せることができるのか...!:thinking:)
  - 2. Sample size:
    - サンプルサイズ(=実験単位がユーザなら、ユーザ数)が多いほど、検出力が高くなり、より小さな効果を検出しやすくなる。
      - サンプルサイズが大きいほど、null distributionとalternative distributionの分布の幅が狭くなるから!ベルヌーイ分布の標本平均の確率分布の分散は $p(1-p)/n$ だから...!:thinking:
      - -> rejection regionの値域が広くなるから、false negative rateが低くなる。
    - プロダクト開発の文脈では、テストに割り当てるユーザ数(or他の実験単位)を増やすか、variant数を減らすことで検出力を高められる。
      - ただし、各ABテストのサンプルサイズと、並行実施できる重複しないABテストの数はトレードオフである。
  - 3. 実験対象集団におけるmetricの変動性
    - (=metricの真の分散みたいな認識...!:thinking:)
    - **集団内のmetricの値の変動性が小さい**ほど、true effectを正しく検出しやすくなる。(要は値が安定したmetricの方が、実験で評価しやすいよねって話...!:thinking:)
      - (これもサンプルサイズと同様に、null distributionとalternative distributionの分布の幅が狭くなるからかな:thinking:)
    - ex.) 再生ボタンを押してからビデオ再生が開始するまでのlatencyを改善する施策のABテスト:
      - **latencyの値は、ユーザ間で多くの自然なばらつきがある**。(デバイスやネット環境に依存)
        - (確かに、latencyはアプリ由来とユーザ由来の2つの要因があるからか...!:thinking)
      - もし施策によってlatencyの指標がわずかに改善したとしても、それをABテストでうまく検出することは難しい。ユーザ間のばらつきによるノイズが、改善の小さなシグナルを覆い隠してしまうから。
      - -> 実験対象を同じデバイスと同じweb接続を使用するユーザ集合に絞ることで、metricの変動性を小さくし、検出力を高めることができる。
    - (**metricのばらつきが施策の効果を薄めてしまう問題か...! テストするmetricをより安定したものに変更したり、Trigger分析で対象ユーザを実際に施策を受けたユーザのみに絞り込むのも、たぶん施策効果の希釈を防ぐアプローチの一つだよね**...!:thinking:)
      - 論文内でも、**non-binary metricであるrevenue(収益)よりも、conversion rateのほうが一般的にmetricの変動性が小さいため、他の全ての条件が同じでも検出力はconversion rateを採用した方が高くなる**、という話があった...!:thinking:

### Confidence level (信頼度)

### Power (検出力)

### A/A test

## 意思決定のためのツールとしての統計的仮説検定
