## link

- <https://arxiv.org/pdf/2306.13662>

# Best Practices for Machine Learning Systems: An Industrial Framework for Analysis and Optimization

## abstract

In the last few years, the Machine Learning (ML) and Artificial Intelligence community has developed an increasing interest in Software Engineering (SE) for ML Systems leading to a proliferation of best practices, rules, and guidelines aiming at improving the quality of the software of ML Systems. However, understanding their impact on the overall quality has received less attention. Practices are usually presented in a prescriptive manner, without an explicit connection to their overall contribution to software quality. Based on the observation that different practices influence different aspects of software-quality and that one single quality aspect might be addressed by several practices we propose a framework to analyse sets of best practices with focus on quality impact and prioritization of their implementation. We first introduce a hierarchical Software Quality Model (SQM) specifically tailored for ML Systems. Relying on expert knowledge, the connection between individual practices and software quality aspects is explicitly elicited for a large set of well-established practices. Applying set-function optimization techniques we can answer questions such as what is the set of practices that maximizes SQM coverage, what are the most important ones, which practices should be implemented in order to improve specific quality aspects, among others. We illustrate the usage of our framework by analyzing well-known sets of practices.

## Introduction

In Software Engineering, Software Quality Models (SQM) are central when it comes to achieving high quality software, as highlighted for example by [10]: "A quality model provides the framework towards a definition of quality". A Software Quality Model is the set of characteristics and the relationships between them that provides the basis for specifying quality requirements and evaluation [19]. In practice, a SQM is a structured set of attributes describing the aspects that are believed contribute to the overall quality. Machine Learning (ML) systems have unique properties like data dependencies and hidden feedback loops which make quality attributes such as diversity, fairness, human agency and oversight more relevant than in traditional software systems [29]. This makes traditional　quality models not directly applicable for ML applications. Moreover in recent years there has been a rise in the publication of best practices tailored for ML systems [26], [2], [34], [35], [27], however understanding their impact on overall quality and the systematic prioritization for their adoption has not received enough interest. Improving the quality of ML systems, especially in an industrial setting where multiple ML systems are in production, does not only require a set of practices, but also a deep understanding of their contribution to specific aspects of the quality of the system, as well as criteria to prioritize their implementation due to their large number and high implementation costs. Without a systematic prioritization based on their contribution to each individual aspect of software quality, it is challenging for practitioners to choose the optimal practices to adopt based on their needs which might lead to limited adoption, undesired biases, inefficient development processes and inconsistent quality. The challenge lies on the fact that some best-practices have a narrow impact, strongly affecting a few specific quality aspects while others have wider impact affecting many aspects, which might lead to redundancy or gaps in the coverage of the all the relevant quality aspects. Another challenge is that the importance of each quality aspect depends on the specific ML application, hence there is no single set of best-practices that satisfies the quality requirements of all ML applications. To address these challenges we introduce a reusable framework to analyse the contribution of a set of best practices to the quality of the system according to the specific needs of the particular application. The framework consists of a general-purpose Software Quality Model for ML Systems, expert-based representations of a large set of well established best-practices, and a criterion to assess a set of best practices w.r.t. our SQM: the SQM Coverage Criterion, which quantifies how many of the attributes receive enough attention from a given set of best practices. Applying set optimization techniques we can answer questions such as what are the practices that maximize the coverage, which practices can be implemented to address specific quality aspects and which aspects lack coverage, among others. Concretely, our contributions are the following: 1) A generalpurpose software quality model tailored for ML systems. 2) A framework to analyse and prioritize software engineering best practices based on their influence on quality, with the flexibility to be adaptable according to the needs of each organization. 3) We apply the proposed framework to analyze existing sets of best practices for ML systems and identify their strengths and potential gaps.

The rest of the paper is organized as follows. Section 2 discusses related work with emphasis on Software Quality Models and software best-practices for ML systems, section 3 introduces our Software Quality Model and describes its construction process. Section 4 introduces our best-practices analysis framework with details about its construction process and relevant algorithms. In section 5 various best-practices sets are analysed using our framework, we present our findings and insights. Finally, section 6 summarizes our work and discuses limitations and future work. Appendices include all the details, such as proofs, extensive results, and computer code to facilitate reusability and repeatability of our framework.

## Related Work

### Software Quality Models for ML Systems

Defining and measuring software quality is a fundamental problem and one of the first solutions came through the means of a software quality model in 1978 [8]. Such models include general software characteristics which are further refined into sub-characteristics, which are decomposed into measurable software attributes whose values are computed by a metric [6]. Software quality models developed until 2001 [5], [15], [11], [19] are characterized as basic since they make global assessments of a software product. Models developed afterwards, such as [4], [1], [18] are built on top of basic models and are specific to certain domains or specialized applications, hence are called tailored quality models [22]. Such a quality model tailored for data products has been presented in [18]. Software for ML Systems exhibits differences when compared to traditional software such as the fact that minor changes in the input may lead to large discrepancies in the output [20]. Moreover due to the dependencies on data, ML systems accumulate technical debt which is harder to recognize than code dependencies, which are identified via static analysis by compilers and linkers, tooling that is not widely available for data dependencies. Other peculiarities of ML systems include direct and hidden feedback loops where two systems influence each other indirectly [26]. Additionally, software quality aspects such as fairness and explainability as well as legal and regulatory aspects which are relevant to ML software are not covered by existing software quality models [32]. Furthermore, existing quality attributes such as maintainability and testability need to be rethought in the context of ML software [16]. All these peculiarities make existing software quality models only partially applicable to ML software. In [29] the authors present the systematic construction of quality models for ML systems based on a specific industrial use case. The authors focus on the process of constructing a quality meta model, identifying ML quality requirements based on the use case and instantiating a quality model that is tailored to the business application. In our work however, we introduce a general software quality model for ML systems that can be directly applied on a large set of industrial applications, without the need to go through a construction process. The key difference between our work and [29], is that their main contribution a development process for quality models, while one of our main contributions is the quality model itself, which can be used with no or minimum modifications for a broad range of ML systems. This allows the usage of the same quality model for multiple use cases within an organization which reduces the effort of its adoption and allows to create a common communication language regarding the quality of the ML systems in the organization. In [23] the authors conclude that the majority of the studies on software quality for ML either adopt or extend the ISO 25010 Quality Model for software product quality [17]. They find though that there is no consensus on whether ISO 25010 is appropriate to use for AI-based software or which characteristics of AI-based software may be mapped to attributes of traditional quality models. Unlike other studies, we did not adopt or extend ISO 25010 but rather followed a systematic approach to build our quality model from scratch by adding quality sub-characteristics based on their relevance to ML systems.

### Software Best Practices for ML Systems

Best practices for increasing the quality of ML systems are presented in [7], [2] and [34] however a systematic way to link the influence of the recommended practices to the software quality attributes of ML systems is not included. This makes it particularly challenging for ML practitioners to prioritize the adoption (or even understand the impact) of the large set of best practices based on the specific needs of their organizations. In [35] the authors present published ML practices targeting several testing properties (relevance, robustness, correctness, efficiency, security, privacy, fairness and interpretability) however their influence on quality aspects is not being studied. The authors in [27] conducted a survey of ML practitioners from multiple companies and present the effect of various published ML practices on four categories (Agility, Software Quality, Team Effectiveness and Traceability). They present the importance of each practice for each of the categories, as perceived by the surveyed practitioners. However, these categories are generic, and in fact only two of them are directly related to software quality (Software Quality and Traceability), in contrast, we study the influence of each best practice on a full-blown general purpose Software Quality Model specifically built for ML system with fine-grained aspects such as testability and deployability. Furthermore, we study the influence on each quality aspect of the quality model when a set of practices is applied, which is key to understand and prioritize best-practices since the overall impact is different depending on which other practices are also implemented. In [21] the authors extracted challenges and solutions for large scale ML systems synthesized into four quality attributes: adaptability, scalability, safety and privacy. They categorized software practices based on the step on the ML lifecycle and the addressed quality attribute. A difference of this work with ours, is that in [21] each practice targets a single quality attribute while its effect on multiple attributes is not explicitly studied. Even though there is work that studies the effect of practices on software quality [29], [21], [27] to the best of our knowledge, no study has been published about the interrelationship of software best-practices for ML Systems with multiple fine-grained quality attributes, nor about their prioritization in order to balance Software Quality and implementation costs.

## A Software Quality Model for ML Systems

### The model

A quality model determines which quality aspects are considered when evaluating the properties of a software product [17]. Our software quality model for ML systems comprises 7 quality characteristics further divided into sub-characteristics. Quality characteristics are general properties of quality that comprise the fundamental factors, which cannot be measured directly. Each characteristic consists of sub-characteristics, which are concrete quality aspects that can be directly influenced and measured. A graphical illustration of our software quality model for ML systems is presented in treestructure in Figure 1. We define quality characteristics as follows: Utility — The degree to which a machine learning system provides functions that meet stated and implied needs when used under specified conditions. Economy — The level of performance relative to the amount of resources used under stated conditions. Robustness — The tolerance to degradation by the machine learning system under consideration when exposed to dynamic or adverse events. Modifiability — The degree of effectiveness and efficiency with which a machine learning system can be modified to improve it, correct it or adapt it to changes in environment and in requirements. Productionizability — The ease of performing the actions required for a machine learning system to run successfully in production. Comprehensibility — The degree to which users and contributors understand the relevant aspects of a machine learning system. Responsibility — The level of trustworthiness of a machine learning system. The definitions of all sub-characteristics can be found in Appendix B. Notice that there are no data quality attributes in the quality model, as these are defined in well established software quality models tailored for data [18]. This existing data quality model can be used in addition to our software quality model, to analyze the quality of data which are used as input to an ML system.

### The development process

We started by creating a list of the quality sub-characteristics to be included in our model. To achieve this, we went through the list of all the known system quality attributes in [33] and all software quality models in [22] from which we shortlisted and adapted the ones we judged applicable to machine learning systems. The shortlisting was done based on the relevance of each quality attribute to any stages of the ML development lifecycle defined in [3] and taking into account the various types of ML use cases e-commerce platforms like Booking.com has. Next, we added attributes related to machine learning that were not part of the initial list, such as fairness and explainability (as defined in Appendix B). With the final list of attributes, we created clusters of factors (characteristics) comprising related sub-factors (sub-characteristics), following the standard nomenclature for quality models [22].

We validated the completeness of our quality model using published sets of machine learning practices [26], [7], [2], [27], [28]. Concretely, we checked if we can relate these practices to at least one of the quality sub-characteristics in our quality model. We iterated on this procedure a few times before we concluded on an first version, which was further refined using feedback from 10 internal senior ML engineers and scientists working in the industry and building ML systems for a minimum of 5 years. Given the speed with which the field is evolving, it is important to remark that the software quality model for machine learning is a live artifact constantly reviewed and updated in order to keep its relevance to the current machine learning needs. Another development process for a quality model for machine learning has been presented in [29], in which the authors explain the implementation process of quality models for particular machine learning related use cases. Our development process aimed at creating a general-purpose quality model which is relevant for a wide range of machine learning applications. Different applications and organizations will put different emphasis onto different sub-characteristics (for example external facing systems should be invulnerable even at the cost of accuracy) something that can be achieved by using importance weights per quality sub-characteristic. Having a common quality model for all the machine learning systems allows its usage as a common language for quality related initiatives and for identification of gaps on quality attributes both at the system and organizational level.

## A Framework to Prioritize Software Practices

Choosing practices in order to improve ML quality is a challenging task mainly due to their large number, varying implementation costs, and overlapping effects. To tackle this, we propose a framework to analyze and prioritize software practices. Given a Software Quality Model represented by a set of sub-characteristics 𝐶, and a set of software best practices 𝑃 we want to choose a subset of practices maximizing the coverage of a given set of sub-characteristics, under a constraint of implementing at most 𝐵 practices 1 . Having an influence 𝑢(𝑝, 𝑐) for a practice 𝑝 on a sub-characteristic 𝑐 we can define coverage as a minimum threshold 𝑘 of influence. Formally we have: (1) A Software Quality Model, represented by its set of subcharacteristics 𝐶 (2) A set of software practices 𝑃 (3) For each practice 𝑝 ∈ 𝑃 and each quality sub-characteristic 𝑐 ∈ 𝐶, the influence defined by a function 𝑢 : 𝑃 × 𝐶 → R + (4) A sub-characteristic importance vector 𝑤 ∈ [0, 1] |𝐶| representing the relevance of each sub-characteristic 𝑐 ∈ 𝐶 (5) An effort budget in the form of number of practices to be adopted 𝐵 ∈ N (6) An integer 𝑘 representing the minimum influence necessary to consider any sub-characteristic covered We define the coverage function as a set function that given a set of sub-characteristics 𝐶 with importance weights 𝑤 and a coverage threshold 𝑘 maps a set of practices 𝑋 ∈ 2 𝑃 to a real number, formally:

The objective is to choose a subset of practices that maximizes the coverage of the quality model weighted by its importance under the budget constraint:

### Eliciting the relationship between best practices and quality sub-characteristics

In order to apply the framework in practice, we first needed a set of practices 𝑃. To achieve this, we conducted a survey with our internal ML practitioners at Booking.com where we asked them which 3 best practices for ML systems, from the ones they apply in their day to day work, they find the most useful. In total we received 25 responses from ML engineers and scientists with a minimum of 3 years of industrial experience building production ML systems. Based on the responses we created a list of 41 practices, which can be found in Appendix D.1. Then, we obtained the values of the function 𝑢(𝑝, 𝑐) to be used as inputs in the framework by going through the following procedure. We conducted a workshop with 13 internal ML practitioners (ML engineers and scientists with a minimum of 3 years of industrial experience building ML systems) who were given a lecture on the proposed Software Quality Model and had interactive exercises to ensure a deep understanding of all the quality sub-characteristics and their nuances. In the end of the workshop, the practitioners were given a quiz to assess their understanding. After the quiz, the practitioners were asked to score the set of 41 practices against each quality sub-characteristic (𝐶) on a 0-4 scale indicating their influence: irrelevant (0), weakly contributes (1), contributes (2), strongly contributes (3) and addresses (4) 2 . Finally by taking the median of the scores of all the practitioners we obtain the influence of each practice 𝑝 on each quality sub-characteristic 𝑐, 𝑢(𝑝, 𝑐). To make this more concrete, we provide some examples of scores 𝑢(𝑝, 𝑐) for several pairs of quality sub-characteristic and practices in Table 1. Influence scores for each sub-characteristic can be found in Appendix F. Given the influence per practice and sub-characteristic 𝑢(𝑝, 𝑐) and a coverage threshold 𝑘, we can determine when a sub-characteristic is considered covered. For example, given that we want to cover Understandability, if 𝑘 = 10 then the practices documentation, peer code review and error analysis with influence scores 𝑢(𝑝, 𝑐) of 4,3 and 3 respectively, do cover it. However the practices logging of metadata and artifacts, data versioning and alerting, with influence scores of 2,1 and 0 respectively, do not cover Understandability.

### Scaling of Influence Scores

Based on ML practitioners’ evaluation, four practices scored with an influence of weakly contributes = 1 should not be treated equally as a practice scored with addresses = 4, hence to penalize weak contributions we re-scale the scores. To achieve this we chose a piecewise linear function where we define the addresses influence score = 4*strongly contributes, strongly contributes = 3* contributes, contributes = 2 * weakly contributes. For continuous values, after averaging multiple ML practitioners scores, we apply a piecewise linear function between these values which we depict in Figure 2. We defined coverage in Equation 1 as the minimum threshold of influence 𝑘. We chose one addresses influence to cover a subcharacteristic, and after applying our re-scaling function we get 𝑘 = 24. In general, the parameter 𝑘 defines the coverage threshold, and the re-scaling allows to parameterize the relationship of the influence scores while keeping the scoring of the sub-characteristic and practice pairs on a small linear scale of [0; 4] ∈ Z 0+ . The choice of 𝑘 and of the re-scaling function depend on the application where the ML System is deployed and on the risk of wrongly treating a sub-characteristic as covered.

### Inter-annotator Agreement

Assessing the influence of a practice in a quality sub-characteristic is a subjective task and therefore subject to annotator disagreement. We used two tests for agreement - whether two scores are identical (referred as plain agreement) and whether two scores differ by more than one level (referred as practical agreement). The practical test is more aligned with the complexity of the task and the variance coming from the practitioners experience and knowledge. We found an average agreement rate (between a pair of annotators) of 73.56% (plain) and 86.38% (practical). We used Cohen’s Kappa to check the agreement rate while neutralizing the probability of agreement happening by chance, and reached 0.4 (plain) and 0.69 (practical). These scores represent an agreement rate which is between fair (plain) and substantial (practical) according to [31]. The observed consistency suggests that we can have new best practices sets (or new quality sub-characteristics), scored by substantially fewer practitioners, which we consider an important insight when it comes to adopting new practices in an industrial setting. For example, considering the case of only two annotators, we estimate the sampling distribution for both the agreement-rate and Kappa statistic by computing the metric for every possible pair of annotators among the 13. For the agreement rates, the standard deviation is 1.38% (plain) and 1.68% (practical), and for the Kappa statistic the standard deviation is 0.043 (plain) and 0.05 (practical). Both figures are low enough which enables us to substitute a large group of annotators with only a pair and still get reliable scores.

### Algorithms

The maximization problem we want to solve is similar to the Generalized Maximum Coverage (GMC) problem [9], with a clear difference: in GMC if a set 𝑋 covers an element 𝑎, then at least one subset 𝑌 ⊂ 𝑋 covers 𝑎. In our case, if a set of practices 𝑄 ⊆ 𝑃 covers a sub-characteristic 𝑐 ∈ 𝐶, it might be the case that no subset of 𝑄 covers 𝑐. Consider two practices 𝑝1, 𝑝2 and sub-characteristic 𝑐 with 𝑢(𝑝1, 𝑐) = 𝑢(𝑝2, 𝑐) = 𝑘/2. In this case the set 𝑄 = {𝑝1, 𝑝2} covers 𝑐 since 𝑓 (𝑄; {𝑐}, 1, 𝑘)) = 𝑘 but no subset of 𝑄 does since 𝑓 ({𝑝1}; {𝑐}, 1, 𝑘)) = 𝑓 ({𝑝2}; {𝑐}, 1, 𝑘)) = 𝑘/2 and 𝑓 (∅; {𝑐}, 1, 𝑘)) = 0. Because of this, a specific analysis is required. The budget expressed as the maximum number of practices to be applied leads to a combinatorial explosion of the search space. To illustrate, the set of 41 practices we collected and a budget of 3 practices yields a search space of size 41 3  = 10660, whereas a budget of 10 practices yields a search space of 1.12e+9 options to explore. To tackle this computational problem we propose a greedy solution based on the observation that 𝑓 is positive monotone submodular (proof in Appendix A). Maximizing a monotone submodular function is known to be NP-Hard [14], [13], however a simple greedy approach yields a (1 − 1 𝑒 )-approximation [25] even for one general knapsack constrain [30], and it is the best polynomial time solution, unless 𝑃 = 𝑁 𝑃 [24], [12]. We propose two solutions: brute force and greedy, in Algorithm 1 and 2 respectively. In practice we found that the greedy approach rarely yields sub-optimal results for this case.

## Applying the Framework

In this section we illustrate the usage of our framework by analyzing our own best-practices set and three well-known ML best-practices sets [7], [2], [27] and [28] (we combine the last two as they intersect) including 28, 7, and 45 best practices respectively. In each case we compute the coverage function, optimal practices sets for different budgets, and highlight gaps as well as general trends. We also provide a global analysis combining all sets of best practices.

### Analyzing sets of best practices

#### Internal Set

Using the influence vectors of the internal set of 41 practices applied at Booking.com, we can visualize the total contribution of the set to all the quality sub-characteristics and assess its completeness. We plot the contributions of the internal set in Figure 3, where we mark the threshold 𝑘 = 24 contribution points indicating coverage of a quality sub-characteristic. We observe that 22 out of 29 sub-characteristics are being covered indicating a coverage rate of 75%. The sub-characteristics with the largest influences are mostly associated with traditional software systems, such as effectiveness and monitoring, while the ones with the least influences are more specific to ML systems, such as explainability and discoverability. This is due to the fact that historically, engineering best practices are more closely related to traditional software systems and only in the recent years ML specific best practices started becoming popular. Based on this analysis we were able to identify the areas for which practices are lacking and work towards their coverage, by creating new ones. Concretely, to address the gaps in Vulnerability, Responsiveness and Discoverability we created the following practices: "Request an ML system security inspection", "Latency and Throughput are measured and requirements are defined", "Register the ML system in an accessible registry", which increase the coverage for each of the sub-characteristics respectively (see Appendix D.1 for their descriptions). To gain further insight, we use the Greedy algorithm to find the top 3 influential practices on all quality sub-characteristics, considering them all equally important. The algorithm outputs a set of the following top 3 practices: "Write documentation about the ML system", "Write modular and reusable code", and "Automate the ML lifecycle". This result has been used to guide the ML practitioners at Booking.com on the prioritization of practice adoption in their daily work, by highlighting the value of these practices on the overall ML quality. The actual prioritization of their adoption depends on the team, since different teams and departments use different priorities for the quality sub-characteristics.

#### External Sets

We analyze three ML best practices sets of 80 practices in total. Since it is impractical to have the same 13 ML practitioners scoring the 80 practices, we limit the number of annotators to 2, based on the high agreement rate for a pair of annotators observed in Section 4.3. After the scoring, we compute the plain agreement rate for the 2 annotators to be 63.5% and the practical agreement rate 94.5%. With these vectors, we can visualize the total contribution of the whole set of practices to each of the quality sub-characteristics and based on that assess which of them are being covered. In Figure 4a we see that applying all the practices presented in [27] 25 sub-characteristics are covered. In this set of practices the strongest emphasis is on sub-characteristics related to cost-effectiveness, responsibility and modifiability. On the other hand, sub-characteristics such asscalability, discoverability, operability and responsiveness, remain uncovered even when applying all the 45 practices from this set. Figure 4b illustrates the contributions by applying all the 28 practices mentioned in [7] and we observe that this set covers 17 sub-characteristics: we observe the top contributions to be on non-ML specific quality sub-characteristics, although ML specific ones such as accuracy and fairness are also covered. The least covered are related to collaboration such as ownership, discoverability and readability. Lastly, the contributions of [2] to the software quality are depicted in Figure 4c. This set of 7 practices manages to cover 9 quality sub-characteristics with a focus on those related to economy and modifiability. The least contributions are achieved on aspects related to the comprehensibility of ML systems. In general we find that all practice sets focus on different quality attributes and have gaps on different areas of our SQM. This indicates that the sets complement each other, which motivates our next analysis. In Figure 4d we look into the quality coverage in the scenario where we apply all the practices combined. After removing overlapping practices (see Appendix D.2), this set includes 76 practices. We observe that when we apply the full set of 76 practices, 28 sub-characteristics are covered which verifies that the practices complement each other. An example that shows this is scalability, which is not covered by any set in isolation, but only when the practices are combined. We also see that even when applying all the 76 practices, discoverability remains uncovered. This shows that there is lack of practices addressing this quality sub-characteristic, something that was also observed in the analysis of the internal practice set. Moreover, the low scores for sub-characteristics like scalability, operability, usability and responsiveness indicate that they receive less attention compared to the rest. On the other hand, it is encouraging to see large scores for sub-characteristics related to trustworthiness such as fairness and explainability.

### Score and coverage threshold sensitivity

To further assess the sensitivity of the results to the scores assigned by the ML practitioners, we perturb the scores by adding a random integer in the range [−1; 1] and [−2; 2]. We then take the original scores and perturbed ones, and compute the scores of each subcharacteristic as if all practices were applied and rank them by the sum of scores. Then we measure the Pearson correlation coefficient of the original ranking and the ranking after the scores were perturbed. After 1000 perturbation iterations we obtain a mean correlation coefficient of 0.94 with a variance of 0.0002 for perturbing by [−1; 1], and a mean of 0.91 with a variance of 0.0006 for perturbing by [−2; 2] respectively. A random integer in the range [−3; 3] yields a mean of 0.86 and a variance of 0.0016. This shows that our results are robust to scoring variance. Regarding the coverage threshold 𝑘 we remark that 24 points is rather low since one single practice with addresses score would cover the sub-characteristic, at the same time, in Figures 3 and 4 we can see that small changes in 𝑘 do not lead to big changes in which quality sub-characteristics are covered, more importantly, the general observations hold even for moderate changes in 𝑘.

### How many practices are enough?

To evaluate how many practices are enough to maximize quality, we analyze the internal and open source sets combined (after removing overlapping practices the combined set has 101 practices, see Appendix D.2 for details). Using our prioritization framework we find the minimum number of practices which cover the same number of quality sub-characteristics as the full set of those 101 practices. To achieve that, we find the top 𝑁 practices from the combined set of practices using our greedy algorithm (brute force takes too long), for 𝑁 ∈ [1, 101] and we evaluate what percentage of the quality sub-characteristics is being covered with each set of practices. Figure 5 illustrates the coverage percentage for all the values of 𝑁. We see that applying 5 practices covers almost 40%, 10 cover 70%, and to reach 96%, 24 are needed. The coverage does not increase further with the practices. This result shows that using a relatively small number of practices can achieve similar results in terms of quality coverage to the full set of 101 practices. This means that when applying the right set of practices, a significant reduction in the effort of adoption can be achieved, which is especially relevant in an industrial setting.

### Which are the best practices?

To gain further insights as to which are the 24 practices which maximize coverage, we provide the optimal set in Table 2, along with the source of each practice (some practices have been renamed for better clarity, see Appendix D.3 for details). It is important to note that here we assume equal importance for each quality subcharacteristic, something that needs to be taken into account from ML practitioners wanting to use this set as guidance. In case a different importance weighting is desired, one needs to re-create this set after applying importance weights to each sub-characteristic. Prioritization within the final set, can be achieved by taking into account the specific needs of an organization (for example if safety is top priority, practices focusing on robustness should be prioritized) or the cost of adoption per practice.

### Futher applications of the framework

The proposed SQM is currently being used to construct a quality assessment framework for ML systems. Concretely, the framework assesses the coverage of each quality sub-characteristic on an ML system level, to pinpoint improvement areas. Implementing an ML quality assessment framework without an SQM for ML systems, would lead to an incomplete picture of ML quality. Moreover, the prioritization framework is being used alongside the quality assessment framework: After the quality of an ML system is assessed, by assigning a quality score per quality sub-characteristic, the sub-characteristics with low scores are provided as input in the prioritization framework in order to recommend the best 3 practices to apply in order to cover them. This has been very helpful for ML practitioners as it allows them to prioritize the improvements to be made efficiently, by focusing on practices that have the largest influence in the quality attributes that are considered the most important for the use case at hand.

Additionally, the SQM has created a common language for ML practitioners to discuss ML quality topics and quality related initiatives are easier to be justified. For example, it is more straightforward to argue about the value of an initiative targeting to increase the adoption of unit-testing for ML systems, since the benefit of it, e.g. improvement in modifiability of the system, is clear. An advantage of our framework is that it is flexible enough to be adapted to other organizations. For completeness, we describe how this can happen. The organization needs to determine which quality sub-characteristics are the most crucial, by specifying the importance weights 𝑊 for each sub-characteristic. The provided software practices can be used as is or new ones can be added and scored by ML practitioners within the organization. Lastly, a coverage threshold 𝑘 should be chosen based on how strict an organization wants to be for solving a given quality sub-characteristic. To deal with disagreements in the scores 𝑢(𝑝, 𝑐) or the coverage threshold 𝑘, the mean or median can be taken. Then, all an ML practitioner needs to do is to run the prioritization algorithm using as inputs the quality sub-characteristics 𝐶 to be improved, the set of practices 𝑃 to be considered, the allowed budget 𝐵, the importance vectors𝑊 and the coverage threshold 𝑘, and then adopt the optimal practices which are recommended by the framework.

## Conclusions and Discussions

Conclusion. In this work we presented a framework to analyse the relationship between software engineering best practices for ML Systems and their quality with the primary purpose of prioritizing their implementation in an industrial setting. We addressed the challenge of defining quality by introducing a novel Software Quality Model specifically tailored for ML Systems. The relationship between best practices and the various aspects of quality was elicited by means of expert opinion and represented by vectors over the sub-characteristics of the Software Quality Model. With these vectors we applied Set Optimization techniques to find the subset of best practices that maximize the coverage of the SQM. We applied our framework to analyse 1 large industrial set of best practices as implemented at Booking.com and 3 public sets. Our main findings are: (1) Different best-practices sets focus on different aspects of quality, reflecting the priorities and biases of the authors. (2) Combining the different best-practices sets, high coverage is achieved, remarkably, aspects that no single best-practices set covers on its own are covered by integrating different practices proposed by different authors. (3) Even though there is a proliferation of best practices for ML Systems, when chosen carefully, only a few are needed to achieve high coverage of all quality aspects. (4) Even though the influence of best-practices on quality aspects is a subjective concept we found surprisingly high consistency among experts. Our framework was useful to spot gaps in our practices leading to the creation of new ones to increase the coverage of specific quality aspects.

Limitations. A limitation of this work is that in order to add a new quality sub-characteristic or a new practice to the framework, one needs to score the influence vectors which is a time consuming procedure. On the other hand, the addition or removal of an existing practice or quality sub-characteristic does not influence the existing scores. Another caveat regards the subjectivity of the influence vectors based on the individuals who conduct the scoring. However, our sensitivity analysis described in Section 5.2 indicates that our results are robust to scoring variance, which mitigates the subjectivity concerns.

Future Work. Future work will focus on a comparison of our framework with baseline prioritization approaches (such as prioritizing the most popular practices first or the ones requiring the least effort) and on assessing the coverage of sub-characteristics in existing ML Systems. We will also keep evolving the assessment framework mentioned in Section 5.5 since this can provide visibility on quality gaps of ML systems, and along with the prioritization framework can provide guidance to ML practitioners on the optimal actions to take to improve them. Furthermore, exploring more realistic practice implementation cost functions can lead to a better cost and quality trade-off. Lastly, even though we aim at producing a complete software quality model, further validation is necessary especially by the external ML community.
