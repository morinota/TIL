## 0.1. link

https://www.researchgate.net/publication/333136404_Top_Challenges_from_the_first_Practical_Online_Controlled_Experiments_Summit

## 0.2. title

Top Challenges from the first Practical Online Controlled Experiments Summit

## 0.3. abstract

Online controlled experiments (OCEs), also known as A/B tests, have become ubiquitous in evaluating the impact of changes made to software products and services. While the concept of online controlled experiments is simple, there are many practical challenges in running OCEs at scale and encourage further academic and industrial exploration. To understand the top practical challenges in running OCEs at scale, representatives with experience in large-scale experimentation from thirteen different organizations (Airbnb, Amazon, Booking.com, Facebook, Google, LinkedIn, Lyft, Microsoft, Netflix, Twitter, Uber, Yandex, and Stanford University) were invited to the first Practical Online Controlled Experiments Summit. All thirteen organizations sent representatives. Together these organizations tested more than one hundred thousand experiment treatments last year. Thirty-four experts from these organizations participated in the summit in Sunnyvale, CA, USA on December 13-14, 2018. While there are papers from individual organizations on some of the challenges and pitfalls in running OCEs at scale, this is the first paper to provide the top challenges faced across the industry for running OCEs at scale and some common solutions.

# 1. Introduction

The Internet provides developers of connected software, including web sites, applications, and devices, an unprecedented opportunity to accelerate innovation by evaluating ideas quickly and accurately using OCEs. At companies that run OCEs at scale, the tests have very low marginal cost and can run with thousands to millions of users. As a result, OCEs are quite ubiquitous in the technology industry. From front-end user-interface changes to backend algorithms, from search engines (e.g., Google, Bing, Yandex) to retailers (e.g., Amazon, eBay, Etsy) to media service providers (e.g. Netflix, Amazon) to social networking services (e.g., Facebook, LinkedIn, Twitter) to travel services (e.g., Lyft, Uber, Airbnb, Booking.com), OCEs now help make data-driven decisions [7, 10, 12, 27, 30, 40, 41, 44, 51, 58, 61, 76].

## 1.1. First Practical Online Controlled Experiments Summit, 2018

To understand the top practical challenges in running OCEs at scale, representatives with experience in large-scale experimentation from thirteen different organizations (Airbnb, Amazon, Booking.com, Facebook, Google, LinkedIn, Lyft, Microsoft, Netflix, Twitter, Uber, Yandex, and Stanford University) were invited to the first Practical Online Controlled Experiments Summit. All thirteen organizations sent representatives. Together these organizations tested more than one hundred thousand experiment treatments last year. Thirty-four experts from these organizations participated in the summit in Sunnyvale, CA, USA on December 13-14, 2018. The summit was chaired by Ronny Kohavi (Microsoft), Diane Tang (Google), and Ya Xu (LinkedIn). During the summit, each company presented an overview of experimentation operations and the top three challenges they faced. Before the summit, participants completed a survey of topics they would like to discuss. Based on the popular topics, there were nine breakout sessions detailing these issues. Breakout sessions occurred over two days. Each participant could participate in at least two breakout sessions. Each breakout group presented a summary of their session to all summit participants and further discussed topics with them. This paper highlights top challenges in the field of OCEs and common solutions based on discussions leading up to the summit, during the summit, and afterwards.

## 1.2. Online Controlled Experiments

Online Controlled Experiments, A/B tests or simply experiments, are widely used by data-driven companies to evaluate the impact of software changes (e.g. new features). In the simplest OCE, users are randomly assigned to one of the two variants: Control (A) or Treatment (B). Usually, Control is the existing system and Treatment is the system with the new feature, say, feature X. User interactions with the system are recorded and from that, metrics computed. If the experiment was designed and executed correctly, the only thing consistently different between the two variants is feature X. External factors such as seasonality, impact of other feature launches, or moves by the competition, are evenly distributed between Control and Treatment, which means that we can hypothesize that any difference in metrics between the two groups can be attributed to either feature X or due to chance resulting from the random assignment to the variants. The latter hypothesis is ruled out (probabilistically) using statistical tests such as a t-test [21]. This establishes a causal relationship (with high probability) between the feature change and changes in user behavior, which is a key reason for the widespread use of controlled experiments.

## 1.3. Contribution

OCEs rely on the same theory as randomized controlled trials (RCTs). The theory of a controlled experiment dates back to Sir Ronald A. Fisherâ€™s experiments at the Rothamsted Agricultural Experimental Station in England in the 1920s. While the theory is simple, deployment and evaluation of OCEs at scale (100s of concurrently running experiments) across variety of web sites, mobile apps, and desktop applications presents many pitfalls and research challenges. Over the years some of these challenges and pitfalls have been described by authors from different companies along with novel methods to address some of those challenges [16, 23, 43, 48, 50, 61]. This is the first time that OCE experts from thirteen organizations with some of the largest scale experimentation platforms have come together to identify the top challenges facing the industry and the common methods for addressing these challenges. This is the novel contribution of this paper. We hope that this paper provides researchers a clear set of problems currently facing the industry and spurs further research in these areas in academia and industry. It is our wish to continue this cross-company collaboration to help advance the science of OCEs. Section 2 presents an overview of top challenges currently faced by companies across the industry. Later sections discuss specific problems in more detail, how these problems have an impact on running OCEs in different products, and common solutions for dealing with them

# 2. Top Challenges

Software applications and services run at a very large scale and serve tens to hundreds of millions of users. It is relatively low cost to update software to try out new ideas. Hundreds of changes are made in a software product during a short period of time. This provides OCEs a unique set of opportunities and challenges to help identify the good changes from the bad to improve products. Our compiled list of top challenges comes from three sources: the pre-summit survey to collect the list of topics to discuss, the top challenges presented by each company at the start of the summit, and the post-summit survey on top takeaways and list of topics to discuss in future. The relative order of prevalence remained roughly the same across these sources. These top challenges reflect the high level of maturity and scale of OCE operations in these companies. There is literature out there on the challenges and pitfalls some of these companies faced and solved during the early years of their operations [32]. The challenges mentioned here are at the frontier of research in academia and industry. While there are some solutions to existing challenges, there is a lot of scope for further advancement in these areas. 1. Analysis: There are many interesting and challenging open questions for OCE results analysis. While most experiments in the industry run for 2 weeks or less, we are really interested in detecting the long-term effect of a change. How do long-term effects differ from short-term outcomes? How can we accurately measure those long term factors without having to wait a long time in every case? What should be the overall evaluation criterion (OEC) for an experiment? How can we make sure that the OEC penalizes things like clickbaits that increase user dissatisfaction? While there are methods to test an OEC based on a set of labelled experiments [24], how to best collect such set of experiments for evaluation of the OEC and other metrics? While, there are models for estimating the long-term value (LTV) of a customer that may be a result of a complex machine learning model, can we leverage such models to create OEC metrics? Once we have OEC metrics and a Treatment improves or regresses the OEC metric, how can we best answer why the OEC metric improved or regressed and uncover the underlying causal mechanism or root cause for it? Running experiments at a large scale introduces another set of issues. It is common for large products serving millions of users to have 100s of experiments running concurrently, where each experiment can include millions of users. For products running so many experiments, most of the low-hanging fruit get picked quickly and many Treatments may then cause a very small change in OEC metrics. It is important to detect these types of changes. A very small change in a per-user metric may imply a change of millions of dollars in product revenue. How can we best increase the sensitivity of OECs and other experiments metrics without hurting the accuracy of these metrics to discern between good and bad Treatments [18, 42, 75]? If we are running 100s of experiments concurrently how do we handle the issue of interaction between two treatments? How can we learn more from analyzing multiple experiments together and sharing learnings across experiments? For a product with millions of users, there are many ways to segment users. Even a small fraction of users is very significant. Just understanding the average treatment effect on the entire population is not enough. How can we best identify heterogenous Treatment effects in different segments? 2. Engineering and Culture: Culture is the tacit social order of an organization: It shapes attitudes and behaviors in wide-ranging and durable ways. Cultural norms define what is encouraged, discouraged, accepted, or rejected within a group [35]. How do we build a culture to one that uses OCEs at scale to ensure we get a trustworthy estimate of the impact of every change made to a product and bases ship decisions on the outcome of OCEs [46]? Engineering systems and tools are critical aspects to enable OCEs at scale. What are some good development practices, data logging and data engineering patterns that aid trustworthy experimentation at scale? 3. Deviations from Traditional A/B Tests: Traditional A/B tests depend on a stable unit treatment value assumption(SUTVA) [39], that is, the response of any experiment unit (user) under treatment is independent of the response of another experiment unit under treatment. There are cases where this assumption does not hold true, such as network interactions or interactions between multiple experiments. If this issue is ignored, we may get a biased estimate of the treatment effect. How can we detect such deviation? Where deviations are unavoidable, what is the best method to obtain a good estimate of the treatment effect? 4. Data quality: Trustworthiness of the results of an OCE depend on good data quality. What are some best practices to ensure good data quality? While the sample Ratio Mismatch test is a standard industry test to indicate data quality issues in OCEs [13, 22, 23, 45], what are other critical data quality tests to perform during OCE results analysis?

# 3. Estimating the long-term effect

## 3.1. Problem

Though it only takes a few days to change a part of a software product or service, the impact of that change may take a long time to materialize in terms of key product indicators (KPIs) and can vary across products and scenarios. This makes it challenging to estimate the long-term impact of a change. For instance, the impact of a change of ranking results in an online travel service on customer satisfaction may not be fully understood until customers stay in a vacation rental or hotel room months after booking. Increasing the number of ads and hence decreasing their quality on a search results page may bring in more revenue in the first few weeks, but might have the opposite impact months later due to user attrition and users learning that ad results are less useful and ignoring them [38]. Investing in user retention and satisfaction through better user experience can be more beneficial over the long term than what short term measurements indicate. Introduction of clickbaits on a content provider service may cause increase in clicks due to the novelty effect but may induce larger dissatisfaction in the long term as users learn about poor content quality. Further, in twosided markets [71], some changes like pricing in ads, ride-sharing services, or home-sharing services may introduce a market effect with a shift in either demand or supply in the eco-system and it may take a long time before the market finds a new equilibrium.

## 3.2. Common Solutions and Challenges

### 3.2.1. Long-term Experiments or Holdouts

Running experiments for a long duration is not usually a good answer. Most software companies have very short development cycles for planning, developing, testing, ultimately shipping new features. Short development cycles enable companies to be agile and quickly adapt to customer needs and the market. Long testing phases for understanding the impact of changes could harm a companyâ€™s agility and are not usually desirable. Another option is a long-term holdout group consisting of a random sample of users who do not get updates. This holdout group acts as the Control against the set of features shipping to everyone else. This option usually incurs a lot of engineering cost. The product development team must maintain a code fork that is not updated for a long time. All upstream and downstream components to this code must support this fork as well. This still does not solve the challenges of non-persistent user tracking and network interactions described below. In many products and services, the first visit and subsequent visits of users is tracked using a non-persistent user identifier, like a random GUID [72] stored in a browser cookie. This way of tracking users is not very durable over a long time as users churn their cookies and we are left with tracking a biased sample of all users exposed to the variants [23]. Further, a user may access the same service from multiple devices, and the userâ€™s friends and family may access the same service. As time goes on, a user or their friends or family may be exposed to both the treatment and control experience during an experiment, which dilutes the impact of the treatment being measured in the experiment. There is some value in running experiments a little longer when we suspect that there is a short-term novelty or user learning effect. At Microsoft, while most experiments do not run for more than two weeks, it is recommended to run an experiment longer if novelty effects are suspected and use data from the last week to estimate the long-term treatment effect [23]. At Twitter, a similar practice is followed. An experiment at Twitter may run for 4 weeks and data from last two weeks is analyzed. If a user exposed in the first two weeks does not appear in the last two weeks, values are imputed for that user when possible (like imputing 0 clicks). However, it may not be possible to impute values for metrics, like ratio or performance metrics.

### 3.2.2. Proxies

Good proxies that are predictive of the long-term outcome of interest are commonly used to estimate the long-term impact. For instance, Netflix has used logistic regression to find good predictors for user retention. Netflix also used survival analysis to take censoring of user data into account. LinkedIn created metrics based on a lifetime value model. For treatments that effect the overall market, Uber found some macro-economic models to be useful in finding good proxies. There can be downsides to this approach as correlation may not imply causation, and such proxies could be susceptible to misuse, where a treatment may cause an increase in the proxy metric, but ends up having no effect or regression in the long-term outcome. It may be better to develop a mental causal structure model to find good proxies. Bing and Google have found proxies for user satisfaction and retention by having a mental causal structure model that estimates the utility of an experience to users.

### 3.2.3. Modeling User Learning

Another approach followed by Google is to explicitly model the user learning effects using some long duration experiments [38]. In long duration experiments, there are multiple and exclusive random samples of users exposed to the treatment. One group is exposed to the treatment from the start of the experiment. A second group has a lagged start, being exposed to the treatment at some point after the start, and so on. Comparing these groups a day after the second group is exposed to the treatment provides an estimate of user learning from the treatment. Google also used cookie-cookie day randomization to get an estimate of user learning for any duration (in days) since the experiment started. In these experiments and in the subsequent analysis, the authors carefully designed the experiments and did careful analysis to ensure that they were not seeing many confounding effects (e.g., other system changes, system learning, concept drift, as well as selection bias issues due to cookie churn/short cookie lifetimes). They took this information and modeled user learning as an exponential curve, which allowed them to predict the long-term outcome of a treatment using the short-term impact of the treatment directly measured in the experiment and the prediction of the impact of the treatment on user learning.

### 3.2.4. Surrogates

Surrogate modeling is another way to find good estimates of longterm outcome. A statistical surrogate lies on the causal path between the treatment and the long-term outcome. It satisfies the condition that treatment and outcome are independent conditional on the statistical surrogate. You can use observational data and experiment data to find good surrogates. Even if no individual proxy satisfies the statistical surrogacy criterion, a highdimensional vector of proxies may collectively satisfy the surrogacy assumption [8]. Having a rich set of surrogates reduces the risk of affecting only a few surrogates and not the long-term outcome. Facebook used this approach with some success to find good surrogates of the 7-day outcome of an experiment by just using 2-3-day experiment results. They used quantile regression and a gradient-boosted regression tree to rank feature importance. Note that there is still a risk that having too many surrogates for the long term may make this approach less interpretable.

# 4. OEC: Overall Evaluation Criterion Metric

## 4.1. Problem

One key benefit of evaluating new ideas through OCEs is that we can streamline the decision-making process and make it more objective. Without understanding the causal impact of an idea on customers, the decision-making process requires a lot of debate. The proponents and opponents of the idea advance their arguments regarding the change only relying on their own experience, recall, and interpretation of certain business reports and user comments. Eventually the team leader makes a call on whether to ship the idea. This style of decision making is based on the HiPPO (Highest Paid Personâ€™s Opinion) [37] and is fraught with many cognitive biases [2]. To help change HiPPO decisions to more objective, data-driven decisions based on the causal impact of an idea from customer response [5], we recommend establishing the OEC for all experiments on your product. Not all metrics computed to analyze the results of an experiment are part of the OEC. To analyze experiment results, we require different types of metrics [22]. First, we need to know if the results of an experiment are trustworthy. A set of data quality metrics, like a sample ratio, help raise red flags on critical data quality issues. After checking the data quality metrics, we want to know the outcome of the experiment. Was the treatment successful and what was its impact? This set of metrics comprise the OEC. In addition to OEC metrics, we have found that there is a set of guardrail metrics which are not clearly indicative of success of the feature being tested, but metrics that we do not want to harm. The remaining bulk of the metrics for an experiment are diagnostic, feature or local metrics. These metrics help you understand the source of OEC movement (or the lack of). It is hard to find a good OEC. Here are a few key properties [19, 24, 55] to consider. First, a good OEC must be indicative of the long-term gain in key product indicators (KPIs). At the very least make it directionally accurate in estimating the impact on the longterm outcome. Second, OEC must be hard to game and it should incentivize the right set of actions in the product team. It should not be easy to satisfy the OEC by doing the wrong thing. For instance, if the OEC is limited to a part or feature of the product, you may be able to satisfy the OEC by cannibalizing other parts or features. Third, OEC metrics must be sensitive. Most changes that impact the long-term outcome should also have a statistically significant movement in OEC metrics so it is practical to use the OEC to distinguish between good and bad changes to the product. Fourth, the cost of computing OEC metrics cannot be too expensive. OEC metrics must be computed for 100s of the experiments and be run on millions of users each and every week. Methods that involve costly computation or costly procedures like human surveys or human judges may not scale well. Fifth, OEC metrics must account for a diverse set of scenarios that may drive the key product goals. Finally, OEC should be able to accommodate new scenarios. For instance, direct answers to queries like current time would provide a good user experience in a search engine, but if you only base the OEC metrics on clicks, those metrics will miss this scenario.

## 4.2. Common Solutions and Challenges

### 4.2.1. Search vs. Discovery

Measuring the success of a search experience in search engines has been a research subject for a long time in academia and for many products including Bing, Google and Yandex. It is well established that metrics, like queries per user, cannot be a good OEC because queries per user may go up when search ranking degrades. Sessions per user or visits per user are considered better OEC metrics [49]. In general there is an appreciation of focusing on HEART (Happiness, Engagement, Adoption, Retention, and Task success) metrics for the OEC and use PULSE (Page views, Uptime, Latency, Seven-day active users [i.e., the number of unique users who used the product at least once in the last week], and Earnings) metrics as your guardrail metrics [62]. Over time, different methods have been proposed to measure HEART metrics in search engines and other goal-directed activities [36, 54]. It is still challenging to find metrics similar to HEART metrics to work for discovery- or browsing-related scenarios, like news articles shown on the Edge browser homepage, or Google mobile homepage, or Yandex homepage. The challenge is to understand user intent. Sometimes users will come with a goal-oriented intent and would like to quickly find what they are looking for. Other times users may have a more browsing or discovering-newinformation intent where they are not looking for something specific but just exploring a topic. In this case it is not clear if lack of a click on an article link with a summary snippet can be viewed as a negative experience or positive because users got the gist of the article and did not have to click further. Further the two intents (goal-oriented and browsing) can compete. If a user came with a goal-oriented intent but got distracted and ended up browsing more, it may cause dissatisfaction in the long term.

### 4.2.2. Product Goals and Tradeoffs

OEC metrics usually indicate improvement in product KPIs or goals in the long term. This assumes that product goals are clear. This is not a trivial problem. It takes a lot of effort and energy to have clarity on product goals and strategy alignment across the entire team. This includes decisions like defining who the customer is and how best to serve them. Further, your team must also create a monetization strategy for the product. In absence of such clarity, each sub team in the product group may set their own goals that may not align with other teamsâ€™ or corporate goals. Even after the product goals are clear, in most companies you end up with a handful of key metrics of interest. It is challenging how to weigh these metrics relative to each other. For instance, a product may have goals around revenue and user happiness. If a feature increases user happiness but losses revenue, in what case is it desirable to ship this feature? This decision is often made on a caseby-case basis by leadership. Such an approach is susceptible to a lot of cognitive biases, and also may result in an incoherent application of the overall strategy. Some product teams like at Bing, tried to come up with weights on different goals to make this tradeoff align with the overall product strategy and help ensure that consistent decision-making processes are used across multiple experiments.

### 4.2.3. Evaluating Methods

We mentioned that OEC metrics help decision making more objective. For it to be widely adopted, it is important to establish a process to evaluate any changes to the OEC metrics. In some cases, there may be an expert review team that examines changes to the OEC and ensure that it retains the properties of a good OEC. To make this process even more objective, we can have a corpus of past experiments widely seen to have a positive, negative or neutral impact. Changes to the OEC were evaluated on this corpus to ensure sensitivity and directional correctness [24]. Microsoft and Yandex successfully used this approach to update OEC metrics. The challenge here is to create a scalable corpus of experiments with trustworthy labels. Other approaches include doing degradation experiments, where you intentionally degrade the product in a treatment and evaluate if the OEC metrics can detect this degradation. One well known example are experiments that slow down the user experience performed at Microsoft and Google [63, 64]. This is also a good thought exercise to go through while designing and evaluating OEC metrics to ensure that they are not gameable.

### 4.2.4. Using Machine Learning in Metrics

Some product teams tried to incorporate machine learning models (MLMs) to create a metric. For instance, using sequences of user actions to create a score metric based on the likelihood of user satisfaction [36, 54, 57] or creating more sensitive OEC metrics by combining different metrics [25, 42, 59]. Also, good proxies for long-term outcomes are often used to find good OEC metrics. This area of experimentation is relatively new. Many product teams are carefully trying to test these methods in limited areas. These methods are more commonly used in mature product areas, like search, where most of the low-hanging fruit is picked and we need more complex models to detect smaller changes. For new products, it is usually better to use simple metrics as the OEC. There are some concerns with using machine learning models to create metrics. MLM based metrics can be harder to interpret and can appear as a blackbox, which reduces trustworthiness and makes it hard to understand why a metric may have moved. Refreshing MLMs by training them on most recent data may lead to an abrupt change in the metric that would hard to account for. If the MLM is being refreshed while an experiment is running, it can create bias in the metric. Further, there are concerns these metrics are easily gamed by optimizing for the underlying model that created these metrics, which may or may not lead to improvement in the longterm outcome of interest.
