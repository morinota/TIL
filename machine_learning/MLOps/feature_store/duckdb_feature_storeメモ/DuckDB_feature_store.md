refs: https://www.hopsworks.ai/post/python-centric-feature-service-with-arrowflight-and-duckdb
<!-- ãªã‚“ã‹Hopsworkç‰¹å¾´é‡ã‚¹ãƒˆã‚¢ãŒã€å†…éƒ¨ã§DuckDBã‚’ã©ã®ã‚ˆã†ã«ä½¿ã†ã‚ˆã†ã«ãªã£ãŸã‹ã€ã¨ã„ã†è©±ãªã®ã§ã€ã‚ã‚“ã¾ã‚Šèª­ã¾ãªãã¦è‰¯ã•ãã†...!:thinking: -->

# Faster reading from the Lakehouse to Python with DuckDB/ArrowFlight  
# ãƒ¬ã‚¤ã‚¯ãƒã‚¦ã‚¹ã‹ã‚‰Pythonã¸ã®é«˜é€Ÿãªèª­ã¿å–ã‚Šï¼šDuckDB/ArrowFlightã‚’ä½¿ç”¨ã—ã¦

## TL;DR
In this article, we outline how we leveraged ArrowFlight with DuckDB to build a new service that massively improves the performance of Python clients reading from lakehouse data in the Hopsworks Feature Store. 
ã“ã®è¨˜äº‹ã§ã¯ã€DuckDBã¨ArrowFlightã‚’æ´»ç”¨ã—ã¦ã€Hopsworks Feature Storeã®ãƒ¬ã‚¤ã‚¯ãƒã‚¦ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚‹Pythonã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹æ–°ã—ã„ã‚µãƒ¼ãƒ“ã‚¹ã‚’æ§‹ç¯‰ã—ãŸæ–¹æ³•ã‚’æ¦‚èª¬ã—ã¾ã™ã€‚
We present benchmarks and results comparing Hopsworks to cloud provider feature stores, showing from 8X to 45X higher throughput. 
Hopsworksã¨ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢ã‚’æ¯”è¼ƒã—ãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨çµæœã‚’ç¤ºã—ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒ8å€ã‹ã‚‰45å€é«˜ã„ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
Our analysis suggests the performance improvements are due to (1) using Arrow end-to-end (for both data transport and query processing in DuckDB, removing expensive serialization/deserialization), and (2) replacing ODBC/JDBC with ArrowFlight, removing expensive row-wise to column-wise transformations.
ç§ãŸã¡ã®åˆ†æã¯ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å‘ä¸ŠãŒï¼ˆ1ï¼‰Arrowã‚’ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ï¼ˆDuckDBã«ãŠã‘ã‚‹ãƒ‡ãƒ¼ã‚¿è¼¸é€ã¨ã‚¯ã‚¨ãƒªå‡¦ç†ã®ä¸¡æ–¹ã«ãŠã„ã¦ã€é«˜ä¾¡ãªã‚·ãƒªã‚¢ãƒ«åŒ–/ãƒ‡ã‚·ãƒªã‚¢ãƒ«åŒ–ã‚’æ’é™¤ã™ã‚‹ï¼‰ãŠã‚ˆã³ï¼ˆ2ï¼‰ODBC/JDBCã‚’ArrowFlightã«ç½®ãæ›ãˆã‚‹ã“ã¨ï¼ˆé«˜ä¾¡ãªè¡Œå˜ä½ã‹ã‚‰åˆ—å˜ä½ã¸ã®å¤‰æ›ã‚’æ’é™¤ã™ã‚‹ï¼‰ã«ã‚ˆã‚‹ã‚‚ã®ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚

## Introduction ã¯ã˜ã‚ã«

Feature stores provide APIs for retrieving consistent snapshots of feature data for both model training and model inference. 
**ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã®ãŸã‚ã«ã€ä¸€è²«ã—ãŸãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®API**ã‚’æä¾›ã—ã¾ã™ã€‚
(ã€Œä¸€è²«ã—ãŸã€ã¨ã€Œç‰¹å¾´é‡ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—ã€ã¨ã„ã†ã®ãŒãƒã‚¤ãƒ³ãƒˆã£ã½ã„...!:thinking:)
Feature stores are typically implemented as dual datastores, with a row-oriented database providing low latency access to the latest values of feature data for online models via an Online API, and a column-oriented data warehouse or lakehouse providing access to historical and new feature data for training and batch inference, respectively, via an Offline API. 
**ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢ã¯é€šå¸¸ã€äºŒé‡ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢ã¨ã—ã¦å®Ÿè£…**ã•ã‚Œã¦ãŠã‚Šã€è¡ŒæŒ‡å‘ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®æœ€æ–°ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å€¤ã«ä½é…å»¶ã§ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã®**Online API**ã‚’ä»‹ã—ã¦æä¾›ã—ã€åˆ—æŒ‡å‘ãƒ‡ãƒ¼ã‚¿ã‚¦ã‚§ã‚¢ãƒã‚¦ã‚¹ã¾ãŸã¯ãƒ¬ã‚¤ã‚¯ãƒã‚¦ã‚¹ãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³ãƒãƒƒãƒæ¨è«–ã®ãŸã‚ã«æ­´å²çš„ãŠã‚ˆã³æ–°ã—ã„ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‡ãƒ¼ã‚¿ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã®**Offline API**ã‚’ä»‹ã—ã¦æä¾›ã—ã¾ã™ã€‚
These two databases are typically called the Online Store and the Offline Store, respectively. 
ã“ã‚Œã‚‰ã®**äºŒã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯é€šå¸¸ã€ãã‚Œãã‚ŒOnline Storeã¨Offline Storeã¨å‘¼ã°ã‚Œã¾ã™**ã€‚

![]()
Figure 1: Arrow is now used end-to-end from the Lakehouse to Python clients that use an Offline API to read feature data for training models and for making batch predictions (inference)
å›³1ï¼šArrowã¯ã€ãƒ¬ã‚¤ã‚¯ãƒã‚¦ã‚¹ã‹ã‚‰Pythonã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¾ã§ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§ä½¿ç”¨ã•ã‚Œã¦ãŠã‚Šã€Offline APIã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„ãƒãƒƒãƒäºˆæ¸¬ï¼ˆæ¨è«–ï¼‰ã®ãŸã‚ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚Šã¾ã™ã€‚

Similar to other feature stores, Hopsworks has used Spark to implement the Offline API. 
ä»–ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢ã¨åŒæ§˜ã«ã€Hopsworksã¯Offline APIã‚’å®Ÿè£…ã™ã‚‹ãŸã‚ã«Sparkã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
This is great for large datasets, but for small or moderately sized datasets (think of the size of data that would fit in a Pandas DataFrame in your local Python environment), the overhead of a Spark job and transferring the data to Python clients can be significant. 
ã“ã‚Œã¯å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯é©ã—ã¦ã„ã¾ã™ãŒã€å°è¦æ¨¡ã¾ãŸã¯ä¸­è¦æ¨¡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã®Pythonç’°å¢ƒã§Pandas DataFrameã«åã¾ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã‚’è€ƒãˆã¦ã¿ã¦ãã ã•ã„ï¼‰ã«å¯¾ã—ã¦ã¯ã€Sparkã‚¸ãƒ§ãƒ–ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚„ãƒ‡ãƒ¼ã‚¿ã‚’Pythonã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«è»¢é€ã™ã‚‹éš›ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒå¤§ãããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
In this article, we introduce a new service in Hopsworks, ArrowFlight with DuckDB, that gives massive performance improvements for Python clients when reading data from the lakehouse via the Offline API. 
ã“ã®è¨˜äº‹ã§ã¯ã€Hopsworksã«ãŠã‘ã‚‹æ–°ã—ã„ã‚µãƒ¼ãƒ“ã‚¹ã€ArrowFlight with DuckDBã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€Offline APIã‚’ä»‹ã—ã¦ãƒ¬ã‚¤ã‚¯ãƒã‚¦ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚‹éš›ã«ã€Pythonã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«å¤§å¹…ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚
This improves the iteration speed of Python developers working with a feature store, opening up feature stores as a general purpose data platform usable by the wider Python community.  
ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢ã§ä½œæ¥­ã™ã‚‹Pythoné–‹ç™ºè€…ã®åå¾©é€Ÿåº¦ãŒå‘ä¸Šã—ã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢ãŒã‚ˆã‚Šåºƒç¯„ãªPythonã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«ã‚ˆã£ã¦ä½¿ç”¨å¯èƒ½ãªæ±ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã¨ã—ã¦é–‹ã‹ã‚Œã¾ã™ã€‚

![]()

In tables 1,2, you can see a summary of the speedup we observed when using DuckDB/ArrowFlight for moderately sized data, compared to existing feature stores. 
è¡¨1ãŠã‚ˆã³2ã§ã¯ã€æ—¢å­˜ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢ã¨æ¯”è¼ƒã—ã¦ã€ä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦DuckDB/ArrowFlightã‚’ä½¿ç”¨ã—ãŸéš›ã«è¦³å¯Ÿã•ã‚ŒãŸã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã®æ¦‚è¦ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
The details are in the benchmark section below. 
è©³ç´°ã¯ä»¥ä¸‹ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã‚ã‚Šã¾ã™ã€‚

## ArrowFlight with DuckDB Service

Firstly, letâ€™s introduce the technologies. 
ã¾ãšã€æŠ€è¡“ã‚’ç´¹ä»‹ã—ã¾ã—ã‚‡ã†ã€‚
DuckDB is a lightweight and efficient database management system that has a columnar-vectorized query execution engine which is optimized for analytical workloads. 
DuckDBã¯ã€åˆ†æãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«æœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ©ãƒ æŒ‡å‘ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³ã‚’æŒã¤è»½é‡ã§åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚
It comes with a zero-copy integration with Pandas and Pyarrow and lots of flexibility in terms of input formats and storage connectors. 
PandasãŠã‚ˆã³Pyarrowã¨ã®ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼çµ±åˆã‚’æä¾›ã—ã€å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚„ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚³ãƒã‚¯ã‚¿ã«é–¢ã—ã¦å¤šãã®æŸ”è»Ÿæ€§ã‚’æŒã£ã¦ã„ã¾ã™ã€‚
Our service uses DuckDB to read from tables on object storage and perform point-in-time correct Joins over tables. 
ç§ãŸã¡ã®ã‚µãƒ¼ãƒ“ã‚¹ã¯ã€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä¸Šã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰DuckDBã‚’ä½¿ç”¨ã—ã¦èª­ã¿å–ã‚Šã€ãƒ†ãƒ¼ãƒ–ãƒ«é–“ã§æ™‚ç‚¹æ­£ç¢ºãªçµåˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
However, as we built a network service, we needed to enable data to flow securely from DuckDB to Pandas clients without unnecessary serialization/deserialization (thanks Arrow!) or conversion of data from column-oriented to row-oriented and back again (bye bye JDBC!). 
ã—ã‹ã—ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚µãƒ¼ãƒ“ã‚¹ã‚’æ§‹ç¯‰ã™ã‚‹ã«ã‚ãŸã‚Šã€DuckDBã‹ã‚‰Pandasã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¸ã®ãƒ‡ãƒ¼ã‚¿ã®æµã‚Œã‚’ã€ä¸è¦ãªã‚·ãƒªã‚¢ãƒ«åŒ–/ãƒ‡ã‚·ãƒªã‚¢ãƒ«åŒ–ï¼ˆã‚ã‚ŠãŒã¨ã†Arrowï¼ï¼‰ã‚„ã‚«ãƒ©ãƒ æŒ‡å‘ã‹ã‚‰è¡ŒæŒ‡å‘ã¸ã®ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚’è¡Œã‚ãšã«å®‰å…¨ã«å®Ÿç¾ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã—ãŸï¼ˆã•ã‚ˆã†ãªã‚‰JDBCï¼ï¼‰ã€‚
For this, we used ArrowFlight, a high-performance client-server framework for building applications that transfer arrow data over the network. 
ã“ã‚Œã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«ã€ç§ãŸã¡ã¯ArrowFlightã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚ArrowFlightã¯ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸Šã§Arrowãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ã™ã‚‹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®é«˜æ€§èƒ½ãªã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ-ã‚µãƒ¼ãƒãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚
Independent benchmarks show that ArrowFlight outperforms existing database network protocols such as ODBC by up to 30x for arrow data. 
ç‹¬ç«‹ã—ãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ã€ArrowFlightãŒODBCãªã©ã®æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«å¯¾ã—ã¦ã€Arrowãƒ‡ãƒ¼ã‚¿ã«ãŠã„ã¦æœ€å¤§30å€ã®æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

Figure 1 shows how ArrowFlight, DuckDB, and Pandas come together in our new Hopsworks service (here with a Hudi tables as the offline store). 
å›³1ã¯ã€ArrowFlightã€DuckDBã€ãŠã‚ˆã³PandasãŒç§ãŸã¡ã®æ–°ã—ã„Hopsworksã‚µãƒ¼ãƒ“ã‚¹ï¼ˆã“ã“ã§ã¯ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã¨ã—ã¦Hudiãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ç”¨ï¼‰ã§ã©ã®ã‚ˆã†ã«çµ±åˆã•ã‚Œã‚‹ã‹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
By leveraging Apache Arrow we have zero-copy data transfer on server- as well as on client-side, while being able to use ArrowFlight as a high-performance data transfer protocol. 
Apache Arrowã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€ã‚µãƒ¼ãƒãƒ¼å´ã¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ã®ä¸¡æ–¹ã§ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ãƒ‡ãƒ¼ã‚¿è»¢é€ã‚’å®Ÿç¾ã—ã€ArrowFlightã‚’é«˜æ€§èƒ½ãªãƒ‡ãƒ¼ã‚¿è»¢é€ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
We secure data transfers using mutual TLS for client authentication and encryption in-flight and we build on Hopsworksâ€™ existing authorization framework for access control. 
ç§ãŸã¡ã¯ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆèªè¨¼ã¨ãƒ‡ãƒ¼ã‚¿è»¢é€ä¸­ã®æš—å·åŒ–ã®ãŸã‚ã«ç›¸äº’TLSã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿è»¢é€ã‚’ä¿è­·ã—ã€ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡ã®ãŸã‚ã«Hopsworksã®æ—¢å­˜ã®èªå¯ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’åŸºã«æ§‹ç¯‰ã—ã¦ã„ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## High Performance Python-native Access to Lakehouse Data é«˜æ€§èƒ½ãªPythonãƒã‚¤ãƒ†ã‚£ãƒ–ã®Lakehouseãƒ‡ãƒ¼ã‚¿ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹

Apache Hudi is a parquet-based Lakehouse format that is the default offline store in Hopsworks. 
Apache Hudiã¯ã€Hopsworksã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã§ã‚ã‚‹parquetãƒ™ãƒ¼ã‚¹ã®Lakehouseãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã™ã€‚
Whenever a feature store client reads Pandas DataFrames containing either training data or batch inference data, the data is read from Hudi tables (feature groups) and ends up in a Pandas DataFrame, which is then used as model input for training or batch inference. 
ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯ãƒãƒƒãƒæ¨è«–ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€Pandas DataFrameã‚’èª­ã¿è¾¼ã‚€ã¨ã€ãƒ‡ãƒ¼ã‚¿ã¯Hudiãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰ã‹ã‚‰èª­ã¿è¾¼ã¾ã‚Œã€æœ€çµ‚çš„ã«Pandas DataFrameã«æ ¼ç´ã•ã‚Œã€ã“ã‚ŒãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¾ãŸã¯ãƒãƒƒãƒæ¨è«–ã®ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

With ArrowFlight and DuckDB we provide a fast Python-native API to the Hudi tables as well as high performance point-in-time correct Joins for reading consistent snapshots of feature data that span many tables. 
ArrowFlightã¨DuckDBã‚’ä½¿ç”¨ã—ã¦ã€Hudiãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®é«˜é€ŸãªPythonãƒã‚¤ãƒ†ã‚£ãƒ–APIã‚’æä¾›ã™ã‚‹ã¨ã¨ã‚‚ã«ã€å¤šãã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¾ãŸãŒã‚‹ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ä¸€è²«ã—ãŸã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’èª­ã¿å–ã‚‹ãŸã‚ã®é«˜æ€§èƒ½ãªæ™‚ç‚¹æ­£ç¢ºãªçµåˆã‚’æä¾›ã—ã¾ã™ã€‚
While Hudi is Parquet-based and DuckDB features a very efficient Parquet reader, DuckDB does not support reading Hudi Tables directly. 
Hudiã¯Parquetãƒ™ãƒ¼ã‚¹ã§ã‚ã‚Šã€DuckDBã¯éå¸¸ã«åŠ¹ç‡çš„ãªParquetãƒªãƒ¼ãƒ€ãƒ¼ã‚’å‚™ãˆã¦ã„ã¾ã™ãŒã€DuckDBã¯Hudiãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç›´æ¥èª­ã¿å–ã‚‹ã“ã¨ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚
We therefore developed a custom reader for Hudi Copy-on-Write tables. 
ã—ãŸãŒã£ã¦ã€Hudi Copy-on-Writeãƒ†ãƒ¼ãƒ–ãƒ«ç”¨ã®ã‚«ã‚¹ã‚¿ãƒ ãƒªãƒ¼ãƒ€ãƒ¼ã‚’é–‹ç™ºã—ã¾ã—ãŸã€‚
For all supported offline stores, such as Snowflake or BigQuery, we will provide a different Arrow reader for DuckDB. 
Snowflakeã‚„BigQueryãªã©ã€ã™ã¹ã¦ã®ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã«å¯¾ã—ã¦ã€DuckDBç”¨ã®ç•°ãªã‚‹Arrowãƒªãƒ¼ãƒ€ãƒ¼ã‚’æä¾›ã—ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->


## Hudi Reader for DuckDB in Detail Hudi Reader for DuckDBã®è©³ç´°

Apache Hudiâ€™s storage format is based on parquet files and additional metadata that are stored on HopsFS. 
Apache Hudiã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯ã€parquetãƒ•ã‚¡ã‚¤ãƒ«ã¨HopsFSã«ä¿å­˜ã•ã‚Œã‚‹è¿½åŠ ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚

One Feature Group is typically not stored as a single parquet file, but the data is broken down into individual files by partitions (based on the Feature Groupâ€™s partition key), and then, depending on the file size, further divided into smaller slices within each partition. 
1ã¤ã®Feature Groupã¯é€šå¸¸ã€å˜ä¸€ã®parquetãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã•ã‚Œã‚‹ã®ã§ã¯ãªãã€ãƒ‡ãƒ¼ã‚¿ã¯ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ï¼ˆFeature Groupã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚­ãƒ¼ã«åŸºã¥ãï¼‰ã«ã‚ˆã£ã¦å€‹ã€…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã•ã‚Œã€ãã®å¾Œã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ã¦å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å†…ã§ã•ã‚‰ã«å°ã•ãªã‚¹ãƒ©ã‚¤ã‚¹ã«åˆ†å‰²ã•ã‚Œã¾ã™ã€‚

Each slice has a unique name and is associated with a particular commit-timestamp. 
å„ã‚¹ãƒ©ã‚¤ã‚¹ã«ã¯ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªåå‰ãŒä»˜ã‘ã‚‰ã‚Œã€ç‰¹å®šã®ã‚³ãƒŸãƒƒãƒˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«é–¢é€£ä»˜ã‘ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

When a new commit is made (`fg.insert(df)`), Hudi will quickly identify the latest version of all slices that are affected by the insert/upsert using a bloom filter index and then create updated copies of those slices. 
æ–°ã—ã„ã‚³ãƒŸãƒƒãƒˆãŒè¡Œã‚ã‚Œã‚‹ã¨ï¼ˆ`fg.insert(df)`ï¼‰ã€Hudiã¯bloomãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ã¦æŒ¿å…¥/ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆã«ã‚ˆã£ã¦å½±éŸ¿ã‚’å—ã‘ã‚‹ã™ã¹ã¦ã®ã‚¹ãƒ©ã‚¤ã‚¹ã®æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’è¿…é€Ÿã«ç‰¹å®šã—ã€ãã‚Œã‚‰ã®ã‚¹ãƒ©ã‚¤ã‚¹ã®æ›´æ–°ã•ã‚ŒãŸã‚³ãƒ”ãƒ¼ã‚’ä½œæˆã—ã¾ã™ã€‚

Hudi will also create a new .commit metadata file, which contains the list of files that have been updated by the commit. 
Hudiã¯ã¾ãŸã€ã‚³ãƒŸãƒƒãƒˆã«ã‚ˆã£ã¦æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å«ã‚€æ–°ã—ã„.commitãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚

That file will be retained for up to 30 commits. 
ãã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯æœ€å¤§30å›ã®ã‚³ãƒŸãƒƒãƒˆã¾ã§ä¿æŒã•ã‚Œã¾ã™ã€‚

To read the latest snapshot view of a Hudi Table (most up-to-date version of the data), the latest versions of all slices have to be identified and all slices have to be read and unioned into a single Table. 
Hudiãƒ†ãƒ¼ãƒ–ãƒ«ã®æœ€æ–°ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ“ãƒ¥ãƒ¼ï¼ˆãƒ‡ãƒ¼ã‚¿ã®æœ€ã‚‚æœ€æ–°ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰ã‚’èª­ã¿å–ã‚‹ã«ã¯ã€ã™ã¹ã¦ã®ã‚¹ãƒ©ã‚¤ã‚¹ã®æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç‰¹å®šã—ã€ã™ã¹ã¦ã®ã‚¹ãƒ©ã‚¤ã‚¹ã‚’èª­ã¿å–ã£ã¦1ã¤ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«çµ±åˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

Figure 2 shows how Hudi data is organized in HopsFS. 
å›³2ã¯ã€Hudiãƒ‡ãƒ¼ã‚¿ãŒHopsFSå†…ã§ã©ã®ã‚ˆã†ã«æ•´ç†ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

Files in Hopsworks are stored in cloud native object storage (S3 on AWS, GCS on GCP, Blob Storage on Azure) but accessed via a fast cache with a HDFS API, called HopsFS. 
Hopsworkså†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ã‚¯ãƒ©ã‚¦ãƒ‰ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆAWSã®S3ã€GCPã®GCSã€Azureã®Blob Storageï¼‰ã«ä¿å­˜ã•ã‚Œã¾ã™ãŒã€HopsFSã¨å‘¼ã°ã‚Œã‚‹HDFS APIã‚’ä»‹ã—ã¦é«˜é€Ÿã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚¢ã‚¯ã‚»ã‚¹ã•ã‚Œã¾ã™ã€‚

To read a Hudi Table/Feature Group with DuckDB, we make a recursive file listing in HopsFS to retrieve all parquet-files that belong to a certain Hudi Table. 
DuckDBã§Hudiãƒ†ãƒ¼ãƒ–ãƒ«/Feature Groupã‚’èª­ã¿å–ã‚‹ãŸã‚ã«ã€HopsFSå†…ã§å†å¸°çš„ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã€ç‰¹å®šã®Hudiãƒ†ãƒ¼ãƒ–ãƒ«ã«å±ã™ã‚‹ã™ã¹ã¦ã®parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã—ã¾ã™ã€‚

Based on the listing, we identify the latest commit timestamp (which is part of the filename) for each parquet file slice. 
ãƒªã‚¹ãƒˆã«åŸºã¥ã„ã¦ã€å„parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ãƒ©ã‚¤ã‚¹ã®æœ€æ–°ã®ã‚³ãƒŸãƒƒãƒˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã®ä¸€éƒ¨ï¼‰ã‚’ç‰¹å®šã—ã¾ã™ã€‚

Slices that are currently in-flight, meaning they belong to a commit that is currently in-progress, are discarded. 
ç¾åœ¨é€²è¡Œä¸­ã®ã‚³ãƒŸãƒƒãƒˆã«å±ã™ã‚‹ã‚¹ãƒ©ã‚¤ã‚¹ï¼ˆç¾åœ¨é€²è¡Œä¸­ã®ã‚‚ã®ï¼‰ã¯ç ´æ£„ã•ã‚Œã¾ã™ã€‚

The correct list of latest parquet files is then passed to DuckDB where we register a unionized temporary table based on all the latest files. 
æœ€æ–°ã®parquetãƒ•ã‚¡ã‚¤ãƒ«ã®æ­£ã—ã„ãƒªã‚¹ãƒˆã¯DuckDBã«æ¸¡ã•ã‚Œã€ãã“ã§æœ€æ–°ã®ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«åŸºã¥ã„ã¦çµ±åˆã•ã‚ŒãŸä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ãŒç™»éŒ²ã•ã‚Œã¾ã™ã€‚

This table represents the exact same data we would get with a Hudi Snapshot read via Spark. 
ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã¯ã€Sparkã‚’ä»‹ã—ã¦Hudiã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’èª­ã¿å–ã£ãŸå ´åˆã¨æ­£ç¢ºã«åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ã—ã¾ã™ã€‚

If we execute a query (e.g. for creating a training dataset), this process is repeated for all Feature Groups that are required by the query. 
ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã™ã‚‹å ´åˆï¼ˆä¾‹ï¼šãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆï¼‰ã€ã“ã®ãƒ—ãƒ­ã‚»ã‚¹ã¯ã‚¯ã‚¨ãƒªã§å¿…è¦ãªã™ã¹ã¦ã®Feature Groupã«å¯¾ã—ã¦ç¹°ã‚Šè¿”ã•ã‚Œã¾ã™ã€‚

After the table registration, the input query will simply be executed by the DuckDB SQL engine on the previously registered tables. 
ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç™»éŒ²å¾Œã€å…¥åŠ›ã‚¯ã‚¨ãƒªã¯ä»¥å‰ã«ç™»éŒ²ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«ä¸Šã§DuckDB SQLã‚¨ãƒ³ã‚¸ãƒ³ã«ã‚ˆã£ã¦å˜ç´”ã«å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

Incremental file listing updates After the initial full file listing is established, we can keep the listing in-memory and update it incrementally whenever a new commit is made, using information from the Hudi .commit file. 
åˆæœŸã®å®Œå…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆãŒç¢ºç«‹ã•ã‚ŒãŸå¾Œã€ãƒªã‚¹ãƒˆã‚’ãƒ¡ãƒ¢ãƒªå†…ã«ä¿æŒã—ã€æ–°ã—ã„ã‚³ãƒŸãƒƒãƒˆãŒè¡Œã‚ã‚Œã‚‹ãŸã³ã«Hudiã®.commitãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«ã«æ›´æ–°ã§ãã¾ã™ã€‚

While file listings in HopsFS are very efficient, this further improves read performance on Feature Groups with deeply nested partition keys and/or a high number of commits. 
HopsFSå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã¯éå¸¸ã«åŠ¹ç‡çš„ã§ã™ãŒã€ã“ã‚Œã«ã‚ˆã‚Šã€æ·±ããƒã‚¹ãƒˆã•ã‚ŒãŸãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚­ãƒ¼ã‚„å¤šæ•°ã®ã‚³ãƒŸãƒƒãƒˆã‚’æŒã¤Feature Groupã®èª­ã¿å–ã‚Šãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒã•ã‚‰ã«å‘ä¸Šã—ã¾ã™ã€‚

Filter Pushdown When Hudi Feature Groups are partitioned and you have defined a filter on the partition column in your Query, we leverage that to prune out paths from the full file listing that do not match the filter condition, before registering them in DuckDB. 
ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ Hudi Feature GroupãŒãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åŒ–ã•ã‚Œã€ã‚¯ã‚¨ãƒªã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ—ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’å®šç¾©ã—ã¦ã„ã‚‹å ´åˆã€ç§ãŸã¡ã¯ãã‚Œã‚’åˆ©ç”¨ã—ã¦ã€DuckDBã«ç™»éŒ²ã™ã‚‹å‰ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã«ä¸€è‡´ã—ãªã„ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‹ã‚‰ãƒ‘ã‚¹ã‚’å‰Šé™¤ã—ã¾ã™ã€‚

This can significantly reduce the memory requirements and query runtime for DuckDB. 
ã“ã‚Œã«ã‚ˆã‚Šã€DuckDBã®ãƒ¡ãƒ¢ãƒªè¦ä»¶ã¨ã‚¯ã‚¨ãƒªå®Ÿè¡Œæ™‚é–“ã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã¾ã™ã€‚

DuckDBâ€™s HopsFS access Since data is stored on HopsFS, DuckDB needs to be able to read files directly from HopsFS. 
DuckDBã®HopsFSã‚¢ã‚¯ã‚»ã‚¹ ãƒ‡ãƒ¼ã‚¿ãŒHopsFSã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€DuckDBã¯HopsFSã‹ã‚‰ç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

For this we leverage the extensibility of DuckDBâ€™s filesystem layer via fsspec. 
ã“ã‚Œã«ã¯ã€fsspecã‚’ä»‹ã—ã¦DuckDBã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ å±¤ã®æ‹¡å¼µæ€§ã‚’æ´»ç”¨ã—ã¾ã™ã€‚

Thanks to fsspecâ€™s Hadoop File System Implementation, and HopsFSâ€™s compatibility with HDFS, we can achieve that with minimal friction. 
fsspecã®Hadoopãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ã¨HopsFSã®HDFSã¨ã®äº’æ›æ€§ã®ãŠã‹ã’ã§ã€æœ€å°é™ã®æ‘©æ“¦ã§ãã‚Œã‚’å®Ÿç¾ã§ãã¾ã™ã€‚



## Read Training Datasets at Higher Throughput é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿

With ArrowFlight and DuckDB, Python clients can read feature groups, batch inference data, as well as create in-memory training datasets at higher throughput than with Spark. 
ArrowFlightã¨DuckDBã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€Pythonã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¯ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚„ãƒãƒƒãƒæ¨è«–ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€Sparkã‚ˆã‚Šã‚‚é«˜ã„ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã§ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã§ãã¾ã™ã€‚
Furthermore, users can also read materialized training datasets in Hopsworks at improved throughput compared to the previous REST API solution.
ã•ã‚‰ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ä»¥å‰ã®REST APIã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã¨æ¯”è¼ƒã—ã¦ã€Hopsworksã§ãƒãƒ†ãƒªã‚¢ãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ”¹å–„ã•ã‚ŒãŸã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã§èª­ã¿è¾¼ã‚€ã“ã¨ã‚‚ã§ãã¾ã™ã€‚



## Client Integration ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆçµ±åˆ

There are no API changes needed for Hopsworks clients to use ArrowFlight with DuckDB in the Python client, beyond upgrading to Hopsworks 3.3+. 
Hopsworksã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒPythonã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§DuckDBã¨ArrowFlightã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã«å¿…è¦ãªAPIã®å¤‰æ›´ã¯ã€Hopsworks 3.3+ã¸ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’é™¤ã„ã¦ã‚ã‚Šã¾ã›ã‚“ã€‚

All notebooks and Python scripts you built with Hopsworks can remain the same. 
Hopsworksã§ä½œæˆã—ãŸã™ã¹ã¦ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¨Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ãã®ã¾ã¾ä½¿ç”¨ã§ãã¾ã™ã€‚

Once a cluster is deployed with â€œArrowFlight Server'' (see installation guide), the following operations will automatically be performed by the new service: 
ã€ŒArrowFlight Serverã€ï¼ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¬ã‚¤ãƒ‰ã‚’å‚ç…§ï¼‰ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒå±•é–‹ã•ã‚Œã‚‹ã¨ã€æ¬¡ã®æ“ä½œãŒæ–°ã—ã„ã‚µãƒ¼ãƒ“ã‚¹ã«ã‚ˆã£ã¦è‡ªå‹•çš„ã«å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

- reading Feature Groups 
- Feature Groupsã®èª­ã¿è¾¼ã¿
- reading Queries 
- ã‚¯ã‚¨ãƒªã®èª­ã¿è¾¼ã¿
- reading Training Datasets 
- ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
- creating In-Memory Training Datasets 
- ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
- reading Batch Inference Data 
- ãƒãƒƒãƒæ¨è«–ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿

For larger datasets, clients can still make use of the Spark/Hive backend by explicitly setting read_options={"use_hive": True}. 
å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å ´åˆã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¯æ˜ç¤ºçš„ã«`read_options={"use_hive": True}`ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã€Spark/Hiveãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’å¼•ãç¶šãåˆ©ç”¨ã§ãã¾ã™ã€‚



## Offline Feature Store Benchmarks ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

We benchmarked the main four publicly available cloud-native feature stores using the NYC Taxi Dataset. 
ç§ãŸã¡ã¯ã€NYCã‚¿ã‚¯ã‚·ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ã€ä¸»ãª4ã¤ã®å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã‚¯ãƒ©ã‚¦ãƒ‰ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã—ã¾ã—ãŸã€‚ 
We omitted open-source feature stores, as they require manual configuration for their offline data lake/warehouse, or feature stores that are not generally accessible. 
ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢ã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚¤ã‚¯/ã‚¦ã‚§ã‚¢ãƒã‚¦ã‚¹ã®æ‰‹å‹•è¨­å®šãŒå¿…è¦ã§ã‚ã‚‹ãŸã‚ã€ã¾ãŸã¯ä¸€èˆ¬çš„ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢ã‚’çœç•¥ã—ã¾ã—ãŸã€‚ 
You can reproduce this benchmark using the code published on github. 
ã“ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ã€GitHubã«å…¬é–‹ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦å†ç¾ã§ãã¾ã™ã€‚

The â€œOffline APIâ€ to a feature store is a batch API for reading point-in-time consistent feature data. 
ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢ã¸ã®ã€Œã‚ªãƒ•ãƒ©ã‚¤ãƒ³APIã€ã¯ã€æ™‚ç‚¹æ•´åˆæ€§ã®ã‚ã‚‹ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚‹ãŸã‚ã®ãƒãƒƒãƒAPIã§ã™ã€‚ 
The Offline API is used by both training pipelines (you read feature data and output a trained model) and batch inference pipelines (you read a batch of new feature data and the model, and then output the modelâ€™s predictions on the batch of new feature data). 
ã‚ªãƒ•ãƒ©ã‚¤ãƒ³APIã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚Šã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’å‡ºåŠ›ã™ã‚‹ï¼‰ã¨ãƒãƒƒãƒæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆæ–°ã—ã„ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒã¨ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿å–ã‚Šã€ãã®ãƒãƒƒãƒã®æ–°ã—ã„ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’å‡ºåŠ›ã™ã‚‹ï¼‰ã®ä¸¡æ–¹ã§ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

There are typically two versions of the Offline API: 
ã‚ªãƒ•ãƒ©ã‚¤ãƒ³APIã«ã¯é€šå¸¸ã€2ã¤ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã™ï¼š
1. read the feature data directly as a Pandas DataFrame or 
1. ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥Pandas DataFrameã¨ã—ã¦èª­ã¿å–ã‚‹ã“ã¨ã€ã¾ãŸã¯
2. create a batch of feature (and label) data as (parquet or csv) files that will be used by either a subsequent model training pipeline or batch inference pipeline. 
2. æ¬¡ã®ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¾ãŸã¯ãƒãƒƒãƒæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ä½¿ç”¨ã•ã‚Œã‚‹ï¼ˆparquetã¾ãŸã¯csvï¼‰ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ï¼ˆãŠã‚ˆã³ãƒ©ãƒ™ãƒ«ï¼‰ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒã‚’ä½œæˆã™ã‚‹ã“ã¨ã§ã™ã€‚ 
As such, we provide benchmarks for both (1) the Read an In-Memory Pandas DataFrame use case and the (2) create feature data (training data) as files. 
ãã®ãŸã‚ã€ï¼ˆ1ï¼‰ãƒ¡ãƒ¢ãƒªå†…Pandas DataFrameã®èª­ã¿å–ã‚Šãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã¨ï¼ˆ2ï¼‰ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’æä¾›ã—ã¾ã™ã€‚

In figure 4, we can see that Hopsworks has the lowest time required to read 5m, 10m, 20m, and 50m rows as a Pandas DataFrame. 
å›³4ã§ã¯ã€HopsworksãŒ5ç™¾ä¸‡ã€10ç™¾ä¸‡ã€20ç™¾ä¸‡ã€50ç™¾ä¸‡è¡Œã‚’Pandas DataFrameã¨ã—ã¦èª­ã¿å–ã‚‹ã®ã«æœ€ã‚‚å°‘ãªã„æ™‚é–“ã‚’è¦ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ 
The performance differences can be explained by a combination of 
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®é•ã„ã¯ã€ä»¥ä¸‹ã®çµ„ã¿åˆã‚ã›ã§èª¬æ˜ã§ãã¾ã™ï¼š
1. no serialization/deserialization in Hopsworks due to use of Arrow end-to-end, 
1. Arrowã‚’ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã«ã‚ˆã‚‹Hopsworksã§ã®ã‚·ãƒªã‚¢ãƒ«åŒ–/ãƒ‡ã‚·ãƒªã‚¢ãƒ«åŒ–ã®ä¸åœ¨ã€
2. no conversions from columnar-to-row-oriented or row-oriented-to-columnar (as happens when you use JDBC/ODBC), and 
2. åˆ—æŒ‡å‘ã‹ã‚‰è¡ŒæŒ‡å‘ã€ã¾ãŸã¯è¡ŒæŒ‡å‘ã‹ã‚‰åˆ—æŒ‡å‘ã¸ã®å¤‰æ›ãŒãªã„ã“ã¨ï¼ˆJDBC/ODBCã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã«ç™ºç”Ÿã™ã‚‹ã‚ˆã†ã«ï¼‰ã€ãŠã‚ˆã³
3. DuckDB is a higher performance point-in-time-join engine. 
3. DuckDBã¯é«˜æ€§èƒ½ãªæ™‚ç‚¹çµåˆã‚¨ãƒ³ã‚¸ãƒ³ã§ã™ã€‚

The other feature stores use distributed query engines, Spark/Photon (Databricks), Athena (Sagemaker), and BigQuery (Vertex). 
ä»–ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢ã¯ã€åˆ†æ•£ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã€Spark/Photonï¼ˆDatabricksï¼‰ã€Athenaï¼ˆSagemakerï¼‰ã€ãŠã‚ˆã³BigQueryï¼ˆVertexï¼‰ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ 
The Python APIs for these frameworks act as wrappers for ease-of-use but there is significant overhead when using these systems. 
ã“ã‚Œã‚‰ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®Python APIã¯ã€ä½¿ã„ã‚„ã™ã•ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ãŒã€ã“ã‚Œã‚‰ã®ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã™ã‚‹éš›ã«ã¯å¤§ããªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã™ã€‚ 
While Spark, Athena, and BigQuery do support Arrow as a data interchange format, it is not enough in these cases to remove the additional overhead that comes with a distributed and serverless engine. 
Sparkã€Athenaã€ãŠã‚ˆã³BigQueryã¯Arrowã‚’ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ã‚¿ãƒ¼ãƒã‚§ãƒ³ã‚¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¨ã—ã¦ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ãŒã€ã“ã‚Œã‚‰ã®ã‚±ãƒ¼ã‚¹ã§ã¯åˆ†æ•£å‹ãŠã‚ˆã³ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ã‚¨ãƒ³ã‚¸ãƒ³ã«ä¼´ã†è¿½åŠ ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å–ã‚Šé™¤ãã«ã¯ä¸ååˆ†ã§ã™ã€‚ 
Spark is designed to handle large-scale data processing across multiple nodes in a cluster. 
Sparkã¯ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…ã®è¤‡æ•°ã®ãƒãƒ¼ãƒ‰ã§å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’å‡¦ç†ã™ã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚ 
While this architecture allows Spark to scale horizontally and process massive datasets, it introduces additional overhead in data distribution and communication between nodes. 
ã“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚Šã€Sparkã¯æ°´å¹³ã«ã‚¹ã‚±ãƒ¼ãƒ«ã—ã€å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‡¦ç†ã§ãã¾ã™ãŒã€ãƒ‡ãƒ¼ã‚¿ã®åˆ†é…ã¨ãƒãƒ¼ãƒ‰é–“ã®é€šä¿¡ã«è¿½åŠ ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒç™ºç”Ÿã—ã¾ã™ã€‚ 
Often, Spark can require data to be repartitioned across workers to enable parallel processing. 
ã—ã°ã—ã°ã€Sparkã¯ä¸¦åˆ—å‡¦ç†ã‚’å¯èƒ½ã«ã™ã‚‹ãŸã‚ã«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¯ãƒ¼ã‚«ãƒ¼é–“ã§å†åˆ†å‰²ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ 
This additional step called shuffling can introduce significant latency in the process affecting performance. 
ã“ã®è¿½åŠ ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã‚ã‚‹ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã¯ã€ãƒ—ãƒ­ã‚»ã‚¹ã«ãŠã„ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹é‡å¤§ãªãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ 
Once Spark is finished doing transformations, there is an additional wait for it to collect all the data from each of its executors and then convert that data to a Pandas DataFrame. 
SparkãŒå¤‰æ›ã‚’å®Œäº†ã™ã‚‹ã¨ã€å„ã‚¨ã‚°ã‚¼ã‚­ãƒ¥ãƒ¼ã‚¿ã‹ã‚‰ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã€ãã®ãƒ‡ãƒ¼ã‚¿ã‚’Pandas DataFrameã«å¤‰æ›ã™ã‚‹ã¾ã§ã®è¿½åŠ ã®å¾…æ©Ÿæ™‚é–“ãŒã‚ã‚Šã¾ã™ã€‚ 
With Databricks Photon integration with Spark we see also that Photon is a library loaded into the JVM from where it communicates with Spark via the Java Native Interface (JNI), passing data pointers to off-heap memory. 
Databricks PhotonãŒSparkã¨çµ±åˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã§ã€Photonã¯JVMã«ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã‚ã‚Šã€Java Native Interfaceï¼ˆJNIï¼‰ã‚’ä»‹ã—ã¦Sparkã¨é€šä¿¡ã—ã€ã‚ªãƒ•ãƒ’ãƒ¼ãƒ—ãƒ¡ãƒ¢ãƒªã«ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ã‚¿ã‚’æ¸¡ã™ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ 
Despite Photon itself being written in C++ as a native, vectorized engine, it passes data back through the Spark pathway before it reaches the client. 
Photonè‡ªä½“ã¯C++ã§ãƒã‚¤ãƒ†ã‚£ãƒ–ãªãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã¨ã—ã¦æ›¸ã‹ã‚Œã¦ã„ã¾ã™ãŒã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«åˆ°é”ã™ã‚‹å‰ã«ãƒ‡ãƒ¼ã‚¿ã‚’Sparkã®çµŒè·¯ã‚’é€šã˜ã¦æ¸¡ã—ã¾ã™ã€‚ 
With BigQuery we see another issue when materializing data as a DataFrame. 
BigQueryã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã¨ã—ã¦ãƒãƒ†ãƒªã‚¢ãƒ©ã‚¤ã‚ºã™ã‚‹éš›ã«åˆ¥ã®å•é¡ŒãŒç™ºç”Ÿã—ã¾ã™ã€‚ 
BigQuery doesnâ€™t materialize directly to the client. 
BigQueryã¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ç›´æ¥ãƒãƒ†ãƒªã‚¢ãƒ©ã‚¤ã‚ºã—ã¾ã›ã‚“ã€‚ 
Instead, a temporary, intermediate table is first created which acts as the materialization source from which the DataFrame is then served and the table is deleted after use. 
ä»£ã‚ã‚Šã«ã€ä¸€æ™‚çš„ãªä¸­é–“ãƒ†ãƒ¼ãƒ–ãƒ«ãŒæœ€åˆã«ä½œæˆã•ã‚Œã€ã“ã‚ŒãŒãƒãƒ†ãƒªã‚¢ãƒ©ã‚¤ã‚ºã‚½ãƒ¼ã‚¹ã¨ã—ã¦æ©Ÿèƒ½ã—ã€ãã“ã‹ã‚‰DataFrameãŒæä¾›ã•ã‚Œã€ä½¿ç”¨å¾Œã«ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå‰Šé™¤ã•ã‚Œã¾ã™ã€‚ 
This introduces a latency almost equivalent to doing a file write for in-memory datasets. 
ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¡ãƒ¢ãƒªå†…ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ã‚’è¡Œã†ã®ã¨ã»ã¼åŒç­‰ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒå°å…¥ã•ã‚Œã¾ã™ã€‚ 
Sagemakerâ€™s Athena does somewhat better, but is doing row-oriented to/from columnar conversions. 
Sagemakerã®Athenaã¯ã‚„ã‚„è‰¯å¥½ã§ã™ãŒã€è¡ŒæŒ‡å‘ã‹ã‚‰åˆ—æŒ‡å‘ã¸ã®å¤‰æ›ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚

Given the improved performance in Hopsworks, we can see that there is a clear benefit when data processing engines are optimized for the storage format against which they are interacting. 
Hopsworksã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å‘ä¸Šã‚’è€ƒæ…®ã™ã‚‹ã¨ã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ãŒç›¸äº’ä½œç”¨ã™ã‚‹ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã‚‹å ´åˆã«æ˜ç¢ºãªåˆ©ç‚¹ãŒã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ 
DuckDB optimizes query execution with filter pushdowns and predicate scans and is partition-aware. 
DuckDBã¯ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ã¨è¿°èªã‚¹ã‚­ãƒ£ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚’æœ€é©åŒ–ã—ã€ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’èªè­˜ã—ã¦ã„ã¾ã™ã€‚ 
Additionally, DuckDB can operate directly on Arrow Tables and stream Arrow data back and forth which allows it to utilize Arrowâ€™s zerocopy data transfer mechanism for fast data transfer. 
ã•ã‚‰ã«ã€DuckDBã¯Arrowãƒ†ãƒ¼ãƒ–ãƒ«ä¸Šã§ç›´æ¥æ“ä½œã—ã€Arrowãƒ‡ãƒ¼ã‚¿ã‚’åŒæ–¹å‘ã«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ãã‚‹ãŸã‚ã€Arrowã®ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ãƒ‡ãƒ¼ã‚¿è»¢é€ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’åˆ©ç”¨ã—ã¦é«˜é€Ÿãƒ‡ãƒ¼ã‚¿è»¢é€ã‚’å®Ÿç¾ã—ã¾ã™ã€‚ 
With a format-aware and storage-optimized engine consistently using Arrow from lakehouse all the way to the client we can see data transfer happening at near network-line speeds. 
ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’èªè­˜ã—ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«æœ€é©åŒ–ã•ã‚ŒãŸã‚¨ãƒ³ã‚¸ãƒ³ãŒã€ãƒ¬ã‚¤ã‚¯ãƒã‚¦ã‚¹ã‹ã‚‰ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¾ã§ä¸€è²«ã—ã¦Arrowã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ‡ãƒ¼ã‚¿è»¢é€ãŒãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ³é€Ÿåº¦ã«è¿‘ã„é€Ÿåº¦ã§è¡Œã‚ã‚Œã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ 
Arrow Flight securely transfers data across all languages and frameworks without requiring any serialization/deserialization. 
Arrow Flightã¯ã€ã™ã¹ã¦ã®è¨€èªã¨ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é–“ã§ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«è»¢é€ã—ã€ã‚·ãƒªã‚¢ãƒ«åŒ–/ãƒ‡ã‚·ãƒªã‚¢ãƒ«åŒ–ã‚’å¿…è¦ã¨ã—ã¾ã›ã‚“ã€‚ 
Arrow Flight achieves this by operating directly on Arrow RecordBatch streams and does not require accessing data on a row level as opposed to JDBC/ODBC protocols. 
Arrow Flightã¯ã€Arrow RecordBatchã‚¹ãƒˆãƒªãƒ¼ãƒ ä¸Šã§ç›´æ¥æ“ä½œã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€ã“ã‚Œã‚’å®Ÿç¾ã—ã€JDBC/ODBCãƒ—ãƒ­ãƒˆã‚³ãƒ«ã¨ã¯ç•°ãªã‚Šã€è¡Œãƒ¬ãƒ™ãƒ«ã§ãƒ‡ãƒ¼ã‚¿ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ 
Our higher performance results are not surprising as VoltronData have shown how the ADBC protocol massively outperforms JDBC/ODBC for columnar datastores. 
ç§ãŸã¡ã®é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµæœã¯é©šãã¹ãã“ã¨ã§ã¯ãªãã€VoltronDataãŒADBCãƒ—ãƒ­ãƒˆã‚³ãƒ«ãŒåˆ—æŒ‡å‘ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢ã«å¯¾ã—ã¦JDBC/ODBCã‚’å¤§å¹…ã«ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

Similarly, when we create files containing feature data, for training data or for batch inference data, we can see similar performance gains using Hopsworks, compared to other feature stores. 
åŒæ§˜ã«ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚„ãƒãƒƒãƒæ¨è«–ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹éš›ã«ã‚‚ã€ä»–ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢ã¨æ¯”è¼ƒã—ã¦Hopsworksã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§åŒæ§˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸ŠãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚



## Performance Benefits for Python Developers in Hopsworks Pythoné–‹ç™ºè€…ã®ãŸã‚ã®Hopsworksã«ãŠã‘ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®åˆ©ç‚¹

To provide a more concrete example for Python developers, we compare the runtime of our Hopsworks Fraud Detection Batch Tutorial using ArrowFlight w. DuckDB against Spark (see Table 1). 
Pythoné–‹ç™ºè€…ã®ãŸã‚ã«ã€ã‚ˆã‚Šå…·ä½“çš„ãªä¾‹ã‚’æä¾›ã™ã‚‹ãŸã‚ã«ã€ç§ãŸã¡ã¯ArrowFlight w. DuckDBã‚’ä½¿ç”¨ã—ãŸHopsworks Fraud Detection Batch Tutorialã®å®Ÿè¡Œæ™‚é–“ã‚’Sparkã¨æ¯”è¼ƒã—ã¾ã™ï¼ˆè¡¨1ã‚’å‚ç…§ï¼‰ã€‚

The total runtime of all compute-intensive tasks goes down from 4.6 minutes to less than 16 seconds. 
ã™ã¹ã¦ã®è¨ˆç®—é›†ç´„å‹ã‚¿ã‚¹ã‚¯ã®åˆè¨ˆå®Ÿè¡Œæ™‚é–“ã¯ã€4.6åˆ†ã‹ã‚‰16ç§’æœªæº€ã«çŸ­ç¸®ã•ã‚Œã¾ã™ã€‚

Instead of waiting 10s for a training dataset to load, it now only takes 0.4s and feels almost instantaneous, which demonstrates the practical benefits that our new service brings to interactive Python client environments. 
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã«10ç§’å¾…ã¤ä»£ã‚ã‚Šã«ã€ä»Šã§ã¯ã‚ãšã‹0.4ç§’ã§æ¸ˆã¿ã€ã»ã¼ç¬æ™‚ã«æ„Ÿã˜ã‚‰ã‚Œã¾ã™ã€‚ã“ã‚Œã¯ã€ç§ãŸã¡ã®æ–°ã—ã„ã‚µãƒ¼ãƒ“ã‚¹ãŒã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªPythonã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç’°å¢ƒã«ã‚‚ãŸã‚‰ã™å®Ÿéš›ã®åˆ©ç‚¹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

This will significantly improve the iteration speed for Python developers working with feature data in Hopsworks. 
ã“ã‚Œã¯ã€Hopsworksã§ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†Pythoné–‹ç™ºè€…ã®åå¾©é€Ÿåº¦ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹ã§ã—ã‚‡ã†ã€‚

Note, we are big fans of Spark - we have worked hard on improving Point-in-Time Join performance in Spark, but for Python clients and moderately sized data, ArrowFlight and DuckDB is a better fit. 
ãªãŠã€ç§ãŸã¡ã¯Sparkã®å¤§ãƒ•ã‚¡ãƒ³ã§ã™ - Sparkã«ãŠã‘ã‚‹Point-in-Time Joinã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã«åŠªã‚ã¦ãã¾ã—ãŸãŒã€Pythonã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ä¸­ç¨‹åº¦ã®ã‚µã‚¤ã‚ºã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã¯ã€ArrowFlightã¨DuckDBã®æ–¹ãŒé©ã—ã¦ã„ã¾ã™ã€‚



## Support for Other Offline Stores ä»–ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã¸ã®ã‚µãƒãƒ¼ãƒˆ

We are soon going to add support for DuckDB reading from External Feature Groups (external tables in Snowflake, BigQuery, etc). 
ç§ãŸã¡ã¯ã€DuckDBãŒExternal Feature Groupsï¼ˆSnowflakeã€BigQueryãªã©ã®å¤–éƒ¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰ã‹ã‚‰èª­ã¿å–ã‚‹ã‚µãƒãƒ¼ãƒˆã‚’é–“ã‚‚ãªãè¿½åŠ ã™ã‚‹äºˆå®šã§ã™ã€‚
This will include the support of creating point-in-time correct training datasets across Hudi Feature Groups and External Feature Groups.
ã“ã‚Œã«ã¯ã€Hudi Feature Groupsã¨External Feature Groupså…¨ä½“ã§æ™‚ç‚¹ã«æ­£ã—ã„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹ã‚µãƒãƒ¼ãƒˆãŒå«ã¾ã‚Œã¾ã™ã€‚



## PyArrow-backed Pandas 2.0 DataFrames

Arrow already supports zero-copy conversion from Arrow tables to Pandas 1.x DataFrames for a limited subset of types (foremost int and float types). 
Arrowã¯ã€é™ã‚‰ã‚ŒãŸã‚¿ã‚¤ãƒ—ã®ã‚µãƒ–ã‚»ãƒƒãƒˆï¼ˆä¸»ã«intãŠã‚ˆã³floatã‚¿ã‚¤ãƒ—ï¼‰ã«å¯¾ã—ã¦ã€Arrowãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰Pandas 1.x DataFrameã¸ã®ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼å¤‰æ›ã‚’ã™ã§ã«ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚
Other types, such as strings, usually come with a minor runtime and memory overhead for the conversion (<10% end-to-end for an in-memory training dataset with 50M rows and 3 numeric, 2 string columns). 
æ–‡å­—åˆ—ãªã©ã®ä»–ã®ã‚¿ã‚¤ãƒ—ã¯ã€é€šå¸¸ã€å¤‰æ›ã«å¯¾ã—ã¦ã‚ãšã‹ãªãƒ©ãƒ³ã‚¿ã‚¤ãƒ ãŠã‚ˆã³ãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒä¼´ã„ã¾ã™ï¼ˆ50Mè¡Œã¨3ã¤ã®æ•°å€¤ã€2ã¤ã®æ–‡å­—åˆ—åˆ—ã‚’æŒã¤ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å ´åˆã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§10%æœªæº€ï¼‰ã€‚
With Pandas 2.0â€™s PyArrow-backed Pandas DataFrames this overhead can be fully alleviated. 
Pandas 2.0ã®PyArrowãƒãƒƒã‚¯ã®Pandas DataFrameã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ã“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯å®Œå…¨ã«è»½æ¸›ã•ã‚Œã¾ã™ã€‚
Since downstream libraries like scikit-learn do not fully support such types, yet, we will maintain support of regular Pandas types by default and offer PyArrow-backed DataFrames as an optional feature in the future. 
scikit-learnã®ã‚ˆã†ãªä¸‹æµãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã¾ã ãã®ã‚ˆã†ãªã‚¿ã‚¤ãƒ—ã‚’å®Œå…¨ã«ã¯ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ãŸã‚ã€ç§ãŸã¡ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§é€šå¸¸ã®Pandasã‚¿ã‚¤ãƒ—ã®ã‚µãƒãƒ¼ãƒˆã‚’ç¶­æŒã—ã€å°†æ¥çš„ã«ã¯PyArrowãƒãƒƒã‚¯ã®DataFrameã‚’ã‚ªãƒ—ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã¨ã—ã¦æä¾›ã—ã¾ã™ã€‚



## Summary æ¦‚è¦

This blog post introduces a new service in Hopsworks, ArrowFlight with DuckDB, which offers significant performance improvements for Python clients reading/writing with feature data. 
ã“ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ã§ã¯ã€Hopsworksã«ãŠã‘ã‚‹æ–°ã—ã„ã‚µãƒ¼ãƒ“ã‚¹ã€DuckDBã‚’ç”¨ã„ãŸArrowFlightã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ã“ã®ã‚µãƒ¼ãƒ“ã‚¹ã¯ã€æ©Ÿèƒ½ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿æ›¸ãã‚’è¡Œã†Pythonã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«å¯¾ã—ã¦ã€é‡è¦ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã‚’æä¾›ã—ã¾ã™ã€‚

We chose to build a service rather than making our feature store clients heavier by embedding DuckDB and the drivers required to access the many different offline stores supported in Hopsworks. 
ç§ãŸã¡ã¯ã€Hopsworksã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã•ã¾ã–ã¾ãªã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã«å¿…è¦ãªDuckDBã¨ãƒ‰ãƒ©ã‚¤ãƒã‚’åŸ‹ã‚è¾¼ã‚€ã“ã¨ã§ã€æ©Ÿèƒ½ã‚¹ãƒˆã‚¢ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’é‡ãã™ã‚‹ã®ã§ã¯ãªãã€ã‚µãƒ¼ãƒ“ã‚¹ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã‚’é¸ã³ã¾ã—ãŸã€‚

We showed in benchmarks up to 45X throughput improvements compared to existing feature stores, showing the value of working with Arrow data end-to-end, from the lakehouse to Pandas clients. 
ç§ãŸã¡ã¯ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æ—¢å­˜ã®æ©Ÿèƒ½ã‚¹ãƒˆã‚¢ã¨æ¯”è¼ƒã—ã¦æœ€å¤§45å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã‚’ç¤ºã—ã€ãƒ¬ã‚¤ã‚¯ãƒã‚¦ã‚¹ã‹ã‚‰Pandasã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¾ã§ã€Arrowãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§æ‰±ã†ã“ã¨ã®ä¾¡å€¤ã‚’ç¤ºã—ã¾ã—ãŸã€‚

We have built a bridge for Python-native access to Lakehouse Data in Hopsworks, and we hope it will enable Python developers to be more productive working with our feature store. 
ç§ãŸã¡ã¯ã€Hopsworksã«ãŠã‘ã‚‹ãƒ¬ã‚¤ã‚¯ãƒã‚¦ã‚¹ãƒ‡ãƒ¼ã‚¿ã¸ã®Pythonãƒã‚¤ãƒ†ã‚£ãƒ–ã‚¢ã‚¯ã‚»ã‚¹ã®ãŸã‚ã®ãƒ–ãƒªãƒƒã‚¸ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€Pythoné–‹ç™ºè€…ãŒç§ãŸã¡ã®æ©Ÿèƒ½ã‚¹ãƒˆã‚¢ã‚’ä½¿ã£ã¦ã‚ˆã‚Šç”Ÿç”£çš„ã«ä½œæ¥­ã§ãã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¦ã„ã¾ã™ã€‚



## References å‚è€ƒæ–‡çŒ®
### Interested for more? ã‚‚ã£ã¨èˆˆå‘³ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ
- ğŸ¤– Register for free on Hopsworks Serverless 
- ğŸŒ Read about the open, disaggregated AI Lakehouse stack 
- ğŸ“š Get your early copy: O'Reilly's 'Building Machine Learning Systems' book 
- ğŸ› ï¸ Explore all Hopsworks Integrations 
- ğŸ§© Get started with codes and examples 
- âš–ï¸ Compare other Feature Stores with Hopsworks 



### More blogs ã‚‚ã£ã¨ãƒ–ãƒ­ã‚°

#### Common Error Messages in Pandas Pandasã«ãŠã‘ã‚‹ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
We go through the most common errors messages in Pandas and offer solutions to these errors as well as provide efficiency tips for Pandas code. 
ç§ãŸã¡ã¯Pandasã«ãŠã‘ã‚‹æœ€ã‚‚ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è§£èª¬ã—ã€ã“ã‚Œã‚‰ã®ã‚¨ãƒ©ãƒ¼ã«å¯¾ã™ã‚‹è§£æ±ºç­–ã‚’æä¾›ã™ã‚‹ã¨ã¨ã‚‚ã«ã€Pandasã‚³ãƒ¼ãƒ‰ã®åŠ¹ç‡åŒ–ã®ãŸã‚ã®ãƒ’ãƒ³ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚

#### Migrating from AWS to a European Cloud - How We Cut Costs by 62% AWSã‹ã‚‰ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘ã®ã‚¯ãƒ©ã‚¦ãƒ‰ã¸ã®ç§»è¡Œ - 62%ã®ã‚³ã‚¹ãƒˆå‰Šæ¸›æ–¹æ³•
This post describes how we successfully migrated our serverless offering from AWS US-East to OVHCloud North America, reducing our monthly spend from $8,000 to $3,000 with no loss in service quality. 
ã“ã®æŠ•ç¨¿ã§ã¯ã€ç§ãŸã¡ãŒAWS US-Eastã‹ã‚‰OVHCloud North Americaã«ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ã‚µãƒ¼ãƒ“ã‚¹ã‚’æˆåŠŸè£ã«ç§»è¡Œã—ã€æœˆã€…ã®æ”¯å‡ºã‚’$8,000ã‹ã‚‰$3,000ã«å‰Šæ¸›ã—ãŸæ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚ã‚µãƒ¼ãƒ“ã‚¹å“è³ªã®ä½ä¸‹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

#### Hopsworks 3.0: The Python-Centric Feature Store Hopsworks 3.0: Pythonä¸­å¿ƒã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢
Hopsworks is the first feature store to extend its support from the traditional Big Data platforms to the Pandas-sized data realm, where Python reigns supreme. A new Python API is also provided. 
Hopsworksã¯ã€å¾“æ¥ã®ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‹ã‚‰Pandasã‚µã‚¤ã‚ºã®ãƒ‡ãƒ¼ã‚¿é ˜åŸŸã«ã‚µãƒãƒ¼ãƒˆã‚’æ‹¡å¼µã—ãŸæœ€åˆã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢ã§ã™ã€‚ã“ã“ã§ã¯PythonãŒå„ªä½ã«ç«‹ã£ã¦ã„ã¾ã™ã€‚ã¾ãŸã€æ–°ã—ã„Python APIã‚‚æä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚

Â© Hopsworks 2025. All rights reserved. Various trademarks held by their respective owners. 
Â© Hopsworks 2025. å…¨è‘—ä½œæ¨©æ‰€æœ‰ã€‚ã•ã¾ã–ã¾ãªå•†æ¨™ã¯ãã‚Œãã‚Œã®æ‰€æœ‰è€…ã«å¸°å±ã—ã¾ã™ã€‚

Google Tag Manager (noscript) 
Google Tag Manager (noscript)

End Google Tag Manager (noscript) 
Google Tag Manager (noscript)ã®çµ‚äº†

Cloudflare Web Analytics 
Cloudflare Web Analytics

End Cloudflare Web Analytics 
Cloudflare Web Analyticsã®çµ‚äº†



## Notice ãŠçŸ¥ã‚‰ã›

We   and selected third parties   use cookies or similar technologies for technical purposes and, with your consent, for other purposes as specified in the cookie policy.
ç§ãŸã¡ã¨é¸ã°ã‚ŒãŸç¬¬ä¸‰è€…ã¯ã€æŠ€è¡“çš„ç›®çš„ã®ãŸã‚ã«ã‚¯ãƒƒã‚­ãƒ¼ã¾ãŸã¯é¡ä¼¼ã®æŠ€è¡“ã‚’ä½¿ç”¨ã—ã€ã‚ãªãŸã®åŒæ„ã«åŸºã¥ã„ã¦ã€ã‚¯ãƒƒã‚­ãƒ¼æ–¹é‡ã«è¨˜è¼‰ã•ã‚ŒãŸä»–ã®ç›®çš„ã®ãŸã‚ã«ã‚‚ä½¿ç”¨ã—ã¾ã™ã€‚
Use the â€œAcceptâ€ button to consent. Use the â€œRejectâ€ button or close this notice to continue without accepting.
ã€Œå—ã‘å…¥ã‚Œã‚‹ã€ãƒœã‚¿ãƒ³ã‚’ä½¿ç”¨ã—ã¦åŒæ„ã—ã¦ãã ã•ã„ã€‚ã€Œæ‹’å¦ã™ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ã“ã®é€šçŸ¥ã‚’é–‰ã˜ã¦å—ã‘å…¥ã‚Œãšã«ç¶šè¡Œã—ã¦ãã ã•ã„ã€‚
