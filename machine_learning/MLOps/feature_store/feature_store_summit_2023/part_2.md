(3:11:59) back I had a nice quick break there I hope um hey Claire how's it going um so we have uh now we have another session starting we have four talks from arise bite wax dag works and data bricks and I'm delighted that we have arise again here at the at the conference I think for the second time now and Claire is a CLA Longo is a machine learning engineer and data scientist and she's focused her career on building and scaling ml engineering teams she's currently head of ml Solutions engineering at rise leading post sales
(3:12:31) machine learning Solutions engineering efforts and assisting arise customers in integrating ml observ observability into their mlops infrastructure and workflows and prior to rise she led the mlops team at Open Door and establish a new machine learning team to deliver datadriven products at twio all right Claire um are you ready to go yeah let's do it thank you Jim for the introduction um should I go ahead and just uh share screen here yeah all right and we're live great so I'm talking about monitoring feature stores today um shout
(3:13:13) out to chat gp4 for the catchy Title Here stop blaming the algorithm it's your feature store um the reason I love this title though is that it really does hit home a trend that I've seen over and over with feature stores and with models in production where a lot of the issues we see with the model and production once it's deployed it's not really with the model itself but it's actually with the like data quality of the model input so that can be traced back to feature stores to ETL pipelines that kind of
(3:13:39) thing um so what I'm going to be talking through here is kind of a guide to monitoring your feature store what kinds of monitors we should configure and that kind of thing to kind of maintain the quality both of the features and the models Downstream that depend on it so I think the group here already knows what a future store is I don't want to kick off the talk by explaining that one to you all um I think we're all caught up here so instead I'm just going to go ahead and Dive Right In and talk
(3:14:05) about what can go wrong with a fature and a future store what does future failure look like um across my um experience as an ml engineer data scientist I've actually been on teams that are building future stores I've been on teams that put models into production I've actually seen a lot of this stuff um on my own teams and so definitely speaking from experience here I'm going to talk through three examples of just future failure that I've seen and then we'll talk to monitoring and I'm going to bring it back home and talk
(3:14:35) about how we could have monitored in these situations to uh prevent this um and these are like really common issues that I I think people see in mod models in production across the board so number one that's super common is vendor data shifting so let's say you're data scientist you trained a model and that model relies on data that maybe you're purchasing for from a third party maybe you're even like scraping this data um there's a lot of different ways to get rich data into your models but if you're
(3:15:03) not generating the data your team doesn't own the data you're not um able to really control if that data changes under your feet so we've seen situations where that data will just shift maybe we're changing the data type we decided for that it could be a string instead of an integer those kind of kinds of things are breaking changes that will break a model that depends on this data Downstream um so it's definitely something that we want to be able to monitor for number two is statistical
(3:15:30) drift so I'll get into a little bit more detail exactly what this is and metrics to evaluate it but essentially what this is is this is a natural thing that happens where data just changes naturally over time you have a statistical distribution you trained your model at a certain point of time and you capture those distributional patterns but then you deploy that model and let's say 3 four months later maybe that distribution has changed a little bit maybe something has changed in the economy something's changed in the world
(3:15:57) world events that can really impact the distribution of your data and so when you see that kind of like natural drift um it's important to retrain your model keep your model fresh but those patterns show up in your features in the first place the last one is online and offline skew so this one here is let's talk through the scenario a data science creates a model in a notebook um let's say they do some iterations of hyper param parameter tuning uh feature engineering and they come up with a great model now it's time to deploy it
(3:16:29) and now you have a ml engineer in some situations will step in and do that deployment step and in that situation they may be rewriting that online and offline uh feature transformation so the data scientist already wrote some feature transformations in a notebook you've got to as an ml engineer take those Transformations out of the notebook and scale them and deploy them and so you're essentially running the same transformation in like two points in the ml modeling life cycle during model training and like research and
(3:16:57) exploration and then again during um deployment and it's important to make sure that those Transformations actually match up and are maintained together great so different types of monitoring there's three kind of veins of monitoring here that I'll talk about where we're Mo mostly focused right now is in the middle the data monitoring and a little bit to the right with machine learning um the first one I think is very familiar to any software engineer here that's system and infrastructure
(3:17:30) monitoring you wouldn't deploy software into production without solid monitoring on that um and that same kind of concept applies to your data pipelines and your actual models once they're deployed and part of working software um the difference though is that you've got different people involved across the this Pipeline and usually like different tools different needs different text TXS um so starting with systems and infrastructure you may have a devops engineer software engineer that owns that um they will tell us the mlops is
(3:17:58) just devops for machine learning it really is um then you'll have like a data engineer that's maintaining quality of data pipelines and you'll have the ml engineer who's responsible for maintaining the model itself so the interesting thing here and something that actually get um asked often is let's say you're already on the end of the funnel and you're doing machine learning monitoring you're monitoring your model inputs and outputs together as a model and looking at how those drift and move together um do we
(3:18:26) need the data monitoring upstream and my answer that is yes definitely I think there's a lot of value in having monitoring at every piece in the life cycle that the data travels through so you can really pinpoint and root cause an issue with your data so even if you have like a good monitoring solution around like your uh machine learning models your software systems and infrastructure layering in some additional quality monitoring into your feature store is kind of a no-brainer because that way you can like catch
(3:18:53) issues with your data Upstream before it really impacts those like Downstream Downstream systems which would be like your machine learning models cool so I thought a good way to kind of uh present this would be to walk through a day in the life of a feature what is the life cycle that features that a feature goes through so starting on the left here you're looking at your raw data this can come from different sources it could be streaming data that's collected in real time could be the vendor open source data that I
(3:19:28) mentioned maybe you're web scraping uh purchasing data or it could be internal databases this could be a database that um a different team owns that you're trying to access some some data or it could be a database that you own um so the first step here is really to like collect a bunch of raw data the next step is data cleaning this is typically data scientists data Engineers doing this type of work um they're really cleaning pre-processing that data for a machine learning project then you go into feature engineering this is where
(3:19:58) we're actually making the future the future is born in this step so this is really the beginning of their life cycle but there's still that Upstream kind of raw data that um it's dependent on and the quality of the feature itself is also going to be dependent on that Upstream data um so feature engineering is again I'm not going to go into depth here because I think we all know but we're essentially applying transformations to this data to create features that will be useful in a machine learning model and that looks
(3:20:24) different depending on like if you're doing deep learning like llms there's definitely some different kind of data processing steps here but essentially you're kind of creating features going to be used in a machine learning model uh both pre and post production um most people are dumping that into a feature store very good idea and then um there might be a second feature engineering step here this one I see is actually quite common where you already have some features in your feature store but for a specific team or
(3:20:52) modeling use case that's drawing on that feature they might apply additional Transformations so you might have different kind of Transformations being applied to your features as they come out of the feature store and get used in the these uh different use cases then the features get used across um some different kind of steps in the modeling life cycle here so um definitely useful to Tinker with them in like a research and experimental phase of a project seeing what features are valuable in your modeling
(3:21:20) project um it's going to be useful in model training this can be like initial training and testing models against each other it could be like an automated retraining cycle and then it's also going to be used in model serving when you're actually getting inferences and predictions out of that model so what can go wrong um and really where should we monitor this feature So my answer to this is we should monitor it every step that we can because it really helps put together a picture and root CS exactly where the breaking
(3:21:50) points are um but there's a lot we can do within the feature store which I'll get into so first place where things can go wrong is that data shifts you can have changes in your raw data the second one sorry um second one is data uh cleaning there could be quality issues there an example of like quality issues that I've seen is like missing values Maybe there's suddenly like a new value that shows up in a system um that the model hasn't seen before and that can definitely cause problems Downstream uh feature engineering so
(3:22:22) this can be broken feature engineering code you're writing ETL maybe you've got code in a notebook maybe it's actually U deployed somewhere else but essentially like you've got these Transformations on the data and that can definitely have bugs because it's code um then within the features themselves there can be feature quality issues there and then again you're doing more future transformation so again another opportunity to break things and then on this side of things um you have that kind of online
(3:22:51) offline skew with the feature serving and then in um training oops so yeah you're going to have in this phase you're going to have features as model input so the models the features essentially going into the model to get that um inference so you're going to have like training and serving skew online offline skew the difference here so like training and serving is like your feature look different when you train the model than when you're actually trying to serve it and get a prediction and so that's going to cause
(3:23:21) some discrepancies in your model quality um online offline is like you've got um an online feature Store offline features maybe there's some features that you're able to like pre-compute before inference time because obviously if you don't have to put something in your critical path don't do it so feature serving we like to keep like really lean so anything you can precompute is important there and you want to make sure those um will always line up great so that is everything that can go
(3:23:48) wrong um but wait so here's kind of the challenge if you own a feature store if you're working on a feature store you don't necessarily like own that Upstream data you may not even own the infrastructure for like the model deployment and so how do we get like that end to end monitoring that I'm always advocating for monitoring for the data in every step of that machine learning life cycle it's actually very hard to do this is typically going to cross like multiple teams so as like a feature store team and someone who owns
(3:24:18) a feature store what I want to do here is I want to set up monitors that are G give me like the most coverage as possible I want to make sure that I can like preemptively detect issues so that my like Downstream customers the models that depend on my system um are going to be kept healthy and I can like mitigate any issues with my features that those systems are dependent on and I also want like a little bit of on like the Upstream data issue like I do want to trust my other teams to be monitoring really well and for the most part like I
(3:24:47) have seen people monitor that data very well but they're not necessarily monitoring it for my use case specifically so they might not like have full coverage of every issue that could really impact me it's a customer to that data so being able to set up my own monitoring across my feeder store super valuable but the question is what monitor should I really set up so I like to think of three kinds of monitors here number one data quality this is a no-brainer this is what I talked about in terms of like missing
(3:25:19) values new values suddenly appearing um things that would really be like a result of like a broken data pipeline the next one is going to be tabular drift monitors so what I mean by this is this is that statistical distribution drift where you're seeing the data change naturally over time this can be gradual it can be sudden but it's usually happening due to like natural events and we want to keep the models up to date and healthy with that and this one requires like some fancy statistical calculations
(3:25:50) to really monitor this because essentially what you're doing is you're looking at the you're measuring the distance between two statistical distributions so there's some great metrics around that including PSI K Divergence um that really what these are essentially doing is like they're looking at two histograms and they're measuring like how similar are these two histograms and if you have like true drift you're actually going to see like kind of what we're seeing in the illustration here we're going to see two
(3:26:16) histograms that really don't look the same so if you're features see this over time um you have drift in your features so the last one is embedding drift monitors and this gets uh this is really interesting because um this is important for deep learning but it's also really important for like the llm use case which we're seeing so much of these days this is really a popular thing to be building right now it's defly the Buzzy thing so if you're working with like generative AI the question is like how
(3:26:46) do I even like monitor a system like that it's really hard but and the reason the reason it's hard there is like your data is not tabular thinking about monitoring like a tabular feature like I don't know some a feature like credit score maybe like income like those kind of like strong tabular features it's very easy to just visualize in the 2D and monitor for like data quality and Drift But when you look at the data that is used in these like llm systems and dup learning systems NLP computer vision
(3:27:14) you're looking at embeddings which are like nend dimensional so how do you monitor drift on that um I'll talk through some approaches on that one as well so those are the three I'm going to dive into all three of these and just kind of give examples of where they're adding value so number one data quality the areas that I've seen these be very valuable kinds of monitors um for like catching issues is things like missing values new categorical values which I mentioned catching an outlier suddenly
(3:27:43) you're seeing just a really high really low Val value in your data Maybe it's like generated by a new bot or something like that um catching online offline skew can be important here you can create some uh pretty fancy custom metrics to compare different data sets so configurability is what you want here you want the ability to really kind of have some great out-of the Box um Quality metrics but also be able to kind of write your own and con configure the data quality monitors that are really going to be like most valuable for you
(3:28:11) use case um I do get asked a lot of like what should I monitor for and I think it really depends on your use case and no one knows your data better than you and the people using it Downstream like the uh data scientists machine learning Engineers that are creating models so coming in with a strong hypothesis of like what could go wrong with the kind of data that you're looking at is really helpful here but in general like missing values outliers categorical online offline skew are kind of the things to think about
(3:28:40) here so that's data quality so next is drift there's two kinds of drift so sudden drift can happen if there's just like an immediate thing that changes the patterns in your data suddenly and gradual is obviously what it is is that kind of like just kind of gradual overtime Divergence of the two data sets when you're thinking about drift you've got two data sets you've got like a baseline data set and a current data set and you're probably working with some moving Windows there um and you want to
(3:29:11) be able to catch both of these like the sudden and the gradual the sudden drift could be like an immediate issue with like data pipelines breaking it could be like an immediate like event like something like coid I think would suddenly change a lot of data patterns naturally gradual drift can just be like things kind of change slowly over time and you also want to be able to detect that but it might not require that kind of sudden immediate response to it but this is all in art in like choosing your uh Baseline data set one of these
(3:29:39) distributions that's used as the Baseline for comp comparison and then the current one and what I've seen work really well here is training data versus production data if you're looking at the model Downstream but if you're looking at just the feature and you're not tying it to the model itself uh what is really useful here is going to be just like a moving window what was last month versus this month what was last week versus this week U and being able to track those over time to catch both kinds of
(3:30:06) drifts is um definitely the recommendation here I think one point to note is like the difference between monitoring a feature store and a model so the model you're monitoring the um model inputs and outputs together so you're monit you're already monitoring some of these features with the model output and that's very valuable because if there's an issue with your model outputs you're able to tie that back directly to any issue with the feature but when you're doing that you're
(3:30:34) already kind of Downstream of any feature issue so when you're monitoring Upstream as well just on the feature store and you're not Cor relating it to the model output because there's probably a lot of models honestly that are depending on those features you're able to kind of like preemptively catch stuff before the data scientists have to look for a problem there I know we're running into time um so really quickly this is the embedding monitoring approach so you're now um in the ND Dimension you're looking at em at
(3:31:03) monitoring unstructured data text Data computer vision data essentially the way to approach this one is you create these um kind of representations of the data through embeddings and then you're monitoring the embeddings themselves so the way to monitor drift in an embedding would be to look at like ukan distance cosine similarity you've essentially reflected your data into lat into 3D latent space here and you can identify like natural clusters of of things so you'd be able to see like if there was
(3:31:33) sudden drift gradual drift if there was like a whole new cluster that appeared in a visualization like this the way to detect that would be comparing the distance of the Clusters from each other and monitoring that over time so that can be done actually really simply with like cosine similarity and ukan distance metrics on this kind of data so wrapping it all up here we've got these kind of three scenarios I talked about you got vendor data shifting how do we monitor for that I put data quality monitors um on the
(3:32:05) feature store so you could see like the changes in like data types missing values that kind of thing statistical drift you want to monit tabular and embedding drift with those kind of statistical metrics and then online offline skew um that scenario of like taking the model from like uh notebook to production as well you want to be looking at custom metrics for that data skew itself um so kind of a data quality scenario scenario there as well great I want to pause for any questions H you can hear he me um that
(3:32:42) brilliant we actually have a couple of minutes for questions so you you you're you have time um I'm going to I'm going to start by asking a question that I hear quite a lot uh I think a lot of people are confused about it um when we store data in a feature store uh at least at hopss we would say store that data unencoded if it's a categorical feature and if it's a numerical feature um you know that you want to standardize or normalize or scale uh we would say store that unencoded in the feature
(3:33:10) store and then do the encoding in the training and inference pipelines consistently um but you know in some cases people might say well I want to I want to store the encoded data and the feature store can you do monitoring of encoded categorical variables or scaled numerical features and does that make any sense you totally can um I would want to monitor it honestly like both spots if possible but once it is encoded like data scientists are probably monitor monitoring it with the model output there so they're probably like
(3:33:44) doing the encoding like they take unstructured data turn to embedding they take categorical data one hot encod it they're probably monitoring those Transformations it kind of gets to this diagram here where you have that additional step of feature engineering before it's going into the models um the thing that's interesting though is like I would monitor both because there's probably different feature transformations they're making there and if an issue shows up with a model you want to be able to tie it back
(3:34:18) to like their step of the feature engineering code or like some quality issue with the feature Upstream so being able to kind of monitor in both spots I think is valuable here yeah I I guess one of the challenges there that I would see like particularly if you take gradient based uh models gradient descent based models is that we often normalize the numerical features for example we we want numerical features in deep learning um is it's very hard then to Define rules you know so in maybe in in in your raw
(3:34:47) data you have a very wide distribution of your feature values outliers are easier to identify but then what what's the standard deviation in your raw data is not a standard deviation in your uh you normalize data and then it becomes quite challenging um to for people to to Really to understand that I don't know I think that's something I find very challenging to to to discuss with people yeah I think there's definitely some stuff to in unpack there are you thinking about like the approach for
(3:35:15) monitoring it before yeah well I guess I guess what I'm what I'm curious about it's just Tech like we have standard techniques like you showed some very interesting techniques of course comparing data distributions and distances between those data distributions but they're really typically for unencoded uh feature values that but then you know when you encode them you're obviously Shifting the distribution and then those techniques um you know become less you know Le I guess there's less of a signal
(3:35:41) to to work with so I think I think it's a very open area for work and and I'm curious to see um what's going to happen in US yeah that's very interesting because I love monitoring at every point you can but I mean what you're raising there is like there might be some challenges to monitoring pref feature transformation um spe specifically in terms of like the uh statistical distributions too so yeah and the LM ones is is really interesting so so you know you mentioned that you could take
(3:36:13) um you know you could you could compute the cosine similarity between two embeddings to get get difference between them do you do you think do you in your case do you need a a vector database to do that or do you do it um on demand or what's AR's kind of way of doing that uh great question um arise H will create embeddings for you so we have um some code that will actually create these but there's also the option of like extracting embeddings right out of your model which is again like a downstream Thing Once you got the model
(3:36:46) involved um or getting embeddings out of something like open AI which I think is really cool uh way to just kind of generate and extract embeddings but there is like the step here of like yeah you've got the raw data like computer vision NLP you need to turn it into embedding to do this so that's definitely um kind of part of this pipeline here I think of that as like a feuture transformation step for sure cool now there were two question questions in the Q&A section I kind of got carried away maybe we can we can
(3:37:14) take them on slack um thanks everyone um thanks for those questions thanks very much Claire talk to you again uh great now we have Xander Xander Matson and Xander Matson is the CEO of and founder of bwax uh before starting bwax he worked across machine learning infrastructure and data science teams at Heroku and GitHub he started bite wax to try and make it easier for engineers to add real-time capabilities to products we really like white wax because we really like the fact that you know python developers can now get into streaming
(3:37:46) without having to to build this massive infrastructure and I know sandre likes to Surf as well which I also like so we have that in common sandre over to you hey everybody um I'm gonna do the little share my screen dance so we'll see if we get this working correctly the first time yes we're good to go right on well I'm really excited to be here um I've been listening either uh directly or in the background all day and there's been some really interesting things um and what has really stood out to me in many
(3:38:22) of these talks is just how complicated some of the architectures and setups can be when you're talking about these uh production machine Learning Systems so Props to everyone here who is managing systems like that I know it is not easy so my little introduction that was made my Jim thank you I'm Xander uh founder and CEO of bite wax uh and um today I'm excited to talk to you a little bit about realtime machine learning and uh in specific real-time feature pipelines um so you might be asking yourself you
(3:38:57) know in addition to all this complex architecture that I already manage and infrastructure um why do I need to uh add in additional complexity about real time and the truth which is uh its own session is that you may not need to but I'm going to share today how you can start thinking about that uh adding realtime machine learning and real time future pipelines um and potentially bring Major Impact to your business with streaming data and realtime MLS so uh so real time machine learning is all around us today you have uh
(3:39:33) recommendation systems like addictive Tik Tok videos um you have ads on many platforms some of those which are highly relevant and using uh many different inputs streaming inputs you have personalization on many of your favorite apps and websites um and then another good one is fraud detection so when you're traveling you get that text message right after you buy some train ticket or something like that in a foreign country so that's a good example of many of the real-time machine learning uh systems we see today but before I go on
(3:40:06) a little bit more about me and why I'm here to talk to you today uh so as was mentioned um I'm the CEO and founder of bwax and before that um I'll talk a little bit about bwax what bwax is it's a stateful stream processor it's an open source project um as Jim mentioned it's Python and we made it so it's easier to work with streaming data um but before bite wax I worked at GitHub and before that at Heroku uh and was working in many different data hats I suppose um between data science data engineering
(3:40:40) machine learning learning and machine learning infrastructure kind of migrated my way through a few of those things and at the end I was working on machine learning infrastructure uh at GitHub and I left GitHub specifically to start a company focus on enabling real-time machine learning as it was a big point of friction uh for us at GitHub um so I'm also uh outside of working on bite wax a proud human and dog dad uh I had uh my first daughter about year and a half ago um and I have a eight-year-old Aussie Shepard border
(3:41:15) col mix uh this photo here is me trying to look cool hiking up to go skiing outside of Surfing I also like to go skiing um and uh you can find me in the bwx slack or in the featur store.org if you want to chat more uh or if you're in happen to be in Santa Cruz where I live send me a LinkedIn message and we can grab a coffee okay on to the talk today's agenda uh we're going to cover a little bit about um background of what realtime ml is and then we're going to dive into how we can enable this with streaming data
(3:41:49) and stream processing uh so moving right along here so we all know that timeliness matters uh but how much does it really matter it has been shown to affect the bottom line of businesses directly and indirectly uh in this case we're talking about latency but there's also freshness that is another aspect of this uh so these statistics from Google Walmart and others may not look like much but at their scale it is millions or billions of dollar dollars sorry um but uh you may be thinking to yourself oh it's only
(3:42:23) for big companies it can also affect smaller companies um timeliness or latency uh can impact in additive ways um the uh how well your organization or big business um will do uh with with things like customer retention and customer lifetime value um but not only that but in some of the uh Tech enabled um businesses it's a win or take all type of market and the uh and you're competing directly to provide the most impact or value to the user and uh this will result in either thriving or going out of business
(3:43:05) um so uh what does real-time machine machine learning really look like broadly when we talk about real-time machine learning it fits into one of two categories um and uh the first of these being offline training with and off online serving um so uh the difference or sorry what happens with the online offline uh training and online serving is you in a batch method train some train your model and then you promote it and then you serve it in real time and the other method is when both that training and serving happen in an online
(3:43:39) fashion so you're con conly updating um the model so we'll kind of talk about what these are in more depth uh this is a simplified uh version based on some of the diagrams I've seen today but uh uh it's a sketch to illustrate um from a high level how the offline online system works so you have a mechanism to train your model in a batch fashion with some historical observations and then is promoted to a serving layer where it can then make predictions um and then you have another system where you're receiving new data as it is
(3:44:15) generated processing those observations and then making them available to the serving layer so the fresh features can be used in the model inference uh obviously glossing over some of the little intricate bits of uh of these systems in this um another sketch in contrast to that is for online training um and serving so we have uh the observations um uh coming into the model and then you're making a prediction with those observations and you're also updating uh the model so an example of this that is uh fairly common uh is
(3:44:53) using some of the unsupervised methods one of those which is anomaly detection um various different anomaly detection algorithms where you can update the model incrementally um so today though we are going to focus on the feature side of real-time machine learning um and how you can create real-time feature pipelines with bite wax uh before we do that I would like to diverge a little bit just to talk about streaming data and how we can process streaming data so we'll build a little pipeline to analyze a stream of data um I'm going to
(3:45:31) play a little video here I'm hope it is easily viewable on your screens um okay so uh streaming data is all around us uh I think the best way to kind of think about this if you're a python developer is it's kind of like an infinite um generator and so that's what I'm like what I was typing up in this video here so we've created a little generator and then uh the consumer or the downstream um system uh will request a if there is new information available um and kind of recurring polling Manner and so that in this video
(3:46:13) is us saying next so you can think of the stream of data being the generator and then uh the processor Downstream is saying hey next next next and then it's changing that data um for whatever purpose that uh that you require okay so um the unbounded nature which we were alluding to with that infinite python generator um can really feel quite daunting uh and in order to manage processing this data we often use unique processing tools to Chunk Up or batch our data or to maintain a memory a memory of what has
(3:46:54) happened in the past and the tools we use to to do this fall into the realm of stream processors and they're either referred to as stateful or stateless stream processors the stateful ones are the ones that can retain stuff in memory uh and one of which is bite wax other tools you might be familiar with or heard of uh that can do this is spark with their structured streaming um part of the the project and then Flink is and beam are another common uh open source tools to do this so all of these tools give us
(3:47:25) some semantics um to connect to data streaming data source sources and process them in Windows or micro batches uh to join different data streams together filter it enrich it and then write it out to a sync which in the real time feature pipelines will be the feature store uh so to illustrate what we're talking about with streaming data and this idea of like continually asking for if there's new dat uh data available and then processing it um as a stream we're going to make a or I'm going to show a
(3:48:00) little uh code snippet that basically pulls Hacker News API for new everyone's favorite uh news source Hacker News um it ually polls Hacker News for new updates and then process them in a streaming manner um we're going to do uh a little stateful um operation here where we're going to count uh the number of comments on a story uh and update that incrementally and then we'll just print it print it out to the terminal okay so I have another little video here so we'll run this give it a second to you going um so
(3:48:41) bwax is a python framework with a Ros processing engine but you don't really touch the Ros processing engine much so you're mostly what you see here is just python code it's nothing too fancy um the framework provides a lower level API that we've used here um to build our Uh custom input source where we're pulling Hacker News um and then we've also parameterized this data flow so um we can pass it and an initial ID uh so let's say that we had to go back in time and start from a certain period we could find out what
(3:49:17) that story IDE is we can start from there and uh um pass that through to our input um so in this uh data flow we're setting a periodic input so we're polling um I'm just going to take a quick pause here so most clients that connect to um streaming data sources whether it's Kaa Kafka or red panda or a web socket um they're generally pulling and asking if there's new dat data available so we've just kind of slowed this down and then this instance oh sorry oh no press the wrong button here let me
(3:49:56) get to the right spot apologies okay um I won't be pausing it anymore um so every 15 seconds we're going to call this uh or by Wax is going to call this uh input next item function that we'll show here in a second um and so we're just kind of like pulling and saying hey do you have any new updates do you have any new updates and we've kept uh in memory the max ID of the last time and we'll just find the ID IDs between those and send them Downstream um redistribute them so in the case of multiple workers we can
(3:50:38) split them out and then we will process those individually as a stream of data in this uh code what we're doing is we're finding the root ID so we might get a common ID and we have to Traverse up the thing to get the the parent ID so we can do that with a map operator um and uh the map operators are ENT or sorry The Operators are essentially the the tools you can use to kind of control the flow of data throughout you can think of each step as like a node in a graph so in this case we have our input that brings us many
(3:51:12) IDs and then we flatmap those that's the operator so we split them out and we process them one at a time uh We've re distributed them again uh across workers then we've retrieved metadata then we've traversed back up this graph of comments grabbed the key parent ID and then we're using a stateful map um where we have a builder function and a mapper function which in an Essence is like every time I see a story ID for the first time I'm going to do that build ID which is make a story object and then every additional
(3:51:45) time I see it I'm going to update the count and that's that mapper part so I've Le left some uh print statements in those so you can see when the um story is created and then the comment is updated and then finally in this we're outputting to the terminal Downstream we're outputting that count of comments uh okay so that was all really cool right but what happens if the server room goes on fire and the whole thing collapses or if you want to do something more complicated uh like many different types of Windows or
(3:52:22) many different uh different things in addition to uh what we showed there which was just a simple comment count uh it could be quite untenable or you might end up with the wrong calculation so what is Norm generally advertised as a best practice which we'll show next before we create our actual feature pipelines um is to split out so in our streaming application we had uh getting the raw data doing some stuff with the raw data and then doing this like stateful calculation and then printing it out and it was all in one application
(3:52:55) so the recommended best practice is to sort of split that out with something like kfka red panda in between and the reason you want to do that is in it writes to more durable store storage layer and is backed up and so if our work or if our server room like in this uh um this graphic here really actually goes on fire uh and our data flows go down we can recover them and replay them uh from the correct offset and not lose any data uh for our feature pipelines Downstream um so here's an updated uh update to our previous
(3:53:34) diagram of splitting out our calculations so uh We've added some complexity to the overall amount of infrastructure that you have to manage and that in this case is red panda which is a Kafka API compatible um stream uh stream streaming platform you can think of the streaming platform as like the transport SL sto storage layer for streaming data um so you can send the data through but it also maintains some uh of what you've sent through um at a durable level that you can replay it so anyway so we split this thing out we've added a
(3:54:12) little bit of complexity and the tra the the trade-off we're making for that complexity is we're able to uh do two things one have some more resiliency the most common place our data flow might fail um is in that calculations those comment calculations or the more sophisticated calculations they may fail because we run out of maybe it gets really big and we run out of memory U have to move it to a different machine or maybe the the the data somewhat shifted and you have like some failure because of field changed or whatever it
(3:54:44) might be so we have some resilience there the other benefit is that we can have many consumers uh of the data we put in red panda without hitting the API many many times so we've saved ourselves the um requirement of like overloading that single API to have many of these Downstream pipelines uh so moving on from here we're ready to make some feature pipelines so the first you can think of what I've done so far is like okay let's um get some data we're going to be using for making predictions all of this is a
(3:55:20) little bit um uh you have to use your imagination because the feature is not really used and there's not a model Downstream but um so we have uh pulled some raw data that we thought we could use to make some interesting uh predictions with the model um we've now created uh a pipeline to do something like that to create one feature which is probably not the right feature but is Count comments and now we're going to split these things out so we can have a more of a production ready system and then we can have many feature pipelines
(3:55:51) so once more we're going to have a little video that I'll try to talk through um so we're going to modify our previous example that you already saw to write the data out uh to fit into that architecture so we're going to use um red panda uh as the input and we or as the as the stream platform and we already have that running in the background in this instance um so what we need to do to connect to red panda is we can use the bite wax uh kofka connector so we'll replace the kofka output or sorry the
(3:56:25) the standard output where we're writing to the terminal we're now riding to red panda um the reason it says cof output is because it's CU API compatible so it's one works for both so we replace that output step in the in the flow and now there's a couple more things that we're going to do after we replace that output step uh one of which is we don't need our stateful map um uh count comments anymore we're going to put that in a SE separate feature um pipeline that's going to be uh taking input from
(3:56:58) Hacker News raw so we can remove that from our code and we'll remove the corresponding story um class that we made uh and the update count and we're going to do in what we're going to replace that with is a serialization step red panda requires that our data comes in as a b as by string um the topic requires it comes in a by string so we just need to serialize our data into a by string uh and then pass it through so we'll Json serialize it and then into bytes and then we can send it through uh or it will work sending it
(3:57:35) through that kofka or the red panda output so here you have you see me in the console UI create the topic um ACR raw uh it's we won't we'll skip over the details of what the topics are and partitions and everything like that so we have our topic now we can start writing data to it so we run our Hacker News flow and when we um uh find out there's an update we shove that into red panda um and in the red panda UI can do some cool stuff like you can see the um the data so we could leave this thing
(3:58:15) running and uh um yay now we can move on to the next part which is how can we create these feater pipelines from red panda um so that's also nice little video here uh and uh our our data flow has been chugging along nicely in the background and we have a bunch of stories and comments in this red panda topic uh and now we're ready to write some features against this uh to First do what we were previously doing which was count comments so luckily byx has a nice cka input uh adapter that or connector that
(3:58:58) can be used uh with red panda so here we can add it in place of when we had The Hacker News input that we built where we built that custom class here we're just going to use one out of the box and then we're going to take that stateful map code that we used before and we're going to throw it in here as well um and then we can use that uh to count our comments and now we're going to put it back into red panda um and before we do that we'll serialize the data again so uh we deserialized counted and then we
(3:59:33) serialized it and um wrote it back to uh I panned out this let's skip ahead here I went a little bit maybe too quickly here so um let me explain what's going on quickly and then we'll so we added an input uh in our data flow code that you can see up above here um we added the red panda input so now we're consuming instead of consuming from the custom class that we wrote which was polling an HTTP endpoint we're consuming from red panda and then we're deserializing that data because we serialized it on the other side now we
(4:00:16) do our stateful map which is counting comments so we create a story object and then update that story object with a count of comments and then we um now we end up with data in the format of uh story and the count of comments we serialize that once more so that we can into a by string so we can send that to uh red panda and we're sending it to a new feature called called hn feature one we've run this thing in the video already um it started from the beginning of uh the Kafka topic and has crunched through all the data we have a logging
(4:00:50) step in here so it's logging some of these uh comments along the way I think I recorded this two days ago or something so there's probably I know maybe there's something here something about meta headsets um and then we can go back to the you UI oh well what I wanted to show which is I Mis pressed here you back to the UI and you can see the story and the running count of comments which is going up and up and up um so that's that uh someone else today has already showed how you can plug these sort of things into um
(4:01:32) the Hobs Works feature store but that would be our next step say we actually had a a feature that we wanted to use with our model um and we were going to predict the I don't know trending Hacker News or something um we could do that with uh hopss there's a tutorial on it that has bite wax in the hopworks tutorial repo U but basically you would um tell hopworks which what the schema was and what the uh Define that feature and then you can kind of point it to the kofka topic so I that was a lot and it was
(4:02:03) pretty fast uh but I managed to make it in the time with no time for questions uh if you want to check out the code you can find it on my GitHub aw maths and guide periodic not uh news and it will be a guide shortly on our website because I'll make it a guide there and then you can find us at bwax bwax um yeah thank you everyone brilliant thanks sers thanks and great to see code and and that that data scientists can can actually realize the dream of building real-time machine learning in Python um so I'm really looking forward to to see
(4:02:34) what you guys do in the future thanks oh we're going to move on and um so as sandre mentioned we're out of time for questions you can you can write questions to him on the slack and hopefully Sandra will jump in and answer there we're going to move on to Dag works and we have a talk by Elijah Ben Izzy Hi Elijah you there I'll read your bio and great so Elia Benes he built large uh components of the simulation trading infrastructure to Sigma where he led a team to test ensure the reliability of the simulations and then
(4:03:08) he built out the ml platform at Stitch fix that was used by over 100 data scientists and most recently he co-authored the open source Library Hamilton which is a general purpose lightweight framework for building data flows in Python and due to that success he left his job at Stitch FX start a dag works and dag Works goal is really to make it easy for data scientist to build and manage machine learning pipelines looking forward to this um Elijah big fan of of Hamilton as well um and the stage is yours awesome thank you so much
(4:03:39) Jim um I'm having trouble sharing my starting video so but I can share start oh there we go start my video all right see you great and so I exist and I'm gonna share my screen so you can see what I'm actually talking about there we go I want to go ahead and say that everything is good so first of all yeah thank you so much for the introduction I really appreciate it um super excited that Xander went uh before me I think this is I'm going to talk a little B about streaming in my talk uh and they can integrate with a bunch of
(4:04:14) different platforms B wax could totally be one of them um yeah so hey folks name is Elijah uh Jim introduced me but I'm gonna be talking about something I'm really excited about which is the open source Library uh Hamilton and how you can use it to build out feature engineering capabilities specifically how you can use it to write your features run once and run them everywhere in different contexts so today uh what's what do I want to you to come home with I want to convince you that one writing portable
(4:04:43) feature engineering code is hard two state-of-the-art approaches aren't flexible and powerful enough I put a little spicy Emoji by this uh because you're hearing about a lot of interesting state-of-the-art approaches and I think they're all amazing um I want to offer sort of a slight alternative or very different way of thinking about it Hamilton can help you write code to run in multiple contexts and keep your code organized and clean and Hamilton is easy to get started with and easy to use so first a little bit
(4:05:09) about dag Works we're aiming to build the unifying later for data ML and llm pipelines it's all open core so we have a paid product but that's not what I'm talking about today not selling anything I'm going to be talking about Hamilton everything here is free to download and use and all open source so you can install it on piie on cond as well easy to get started uh you've got documentation I'll give the links later and you can try it out at try hamilton.
(4:05:36) deev this is a website that uses pyodide to have a sort of interactive python experience in your browser you can explore Hamilton write one run through a few examples and sort of learn the basic concepts so on the agenda today first I want to talk about the problem or the challenges with feature engineering then I want to talk about the solution we came up with Hamilton next I want to talk about how you can use Hamilton to write once write your features once and run them everywhere I'm going to apply this towards batch and streaming mode
(4:06:04) and then I'm going to talk a little about additional benefits of Hamilton what having a Hamilton codebase does and how it sort of can help you and finally talk about some open source progress in community updates so what are the challenges with feature engineering well I want to look at a common scenario um it's based off of something that I saw at a previous role in e-commerce but I think is generally fairly widely applicable so customers will fill out survey results when they're signing up when they have
(4:06:30) something they want to tell you your model makes predictions and the goal is to get the survey results featurization what are the caveats here well you've got streaming survey results trickling in as you just learned the infrastructure for that can be uh fairly nuanced and complicated data comes in in dumps nightly so there's batch data to handle multiple teams are working together to do this if you're at a bigger company so you've got your featur teams your infrastructure teams and your data teams and features are derived from
(4:06:58) other features that are derived from data right so it's functions of functions of functions of your raw data uh so visually this looks something like this youve got your customer surveys coming in you've got your B data coming in and you've got your live browsing data coming in you put them through these feature Transformations which I'm going to break into map aggregation and joins and queries is sort of the standard shape and then you want to do model training or inference right uh we're going to be focusing
(4:07:21) mostly on feature Transformations with a little touch of inference during this talk so the context you'll want to run this in to actually make it clear you could want to run in a batch on tables in your data warehouse for training data run inside a streaming processor such as bite wax uh right to organize it all or Kafka as a processor for near real time and transform browsing data live we're not going to focus on that one but it's sort of natural to extend this to True online versus streaming contexts and
(4:07:51) complications um you want to ensure that the data is the same in all context so that you don't get a difference between your training and your production serving context so how do you handle joins or alternative data sources and non-batch mode and how do you include aggregations in streaming mode these are all sort of Fairly complicated things and how do you track lineage versions etc for different data sources right these are the things you have to think about when you're uh having a feuture pipeline that's run in multiple
(4:08:17) different contexts so uh back to that spicy point I made at the at the beginning I want to argue that the current approaches kind of fall into two different sides of a spectrum right so on the one side there's context specific execution and what this looks like is you write batch code and you write streaming code these are two separate pieces and they um know so two separate ways of doing it and you uh the first one you have as much power to do each one in whatever way you want but it can be cers to manage right you have two separate ways
(4:08:50) of writing code you have two sets of versions and the big question is do they match right on the other side of the spectrum you've got a single feature DSL to unify so you got to learn a new way of doing things it can sort of be opinionated about how you do these aggregations and how you do these joins and make it very powerful and I think these are sort of really interesting and fascinating but it can be tougher to grock limited specific operations and highly opinionated on how you do aggregations and joins which often I
(4:09:14) find are as much an infrastructure question as they are a modeling question so I want to propose something kind of right in the middle and the idea is can we write normal python code that is dry meaning you don't repeat yourself applicable in all settings so you can reuse pretty much all the same code fully customizable you decide how to do joins differently you decide how your aggregation approach you write map functions however you want want and you bring all your own infrastructure and that are self-documenting and imply
(4:09:42) structure meaning it's really easy to figure out exactly how the uh what is happening in your feature code just by looking at the code so I'm going to talk about how Hamilton is the solution we came up with here solve this problem and it comes from a very simple concept so the idea is what if every feature corresponded to exactly one python function and what if the way that function was written told you everything you needed to know about how that function worked so in Hamilton the artifact feature in this case is
(4:10:10) determined by the name of the function and the dependencies are determined by its parameters so if you've ever seen P test it's kind of taking that idea of fixtures and adding type safety and sort of taking it to the extreme and what does this look like well just to be clear Hamilton Sports all python objects in any sort of dataframe Library it started with panda so I'm going to kind of use that because it's something everybody knows um but really replace it with polers P spark Etc it all they're different certain
(4:10:38) nuances but it'll all kind of work the same and you can have python objects whatever you want to pass between it's just python functions so instead of writing operations on a data frame like you would here DFC equals DFA plus dfb DFD equals transform of DFC you declare your uh your features as functions and write them out individually so as I said the output corresponds to the function name so this is creating two features one called C and called D and D depends on C and C depends on A and B which aren't defined
(4:11:12) here so they could be something from the outside world or they could be another Upstream feature so to tie this all together what do you actually need to run it well this is a full hello world you've got your functions Each of which writes defines a feature and then you've got a driver to say what and when to execute right so all the driver does is import your modules load it up uh and then execute passing some input data you can think about the driver uh the functions is setting up delay execution the driver is
(4:11:40) interpreting that and actually running it so to sum it all up for each transform you write a function functions declare a dag which is a directed as graph if you're not familiar it's just a way of saying like run this use this data to run that use this data to run that Etc it looks like a flowchart and Hamilton handles this dag execution using your driver and it's as simple as that so you're probably asking doesn't Hamilton make your code more verbose um you have to write now functions instead
(4:12:08) of uh individual lines sort of operating on the same data frame I want to say yes but that's not always a bad thing and in most cases that's probably a good thing if you do it right but when it is a bad thing we have decorators so these decorators you can think about as sort of macro dag operations that enable you to uh sort of change the shape of your dag and make it a little more concise there's a bunch of them and this is a whole there's sort of a whole algebra here but the one you need to know about
(4:12:33) for this talk is called config dowin which allows you to use a static configuration parameter to choose between different implementations of the same sort of notor function so now let's talk about how you can use it to write once and run everywhere uh and the basic premise here is that we're going to have one and this is where I a little bit LED you on it's not just one feature per function for most functions you're going to have just one and you can share the vast majority of the code but using that config when
(4:13:01) we're going to allow you to swap out implementations to choose exactly how to work right so you should have probably had an aster by write once run everywhere but I kind of like I I I think this gets really is like a good way to do it because it allows just the flexibility and power for the more complicated operations and everything else you can run the same stuff so with map operations we're going to treat single versus bulk operations to be equivalent oftentimes even the single online operations you're going to want
(4:13:25) to be running in small batches so and you run vectorized aggregations you're going to choose do use store your aggregation computed on the Fly update regularly etc for for joins you can use a we're going to use a query instead of a join although there other ways to do it depending on the data size um and again for aggregation joins you're going to re Implement just the small pieces that differ between them so back to our scenario remember e-commerce survey data coming in we've got a set of simple map
(4:13:54) operations say you have a raw raw survey data where the customer inputs their budget gender and age just sort of spitballing here uh derived features are is their budget high are they high roller is male is female join you got the time since the last login which is a function of sort of the client ID and the login data and aggregations we're going to normalize the age these features are kind of just made up but these the shape of this isn't too far from something that I've seen in real life and that you've probably
(4:14:24) encountered all right so what does this look like in batch we're going to go over a full sort of working example of code and show how the dag changes along long the way but first uh this setting here so the goal is to compute features and infer model infer the model model in batch right so we're going to do uh feature calculation and then model inference kind of if you were doing like a snapshot thing it's pretty trivial to think about how you replace it with the actual training piece and the context we've got a
(4:14:49) database with raw survey results another database with client login data so we can get that information say the model is already trained um again I said you can use this for training but we have a model object that we can access and the data is of a reasonable size that's just I'm going to be using pandas uh but Hamilton can work with any data frame library and has sort of tools to help you scale up p them so what does data look data loading look like well we actually do have some specific data loading adapters and
(4:15:14) decorators but for now we're going to do some very simple code create a connection load from SQL and then we're going to have uh four columns here three of which are features we cared about budget age and gender and the last is client ID which forms the spine right so this is just a single node to start loading your data and this produces a dag that looks like this right we've got our survey results database and table there are strings coming in from the user then we get survey results and we split
(4:15:39) it out into separate columns so now let's do the map functions which are derived features on top of our features that we just uh extracted raw let's say we want to get as male as female as H roller these are sort of very simple implementations you can imagine these getting more complicated one of the nice things about Hamilton is it's separated each one of these is separated into a specific place so if you want to change it if you want to sort of understand exactly it's happening that's fairly easy to
(4:16:03) do now that we've added these you can see uh we've got three more nodes is male is female and is high roller and we're starting to build out our dag and the point a point I want to make is one of the cool things about Hamilton is you can sort of take these dags and add extra pieces to it and start building it iteratively and composing it to turn from a simple uh data loader into a more complex data pipe line now what are our joins going to look like well we're going to do a join in memory but you can easily do a join
(4:16:29) in your Warehouse if you want so this is loading your client login data getting the last logged in uh just a merge there and then a time since last login pretty simple which is just a subtraction it's a map operation and you can see we've added another section to our dag getting another feature that we're going to want to use in our model aggregations we're going to do the same thing that I talked about earlier where you're not using necessarily Panda series but you're using python objects
(4:16:55) taking a series and turning it into a float so going you get mean standard deviation and then normalize it which is just a map operation and finally we've got that added to our dag and we have got all our features so we've got aggregation join and map operations and finally we want to do inference in this case I've just defined model data to make it very explicit what the model is taking in there are other ways to sort of join these series that you can use in Hamilton this is just a way to make it
(4:17:19) very clear and predictions which take in a model that you pass in from the outside world as well as the model data and run your prediction so you've got uh predictions on top of your inference and this is the entire batch dag to put it all together here's what the driver looks like this would be in your etlp script say you're running an airflow or wherever you would uh import your features run it and pass in just the inputs you want based off of sort of your setting simple enough easy to run all right so what does this look like in
(4:17:51) the streaming mode and what specifically do we have to change well let's set up a context first so we know what we need to build uh you see you have a service to give you client login data you've stored aggregations from training so we have those means and standard deviation and your goal is to be near real time so to predict as soon as raw data is available what are the changes that are required uh well we have no aggregations available right so we have the stored aggregations of training we'll use that
(4:18:14) we want to swap out an external join with an API call uh so say we have a service as I said to give us the client login data we're going to be using single datam not data frames uh we kind of treat them the same here um I found that in most cases the performance hit of using uh single items versus vectorized stuff is almost negligible um but there are ways to sort of vectorizing hiled too if you want so we're going to be using this decorator config when to swap out the features you need to change uh there are
(4:18:43) four different things here so note we've got four things that are different versus um like 20 or 30 features that we've defined overall and this is loading in the survey results loading in the last log data and loading your scalers here note that we have this sort of double underscore thing after the feature name all this means is Hamilton's going to chop it off and it's kind of a clever way of defining two features that point to the same thing in the same file right and we're doing config when mode
(4:19:10) equals streaming to say oh in streaming mode we're going to do it this way versus in batch mode we're going to do it another way and what does this look like well the dag is actually simpler because we're loading data from external sources so there's more complexity pushed outside uh but comparing it to the other one we've got the joins the maps and the aggregations we have the data loaders I've highlighted the joins and the aggregations that are different between them the maps are pretty are all
(4:19:35) the same as are most of the S pieces within those in the glue so what does the driver look like um this is kind of like generic streaming stuff taking a list of Records output a list of predictions probably a list of Records in this case I just made a list of floats uh set up your configuration instantiate your driver and process your records which all it does is set your inputs from the records and execute it right so we've got a feature pipeline that works in Bach streaming and most of it is exactly the
(4:20:06) same and you know everything that's different so it should be pretty clear to figure out what's going on and figure out what the corresponding batch operation and streaming operation is all right so Hamilton gives you portable feature engineering code but also gives you a host of other benefits so because it lets you write these python Transformations as simple these Transformations as simple python functions they provide everything you need to know so unit testing is really easy they're all just plain old python
(4:20:31) functions you can even you can unit test individual pieces or unit test an entire path for documentation all you need to do is use the python doc string for modularity you naturally have it broken into small pieces and the small pieces are organized into modules so it makes modularity really easy your code uh naturally forms a central feature definition store so it's not a data catalog per se but it's a very valuable thing to know exactly where your data is coming from and you can sort of visualize the dag as well debugging you
(4:20:58) can execute functions individually and set breakpoints even pass and mock data fairly easily and for trustworthy data to make sure your data is trustworthy uh we have validation included out of the box we have all sorts of cool validators including a Pandera integration that's built into Hamilton right um so how does this integrate with feature stores because we're at the feature stor Summit Hamilton is kind of the transform layer it actually works in basically anything that runs python uh so hops Works Feast
(4:21:21) tecton can be the storage and sort of moving around stuff layer um and we actually have a blog post of feast integration here that I'm really excited about that shows a few different ways to use Hamilton to make it easier to manage Feast features and manage Feast contu constructs all right so taking a step back um it's not just applicable towards feature engineering although that was sort of the bread and butter and how it started but it can improve code wherever Python and data are involved so for llm
(4:21:49) pipelines you've got uh we've got users online writing retrieval augmented generation and fine-tuning for ML training pipelines a lot of people writing sort of training pipelines you can see that's not actually that different from what we showed you earlier and for sort of General data engineering pipelines we've got all sorts of Integrations with pandas P Sparks poers ET ET and it's pretty complimentary with most existing infrastructure it's really a way of organizing your code and isn't imp isn't
(4:22:12) doesn't really care about how you run it so if you got airflow DBT prefect dagster Etc it's sort of complimentary with that um it kind of is a different way of doing chains in L chain although you can use them together nicely if you want uh pypar is complimentary we have a whole blog post and a whole integration about using pypar with kedro it's sort of complimentary to the structure if you like the way that kedro is organized and replaces uh the sort of way that kedro builds dags for Ray and dask it's very
(4:22:41) complimentary and the goal is it for is for it to uplevel your sort of software engineering skills so that if you so that you're thinking less about infrastructure and logic at the same time and can sort of separate them out finally a note on open source progress and updates we've got a thriving Community this is uh we have 110,000 plus downloads these are a bunch of users that we know from uh just having chatted with them there's a whole host more that we know and others that we sort of talk to but they're all using
(4:23:09) it for sort of critical models in production we have a growing set of core contributors a full company dedicated to building it that's us we're looking for contributors we have a whole October Fest thing going on we've gotten a bunch of Engineers who've wanted to really sort of like start contributing to open source and a bunch of tickets that are easy to get started with we're looking for bug hunters and use your feedback nothing better than uh the more people telling us problems and things they like
(4:23:33) about our code the better we can make it all right in progress we're working on more expressive apis so flexible loading and materialization I didn't show it here but we have all sorts of ways to decouple the loading of the code from the actual Transformations and saving of your data new higher power decorators is really exciting um ways to sort of perform macros and make it uh extremely expressive and we're always listening to new ideas for the execution side we're thinking of building out the ability to
(4:24:00) compile a Hamilton code to say airflow so you write your code in Hamilton which is easy to think about press a button and it gets compiled down to the right airflow dag snow park integration and a whole host of ideas um we've had a bunch of good user interaction here from open source so give Hamilton a try we'd love your feedback uh try hamilton.
(4:24:19) DAV EAS way to get started you can install it on piie please give us a start GitHub uh if you're if you like it we really love knowing that people are sort of like what we're working on you can create and vote on issues on GitHub join us on slack and for those later there's a link to a blog post that goes through this talk and code to play with so you you can download all of this and get started and sort of adapt it to your case all right so thank you so much I really appreciate your time um really excited
(4:24:43) to be here among everybody really interesting talks here's links if you can if you want to like yell at me online um QR code to the GitHub repo thank you so much that's brilliant thank you Elijah and it's great to see an in-depth uh discussion of some of the capabilities of Hamilton one of the things I we have a little time for qu a question or two one of the things I really like about Hamilton um is that particularly with new data scientists who maybe are used to writing notebooks and and they don't you know Factor their
(4:25:12) code out in their Transformations into separate functions is that we can easily introduce unit testing and P testing you didn't go into too much details that about that but um you know and you think it's more important as an enabling technology or or where do you see that the testing come into it as well with Hamilton so we have a blog post coming out soon about testing that I'm really excited about um I think it's really important I think that like you can easily get into the like testing too much thing or testing too little
(4:25:41) it's either like you're not doing it at all and then you start like testing all the things you don't need I think Hamilton can help you sort of identify critical flows and then separate out the logic from the sort of glue code behind it because I think the big the hardest part in testing is how do you mock what do you mock how do you like handle external services so that's where I think it's really yeah enables people to get started with that it sort of offers a whole new way of thinking about it and
(4:26:04) can make it easier and and I guess another area where um we often see in in feature stores being used and and it's it's the flip side of feature engineering is batch inference so I could see you know a very good role for Hamilton for runting batch inference applications where you're you're taking your data pre-computed from the feature store but maybe you've got model dependent Transformations and so on have you you looked at that area for Hamilton much oh yeah um I mean that was one of the
(4:26:33) original use cases doing batch inference but I think but like yeah I mean I think we kind of talked over that a bit in the talk and uh right it allows you to take sort of the smaller mini batch stuff you might run in streaming and then apply that and there's a really cool thing that we did with spark in case you want to scale up where you can sort of run your B map operations in pandas as udfs on spark and then handle p uh spark operations sort of keep stuff similar between different contexts brilliant thanks million Elijah
(4:27:01) and and with the mention of spark um I think we'll move on to uh the company behind spark data bricks um Elijah you'll be on slack I hope if there have any questions we can get thank you so much really appreciate Jim brilliant so um without any further Ado we'll move on to the last talk in the session it's dat bricks um we' had dat bricks talking uh before at this feature store Summit so we're delighted to have you back again um we have two speakers um we'll have we have firsty AK
(4:27:32) talati and she she is an MLA engineer working on building the features store for the ml platform at datab bricks prior to datab Bricks she worked on distributed systems and ml infrastructure at Google she's a masters in CS and is passionate about encouraging women to pursue careers in Tech which I think we all can agree with and we also joining her we have M mingyang gay and he is an MLA aai engineer working on building the feature store for the ml platform at datab brakes before datab bricks he worked on
(4:28:01) conversational platform at Google Assistant delighted to have you and um let's see if we can we can get the screen sharing going awesome thanks thanks Jim uh and uh um as as Jim said like my name is akriti and I'm the tech lead for feature serving at data breaks hi everyone my name is minia and I'm a software engineer working on feature store at data breaks um and we are here today to talk about how data breaks feature and function serving can deliver personalized context for llms uh llms are all all the talk now so uh
(4:28:38) let's get started on that um we are very excited uh for the agenda uh today to share with all of you uh first we want to discuss like how traditional hotel booking websites are built we are just taking like an example of hotel booking uh since it's uh easy to follow uh for us um and then the next we'll talk about how we can transition the website uh to an llm chatbot uh we'll then share like current shortcomings of an outof boox chatbot and how structured Enterprise data can fulfill those gaps and uh
(4:29:13) finally my colleague will take you through the demo of how datab bricks feature and function serving Powers the llm chatbot uh and we'll close off the talk by diving into conclusions and any questions you may have for us okay so let's start with how a traditional hadal booking website works for this website like typically it's built on top of a mobile or web application that takes in user city of interest and booking dates it then performs a personalized search and some ml based ranking of the search results
(4:29:47) for the logged in user it is pretty good uh but it cannot capture the users's intent as well as an llm can and in order to capture the user intent better and to provide an intuitive user interface we probably want to build this website with a conversational board that can help with these so let's take let's take a look at how to build personalized chatbot for hotel booking website by using a large language model and we'll see how leveraging the Enterprise data uh that we all know very well about like can
(4:30:20) make the llm chatbot even better uh with personalized context so uh before we dive into that like I want to take an example of what the interaction with outof box chatbot without personalized context would look like and um for that I'm using an outof boox uh conversational model from open AI uh and I asked the chatbot to create a 7-Day vacation plan for a user on my website they want to travel to Amalfi Coast in September so here's what it responded with it said sure I can help uh however I am an AI language model and I don't
(4:31:00) have access to your personal information and it suggested me to contact the travel agency to or use a travel booking website so without any fine tuning or additional context it understands really well that I'm planning a vacation it also understands that the I need to book flights and accommodation but what does it not know it doesn't know anything about the user like what kind of vacation they generally book what is their typical budget what type of accommodations they prefer um but we are all feature store Enthusiast here and we
(4:31:33) know exactly what Enterprise feature data uh in any of the feature store has so uh let's take example of data briak here datab braks already has like all this Enterprise data uh that I need to power my llm and I can use the feature and function serving functionality to serve business data and functions at a low latency in uh in a real-time environment and all I need to do is store the Enterprise data and serve it using one of the serving Solutions so uh let let's see how we can get this information from data
(4:32:10) breaks uh let me Define uh first like I I'm going to be using lak house uh and unity cadog so uh a Lakehouse is primarily like a combination of a data lake and a warehouse to power different workflows such as business intelligence analytics machine learning and unity catalog is uh a a a catalog for governance of the data so uh data brakes lets me use use the entire lake house as the feature store uh what I would do is in unity catalog uh any Delta table that has a primary key can be used as a feature table uh we'll see that later in
(4:32:47) the demo as well and um also Unity catalog makes it possible to access the data from all of my workspaces whether they be staging uh Dev or production environments and uh Unity catalog allows me to specify permissions for all of my data so this hugely simplifies governance model for the data and I can see all the lineage uh upstream and downstream uh for the data and the code that I have so now that we have the data in one place uh let's see how we can access it in real time at low latency so uh for that I'm going to use
(4:33:25) data breaks feature and function serving today uh which supports real time and low latency serving of data and real-time calculation of any on demand features it's supports running arbitrary python functions for calculating realtime features or context features um and it lets me do arbitrary function chaining uh realtime feature serving solution hugely increases the contextual understanding for uh better chatbots uh let's see how I can power the chatbot with the context data from feature and function serving that we
(4:34:02) just talked about um so bringing everything together uh my users feature data and hotel data along with the custom code to calculate discounts is part of unity catalog I use feature and function serving to query the data at low latency next I build some tools for Lang chain I'm going to be using Lang chain here today to call the feature and function serving to fetch that data and embed it into the prom to provide personalized context and finally all of this gets built into the chatbot that knows when to query which
(4:34:36) data so let's see what that llm uh workflow would look like here uh when I ask the user uh when when the user asked the AI bot to create a travel plan for them the chatbot will ask first like what do I know about the user um so here chatbot will query feature serving to find users budget or other preferences uh for example like how many people uh does the user generally travel with and what are their personal preferences next it will ask like what are the best hotels based on users personal preferences and for that this
(4:35:17) will it will do a top case search using Vector search tooling for hotels based on uh users budget and other preferences then it will uh calculate like What's the total amount for the trip and for that uh it will uh again use the feature serving to get the real time time availability and prices for the preferred hotels from this it calculates the total price on demand using a custom function that I have defined in the lake house then it calculates the Final prices after doing some discounts based on the membership
(4:35:53) type as you can see on the right the that data braks allows me to do arbitrary function chaining for all the operations I need uh to calculate the final cost of the trip here okay enough talk uh so now let's see this in practice for the hotel booking website and for that I would like to invite mingyang to do a demo thank you AR yeah cool so uh in today's T demo we will build a oh sorry can we share my screen oh yeah thank you yeah so in today's demo uh we'll be building a hotel recommendation app with
(4:36:41) contact data enhanced the LM um let's first have a look at the goal we want to achieve today in this demo let's say you have a conversational um app and the user ask um ask you to plan a 7-Day vacation to a mfie coast around September for user ID blah blah blah um for comparison I copy over the response from the anguage models I'm going to show you in a little bit so uh for generic LM assistant it doesn't know anything about the user let's say let's see what can can it does right so it ask
(4:37:19) sure I can help with you uh with a plan uh in September in Coast but it doesn't know anything about the user and it doesn't know anything about the hotel um so it ask for details it's pretty smart but it's not super helpful so what happens if we enhance it with context data with this enhanced version it can say based on your preference here are some Hotel available in monthly Coast for a 7-day trip and it list some uh uh like uh available hotels with all the price calculated for 7 Days stay let's see how we can build
(4:38:00) this cool this is a uh a little recap of the diagram I already show you earlier um so besides the L chain and LM part we simply have these three blocks to build to provide context data for the language model in the first block we have a table look up that lookups the user preference by the ID the second block is a vector search API to call to return some relevant hotels and the third one is a little bit complicated it has a table lookup and after the lookup it gets a value and it apply two functions um to do some calculation we all know language
(4:38:42) model are not super good at calculation we have to do it with some help of the feature serving endpoint uh in here you can see all these table lookup and function are trained together yeah in your application you can arbitr train them in any way you want to have a dag to to fit in your custom logic um and all these tables and the functions are stored in unit catalog so unit catalog is the onetop storage that store every entities you have to build to train your model or build an application uh let's jump to the UI to have a quick look so
(4:39:16) this is a catalog explore UI um and I have some tables already stored in my catalog um uh any table if they have a already have a primary key they will be a fature table already this table have a US ID and some budget preference and we're also going to use some functions in this example for example this one you can see the custom logic is built into this function in Python okay let's uh head back to the notebook and see how we build them how we put them together uh for the time constraint I will skip all
(4:39:55) this setup and straighte to the language model okay here uh just to prove I'm not making things up myself so this is a basic uh generic llm model we are going to use um we are using open AIS chat like GPT 3.5 turbo without any customization at the first version We initialize the agent now we ask this question to plan a vacation um at some time for user um it says like I can plan I can make the plan for you but I don't know anything about about you now let's enhance it with context data so with the help with L chain uh we
(4:40:44) just need to extend this base tool and create this several tools um to fetch context data as mentioned before we need to have three tools to query the three apis the first one is a user budget preference tool um called the first uh serving endpoint we give it a name and a description as a hint um for the language model to use and the business logic is pretty simple it's basically a like a HTTP request to the end point we create and host on data brakes uh we give the user ID as the input and the output will be the the
(4:41:21) budget preference for the user the second tool will be a vector search API call which is pretty similar uh it will return the relevant hotels for the like the user preference and the third to tool is also a call to the feature serving endpoint we create an H on data brakes um this time the input will be the hotel ID and the days uh like number of days the user want to stay um and the result will be the final price so this uh serving endpoint will do all the feature lookup and the uh function evaluation under the
(4:42:00) hood now we have all the building blocks ready let's put it together so we basically list uh like put all these tools into a list and pass it into the agent then we give the agent a hint of the system message uh just to describe how these tools can be used uh you can see like we are saying uh please use the budget preference from this trusty Tool uh and we you can also use the uh the hotel IDs and calculate the total price yeah it's not like it's not code with natural language but language model
(4:42:36) is able to understand it okay cool now this uh AI bot is ready let's ask the same question you can see the different the response is different now the language model basically uh first query this first end point to get the user preference and then uh it query the vector search API to get the relevant hotels and then it calls the search endpoint to do the uh the extra lookup and the calculation uh given the our like provided user defined python functions now uh as a result like it list all the available hotels and
(4:43:18) calculate all the price it's pretty powerful right cool this is all I I need to demo today heading back to you arti thanks mang if we can switch to the uh presentation all right yeah um okay perfect so thanks mang for the great demo uh we saw how structure data is critical to providing personalized context for llms and chatbots um so uh uh we saw how feature Engineering in unity catalog uh makes the entire lak
(4:44:22) house a feature store using feature Engineering in UC for llm chatbot helps govern the data presented uh to llm and it makes the lineage of of the data and code available at my finger also on top of that uh using the feature and function serving helps seamlessly deliver personalized context from the data breaks Lakehouse at low latency and in real time uh you can calculate any ond demand features using custom code uh with feature serving and uh you can chain these custom functions together to provide realtime cont text
(4:45:05) for the chat board so uh essentially like all of the Enterprise data Powers the llm uh to provide custom and realtime context uh that makes it it much much more powerful uh than an out of box llm uh so uh thank you for attending the session today uh we are now ready to take your questions and we'll be on the slack Channel after the talk if you want want to talk to us brilliant thanks very much and thanks very much mingang it's great to see a demo as well right to to make it make it real I think this is fascinating because
(4:45:45) this is obviously a very exciting area personalized mlms um and what you see in the community is a lot of interest in Vector databases uh as a place to probabilistically search for data that will help enrich your your prompt but you're making the case that structured data and data from The Lakehouse is as important or more important how would you compare both of the those as as potential places for for um personalizing llms yeah that's that's a great question there's a there's a lot of interest for
(4:46:22) both structured and unstructured data and I would uh basically say that uh structured data provides a very useful context to the llm that uh that uh is is required right like as we saw today uh we needed unstructured search for basically like looking up uh if you take the example that mingyang had shared with us we needed that to look up like the hotels but there was a lot of data that is already available in the lake house that we could event that we could use to provide a lot of uh context information to the llm that it would
(4:47:00) typically not have and it also like helps us ensure that uh if I'm using like an outof boox llm from a third party provider I am very concerned about like governance and security of uh the data as well and so like this ensures that we are not we we have full control of like what data gets shared uh in what Manner here so we have a question on the chat here from Andre Barry um and he asks what would be the format of the user preference in the example how would you build it from your user interaction great question uh mingang is
(4:47:42) the do you want to take this one yeah um yeah so in this demo uh for the user preference for Simplicity we are uh we only have a a the user budget preference which is a estimate of the user like how much they want to spend for a night um this is a integer or double value in our lak house uh table and it has the user ID as a primary key and it can be then it can be used as a normal Feature Feature table that can be looked up yeah and uh to add on top of that uh what we essentially like uh couple of more examples you could think of is uh
(4:48:25) you might have like meal preferences for the user essentially like think thinking about it in terms of like traditional mlops like it has lot to offer when you transfer all of that knowledge to llm Ops and uh thinking about it from that perspective like uh number of people that travel with uh number of the type of hotel that this person usually likes like embeddings for those all of this data can transfer over uh for your user preferences here and uh to answer maybe your second question which is about how can we build it from the user
(4:48:59) interaction so that is uh the prompt engineering part that uh mingang had showed earlier in his demo where uh you would customize like the description uh so that the tool is able to provide all of that information uh to the llm chatbot as well as uh the overall system description that you provide that helps build all of that information into it I I have another another question we have another one on the chap I'll ask my one before I take that one is that one's we've already answered um so when we use
(4:49:31) the term prompt engineering um often we'll take the prompt PRP and then we'll go to a vector database to try and find relevant documents that's kind of the Assumption is that it's prompt and you're using the prompt only but you showed that you can use the user ID and in many websites you're going to log in or you'll have an order number as a way of identifying the user and I see that is the portal that opens up the Enterprise data to integration into your prompt so what should we call that we
(4:49:59) can't I would I would not call it prompt engineering I would call it you know session ID user ID plus PRP engineering I don't know I I that's a great question Jim like I think uh you you bring it up like a very good point like I think that's the whole point that's why we wanted to bring this up conversation like here in this community and say that this is not uh this is basically like integrating your structured data into the LM um so providing it like as part of and and there is like more we can all like do
(4:50:35) together in this journey and um that's why like I think this is more of like a tight integration of structured data uh similar to how um Vector searches are like getting integrated uh into llm chatbot QA bot Etc brilliant okay there may be more questions on the slack so jump over there thanks very much we're going to take a little break after the break we and it's going to it's going to be a six minute bre Break um we're going to have um a bunch of great sessions lined up we have feature form fennel Uber feature
(4:51:12) bite and H2O so um back in six minutes time I see you very soon all right welcome
(4:57:26) back we are live um I'm delighted to welcome Simba Kadar Simba is a a real character in the field he's been around for quite a while and he is the founder and CEO of feature form he started his ml career in recommender systems he architected a multimodal personalization engine that powered hundreds of millions of users experience he later open source and built a company Rand their feature store and that's feature form it's the virtual feature store it enables data scientists to Define manage and serve model features
(4:58:01) using a python API Simba has also published he's a published astrophysicist an avid Surfer which I love and he ran a marathon in basketball shoes which is a little bit crazy but that's Simba forward to you Simba thanks thanks for the intro um yeah um I'm gonna share my screen um really excited to be here again this year um and assume you all can see this or someone will yell at me that they can't yeah we're good awesome so yeah feature stores it's not not just about storage um I would argue and have argued
(4:58:37) that feature stores it's kind of this misnomer that um we're kind of are sort of stuck with because it's a very hard thing to change the name of a category after it's been created but I think that when people hear feature store they think of a specialized database to store features and I don't really think that's a problem to be solve when most people think of feature stores they think of something that looks like this we call it a literal feature store um if you've ever used something like Sage maker or data
(4:59:09) Brick's feature store um it kind of has an architecture which sort of mimics this again everyone does a little differently but I would say generally it looks like this and how that looks like is you would take your data sets you would transform them to build your features and you treat your features as this kind of final artifact that goes into uh the feature store and the value there is that all of your features are in one place they can be served both in kind of olap style fashion for training and in transactional fashion of low latency for
(4:59:49) inference for models that are um interactive like a recommender system or a fraud detection system um the problem here which I'll get into is that the feature is treated as this final artifact like we're treating the feature as like a row of data when in practice the feature is almost always actually tied to the definition this idea of the transformation and if you don't tie the transformation to the feature you end up in kind of this High friction area because a lot of the value of features and stability like the point of the
(5:00:28) feature source to uh increase the duration time or rather decrease the duration time increase the number of iterations and if you have to constantly switch back and forth it causes a lot of pain in practice um especially for a production feature we have a set of features that are batch we have a set of features that are streaming and we have a set of features which can only be produced at request time or on demand features these would be things like if you are doing fraud detection you might take or let's say
(5:01:05) actually you're doing um something where you're checking a a comment for hateful speech there's some features You' probably create off of the comment itself that you can't pre-process that right like that has to happen at request time and that would be the case for non- demand features there are a lot of features which um let's say they have like Long window time Etc which can be done via batch and um there's a set of features which really benefit from having near real time upto-date freshh
(5:01:37) features so on the left side these are features that can be pre-processed and the kind of decision tree of whether to use streaming or batch is really how much um does freshness impact the um features or the model's performance um or that features performance towards the model great so let's talk about the actual workflow of building features a lot of people when they think of feature stores they think of kind of that production like I built my feature now I store in the feature store but again lots of the
(5:02:17) point of of building features is the kind of constant iteration of features and this is actually what I see in practice even if you're using a feature store you end up with something that looks like this so let me break it down there's this fence in the middle which I so artfully put together um but I'll start with the top part which is the experimentation piece which is actually where most of the time is being spent like where most of your time as a data scientist is being spent in that world you're working with
(5:02:44) a lot of notebooks you are interacting with um your infrastructure to build um or maybe even just working locally to come up with signals that you feed into the mall the features some of them are interesting some of them aren't you may be interacting set the bass grips airflow jobs Etc but once you find something interesting what I see in practice a lot is you actually have to throw that notebook or something over the fence to some other team that has to then go implement it and no one's happy because often times they screw up and
(5:03:20) oftentimes they're like what is this kind of garbage notebook that I've just received I have to somehow like cross right into Scala for our production pipeline um so this is where a lot of the friction comes from and just having this kind of literal feature store this place where you just store features doesn't solve this because you still need to handle cases where you have streaming features you still need to handle on demand features which doesn't really exist in the experimental function it's
(5:03:50) almost like you're still having the problem of riding two different pipelines and oftentimes even having two separate teams to handle building or coming up with features and getting those features in production um if you have dealt with this then this might resonate one of the big problems is that I think as uh mlos practitioners we really get caught up I mean just as Engineers I think we like working on what I would call the data processing side of things how how do we do streaming faster like how can we use
(5:04:36) whatever Cutting Edge new framework we can use rust to make this you know 10% faster to handle however many exabytes of data the thing is though that most of the problems that I see companies face is actually more of a workflow problem how can we get n data scientists to work together to collaborate how can we make us so at the workflow of getting a a feature all the way from from experimentation ideation to production to be something that is very um clean has Clear Flow has clear um um history lineage versioning
(5:05:16) governance monitoring all the pieces that would be required how can we get all of that to work together seamlessly it's not necessarily that um that uh the problem is so much that oh spark isn't fast enough if only we had a better spark or if only we had the better flank it's really how can we make all of these desperate pieces work together in a workflow that is coherent from end to end it's a workflow problem and I think most mlops problems are in practice workflow problems and not infrastructure
(5:05:51) problems and I think that we get too caught up with building new Cutting Edge infrastructure and you know solving people problems is much tougher it's really hard to like come up with the right apis and the right abstractions and I think that for the feature store case we've come to the wrong abstraction and I think that it's kind of time to rethink what the feature store is um I know ver I talked earlier about the logical feature store we call it the virtual feature store and I think that is kind of the direction that as a a
(5:06:22) a a an ecosystem we need to move towards these are things that traditional features stores don't solve like sure you can go and you can write your feature to this new sync which is a a feature store but think of all the things that you did on the way your Untitled notebooks your DF final finals your feature table V7 that's in Snowflake or wherever even if it's in the feature store it's still called feature table V7 um you have these random scripts to like trigger materializations these ad hoc spark submits
(5:07:03) all these problems and no documentation it's not really built into the workflow because the feature itself is an artifact which is separate from how it was created so even if you find a feature that may or may not be relevant you don't really know what this thing is how it was created where it came from how to rebuild it you're kind of in this situation where technically your feature is now in production you can train a model of it you can serve for inference but the workflow is just broken like
(5:07:30) it's just completely ad hog um if these things just few more memes um I promise um if these things look familiar to you then you likely are facing something that may or may not look like what I'm describing this is not something or a set of problems that is um unheard like unusual or something new in fact it's interesting if you kind of roll back time and look at the first feature stores that were built internally at companies like yeah companies like uber and my last company we built a future store they would look
(5:08:15) like we call it a physical feature store some people call it feature platform but the idea of tying the transformation to the storage is actually the more common um Paradigm um that we see even in companies that have lit control feature like even if you look at like um companies that have open source feature stores it's actually only one team and the other team did the transformation piece so internally even though there's two separate pieces it still looks like this and the idea of this is now you're
(5:08:47) actually iterating on features as logic and not iterating as features as like this row of data that gets outputed after your pipeline um this really makes things better especially when you're dealing with um streaming Fe features real time features um but just even in general just the iteration you get the lineage automatically you get all of the stuff that you would expect and the workflow becomes very clean but there's another problem here this works really well internally in the sense that if you have a team that
(5:09:21) builds this thing and maintains this thing you can get away with rehousing all of your data on your own internal platform however um as an external vendor um if you're looking at using an external vendor like let's say I come to you and you wherever company you're at you're a larger company let's say and I say hey to use our feature store so that you get all this ability to Define features to manage features to to serve features to do all the things you would expect you need to actually move all of your
(5:09:56) data onto our infrastructure and then you need to uh transform it via our infrastructure and we kind of everything end to end It's A Hard Sell um internally if you're an internal team you can you can do that you get more Buy in because in the end you own the thing um it's your code as a vendor it's it's a hard pill to swallow and in practice there's only so much value you get from doing this right because your problem it's the wrong puzzle piece your problem is not that oh we don't like
(5:10:27) redus redus isn't fast enough or we don't like spark we need something else we need to build something that is completely new a whole new type of spark or a whole new type of redus which is better than what exists today um I don't some people may have this problem but I would say that the majority of companies that use us that we talk to don't that's not really what they're facing another kind of common thing that a lot of the you know feature store that I've seen talk about is making everything
(5:10:58) streaming and I get it like theoretically like streaming is a super set of batch um so like every batch problem can be actually defined as a streaming problem in almost every case of a feature in practice though like that's like a nice theoretical World in practice batches way easier and if you can get away with batch you should probably use batch and only when it makes sense to have streaming should you use streaming and in practice what we see is like 5% about on average of features at most large uh companies are
(5:11:32) actually streaming um in practice almost everything is batch and there's not really much value in taking those batch jobs rewriting them making them all streaming finally the last kind of wrong puzzle piece is that oh what we're missing is this new type of storage layer which is this where we put our pre-computed features that's a feature store as the name implies and again it's just not really solving the problem of how do I iterate quickly on features how do I keep features reliable how collaborate on things which would
(5:12:03) require having things like lineage and governance and Etc um it's really just like a new type of database which going to the first point isn't really the problem to be solved and as I was thinking about this and thinking about the fact that hey we built a physical feature store that feature platform but in practice it's not what people really need in my opinion like externally as a vendor I just don't think it makes sense to force everyone to rehouse all of our data onto us and for us to actually do the
(5:12:39) Transformations and to create some new DSL that every data scientist has to learn so we came up this idea of a virtual feature store where you have a declarative API which you can Define and manage your your features in literally where the idea of feature form and the name came from was I want a terraform for features and that's kind of what we end up building something that looks like terraform but built for feature pipelines from end to end so you can Define your features Define your labels Define your training sets Define your
(5:13:07) Transformations you get built-in collaboration versioning Etc you get monitoring alerting built in pretty much add this layer this application layer on top of the feature store infrastructure and use an orchestrator to make the underlying infrastructure work like a feature platform that was kind of the premise of this and it works really well because what we're trying to solve here is solve the the workflow problem how can we make all this stuff work together cleanly and you can solve things like point in time
(5:13:39) correctness you can solve things like backfill Etc without having to actually replace all the infrastructure um especially in situations where companies have like an on pram and an incloud or they have situations where they have you know Dynamo for some features red this for others like there's way more complex deployments in the real world and really what people are looking for is one paint of glass to Define and manage and service things and that's what terraforms promises for infrastructure and I think we needed the same thing for
(5:14:07) features and that's the kind of premise of the virtual feature store and again to the point of the thing the feature store in this case is almost a misnomer it's not about storing features in the sense of like a row of data it's about storing features as this uh definition treating like a definition language and building in all of the components you would expect to exist in an ml feature around an ml feature from governance to monitoring to collaboration um yeah a little uh a little quick but I actually left a little bit um of time
(5:14:39) for for questions I can kind of go in a lot of different directions here depending on where people want to go brilliant thanks Aman uh Simba great to have you back again um can you tell us a little bit about the open source um uh angle of uh feature form you know I think many in the community have looked at uh Feast as being quite successful but we seen less development let's say on feast in the last while and I think you guys have been growing quite a lot so what what's your experience with um working as an open source feature store
(5:15:12) yeah I think one thing that helps us on so we are open source um we're open core more I guess accurately and um the things that aren't open source like like we are a business we do make money the way we make money is we have purposely not open source a few components the components we don't open source are one governance and two stream stream processing so if you need governance or stream processing then that's only in the Enterprise product in the open source product we do everything else and
(5:15:42) from the beginning we were very clear where that kind of pay wall was and that allowed us to really commit fully into open source and not have to ask the question all the time of should we open sources should we not because we've from the beginning been very clear and it's been the right cut because if you care about governance and you really really care about streaming in real time you probably are in a position where you want to work with um an Enterprise solution um and if not then there's an
(5:16:05) amazing open source solution for you too so the short is that we from the beginning knew we wanted to be open source we view open source as kind of the right there just needs to be a strong open source player and even more so now to your point seeing that Feast is kind of um the maintenance of it has dropped significantly um over the last uh few quarters I I I have another question just because you know you're very active in the community and um you know we mentioned early have a python API what's your take with the I mean feature soures
(5:16:41) originated in the Enterprise it originated with big data with spark and Uber and and and now we're seeing kind of at least we're seeing some movement of of ordinary let's say data scientists and pythonistas to seeing the value feature sour how far do you think where we are along that journey and and and how long do we have left to go we're super early I actually think that um for feature stores to become what I think they'll become and how valuable I think they are feature engineering is such a core piece
(5:17:12) of the process right like the same way that pie torch is just something you would just always use if you were using that sort of if you chose that framework I just view that something like a feature store um would exists in the same way that softw uses G like it just I view it as something that is as fundamental as like using notebooks um the problem is that all the most of the feature solutions that exist we have like a local mode that you can actually pip install feature form start using it with zero infrastructure you get all the
(5:17:42) versioning all the things I talked about and it's all running on your local laptop um without deploying even a Docker container and uh my view of the world is that that's what it needs to be like because for experimentation experimentation has to be a lightweight process production is a very heavyweight process but we want to that Chasm and I think the framework needs to really do both and I think everyone really gets focused on the production part which is really heavy in Enterprise and I think that there's been a huge lack of the
(5:18:13) open source side I also think this is why a lot of data scientists hate emops where like ml platform teams are like yeah like here's another tool for you to do and data scientists are just like holding their head like another tool like we think a lot about the data science workflow and I actually think that unfortunately that's like a unique thing U amongst vendors a lot of there's that mad map has like five million companies on it I always joke that if you were to take that set of five million compan or however many
(5:18:40) companies are on it and limit it to the number of companies that people actually love to use like truly love to use would choose to use it every time given the option it would drop to like 10 15 products um I mean we I kind of have a take on this as well like we we had a couple of Point um speakers earlier talking about some of the complexity inherent complexity particularly in M Ops and as you know mlops is supposed to be like devops and the idea of devops is to start small build something small get it working and then incrementally
(5:19:11) improve it with confidence that the changes that you make won't break the system but where we got to as a community with ML UPS is that we kept adding more so I think Fabio earlier showed the effectively the the the canonical reference paper the most cited reference paper from mlops has the most boxes it it's it is it is the water fall life cycle development model and it's calling itself mlops and I just find the irony in that is just kind of uh there because as a mental map nobody is following that mental map nobody I know
(5:19:45) of I mean there I don't think any company in the world has implemented all those boxes in the system so how you know how do we reset this how do we reset like the the the the way we build machine Learning Systems so that they have the properties that we want they're reliable that we can incrementally improve that we can have good processes for developing them what do you think the way forward is I think it's a great question I I don't I won't claim they have the answer but I do think that we
(5:20:12) can learn a lot from devops and I think like I mentioned terraform which is is a product of Hashi Corp and Hashi Corp is one of those companies that I find to be very fascinating because it's like seven open source projects it like grew faster than slack Revenue Wise It's Kind of a weird company um in this in that sense in like and like how it's defined um it's not just a simple platform or one product but I think it's totally correct for devops because like mlops everyone does it so differently and there is in
(5:20:43) my opinion there's no such thing as like an ml platform that like works for everyone or even works for even 20% of people I think if you build a platform you're cutting to like 5% you might solve all of our problems way better but really everything's so different in the same way devops is what you need is like a tool belt and you can pick and choose the tools and sometimes maybe the tool that you need is a bash script because it's just like that's that's it it's not that complicated like you just I just need a
(5:21:10) script here in other places you need a feature store maybe you don't and I think like being able to build that way and I've always viewed that this was always going to be uh choosing the best in-class vendors mix of like um kind of your own um pre-built simple things where needed and you can kind of that's how I think it should be and that's how D Ops is done and I don't know why in mlops we really treat it as like you need to have like the Ferrari or you need to like or you have nothing I think that the in
(5:21:40) between is missing and I think that's because everyone hasn't built a lightweight Solutions I know we have and we try to but I think there's a lot more to be done okay brilliant thanks M Sim it's been great talking to you and great to hear you uh you know evangelize the virtual feature store concept again and and great to hear from feature for Forum thanks um there may be questions for you on the slack so jump over there and and see see what's cooking we'll do moving on sorry I said thanks again for having
(5:22:08) me of course of course brilliant um now we're moving on so Simba has talked here before at the feature store Summit but we have uh nikel gar who's going to talk here for the first time Nik gar is the co-founder CEO of fennel H at real-time feature store built by an ex Facebook team and previously he was at Facebook where he led teams behind Pine torch in a previous life he was an avid computer programmer and represented India in the finals World Finals with the ACM ICPC twice so you're no longer a programmer N
(5:22:40) I still program I I I I still write I mean uh my my team would tell me that I should not be writing as much code as I'm writing right now but I still managed to sneak in a PR to every now and then I I had the same problem for a long time anyway really looking forward to this uh thank you so much for having me let me share my screen um do you all see my slides wonderful great um great to meet you all um apologies in advance if my voice is a bit broken I'm still recovering from coid but I think I'm all um you know
(5:23:11) patched up enough uh I want to start with you know my journey of feature platform uh you know it started several years ago I was at Kora back then I started the machine learning infrastructure efforts and I gave a talk in 2017 talking about open source ml systems that need to be built out um and I talked about you know you know two layers of the uh very nent mlop stack one was Model Management the other was feature extraction framework uh and I went on to describe what I thought you know based on the needs we had already
(5:23:43) experienced at Cora what a good feature extraction framework may look like those were the early Wild Wild West days you know you can see I was calling it feature extraction framework the term feature store did not exist yet back then and so you know I gave this talk I presented what I thought were you know obvious ideas and I moved on with my life um I from there went on to you know work at Facebook uh I at Kora I read several quality abuse risk related teams at Facebook I was uh closer on the rexus side and some on you know Integrity but
(5:24:16) not fraud space really uh eventually I ended up you know uh running a large part of the team behind pytorch as well and each of these experiences was you know quite transformational in many ways through Kora I first started understanding the problem from first principles at Facebook I saw you know what three generations of a you know solution for this looks like you know kid you not Facebook has I think as of right now over 100% organization working on the feature engineering system I saw real time you know gaining adoption and
(5:24:45) becoming very important everywhere and at py to I truly you know got Doctrine the school of you know beautiful python apis is all that matters and if you can make people productive you everything else can follow from there um when I was leaving Facebook I was you know thinking of doing my own startup I looked around and to my surprise the deck that I had put out in the open in 2017 I still didn't find a feature store as it was called you know uh by then I didn't still find a feature store that more or
(5:25:15) less you know lived up to the manifesto I created and so we just decided to move on and do that and that's what I'll be talking about uh in today's uh discussion you know what Fel does and you know how it is so different from a lot of other options out there and what is the thinking process behind that uh for the first few minutes I'll spend maybe 5 seven minutes not more than that just literally walking you through some code Snippets of what does it feel like to write uh you know features in Fel this is important
(5:25:44) because it sets the tone for the rest of the conversation after that we'll talk about uh a handful of choices uh that are very uh you know countercurrent right now very unique choices and you know the constraints we are taking uh and finally depending on how much time we have we can go a little bit deeper into the infrastructural choice like specific you know uh nuts and bolts of how things work together but first i'm going to um keep sharing my screen but open Panel docs and walk you through a few code Snippets to help you understand
(5:26:19) you know what's the in experience actually feeling like panel has two main abstractions a data set and a feature set I'll only talk about data set right now we don't have enough time for a full walk through data set is effectively a table of data this is a data set called Product it has five columns product ID seller ID price and so on each of them as a type as you can see the schema is written in a identic inspired manner you have some metadata owners and tags and so on and more importantly here we have
(5:26:50) you know system telling us that we should be looking at post table called Product info every 1 minute and using whatever new updates have happened there to hydrate this data set product in in other words this data set product is going to be a near real time mirror of your post table product info and if your data did not live in postr you know we have let's it lived in Kafka we have different decorators for all of those systems all that changes is the source type of data but wherever data lives whether it is you know S3 or you know
(5:27:22) snowflake or red shift or Kafka kesis post my wherever data lives you know how to get it all you need to do is Define the schema in a identic inspired Manner and put the source separator on top of that uh but quality is a very important uh concern for us it's a top three design decision for us and so there are bunch of small Primitives already uh for you to start elevating your data hygiene so for instance you have a very strong typing system uh here for instance you know if we see description you know it
(5:27:53) can be string or it can be null that's fine but if we see a price that is null we would not even admit that data in the data set more generally we have strong typing and so anything that does not meet your types does not get admitted into the data set uh we have new types as well you know we have a type for reject match where you can say that only those strings satisfy this type that let's say satisfy let's say the ZIP code Rejects and only valid zip codes are the type system level are enforced to be
(5:28:18) present in the column so Downstream users don't have to do defensive programming there just one primitive we have many more Primitives here's another one I want to show you right now uh we have native data expectations so in line you're able to describe that you expect the column price to be between 1 and 10,000 in at least 95% of the cases this is a lot like Great Expectations in fact it is literally built on top of that but this is Computing these expectations in a streaming real time as new data is
(5:28:46) arriving and you can set up alerts against that as well now all I've shown you is wherever data lives you can bring it to Fel and start elevating the data hygiene but more often than not you want to derive more data uh using the data set that you already have so imagine this is a data set you wish to derive which given user ID and seller ID is counting the number of orders from that user on that sell in last one day and last one week you a typical rolling window aggregation very common in feach engine this is how you would write that PIP
(5:29:17) plan this is a function in the class you're saying I'm starting from order and product data sets these are the data sets I showed you earlier you're saying if I know these two I know how to derive this out of it and now order gets bound as a function function argument here and product gets bound as a function argument here and you can now describe all your pipelining logic you can join you can transform filter add rows add columns all of that now there are many things that are really nice about this
(5:29:44) I'll highlight a couple and I'll go back to slides after that the first thing that's really nice is it's extremely declarative you are not having to think about you know where should this run how much RAM should be given what if data doesn't fit in Ram how should be partitioned how should be scheduled is it waiting for an upstream job to finish you don't have to think about any of that the second thing that's really nice about this is this is very very python netive this is an actual python Lambda a
(5:30:08) free form python Lambda that will be executed against every Row in this data set this is an actual Panda's data frame and so all of Panda's API is transparently available to you you can import your favorite python libraries and start using them as a part of this Lambda so if you know python if you know pandas you know how to use this and you can bring the full fire power as well the most important thing is this is actually a streaming pipeline it looks really simple to write but as new rows or new updates arrive in orders and or
(5:30:38) products this logic is executed incrementally only for the new data and this data set is updated wherever those updates happen and that process is happening continuously and the exact same code ends up powering irrespective of order itself coming from let's say bad sources like S3 or snowflake or Kafka or you know same for this none of that matter exact same code powers and you're now able to you know bring data from multiple places in the same plane of abstraction and act on that that's all I'm want to show you right now now
(5:31:09) with this you know flavor in mind now let's you know talk about some of the interesting choices we have made that are somewhat counter current right now um to simplify the authoring experience as you can see we let people write pure python with pandas there's no DSL involved there is no py spark involved as well um and so benefit of this is you know people ease of uses obviously there but they can enter off with full python ecosystem as well and this is straight up you know a page you know we took from the by Playbook um and
(5:31:42) our belief is we need to meet people where they are I think what I see a lot of other Technologies in the industry do and this is what tensorflow did as well until you know pyo came and started beating them is build the experience with performance in mind whereas I think what Pyon did was they said experience is important and will solve performance at lower levels by doing a lot of innovation which is what we ended up doing is how do we then solve performance we built everything in Rust we do very fast python rust interupt we
(5:32:10) literally embed the python interpreter inside the rust binaries and some of the new improvements that are coming in in Python but Gill is not a problem anymore it actually turns out to be a lot more feasible than we initially thought early on um on the on the you know data computation side uh you know Simba earlier mentioned that uh most use cases are batched that is true but we also found that almost everyone has some streaming features and you know people are not comfortable having two different Frameworks some for streaming some for
(5:32:41) batch uh and we also believe that the the fraction the utility of streaming is only going to go from here and so what we did was we have no separate badge or streaming subsystems there is no place in the product where you annotate and say this is batch or this is streaming the exact same code works for everything everything is streaming all the time it's built on C architecture and not only it makes the system feature proof it ends up removing a ton of complexity from the end user like for instance you know back filling a streaming uh
(5:33:11) pipeline is so terrible because back filling is inherently batch oriented operation but a streaming pipeline going forward it to streaming it's you know resonance mismatch and so uh you know this completely eliminates all of those uh interaction loans and the way we were able to do this is we literally wrote our own stream processing system written than in Rust uh very memory efficient uh and purpose built for this kind of operation that becomes common um there's another element of real time which is
(5:33:39) you know what are sometimes called as on demand features uh in our world um we as we spoke with more and more companies we discovered that uh you know PE people want features to be able to depend on each other recursively and you know derive from that in arbitrary ways uh and there are also many features which uh you know you have to do some more interesting processing so we came up for this abstraction called Rite separation where our features are not stored data but rather they are stateless functions and those stateless functions run on the
(5:34:10) read side of the request is waiting whereas all the computation uh that is asynchronous we have pipelines is happening on the right path right path is throughput oriented but not latency bound read path is very latency bound but usually not throughput oriented and you now have the ability to keep some computation on right path some on read path all on right path you can you can pick and choose how you want and what this ends up doing is very flexible feature patterns become possible so for instance if let's say in recommendation
(5:34:38) World which is my you know world at Facebook a very common feature is do dot of user embedding and content embedding that's very hard to express um because you have to do that competition on read side you cannot pre compute order n Square user item do products but by keeping it on the read but you know keeping some other competition on right becomes quite natural to express as well to simplify quity this is another thing that we kept hearing we just went ahead and you know made a native um best-in-class tooling and added it to
(5:35:07) the product itself uh and literally what we did is you know we asked ourselves what are all the quality problems we've ever seen in our lifetime related to features and for each of them we asked ourselves how do we design a solution against them can we catch them at compile time can we catch them Upstream if not if none of those preventive mechanics work can we diagnose that somehow can we do monitoring Etc on it um and so what what this ends up doing is as people's features get more complex the confidence still stays up and
(5:35:36) everything from you know versioning immutability strong typing data expectations drift monitoring unit testing all of that you know just natively part of the product itself here I'm showing a couple of screenshots of drift monitoring and data expectations um very contrary to you know the the very previous speaker uh we actually uh to simplify operations we do deep vertical integration we are highly anti- virtual uh we we find that when we are starting to research into this is existing you know feature stores need
(5:36:05) people to bring their own KV stores their own compute engines own spark you own metadata stor sometimes own Kafka as well and you know you could you could make a case that you know customer already has some of them but in practice it is a lot harder than that there's a lot of operation work and more importantly cost work as well you know redis and Dynamo actually do not work for a lot of people at scale in terms of cost um and then even if you know cost is not an issue operation is not not an issue share dependency then leads to a
(5:36:32) lot of confusion like you know who does autoscaling of a shared spark cluster as an example you know what happens if some jobs are failing who's on the on call for that and so this is a very counterculture choice but you know what we have done is whatever fennel needs we bring that up and manage all of that ourselves and there is zero dependency installation for the End customer obviously you know this increases our surface area quite a bit we have to do as good of a job running everything smoothly in a cost- effective manner in
(5:37:02) a compliant manner uh but you know as always every step of the way the choices we take are make it easier for the End customer make it harder for ourselves you know taking a page from The py Playbook so to speak and finally to simplify infoset this is also a counterculture choice we made uh we um have very I mean we we do also have data planes and control planes but our data planes are rather thick and control planes are you know absolutely tiny and as a result of this what happens is our control plane can literally go down and
(5:37:36) your uh your workflows will not be interrupted at all in other words our data planes are thick enough to be nearly self-sufficient even the autoscaling lives inside data plane even the you know the the console UI where you see the code of your features lives inside data plane so your data and the code of your features are never really leaving your uh data plane now all of these choices literally every single one of them requires us to solve hard engineering problems but it ends up creating in in in our you know
(5:38:07) admittedly biased view a much Superior experience for the end data scientist now I probably don't have time to go through all of these but I'll just walk through a couple of these you know things real quick uh as I mentioned we built up our own streaming engine we had to to satisfy you know the constraints that we created for ourselves I'll just walk you through couple of choices related to those uh one very important Cho choice we made by the way this section is now going you know a level or two deeper than the rest of the
(5:38:33) conversation we have built up our streaming system in a way that there is no synchronous communication everywhere it's a full shared nothing architecture you know unlike Flink you know where operators communicate with each other via RPC calls which which has you know some benefits but it has some downsides as well uh fennel does not do any of that communication is either within a node so in memory channels and so on or it is a synchronus mediated via Kafka and since we use Kafka you know it naturally handles back pressure back
(5:39:02) filling things like that uh a single node can go down many nodes can go down and everything else you know continues making progress uh without any Interruption and we also get in exactly once processing for free as a result of this we use uh Rock TV uh for managing the state store of our streaming jobs uh but there are several interesting choices we made here as well all the right sing iteration are buffered in memory and they are applied together at all ically later uh this maintains the exactly once's processing and our jobs
(5:39:34) as a result of this can die anywhere and can wake up on any other node of the cluster and they'll continue working we are taking periodic backups using EFS I don't know if any of you are using EFS or backups but really really good tool for that particular problem uh and we also store some transaction markers in Kafka so that if we have to do recovery we do optimistic recovery they are able to do that effectively uh we are doing continuous checkpointing this is one of the bigger problems with Flink Flink checkpointing
(5:40:02) model is stop the world which means every time you're writing a new feature the topology is changing which then means that you need to stop the world for a few minutes which creates a lot of lag not acceptable in our model every job is checkpointing itself continuously independent of other job so it's distributed checkpointing the local SSD checkpoint happens every second or so so the processes could die and wake up again on the same machine and they just literally pick up right where they started and we do persist consistant
(5:40:30) checkpoints across machine boundaries using object stores every few minutes and so if if the whole machine dies and we have to bring up the job on a different machine it first spends a few seconds catching up and then continues processing from there the benefit of this is the system becomes much simpler for us to operate uh quite quite you know literally built for resilienc in mind a node can go down and there's no issue whatsoever and we can still get fast recoveries uh now contrast this to you know what a customer may have to do
(5:41:00) if they were to you know run Flink on their own and handle you know these kinds of issues uh one very common problem with spark and to some degree Flink in future engineering world is there an out of memory I me they generally happen for all data engineering jobs but particularly for you know large rolling window aggregation jobs in feature engineering uh we have built up a stream processing engine in a way that there is no inmemory state of jobs State always lives on Diss and you know in practice it's built up in a way that uh it it
(5:41:29) lives in either you know the OS you know P page cache or you know block cache of rock may M tables so we still end up not creating that much overhead in reading and writing from it but the state does not you know blow up no matter what and Os you know transparently Pages out certain memory if it is not needed so that it does not go out of memory uh in addition to that we do we don't do any in memory Shuffle operations either as I mentioned all Communications is mediated via Kafka uh we don't use jvm uh we we
(5:41:59) things in Rust there is no GC so memory cannot go up temporarily and we generally have very tight control on RAM and all of it means that SS are structurally impossible and once again you know it's much easier to run a cluster like this compared to you know what our customer may have to do our streaming engine is time aware itself uh you know Flink and Spark are both way too you know general purpose but there's a lot of structure in the problem of feature engineering there's always a notion of time and in fact you do need
(5:42:29) that notion of time to be able to do watermarking handling out of order events time window aggregations and so on so our engine is itself time aware and it you know that is how we are able to do point in time you know feature creation but it now also allows us to do point in time correct streaming joints which are really powerful because you know let's say you have a stream of Kafka events and you want to enrich those event with some information that lives in postr that's effectively a joint between Kafka and postr uh and
(5:43:00) doing that in a Time correct manner is very very hard but making a streaming engine time ofare you know this becomes uh immediately uh easy for us uh there's no Central scheduler everything is horizontally scalable for us uh you know there's a supervisor on each node each note picks up you know divides work and then they just go on their own uh and this is another Choice which means that there's no synchronous communication and no single count of failure uh these are some of you know the tidbits of the engineering that went
(5:43:29) behind in unlocking the C The Experience choices I was talking about you can always read more on a documentation and reach out to me or email as well that's all I have have you take any questions that's brilliant n thanks very much it's uh really interesting to see um you know the level of detail you've gone into and and like I actually work with the Flink guys so I have colleagues Paris cabon and seeri so they were they did the designed the chandel Lamport Global check one algorithm for Flink and
(5:43:57) and wrote the main paper on it so I mean that paper itself the research paper that they published um enabled the the community to build confidence I guess into the the correctness of their algorithms and um you know you've done obviously some Stellar work at Facebook before do you do you intend like giving out the details of your Global checkpointing algorithm and the correctness or um do any plans on on on on filling in more on that we don't want to overtime uh you know write more on that I mean so first of all couple of
(5:44:26) things to say we don't we have a lot of respect for or you know what Flink people did I mean they are true Pioneers you're standing on the shoulders of giants there I think they came up with the they formulated these problems let me put it that way I think problem formulation is a lot harder in solving them usually and now that we have seen a whole generation of people use that for this specific problem you know those are the areas where we are you know optimizing so lot of respect for them uh definitely plan on making more of this
(5:44:54) content available over time we also intend on open sourcing a lot of what described today it's a question of when not a question of if we are a tiny in know company and we only have so much bandwith but yes uh stay tuned hopefully there will be more to share right we we have a question from um SAR sh Shasta on the chat is Python and panas combination able to hander feature generation for large volumes of data pites of data should you be using spark no absolutely it can and once again see the magic is not that we are relegating
(5:45:30) everything to Python and pandas and pandas famously does not work Beyond a certain scale of memory and so what we're doing is all of this computation is mediated via a rust streaming engine and it takes small chunks of data and relegates the control to Python and pandas only for that small chunk but the CH the chunking and the assembling and all of that is still happening in the land and so you know we can almost put you know bounds on how much memory it's going to take and so on and you can you know fearlessly operate this in very
(5:46:01) large data set it's all horizontally scalable in fact as a result of that all right great we we have another question but I think May um from Jeffrey maybe we can take it on the slack um because we're out of time thanks million n has been great and looking forward to see more of fenel in the future thank you thanks for having me folks goodbye great thanks very much so um so drop over to slack if you want to see more ask n more questions or um if you want to see the answer to something more of the questions that are there next up is
(5:46:32) Uber again so we have Uber's uh risk knowledge platforms we have we have I can see Cav here um so Cav serenas is a software engineer in the Uber risk knowledge platform team her role revolves around contributing to Uber's graph database and devising practical solutions that Aid in combating fraud on a larger scale so before joining U her experience was centered on addressing abuse and spam within Yahoo mail or folks on realtime solutions to various abuse scenarios I can see Christopher is here he's a
(5:47:04) machine learning engineer and the risk applied machine learning team at Uber and he builds ml infrastructure tools to assist machine learning Engineers across the risk platform in building effective model training for Fraud and Abuse problems leveraging the tools he creates he also trains and deploys models to tackle some of the most challenging earner abuse issues found at Uber and then we have Jean Kai as well is a senior software engineer in Uber's risk knowledge platform team and she's a key member working on the key large scale
(5:47:32) distributed self-served feature engineering platform for ML purposes and she architect and Implement core components of the streaming computation platform to support various teams across Uber and we're delighted to have you here to talk about I guess use of feature stores in terms of risk so um over to you you can share your screen I think we're a little bit um that's probably me yeah I was zoomed in too much not not everyone else okay try and share your screen and we'll see if if everything's going good
(5:48:00) great thanks so much for the intro Jim yeah so intro but um we can go ahead and get started so yeah I'm I'm Chris uh we have cavia and Jean here and we'll pass it off to Jean to to take us on our uh start of our our talk yeah hi everyone today I'm going to present risk knowledge platform uh together with cavia and Chris and here's today's agenda uh first we'll start with platform overview then Deep dive into uflow and urra feature store then uh later the use case for machine learning
(5:48:35) of risk assessment next so uh why risk build this feature store the motivation is risk needs a scalable self-service feature engineering platform for computing features for predict predictive decisioning and machine leing purpose the use case includes payment fraud promotion abuse GPS moving account takeover and so on next here is the architecture of the risk feature store so when an event happen for example um when user take a trip or user signs up uh this kind of events will hit the risk uh decisioning service and then the risk
(5:49:13) decisioning service will fetch the features from knowledge platform includes Ur graph and uflow so all the features are managed by a feature speack and then uh both Ur graph and U flow have DB log and together with the event log of the decisioning servers uh both will contribute to the feature time travel tool that Chris is going to present later next so here's the and feature engineering flow the data analyst and data scientist will try to explore and analyze what data is useful for building into a feature then they
(5:49:47) will go through the feature Engineering Process and then feed this features into machine learning models and rules and result in potential actioning like ban one or Arrow the l two steps are called decisioning and then the decisioning result will contribute back to the feature iteration next yeah so let's Deep dive into the uflow feature store the uflow cans three types of features batch near real time and real time all the features are managed in the following four aspects uh first is the feature catalog
(5:50:24) it has all the definitions of the features and then we have um next is the feature quality the users are able to access the metrics of feature correctness distribution completeness freshness and anonomous detection when they want to explore and when the users want to debug they can um Trace up and downstream of the feature by the feature lineage and then try to find the feature correlation between each other and when people create evaluate or deprecate the feature then that's the uh when the feature life cycle comes into the
(5:50:57) picture next so here is how our batch Feature commutation work the data source is from hi it can be self-c computed or externally precomputed and then the hi data uh flows uh is dispersed into Cassandra with the support of Auto bootstrap Auto backfield and cicd and uh during the dispersal they will be feature level optimization like for example we only disperse incremental changes to save the computation resource and uh we disperse the data with dtl so it can save some storage costs as well the N1 development is all spec driven so
(5:51:40) it's not engineering friendly next so next is a new real time commutation the data flows from Kafka to various kinds of syns and then uh all the pipelines enables uh feature right lck and um the uh dlq support and all the data quality metrics are handy for people to explore so for some compliance use cases we also enables the end to exactly ones processing for them um so the back field from hive is also supported for this computation and for the um complex use cases we have RPC fanout um early termination is supported for
(5:52:24) optimization uh of performance so for different use cases all of them are uh decoupled for the resource and computation and multi- tennessy supported next yeah so both the mentioned badge feature and neoton features uh support aggregation so our aggregator has aggregation function like sum count and so on and the window can starts from minutes to hour to day or year or lifetime to date um yeah and then all the Computing features will go to Cassandra and will be fetched in our realtime service which comes in the next
(5:53:03) slide yeah so here is our real time service uh architecture it has two parts one is the feature access and one is the feature update so for feature access uh we enables cash for it and hierarchical feature fetching are supported so people can mix and match different gateways different DBS different orders and then uh to achieve the comprehensive fetching and then for some use cases is that after the feature fetching we also update back the feature um yeah and and similarly the r log is also enabled for the feature update in this realtime
(5:53:38) service and for the realtime service we achieve for night availability and this is all for the uflow Deep dive and I'm going to hand over to kabia thank you Jean for that high level overview of uflow um hey everyone I'm cavia um and I work in the ugra platform of the knowledge uh platform team um next slide please so a little bit context about knowledge crafts so we at Uber have a lot of data um about a lot of different entities um to make sense of these entities a graphical representation is very powerful because it helps us
(5:54:15) understand unstructured data and convert it into a structural form by representing it through vertices and edges so the relationship between the vertices help us understand direct and indirect relationship which are not clearly visible from the unstructured data and it's also easier to uh visualize the data and also query it using some of the open source languages that are available in particular this is useful in Risk context where you want to be able to um understand the fraudulent uh activities that are happening which
(5:54:42) are also not easily visible through the data that is as is available next slide please what is ugra uh ug graph is actually an internal term uh and it's an umbrella term that we use for um providing both real time and batch solution to do feature competition and querying uh it supports real time for pointto Point queries and also batch for ml-based feature computation and also bulk queries um mostly used for deriving insights and also facts next slide please so the real time and the batch is what we refer to as olp and olap so I'll
(5:55:22) be talking about the ingestion framework of how data gets inside uh so like how Gene mentioned we also have a spec driven framework work for olp and olap where you can specify whether the data is ingested through a Copic or through Hive and you can specify what VES and in edges needs to be ingested along with the different properties for the vertices and edges um and the hive table ingestion which we offer referred to as the bulk ingestion happens through spark jobs which is either once a day or at different intervals in time Kafka topics
(5:55:51) are used most for streaming inje which is happening through our Flink based streaming inje platform um and we use Cassandra as our primary database for oldp and on top of it we also have a querying layer that almost always gives you a realtime snapshot of what is there in the current time next slide please olap injection framework similar to olp is also a spec driven injection framework but we only operate from The Hive tables because it's a batch processing uh system but here the kind of output that we store is in three
(5:56:22) parts um so one is reconciled graph another one is feature graph and the other one is intermediate graph so reconcile graph is similar to the OTP data where at any point in time it gives you a holistic view of the entities in current time feature graph is more of an ml driven use case which is also a subset of reconcile graph where you don't have all the entities in the graphical data but only select entities based on different ml use cases that we need and it's usually run for a historical point in time so usually ml
(5:56:51) Engineers come up with a specific use case that want to run for a uh historical time in uh data uh um the third one is the intermediate graph which is used for regenerating reconciled graph in case we miss any data and also to generate the feature graph for historical time and it's a date partitioned table so we can go back for um however long we want and then regenerate the data as similar to olp data we also have a querying layer and the querying layer I'll talk about it in later slides which is a common querying
(5:57:20) layer used between the olp and OAB next slide please um so to more explain about the feature graph and how it is built so ml Engineers usually want point in time historical data so they come up with the use case and they also specify what start dates and end dates that they want to run the feature graph for to generate the historical data so we go back in time use the data from the intermediate graphs to generate the feature graph and this is an accumulated data rate so we do this for successive um dates between
(5:57:54) the date ranges and then at the end of it we have one final feature which can be used for querying by the ml Engineers or you also have the ability to uh look at the partition data and see how features are changing in between so this is again a a spec driven framework where you don't have to worry about writing a code you just have to give us a spec and the system will take care of generating the feature graph for you next slide please so now that the inje and the ml feature generation is done I want to
(5:58:22) spend some time about Cipher so Cipher is the open source query language that's available for querying graph database so it was generated U or discovered uh by SQL based out of SQL so you the sytax is very similar between SQL and uh Cipher in how you write the query so here I have a very simple use case where it tries to represent user to user relationship that is connected through a phone number so you have a user vertex and you have a has phone number Edge followed by another vertex which is the phone number vertex and from the right
(5:58:55) side if you see you have a U2 user that's connected also to the same um phone number so this is a very simple query to connect users between um two users who share the same phone number next slide please so if you want to use this query to generate features then I've extended the same case to generate unique uu IDs uh who are connected through the same phone number so the same query is extended and in addition to returning the user U ID you also return the distinct uu ID so you can similarly add relational classes in this um you can
(5:59:29) also um extend it to uh apply filters like how you would do in SQL so it's just about how you write the query to um extend it for different feature uh generation use cases you would need next slide please so how does it all come together um so once you have the cipher query you give it to our service and the service will take care of generating a query plan that is as optimized uh you would want in the query execution and then it gives it to a computational layer where instead of running the query against the
(6:00:00) whole database we fetch a smaller portion of the graph database based on the query that you You' have given us and then from The Returned subgraph you will compute the query or do the feature computation on top of it and return the response back to the um uh service so this is at a high level covers uh what we do with ug graph um in Risk context and now I'll hand it over to Chris thanks cavia that was a amazing overview of your graph uh yeah so my name is Chris I'm a machine learning engineer here on the risk team uh I'll
(6:00:32) talk about machine learning for risk and how we can provide realtime predictions is what I'll start with um sort of going back to the architecture jeene mentioned earlier we have sort of user starting off with requesting a trip and this first flows into our decisioning service uh this decisioning service is ultimately where we can provide our predictions and make a decision on on certain kinds of requests uh for machine learning Engineers this means making calls to mangelo which will serve our machine learning models that can help us
(6:01:02) evaluate risk across a number of domains we may have many different models such as payment fraud models or account takeover models and let's go about thinking about how we might build one of these models and maybe we can start with how might we build a payment fraud model uh maybe the goal here is we want to calculate the probability of some kind of payment resulting in fraud loss when for example user requests a trip of course whenever we train a machine learning model we need data and so the first thing we can start with is in this
(6:01:32) decisioning service we have some log of events for every event that we've provided a decision for in the past and this log may contain many entity values such as the user that's acting the payment uh that they used the email domain of the user maybe the fair that the ride costed the user and maybe the timestamp that that the trip happened at uh this data is called our basis data and we can see that if we combine this data with some kind of label that we determine in in some fashion we can train some kind of maybe naive model
(6:02:06) classifier that can feed in this data and maybe give us a not so good model uh and let's think about why right the type of data that we have here in our basis data might not be very powerful for machine learning for example just using the email domain alone or the rideair alone uh this data May cause the model to overfit on for example the ride being very expensive and or certain email domains being very fraudulent at least when the model is trained and so what can we do to make it better of course we would like more data um maybe in merch
(6:02:44) the model with more features and let's start with how we can do that with batch right so we're I want to show you I want to dive into how we do batch feature backfill uh so the first observation here is that in our basis data tables we have a few different kinds of entities uh we have primary entities these entities sort of like Define the primary actors that are involved in request here we have the user they're the ones making a trip and then we also have secondary entities and these requ these entities are sort of like the
(6:03:13) entities that are shared among multiple primary actors for example the same user may share the same payment or maybe even share the same email domain and the observation there is that when we do batch feure batch feature backfilling we can take those secondary entities and pre-compute some kind of entity feature tables across many different metrics uh to provide feature values for every entity value in this case we sort of see that across many entities such as like maybe an email domain or destination Jero we can compute features such as
(6:03:44) like the email domain chargeback over a 7-Day window given that we also are Computing for like metric of chargeback and time window of seven days and we can do this for many time Windows many metrics and many entities and all the entities that are in our basis table to compute sort of like a product of the number of entities times the number of metrics times the number of Windows and this allows us to extend our you know training set to at least cover all of these entity feature table features uh which gives us a lot of actal additional
(6:04:15) predictive power in our machine learning and you see now we have high performing model um but of course like what are the gaps like when do we need real-time feature engineering or NE realtime feature engineering uh well the answer is like when we want to ask questions like this uh how many trips has a user taken in the last 24 hours we just do this in a bat fashion this wouldn't would might not give us an appropriate level appro appropriate granularity of accuracy um or we want to get like some kind of attribute feature like the
(6:04:47) user's last known phone number or if there's some new entity value observed for the first time and we're sort of observing the cold start problem we may see that that it doesn't have values populating very well yet or anything related to graph where we want to count number of like users linked by the phone number to the current user or even like for the fraud domain number of B users linked by the phone number any kind of feature engineering on primary entities benefits substantially from realtime
(6:05:11) feature engineering which is really relevant for the types of problems that we see in the risk domain such as account takeover new user fraud and Marketplace of use and even safety and so let's kind of dive into how do we do Neal time future backfill and and what are the benefits um so the first observation is that for these sort of attribute features that we have we can use temporal join to stitch their values to our training data set so temporal joins just been a tool kind of a term that's like known about for a
(6:05:39) while where for every event in your basis data you get the value of the feature that happened right before uh and yeah and so this is how we can stitch sort of attribute features to our our training data set and then we can see like the benefit of having near real time aggregation features for this new uh new.
(6:06:00) com domain that we saw in the last on the last slide um New World Time feature aggregation here can sort of help us fill in the gaps that b batch feature backfill would have uh and the caveat here to all of these uh like solutions for for creating features is that the data must exist in some kind of offline storage and so the solution that we Built For This is we are sorted persisting changes into a Roy log uh and so remember from earlier the we have this uh sort of solution architecture for neural time feature computation where data is read from kavka and then
(6:06:32) in our ETL we may do some kind of RPC to an external data source uh before we load into Cassandra uh this value of this RPC may not exist in historical data so we also write it to our right log and this right log is the same right log that lives below urra and uflow and this sort of allows us to persist all the changes toer features and can later be transformed into a change log the why what what can be transformed into a change log you say well one observation we can have is that we can drop redundant rights for attribute
(6:07:03) feature types this sort of like gives us a a way to implement an optimized change dat or capture solution um and for streaming aggregation features like we can maybe relax some of the latency requirements and say that we can do some kind of pre-aggregation to reduce QPS to Cassandra and reduce change log storage as long as we say we can aggregate on like a minute level basis um yeah so this this is the last slide but uh kind of like summing it all up together uh one of the main challenges we see is that we want to
(6:07:31) provide machine learning Engineers access to a tool that can construct training data for them so um this is what we have with this time travel tool users can specify sort of like their basis table join keys and feature groups um and for both batch and Ne real time as long as that we have data in TV change log or in some kind of Hive ingested kopka users can specify basis data um use our time travel tool and ultimately output a new table with bunch of extended batch and streaming features and this allows us to transfer
(6:08:02) from ml with basis and batch feature backfields to ml with NE time NE time future backfills yeah so s that's our slides brilliant that that was a great um tour to force of uh many different contributions you've made to to building this system and um you know I I think I haven't heard the kind of the differentiation between I'm going to ask one question just to start off with um attribute and aggregation features you know you kind of differentiate between them as as different classes is that something you've done internally because
(6:08:34) I haven't seen it too much in the community yeah I you okay well I we used to call these non-aggregation aggregation features um the key observation is just the way that you can like the functionality like how how you can time travel with the features or how you can get the latest value since with aggregation you always have some kind of group by you're grouping by user ID um and some time window but attribute is sort of grouping by like an infinite time window you can go back far like infinitely um yeah yeah I I I have an
(6:09:07) observation about time I mean Sor what you call point and time joins you can tell me what you think about it right so we use the term multim modality in deep learning that you know you have some image and then you have some text and then people are trying to extend it to multiple Dimensions but it's really kind of similar to what we're doing we take an observation maybe some you know some credit risk and then we say well what was the state of this feature at this point in time what was the point so in
(6:09:33) some level we're kind of trying to solve problems I think that other domains in deep learning are just starting to look at would that be reasonable or is that am I off track here um so when you say multimodality right I sort of the first thing that pops into my mind is like the image and text right um in this case we're sort of looking at the problem of um yeah like given this point in time like what's the current state of the world uh that we know about and like can we provide the correct uh like what the best decision
(6:10:07) that we would have been able to provide well I mean think of it as being you have different pipelines updating different parts of the world concurrently at different cadences and we're trying to figure out you know what were the values at at this observation point for this label that that that I'm trying to trying to use as my kind of point in time and get the featur values for I don't know I think it's a challenging area we've looked at at at implementing join up raas for spark at this and so I'm I'm
(6:10:34) I'm too deep in the in the leaves I think I see um yeah I I feel that there's there's there's some opport opportunities for sort of like knowledge understanding up until a point in time and we can maybe even consider like having understanding of like image data um up until a point in time and feeding that as like feature values I feel like is all a data engineering problem yeah I mean another problem I've seen is um we often see that we have this look back window that we're not going to go look further back
(6:11:06) in time and then you have in in the online feature store you might have a time to live for a feature and there's potential for data leakage there so if you make the time to live feature very low but when you're generating historical data if your look back window is larger now you have potential for data leakage so it's a it's a fascinating area yeah definitely brilliant thanks thanks ailan thanks Christopher thanks Jean thanks Ka and um we're gonna move on so I'm delighted to have a new uh
(6:11:39) feature Store featur vice uh join us today we have Javier conort and he's a Visionary data data sciencetist and co-founder feature bite it's a company committed to empowering data scientists and streamlining feature pipelines before co-founding feature B he was ranked as the number one kagle competitor in 2012 2013 and he pioneered automl at data robot and at data robot he decisively shaped an executed the data science role map in his role as Chief data scientist and he's a rich history spanning over 25 years in the
(6:12:11) data domain he started his career as an actu in the IND insurance industry and now he's made the move to data science welcome Javier hi everyone thanks Jim thanks for the introduction uh so my name is Xavier and uh the the co-founder feature bite AI B feature platform and uh today I'm going to talk about the challenges of feature engineering uh the benefits of feature platform and how we use generative AI at feature to make a feature engineering even more accessible so while feature engineering is a critical step
(6:12:58) uh to build machine learning models is still a very challenging task for many companies um so to create and serve features there are three uh skill set uh three skills that are essential domain experties help us understand the data and how the different data assets connect and this makes that we can come up with feature IDs data science skills um help us uh translate those ideas into mathematical forms or code and it's a time consuming uh task and because there the possibilities for features are Limitless and there is a challenging
(6:13:56) task because uh uh uh uh different uh so uh there are different types of features that you can create uh uh in function of the semantic of the data data engineering uh skills are also essential uh you often have to to work with extra large transactional data uh you also have to to work uh uh to build the pipelines that are time accurate to avoid the time leakage and to ensure training serving consistencies and also you have to serve the features in production uh Google B reminds us of another important principle for
(6:14:47) features uh he advised us to create features that are relevant to the use case and also that uh advise us that those features has have to be informative and interpretable and while those three uh skills women uh essential uh there are two recent Innovation and that can improve the uh feature Engineering Process the feature platforms and generative yeah uh feature platforms offer uh uh multiple benefits uh and so so they make the feature engineering more accessible and more efficient in sever several ways first they reduce the latency of feature
(6:15:48) serving by precomputing uh features and storing the feature values in the online store uh second prevent inconsistencies between training and prediction and is important result uh to uh prevent situation where inconsistencies may affect the accuracy uh in production third they simplified they of simplify the creation of complex features thanks to declarative framework uh and this quite often in a python uh forth uh the uh some Fe feature platform accelerate experimentation by offering automated back fing and uh so you there is no uh
(6:16:38) log and weight uh you can experiment your new feature RS uh directly fifth uh they make it easier to find and reuse existing feature so this is important to avoid the feature explosion and Fin they provide a unified solution that makes the transition from experimentation to deployment smoother so in summary the uh uh they make feature engineering more efficient more accessible and more accurate um a long feature uh feature platform Transformers are re revolutionizing data science uh they simplify very complex task NLP uh so
(6:17:33) they allow data scientist to uh to transform a text colon into uh a new outputs that can be more easily aggregated like text edding Tex edding sentiment analysis or extraction of keywords or extraction of uh uh uh ities and these uh this work can be done by data scientist uh by using of the shell Transformers without training data which is awesome results for data scientist and uh as a product man product manager they're opening a new reason they make that product managers can create uh new data uh uh data driven
(6:18:29) products uh uh there are two uh to to to personels that may be concerned with Transformers uh risk manager may find um transer to new to OPAC and to stochastic and mops Engineers may be worried with oper operationalization of the features uh so you may have uh a question do I still need feature platform for Transformers the answer is yes they complement each other uh feature platform can improve uh the oper operationalization of Transformers they ensure point in time correctness they uh a low low latency of uh serving of those
(6:19:29) features thanks to the pre their precomputation and they also um give opportunities to reduce expensive Transformer gos via partial aggregation or uh other mechanism other caching mechanism another question is whether Transformers will replace traditional feature engine here I'm more skeptical uh the user Transformers were successful in some situation uh for example to work with sequences of events and there were successful results for comination system but the use of Transformers on a larger context on many
(6:20:21) more cols and that are less well defined uh uh the the problem is more challenging and even if a solution emerge there will be less magic than for NLP it won't be pre-train of the shell transform that you can use directly but you will have to train those Transformers on your own data and use a lot of data to get meaningful results and for many of us may not pass the model validation process uh because of concerns of exper ibility and also concern on the robustness if the data is not enough large so why do I think Transformers are
(6:21:07) still revolutionary as a product manager uh the what I particularly like with Transformer is their ability to work with semantic to understand semantic and connect semantics and uh uh I think it's a a result that like we can use uh to improve feature engineering uh in feature engineering we are working with many different assets and Concepts that contain semantics and uh there is opportunity to use Transformer Transformers and generative AI to connect all those assets together um so I will give some examples
(6:21:51) how generative AI uh can uh uh help feature engineering and build and how he can help create those features that that b advise us to uh to to create the relevant informative and transparent features so we when we work on the uh feature bite co-pilot we made three key obser obervations uh first we uh found that generative AI is already very familiar with data modeling concept uh they understand what is what are the best practices um in data modeling they understand what is a slowly changing Dimension table they understand what are
(6:22:49) entities they also can recognize the semantics of data columns when Beyond numeric and string uh if meaningful names or and good descriptions are provided and there is again a very important result because uh uh the this allow to connect colons together and make appropriate choice of uh type of features to create from those colons uh we also found that generate AI processes deep domain knowledge for many different domains and this can help take some important decisions in the future Engineering Process and it also gives
(6:23:41) the opportunity to use a generative AI to assess how relevant a feature is to a use case uh here I'm giving a simple example uh uh so we all know that there are different type ofic colons and some colons cannot be added to together and good example of that are rates or any measure of intensity and it's a concept that generative AI understand well so I just ask him what what I can do with the whether a speed colon is additive semi additive nonadditive circular or categorical colon and he finds that the
(6:24:35) the speed colon is non additive and and reminds me that non additive col cannot do any sum on the non additive colon so you cannot do sum of the speed of the cars in the city it won't be uh it w it may be cre with your target but won't be a m full uh uh and Rob robust feature another uh example how generative AI can uh help you uh uh in the feature engineering is uh so some sometimes you have cols in your tables that are represent even type uh so here we have the example is with credit card transaction table where we
(6:25:30) have a transact type transaction type with seven types and I I can ask gen generative AI which uh type of transaction is the most meaningful for uh the use case of fud uh and uh so and generative AI is capable to make good recommendation and advice to First focus on purchase transaction then on cash advance and then on reversal transactions and will tell me that bank fees features will not be useful for this use case another relevant result is that gen can give plain of explanation of why features is relevant uh so it can uh provide more
(6:26:26) detailed description of the features explain what is this score that was used by this feature can give examples and can also assess whether these features is uh is relevant to the use case and here he gives a score nine out of 10 and the generative AI is also capable to do this even for complex features um here is a diagram how feature B uh is currently aaging generative AI so we use generative AI in two uh areas uh so first we use generative AI to uh uh understand the description of the data the description
(6:27:17) of table columns entities uh we uh uh and also we provided the description of the use case which depends this use case belong to the context and the Target and the generative AI is capable to make sense of all this information and map this to our onology and this information is used as a input of our feature radiation engine uh that uh uh suggests features uh with code and distri and we again use generative AI to evaluate the feature and assess its relevance and the the result is a context aware feature suggestion where we provide we sort the
(6:28:13) features by their relevance we provide the explanation F by generative Ai and also we organize the the results uh by uh categorizing the signal type uh that capture was captured by the feature so you can either choose to add those features directly to the catalog or generate notebooks to uh to uh to to have the SDK uh code related to to this feature and edit those uh those features to create similar features uh here I gave a link of examples of feature hdk notebooks that we provided in our tutorials uh this is my conclusion so
(6:29:11) future platforms and generative are accelerating data science uh uh in three ways uh first they reduce time to Value thanks to quicker experence experimentation thanks to back filling uh also by reducing the manual work uh you reduce the effort and uh uh we saw that feature platforms allow smooth transition from experimentation to deployment they also the two solution the two Innovation combin increased production accuracy uh we saw that training serving consistency is important to ensure that the production
(6:29:59) accuracy is align with the prodution that you got during validation and there opportunities to create a better features uh one with a Transformers uh by doing a better NLP from uh from Tex and uh and also by getting better uh feature suggestion that are Contex aware and another important outcome of those Innovation is that they enhance transparency and give the opportunity to bridge the gap between features and non- Technical stakeholders thanks to plan English distributions of features and their relevance to a use case uh I think is a
(6:30:55) uh something really important uh we to uh achieve broader adoptions of AI in companies uh it's important that uh n data scientist can understand well what we are doing and uh and this transparencies this transparency can improve increase the trust that uh those non stakeholder give us um so that was my uh last slide uh thank you for uh sharing with me your this experience and uh yeah thank very much Javier fascinating stuff and I think um you're doing some really in inovative work here on on gen um I'm going to start by asking a
(6:31:46) question just about um you you mentioned some of the potential contributions of generative AI to feature engineering um from ontology to domain insights to generating code and explaining to stakeholders where should you start or where are you starting how are you ranking or prioritizing those different ways in which you you we can add value oh yeah so we don't use yet generative AI to generate Cod uh I I do that's why I think I threw that in there oh doing it I've asked I have a surfing um example where I asked I said can you
(6:32:24) please ract the height of the wave at this beach I'm interested surfing and and and uh please put in a data frame and so on and it wrote the code for me and and that saved me like I I wrote the code a year before it took me about a day to write I'm not a great programmer but um but the but gp4 was very good anyway so anyway sorry yes something that I would love to do and the fact that we have a uh we have a SDK uh the declarative framework make this easier so you don't need to teach generative AI to write a complex xql
(6:33:05) that can support a time travel uh but we first focus on uh yeah mapping data to our autology to improve to make that or feature are Contex well and we and also personally I find that it's really important uh before using a feature to build intuition on how this feature can contribute to a model uh I don't like to look only at the the the correlation but but to have my own expectation of how this feature can be useful and I found that generative AI was quite good in doing that and also I found that sometimes as a data scientist uh you
(6:34:08) um you have nobody to talk to to show the feature nobody really cares and uh and I was pleased to see that generat I la some of my complex feature and they said oh yeah they are very relevant to the use case uh and so so but yeah I agree with you there are many opportunities to use a generative AI uh so yeah and the the uh so the the the our next step will be to generate uh description automatically descriptions from manually crafted features and the other step uh will be to generate the Cod automatically like you did very I mean I
(6:34:58) I find it interesting you had the the you know often when we talk about the types of features apart from data types we think of feature types we we often talk about ordinal you know where we have an order and then we can say well we have another categoric word we don't care about the ordering and so on but you talked about summable and um so you're really thinking about feature types and and how the the feature computations are performed on them yeah yeah and I I think that the feature to categorize the features uh is
(6:35:31) something important to organize the feature catalog and to make the search of the features easier uh so uh so we try to yeah we want to categorize the type of colons uh with orology and the type of features uh in a Noel way that make it easier to yeah maybe to use the colons and also make it youer to use the features brilliant and I'm I'm really looking forward to seeing the progress you make in in using gen to help build better feature platforms HKS you're calling in from Singapore are you correct yeah that's great I'm sure
(6:36:14) I'm sure it's a strange hour of the day I won't even ask thank you very much thanks okay um rice we we're reaching we're we're getting close to the end we've got one talk left so I'm delighted now to um have another uh new we can say relatively new entrance into the field I mean H2O have been around for a long time and of course AT&T are very uh storyed established company H but we haven't had you here at the feature store Summit before so we're delighted to have you here we have Chris simad
(6:36:47) who's director of customer solutions for FSI and strategic products at H2O he serves as product manager for H2O feature store is responsible for key client relationships with global system the important Banks and other financial institutions he has over a decade of experience a data and AI leader and he's worked at Fortune 50 and and, Enterprises and startups he's successfully spearhead and contributed over 100 use cases spanning fraud cyber security pricing customer acquisition recommendations forecasting with Revenue
(6:37:17) growth surpassing $250 million Chris thrives in tackling data and AI challenges crafting Solutions and delivering impact and jooy joining him we have Prince Paul Raj who's an accom he's the head of Chie of Chief data office at AT&T India development off center accomplished leader in the field of data and AI made contributions to AT&T through his Works in very in various initiatives um he works on particularly on inov Innovation especially Realms of AI gen AI um and his achievements include delivering over
(6:37:46) a billion dollars in benefits to the company over multiple departments strong technical background and Leadership skills have enabled him to execute and deliver multiple numerous AI products including the H2O feature store developed in conjunction with AT&T fraud AI Watchtower hyperlocal off recommendation system and AI driven domain agnostic matching and mapping great to have you both here um go ahead and the stage is yours Prince I don't think we hear you yet okay well we have an X showing I'm I'm um it wouldn't be it wouldn't be a
(6:38:27) live event if we didn't have a a technical hitch at some point so um right so Jim this is Prince are you able to hear me yes we hear you great loud and clear awesome Jim and Chris can you just uh bring the slides for me please all right guys everything's good we're good to go awesome okay hello everyone um so today I'm tag teaming um you know from with Chris and there are three important topics that you know we're going to discuss it today all right so the first one is talking about H2O and AT&T
(6:39:09) together Cod developed a product called feature store and want to give you simple overview about that particular product the second thing that I want to more focus on how actually we use this H2O feature store in AT&T and it's not just for a data scientist also for citizen data scientist so we're going to show you the entire flow of how we are operationalizing this Entre AI you know at the scale of you know AT&T using the feature store the third one is feature store and gen um we put together a small
(6:39:43) video clip to show you guys um you know how actually feature store also really helping us in the world of gen application side of it so those are the three important topics today we're going to cover so first one let me talk about the overview about H2O feature store so if you ask any data scientist you know if you ask them where do you spend most of your time um you know 80 percentage of their time they spend you know finding the data collecting the data cleansing it and and creating this valuable features um that's really
(6:40:18) helping you know anyone if they building any sort of a machine learning model so most of the time um you know they spend spend you know time in preparing those you know features and these all features you know if you think about it uh it's sitting in you know data center laptop or maybe somebody stored in a database or somebody trying to put it in some cloud storage place uh but you know this is such a valuable asserts uh if you take any company you really want to keep it in one place and try to make use of
(6:40:49) it how uh you know across the entire organization right so that's the that's very key important factor so 80% of your time um you know if you think about it and you create that hundreds and thousands of features you want to store it in one place and reuse it as many many times you can so move on to the next slide please so the next slide you think you know I just want to talk about the data that it's been you know produced in the organizations right um it's there in the form of real time or it is there in the
(6:41:21) form of in a batch mode or even in real time and we get all these data sets and um you know we use multiple data pipelines uh in order to do that feature engineering right um you know just naming few products in Industry right so we have data breaks snow flick and you know paler and you know use Azure ml or AWS or somebody can even simply use Jupiter notebook right doesn't matter which pipeline actually you are producing those features um because these all features can come a different pipeline but how about we store all of
(6:41:57) them in one place um you know uh we store them in offline mode uh because that really helps the data scientist to uh really train their model right they can experiment uh looking at all the different features and they can create you know best model that they want but at the same time how about all those featur also available for us in the form of an online store um so that you can quickly do the lookup um sometimes you need to score the models in milliseconds you know you can use definitely the online uh feature store and if you have
(6:42:28) both these features either sitting in offline or online but you got to know the metadata about these features so that is also if you package all of them in one single storage and either you're using it in the consumption standpoint training side or you know real time predictions or batch predictions you know you can easily use it these are all the four important steps if you take any data pipeline uh whether you're working with a data or preparing the machine learning model uh during a training or you know predictions you know it can be
(6:43:00) used so that's the concept behind this you know H2 feature store um keeping it all in one place and making it available either offline or online so the next slide um um I will you know talk about uh you know how actually we are using it in AT&T right um you know you can think about ad being as such a big company you know we have more than you know 250 professional data science out there but today uh all of a data SCI has been onboarded into feature store right and they derived or they produced the features there are about
(6:43:35) 40,000 features sitting in the feature store and there are more than 200 machine learning models has been you know onboarded into the feature store so you you take any particular machine learning model but if you try to look at the lineage about the model and these features you know they are categorical variables or like new numerical variables you know some of them are Velocity features um something you derived it you know uh in a direct way or indirect way or relational manner um some of them are streaming features
(6:44:06) doesn't matter all these features because I explained you we have the online feature scoring capability as well as you can do the offline it's all sitting in one place that's the you know good about U H2 feature store so today because we have these 40,000 features and 200 machine learning model um it's not just our professional data centes the very important element that I want to highlight here there are more than 20 citizen data scientists are using it um so um these features that they created
(6:44:38) it's not just only you know used at the time of you know model training or scoring uh but citizen data scientist can also come and do a simple data lookup to understand those features maybe they can use it part of their um you know business process they can insert the data if they want to take some you know actions out of it so end of the day these features we think it's a valuable assets for um the data science overall uh but we are able to handle those features even in citizen data scientists you know some of them
(6:45:09) are SQL saice right some of them they don't even know anything about it but they can able to use the these features and part of their business process and the second thing that I just also want to highlight here um in fact I'm going to talk about two uh realtime model serving uh use cases um the first use case is you know it's actually taking multi-million transactions and it has to the model that what we deployed in the platform needs to respond in in milliseconds also so if you go to the next line you know I'm going to talk
(6:45:42) about uh fraud use case right um you know ATD being a telecommunication company I mean anyone else in the industry you know fraud is a billion dollar problem for us right um so the the Bad actors um you know they daily come with the different you know new scheme of things and you know come and try to you know um uh victimize our customers so it's very important for AT&T how we protect our customers right um doesn't matter what form of this um you know the fraud act fraud are attacking us but we have to protect our
(6:46:17) customers at the the same time you know we have to give a very balanced approach in in customer experience as well we don't want to create more frictions so it is important that um you know uh these all features that all of data scientists in AT&T they produce um we want to keep it in one place and really make it happen uh when we talk about like millions of transactions you know fired across a different you know the channel uh I'm showing actually in the next slide um overall high level view right if you just you know go with me
(6:46:51) from the left to right um there you can see uh anyone you know showing up to AT&T retail store and they want to buy a phone or upgrade a phone um you know add family member whatsoever uh or you go to you know at&t.com and trying to do those uh you know activities or you're calling our customer carriage and and trying to you know purchase a phone doesn't matter you come any one of our Omni channels and um you know we collect all these features uh you know the moment you engage with us in that sort of of a
(6:47:24) platform um generating those features and storing them in feature store that's why you see even part of the injection uh process itself the feature store has been kicked in and actually is working behind the scenes for us and we have put together the pipeline in place and it's actually collecting those features you know for the particular transactions um so then doesn't matter you know you come through any one of the channels either you're logging in or adding stuff to a card checking out or shipment these are
(6:47:54) all the four high level you know transactions that I'm talking about but there are like you know uh hundreds of transaction happens in the overall workflow uh but anytime you do any one of those things um the machine learning models there are like more than 50 machine learning models running behind the scenes and trying to understand you know this particular transaction is fraudulent or authentic right and that you can see we are also using you know H2 DS AI so it's very important for us to not just only use the features that
(6:48:26) what we created and trying to see that in the combination of tress AI it is really you know uh making sense to us uh to understand a particular transaction is a fraud or not fraud and of course you know based on the model outcome uh in a real time in during the serving time we are you know taking some sort of an action so um in in a nutshell even the serving side also our online feature store has been used so if you think about as simple real time you know AI pipeline uh right from where we are gathering the data because a particular
(6:49:00) transaction is happening then creating those valuable features then keeping them part of this machine learning model and scoring them and then really taking you know the remediation during the serving time all the angle this F SW has been you know plugged in part of the platform so that shows you uh you know how this H2 feature store is really helping us um then journey of one particular transactions this all needs to happen in milliseconds by the way right and 24 by7 that's the sort of an operations we are running against the
(6:49:32) H2O featur to so if you go to the next slide so here I'm talking about another use case right so this use case you know like anyone else in Industry obviously we have to take care of our customer churn um you know someone you can think of it um you know the feature store is not just a storage standpoint for us to keep these features uh but at the same time it's giving a shopping experience right uh anyone can get into the feature store and they can shop around the features um it was created for a different context with different data
(6:50:08) scientist but maybe those features are useful us right I mean can we just do the experimentation and see like you know do we get any any performance right that's what happened in in one of our machine learning model uh by our data scientist and before you know um they use the feature store they had the accuracy about 66 in know percentage but obviously they went to the feature store and they looked for some of the features and in just matter of two hours uh the feature store when they looked at the features and especially some features
(6:50:39) related to the account activity that really lifted the model uh you know the performance and so you know we got about 11 personage increase um in our in you know kind of a lift we got it in our machine learning model so this is another use case that I'm just talking to you uh when you operationalize this feature store um you know one side I shown to you how it's really helping us uh during the real-time pipeline the other side even for a shopping experience you go to look at those you know features like I mentioned we have
(6:51:10) 40,000 features that's really helpful right uh when you have 200 model you know in place you know you can shop around and getting this sort of a lift it's really awesome for us so these are the you know the two important use case that I just want to explain to you guys um when we are operationalizing this feature store um you know in and fashion um not necessarily from the you know the model uh training but all the way to the end point of model serving as well so let's go to the next slide please so the next one you know um we're
(6:51:46) going to show to you a small um uh demo of how actually this features were also uh you know used in the context of gen we all know that gen is kind of a a big disruption I would say right now in the market it's a htic growth um but how actually we are using the feature store how it's really getting into the context and um Jim um um sorry Chris do you have a video yeah so I'm going to you know uh search one particular feature set you know kaggle um you all know that this is a Titanic disaster you know the feature
(6:52:30) set and you can look at our GUI that how it just looks like the H2 feature store you have all sort of you know description about this particular feature set and all the features the time to live or time travel or even the governance standpoint whether it's a spy data or not along with the Version Control all those things in place in the right hand side you can kind of see the features I'm just clicking of some of the features here you know the people who surveyed either it's a numerical feature or categorical features you can
(6:53:00) kind of see all those features in the right hand side right so now I'm getting into this ad GNA you know use case Point standpoint now a citizen data s is asking a question just to brief me the description about this particular feature set right so this is the citizen data scientist and you know when they ask you know it's definitely brings the description about that particular uh you know feature set it's talking to the feature show and getting it and second time now I'm asking bit more complicated
(6:53:29) question show me the count and percentage of all the passengers who survived and did not survive by ticket class right and here I'm also asking Group by some of the factor and look at that the answer that is coming from actually from feature store right so um that's the that's the way you know uh we are using the feature store I'm just asking some other question over here um so the contest here is um a citizen data scientist who can simply ask using a natural language simple English and when
(6:53:59) they ask a questions um we are able to generate the query very native to the feature store um you know and from the behind the scenes of the feature store we are able to pull the data and present it to the users where they can able to consume it in a simple cable fashion so this features what sitting in featur tore it's not just there in offline and online uh because we have a very good metadata associated with these features um now you can get into the Gen mode and you can just uh you know ask a simple
(6:54:32) question it can able to produce the you know uh the answers straight away from uh you know the feature stores so those are the three things today we want to talk about it and how this H2O feature story is so unique in in a manner and also how we are using it in in AT&T um you know with that you know I just want to conclude I mean any questions about H2O feature store you know I have my contact details over here as well as Chris from H2O so this is a code developed between at& and H2O and um please you know we have our LinkedIn
(6:55:06) profile as well so any question please you know talk to us and we are ready to help you thanks a milon prince and thanks Chris this has been really great to hear some of the pro that that you have all those data on to hundreds of them working on the same platform um I I'm really fascinated about the citizen dat data scientist because you know we we we've seen at the previous talk we had feature bite talking about um generative AI as a way to help people Define features and and and to understand the domain and have
(6:55:41) domain knowledge um what's your take on the role of I guess data citizen data scientists and feature stores like what what will they be doing to help create features and and and will we make it easier for them to create features we had a lot of talks today about very complex feature engineering you know with streaming and batch and so on um what's your take of the role of the the data analyst or and data scientist so you know maybe I I I just you know talk about the two things right so one is that these features if you
(6:56:16) think about it um some of them are very simple features right you get into a simple Pipeline and you compute those features and and publish them into the feature store then you know it's really available for anyone either data analyst or citizen data centers this they can consume it but if you really think about the other side of it um you know sometimes we create very complicated features right and especially when we use this H2O feature store even we use the H2O driverless AI that can even create an exponential number of the
(6:56:48) features uh looking at the features that what we have it in feature store so it's it's also from a data scientist standpoint it's not just a store where you can keep these features now we can also recommend some of the features to the data scientist that is number one that's coming as a value ad and second thing you know there are new features has been introduced uh that is you know another you know good thing about it and all these features are now available in one place um the citizen data scientist
(6:57:16) don't really know about how this feature has been computed and all the complex and all those things be logic behind it uh they're going to access it like one database you know one table where you have all the data that's the simplest form they can just you know query the data and now with the you know with with with all this gen into play now even you don't need to write a query right you ask simple questions and it can able to produce those features then you try to see those features is helping and some
(6:57:45) of the features are really helpful because uh they are sometimes easy to understand and inserted part of their business process so you know two four one is the data scientist also getting a benefit here and second one is as citizen data scientists they don't worry about the pipeline they don't worry about how it has been computed but the end of the day they can use it for part of their business process yeah I mean actually one one point that might be interesting would be to think about the metadata aspect like so you know if
(6:58:12) you're a citizen data scientist you can obviously describe the the feature you can describe its domain and so on and its usage and and you know I think there's there's opportunities there free and gen to to link that that extra description that you get with the computations and the usage and and and the models that will be put into I don't know if you if you've been looking at that at all yeah um one thing that's a little bit unique in the feature store that we co-develop is that uh the fact
(6:58:42) that you can introduce artifacts to associate with what we call feature sets or with the projects and so um being able to incorporate that with the language models so then you can have a conversation to help contextualize uh not just the technical metadata like show me how many you know what's the cardinality of this you know categorical what's you know the IQR of of of of a continuous feature but also like help me understand the business context is this a fraud data set or is it a customer relationship churn data set and you know
(6:59:13) when might I you know have an opinion on when I might need a little bit of both yeah I I think you know Chris pointed out great Point here um so you take one particular feature set right I mean I think you have seen it you know while we are showing that video demo here um we have a you know something called artifacts where you can even point out your wiki page about that particular feature set or features that you're talking about right or it could you can upload a document uh you know about that particular feature set or how
(6:59:44) you use those features part of your model development right or you have model lineage document you can also attach to it so now you have the data the features along with that you have these documents you know now the J definitely you know connecting the dots now we are able to produce the desire result in fact you saw the demo now when you know Chris was showing a demo somebody's asking a question it's giving answers back um you know it's not just U you know a hot coded query or something right it is autogenerated query that
(7:00:17) it's able to do it with the help of the metadata that we captured behind the scenes for all the features and also user is adding more data towards the you know that particular feature set like you know let me share a v link there you know maybe I put my word documents there maybe I got some PowerPoint let me host it there but all those things now available right so with the help of a gen we are able to you know make use those valuable metadata and I definitely think the more data the better we know that with
(7:00:47) AI brilliant guys thanks a Millian um it's been great to have you and um you know if there's questions on the slack go ahead and jump on there and with that um we're at the end that's it um been a I think we've had the best year of I will say we think we've had the best year of talks so far um great quality no hitches live event there always potential for hiccups gone very well and I'm really Del to have uh a growing and and thriving Community which is which is fantastic and thank you to everybody who
(7:01:20) stuck here stuck it out for the for the entire ire day I know we've had people join joining globally from all over so we have people joining and leaving as as the sun sets and the sun rises and all for the speakers whove who've come in at the the awkward hours of the day for many of them um and we'll be back again next year and if you're interested more about feature stores of course you can go to featur store.
(7:01:45) org you can join the slack Channel you can um contribute look at things like The Benchmark that we presented this year and um you know we we're really happy that that that people have been here the videos will be out by the end of next week it'll be on YouTube and um thanks very much for attending have a great rest
