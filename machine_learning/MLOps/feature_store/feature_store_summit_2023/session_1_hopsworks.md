## refs:

- https://www.youtube.com/watch?v=Pnnr2gpdXRQ

# Hopsworks - From building to using feature stores for ML systems - FS Summit 2023

Hopsworks - From building to using feature stores for ML systems - FS Summit 2023 - YouTube
https://www.youtube.com/watch?v=Pnnr2gpdXRQ

Transcript:
(00:00) [Music] thanks everybody um thanks Jame um I'm Fabio um as a as we said a lady engineering team here at obor um and today we're gonna be talking we're gonna be taking a look at um how to use uh feature source to build machine Learning System um why is that right so if I if we started looking at um uh you know what what is mlops today right and what does mlops over the past couple of years is is what I consider a massive you know spaghetti code right this is from one of the more SED paper on mops architecture you can
(00:41) see there is like um arrows going everywhere components everywhere uh and it's really a complex systems and there out to build out to manage and you know in generally speaking quite quite fragile right um and one of the reason for that is and I believe is that the um the deploy model to Productions is is actually um um a data challenge right so the um the architecture that we can see here is actually meant to address the underly problem of managing data for for AI and there are actually a couple of of different challenges um that that we can
(01:16) look at um these are the challenges that every time we uh we talk to someone about about OBS and about feature stores these are are things that that that come up over and over and over right so the first one being um when I'm building a model I'm usually relying on a variety of data sources I'm not lying on a single CSV file or like you know just a bunch of data on a data warehouse but I'm looking I'm joining data coming from from streams I'm coming data from um you know data houses data L um specialized
(01:43) database like R databases um and I'm tell them everything together um to be able to to build a model and and and and make predictions um the second the Second Challenge is that uh there isn't one single tool to do any of those tests right there is a bunch of different tools to do feature engineering there's a bunch of different tools and Frameworks to train model and and serve them um and there are new tools coming out every every day and every every every year right so for instance you know stuff like polar or dctb um couple
(02:11) of years ago were not well known and today they are pretty much everywhere right um another thing that it's it's quite important um that we see over and over is disconnection between um there's a disconnect between what we uh what like for instance it's experimentation compared to what maybe it's it's it's training or or production right so you have you know different teams kind of making taking care of different different stages of the let's say um you know mlops uh um mlops process and um
(02:42) then you know there are walls and you know um there are situations where you throw the your work over the wall to the next team or the next person responsible for it and that obviously creates um frictions and creates um you know points of failures and and and and things like this um one other um thing thing that we we see often is that there are often times custom like oneof basb pipelines to do different things right so um a team might build a pipeline to train a model that's taking data from data warehouse do some Transformations and
(03:14) and train a model but then the same model needs to be served in in production in real time be rest CPI and the team goes off or some other team goes off and and build totally different pipeline that maybe relies on a CF stream or maybe relies on on other on other like Technologies underneath and then it's a nightmare to keep those things and things right um so these are kind of the um the kind of the eye level the ey level challenges um and and feature stores to a large degree try to address those challenges um but what we
(03:44) found is that um tools on you get you so far right so um the challenges um you know you can't just you know magically drop in the future store um and all the challenges go away um you need U you need proper process in place so um we work with users um and we work with customers um to um you know try to identify some of the patterns and things that we obviously make for a successful implementation uh of of a feature store right so the goal for us is to go to a complex system like this um to a more well- defined system a more simple
(04:18) system to understand and and and reason about which is the FTI pipelines uh we call it FTI pipelines FTI stands for feature training inference pipeline um and the goal is essentially to um you know split the process of De developing a model and deploying a model into production into three independent um logical components uh which are responsible for different part of this of this process right so um we have feature pipelines are responsible for you know reading the data and and Computing the features and and and um um
(04:51) you know um producing the features training pipelines are responsible for actually building the models and that includes everything from you know experimentation and actually training the model final model um and finally we have the inference pipeline which is responsible for um you know taking the model um and making inference um both real time uh or also in batches um these are independent components um the the the nice aspect about this is that you can reason you can reason about every single component um individually um you
(05:23) can have for instance different teams working on different components you can have different uh you can use different Frameworks um you can orchestrate for instance the different component different pipelines independently so you might have a feature pipeline runs in a streaming our way you might have a feature pipeline that runs in a in a in in a batch fashion every day you might have a training pipeline R once a once a um once a month or something like this right so the the idea is not to have a single um UniFi pipeline but you have
(05:50) actually split them up one thing that is missing here from picture is how the three different pipelines work together to you know build this ml systems right um and that's where where obso comes into the picture right so obso as a FID store provide new things provides the capabilities to to manage the artifacts to manage the features um to store them and serve them and so on but it also provides um an open API um which are in Python mainly but also available in Scala and Java to be able to actually interact for for this pip to actually
(06:22) interact with with the feature store right so if you writing a feature pipeline um then you will call the API to register the um the uh the features with the feature store and if you are writing a new trending pipeline you will need API to retrieve the features from the future store um and also for the inference Pipeline and so on right so OBS provides the capabilities for these three pipelines to um you know share the outputs and build on top of each other um while keep while keeping them um you know separated and
(06:53) independent now if if we look a little bit more in detail about the different steps so if we start from from the feature pipeline right the future you said is responsible for um you know Computing the features um um and um you know producing the features and you potentially also the the labels if necessary right um the the expectation is that you have a really clean system uh where basically uh you get a bunch of data and you do Transformations and then there you go you you basically have your features as we said earlier like one of
(07:22) the challenges that you add when you're building machine Learning Systems is that you're dealing with um a bunch of different data sources um with different time with different like timelines in terms of ingestions so you might have set a streaming pipeline coming in that needs to be processed in real time you might have a data warehouse you know that is loaded once a day once a month you might have you know existing pipelines that you need to combine um so there's a lot of complexity and and a lot of like different tools and and
(07:50) different processes that are that are required um one of the challenge is that um you don't have one tool to Ru them all so you don't have one single framework to do you everything right so you have different Frameworks and different things actually you know prefer different um different different Frameworks right so if you for instance if you're doing um if you're working with small data sets um you can probably get away with pandas and paas maybe you're dealing with like um streaming API with streaming data um you might be
(08:18) dealing you might want to look at you know stuff like either beam flank or spark streaming you might have data and data what El so you might be looking at you know um B qu Snowflake and leveraging their their their execution engine and and so on right so there's different tools um and different pipelines are written different tools and they need to comb be combined with each other right the the idea and the the benefits of of the of the feature pipeline is that all this complexity is actually um hidden and and taken away
(08:46) from the rest of the training pipel inference pipeline is all combined into the in the feature pipel um OBS has always adopt a philosophy of being an open platform um also being and the fure pipeline kind of adopts that philosophy as well um to basically allow different Frameworks to be used um not only the Frameworks exist today but also future proofing your infrastructure and the infrastructure of our user and customers um so that if let's say in two years from now the next ACB comes out or the next polus comes out um we can plug it
(09:19) in and you can leverage it to your compute features right you don't have to wait for um a specific um you know tool toward supporting the DSL use it and and call the API um to to register what we call in OB register data frame um data frame are common concept across um all the data processing tools out there um so um that's essentially that's essentially what we what we provide in terms of platform um to implement um the feature pipeline um API um Oneal thing that I is something that that it's new in OBX that um we
(09:55) have released as a preview right now um it's it's feature monitoring right so the goal is that when features leave the feature pipeline everybody can trust it right so if I'm a de scientist building a training pipeline or if I'm building like an inference pipeline I know that the data coming in in the the data that's available in the feature store is actually high quality and correct right obso has a support uh for great expectation for many many releases now um that's mostly covering bad data so
(10:25) for instance I have NS in my data or I have you know negative values when supposed to be positive um these new capabilities feature monitoring um actually looks at more um statistical data statistics properties of the data and making sure if the data is shift drifting um the you can basically you can basically get an alert and you know um you know either ret Trin your model or or adapt your model essentially and we have again open python API to do that um it's a same API for for batch and the same API to monitor um uh real time uh
(10:59) real time pipelines and you can basically you can basically have a look at documentation is going to be available soon as well to to look at this um this API the next step is um you know once the feature has been produced is actually um it's time to actually train um a specific um a specific model um training a specific model again the the underlying concept is the platform needs to be able to support um you know all the different Frameworks that you that you might scientist might want to use right so everything from Deep learning
(11:32) Frameworks like Pythor and so on but all the way to let's say if you're doing working with TBL data and so on H you might want to look at I know ex boot second or something like this um so for for up and for for FID Stores um the the key concept here is to provide an API to give back um a panda data frame right um ABS supports that all the F out there they support same API um one important aspect I will say that you you really want to be careful about is the way you're retrieving data right so um if
(12:04) you look at F out there um and you know to a large degree OBS back in the days um they either built on two underlying Technologies right either built on spark or they built on a data warehouse right um then you know they ship the data to the clients um using gtbc protal essentially right um the the challenges of this architecture is that for instance spark as a really um like a long cold start like if you want to start a spark application it takes a couple of seconds um to get up to do anything useful um to even like start
(12:38) retrieving the data um and that has an impact on the user experience right so one of the things we see oftentimes when we work with the scientist um using obser and using fidget stores is that what they want is um they want to be able to have the same let's say laptop experience um that they would have if they were to train on CSV file right obviously if you are a large Enterprise or if you're you know if you care about data governance and you know you have working with sensitive data you can just have bunch of CSV file flying around on
(13:09) the laptops of people because that's a recipe for disaster so you want to adopt a centralized architecture but at the same time you don't want to take away um the flexibility in the user experience that you know people are used to right so that's what we actually try to address um in the summer with OBS 3.
(13:27) 3 um which essentially allow replacing the spark spark qu engine with a combination of dub and autoflight right um the basic idea here is TB has a really good performance especially for Point time correct joints and it's a typical work use case that you see when you're dealing with feature data um and autoflight is a protocol to be able to um ship data between client and server in a way so that you don't have to uh you know you don't have to do a of distalization you don't have to move go from colon format which is the typical
(14:02) the typical way the data is store in a in a feature store to row format which isbc to back into a columnal format which is a data frame in pandas right so we flight you can do end to endend without um the serialization serialization on right so um that's something that as J was mentioning earlier uh we have benchmarked uh we you know worked with kth and K Institute to look at you know um typically use cases as I said typical use case would be sending data to a pandas um application for for for training your model um so we
(14:38) had done actually two um main experiments one is as I said um this one which is basically sending data itself using using dctb and arlight in know for ups and you know use all the Technologies for the other um for the other vendors um you can see that like performance are extremely extremely well compared to tools out there um similar thing um you know often times you don't want to um just get P the data frame often times you want to um you know get a materialized version of the train data set maybe you're doing upet tuning you
(15:11) want to have a fixed Target that you're training on um so in this case you can potentially say obso offers the API and the all tools as well um to be able to say okay collect all these features um in a in a CSV file or a TF records file um and you know we we we Benchmark it as well um again for obss using the combination of DB and and and and nutof flight um and you can see that you know again the performance are significantly higher both because of this um timization of Point time Jo indb but also because of the um you know outof
(15:44) flight protocol um so yeah finally um we looking at the inference pipeline so the last part of the of of the mlops life cycle um toally it um the the the INF pipeline is responsible for outputting predictions um whether you are predicting on a single vector or you are predicting on uh you know like a 500 500 um uh Vector input or you know a million rolls or something doesn't matter um one of the benefit of the FTI Pipeline and you can you can really see it here in the inference pipeline part is the unified architecture between um you know
(16:24) batch and train right so you know the the the feature pipelines and the the training Pipelines don't really need to know whether or not you you're going to be um you're going to be um you know doing real time inference or you're going to be doing best inference right so the inference takes care of that complexity all the complexity for for that is included and and limited to the um to the inference Pap itself um the inference papul does essentially two things right so um and this is really important in terms of real time um use
(16:54) cases the inference pul retrieves is responsible for retrieving data from the uh from the feature store um in upso if you're retrieving a small set of data because you are doing real time inference we go to the online feature store um if you're retrieving data for like large batches uh we go to the offline part and the the other aspect it does the F the future pipeline sorry the pipeline is responsible for is actually um doing on demand Transformations right so there is a you know Cass of use cases I we the majority of the use cases for
(17:27) which um the some the features data is not going to be you're not going to be able to precompute it right it's going to come in using the request data from the request data and what we do in OBS works is we actually um do that from we use something called like pandas udfs right pandas udfs are um really powerful because you can run them on top of you know Panda series and so you can get you know if you are in inference pipeline you have you are in a pure Pyon environment um you can basically just retrieve the comput the features on top
(17:58) of the panda series um but you can actually apply them if you're if you're if you're doing for instance um large feature engineering or you're back fitting large amount of datas you can take the same feature um you could take the same feature functions um as P UDF and give them to spark spark is optimize for that it's going to push them down to the executors um and it's going to use Arrow to communicate between the executor and the the the PF uh make it extremely fast and extremely performant um again the future
(18:27) pipeline the inference pipeline again it merges to merges to combines the data coming from the feature store and combines the data from the request data and the on demand transations and provide it to the model for making predictions right and um obso as a platform um provides the API to combine this to and to you know retrieve the data and so on and all metadata about which features are needed and from where they needed whether they need to be computed on demand or it can be retrieved immediately it's all managed
(18:55) by opso and it's all part of the opso metadata um layer you can make predictions um we've done a bunch of benchmarks as well um for um for the real time aspect as well um we'll be looking at retrieving data from the from from the feature store uh we've done it for two um well we've done it for more than two you can you can actually look at the um feature store benchmarks URL that you have here um and um with show you like two examples one for batch size one um and the other one for batch size 500 um 500 being a good
(19:29) number that's usually use case where you're doing for instance recommendation systems um that's you know you have a like a large pool of candidates um to be able to rank you want to rank to be able to you know recommend to the user and you know the larg the candidate pole then you know potentially um you're going to get better better results in terms of uh you know more diversity of candidates to to to give back to the users um you can see the difference between um couple different vendors out
(19:56) up and a couple different vendors um um and again you can find the code of the benchmarks to be if you want to reproduce them if you want to add your own system um you can find the code in the in the in the URL um itself and um yeah and some of the other benchmarks that that that we have done now if you go back and we take a a step back and look at the full pictures of what are the you know the benefits of this new architecture um this new FDI pattern um and and we compareed against the data challenges that we had at the
(20:28) beginning um the first one being Vari data sources um the feature pipelines is the one responsible for dealing with all the this data sources all the um all the uh you know complexity of changing or dealing with different data is all is all combined and and constraint into the feature pipelines um in obso the feature pipeline can be expressed with any different any framework and so um we going to solve also the Second Challenge which is basically say okay I have you know bunch of different pipelines running bunch of different Frameworks
(20:59) UPS can take care of all of them um again we provide an opening ey to be able to actually um you know WR the feature store and later on retri features um the other challenge is disconnection between experimentation and training and production um you know the new uh the new setup and the new pipelines actually allow you to you know register features for for for experimentation and whether or not you're going to train model whether you're not you're productionizing the model um it's the same pattern the same
(21:28) I um it's it's it's one tool um to to kind of go through all all the steps um just one p and for them and it's also one PN for doing both BS in real time as I said earlier um the only really component that needs to be aware of whether or not is retrieving features for a real time request and for which he has a bunch of slas against or as more like relax slas because it's it's a bad job running every night for instance um it's actually only the inference pipeline right training Pipeline and and
(21:58) feature pip P are not going to be responsible for uh for for for that and they not need to be aware about the um about the implication where the when the data is going to be needed now um if you want to try out this pipeline uh if you want to try out you know these platters um we actually have as OB available um a cus platform right it's it's a free sandbox um that you can register um we launched it over um over a year ago and we have reached over 3,500 users right now on the platform um some like the platform is
(22:36) same as you were an OBS customer and deploy it in your own VPC on the cloud or on PR um it's the same API the same user experience as the same components for feature store so feature management um but also model registry and model service you can also you can also go and and serve your model um directly into into upso as said the same user experience same API that you will use um if you were upsource customer Enterprise customer um you don't have any infrastructure to manage um so everything is managed by us it's free um
(23:08) and you can just register and there is no there is no um time limit um we build a community around it um there's if you go on the csl.org website you can find a bunch of different examples that you can take inspiration from um all of them follow this FTI P pattern and so you can get a feeling of what the API what what this butn looks like in practice and we also have two amazing Educators Paul and po um they have been building a lot of tutorials and a lot of the um you know a lot of uh examples uh for the community
(23:40) um and you know they're really active in the community um and on upso now the S platform is is great uh but it's not meant for Enterprises it's meant for as a as a Sandbox for you to build your you know examples or to build your own um you know site project and so on uh we don't provide any strict SLA it's a sh infrastructure for everybody and you have limited quarters um that we also you know one of the things we actually get once we Lunes um we got a bunch of people asking for okay can do I get can
(24:12) I get more ports can I actually deploy my own um my own you know Enterprise applications on top of obss right so that's why today I'm super excited to announce that we are actually releasing um the same user experience of s as OB um so it's the same user experience you today in serverless but targeting Enterprises with you know both Enterprise slas and managed platform as a fully managed platform um it has the same set of functionalities uh both F store model registry and model survey it's currently being used by a
(24:45) couple different small set of users uh but you can register and you can join the waiting list uh right now by the QR code um and we open up a bunch of different um know slots every every couple every couple of weeks and so we're going to reach out to you uh once new available um you get a bunch of qus in terms of both online offline and number of model you can actually serve uh with again Enterprise slas and support from um the obso team one of the things that really excited that we worked on really hard
(25:18) throughout summer um we actually made OBS available um across multiple regions um this was one of the requirements that we work with um forun 500 company um and um this is targeting for 500 companies essentially right so um if you are for instance on Prem and or else on the cloud and you want to be able to um uh you know be available if one of your region goes down uh now you can deploy Ops so that um the data and metadata gets replicated uh between different regions so that if your region goes down um you can still um you can still be
(25:53) able to serve your uh applications and serve your business processes um um through from from from essentially the um the other region um we have again this is mostly to um you know help fortun for companies I said we we build it with a for for company um we're going to run webinars we're going to run a bunch of different events um going in details about um about these capabilities and what it means and how to deploy and so on um so you can you know feel free to um like join us um we have a um a bunch of different events
(26:27) coming up uh we're going to talk about it we are going to be um in several US cities um over the course next next couple of weeks you can find all the events in this QR code um and that's everything for me um yeah I don't know if there are any questions I think I don't have too much time but yeah let's see if we can take them thanks Fabio very exciting very interesting and great to see the the the feature store platforms being pushed forward with with features like multi- Region High availability and and serverless
(26:58) [Music]
