## CHAPTER 7: Model-Dependent and On-Demand Transformations
## ç¬¬7ç« : ãƒ¢ãƒ‡ãƒ«ä¾å­˜å‹ãŠã‚ˆã³ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰å¤‰æ›

In this chapter, we will look at data transformations in training and inference pipelines and how to ensure that transformations in both pipelines are equivalent. 
ã“ã®ç« ã§ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ãŠã‘ã‚‹ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã¨ã€ä¸¡æ–¹ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã®å¤‰æ›ãŒç­‰ä¾¡ã§ã‚ã‚‹ã“ã¨ã‚’ä¿è¨¼ã™ã‚‹æ–¹æ³•ã«ã¤ã„ã¦è¦‹ã¦ã„ãã¾ã™ã€‚

We introduced model-dependent transformations (MDTs) in Chapter 2 as data transformations that are performed on data after it has been read from the feature store and that create features that are specific to one model. 
ç¬¬2ç« ã§ã¯ã€ç‰¹å¾´ã‚¹ãƒˆã‚¢ã‹ã‚‰èª­ã¿å–ã£ãŸãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦è¡Œã‚ã‚Œã€ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã«å›ºæœ‰ã®ç‰¹å¾´ã‚’ç”Ÿæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ä¾å­˜å‹å¤‰æ›ï¼ˆMDTï¼‰ã‚’ç´¹ä»‹ã—ã¾ã—ãŸã€‚

There are two broad classes of MDTsâ€”feature transformations (for numerical and categorical features) and transformations that are tightly coupled to only one model. 
MDTã«ã¯å¤§ããåˆ†ã‘ã¦2ã¤ã®ã‚¯ãƒ©ã‚¹ãŒã‚ã‚Šã¾ã™ã€‚æ•°å€¤ãŠã‚ˆã³ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´ã®ãŸã‚ã®ç‰¹å¾´å¤‰æ›ã¨ã€ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã«å¯†æ¥ã«çµã³ã¤ã„ãŸå¤‰æ›ã§ã™ã€‚

An example of the former is one-hot encoding of categorical variables, while an example of the latter is text encoding for an LLM. 
å‰è€…ã®ä¾‹ã¯ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ã‚ã‚Šã€å¾Œè€…ã®ä¾‹ã¯LLMã®ãŸã‚ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ã™ã€‚

We also look at how to prevent _skew between MDTs that are applied separately in_ training and inference pipelines. 
ã¾ãŸã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§åˆ¥ã€…ã«é©ç”¨ã•ã‚Œã‚‹MDTé–“ã®_åã‚Šã‚’é˜²ãæ–¹æ³•_ã«ã¤ã„ã¦ã‚‚è¦‹ã¦ã„ãã¾ã™ã€‚

This is not always as trivial as applying the same versioned function in both training and inference pipelines, as many MDTs are stateful, requiring the same state (the modelâ€™s training data statistics) as a parameter in both training and inference pipelines. 
ã“ã‚Œã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¸¡æ–¹ã§åŒã˜ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®é–¢æ•°ã‚’é©ç”¨ã™ã‚‹ã“ã¨ãŒå¸¸ã«ç°¡å˜ã§ã¯ãªã„ãŸã‚ã§ã™ã€‚å¤šãã®MDTã¯çŠ¶æ…‹ã‚’æŒã¡ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¸¡æ–¹ã§åŒã˜çŠ¶æ…‹ï¼ˆãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆï¼‰ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦å¿…è¦ã¨ã—ã¾ã™ã€‚

We start by introducing common examples of feature transformations and different classes of model-specific transformations. 
ã¾ãšã€ä¸€èˆ¬çš„ãªç‰¹å¾´å¤‰æ›ã®ä¾‹ã¨ã€ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®å¤‰æ›ã®ç•°ãªã‚‹ã‚¯ãƒ©ã‚¹ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

We then look at different mechanisms for preventing skew, including Scikit-Learn pipelines, PyTorch transforms, and transformation functions in feature views for Hopsworks. 
æ¬¡ã«ã€Scikit-Learnãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€PyTorchå¤‰æ›ã€Hopsworksã®ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ã«ãŠã‘ã‚‹å¤‰æ›é–¢æ•°ãªã©ã€åã‚Šã‚’é˜²ããŸã‚ã®ã•ã¾ã–ã¾ãªãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚

We also cover our final class of data transformationâ€”on-demand transformations (ODTs) that are found in online inference pipelines and feature pipelines and are typically stateless transformation functions. 
æœ€å¾Œã«ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚„ç‰¹å¾´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«è¦‹ã‚‰ã‚Œã‚‹ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰å¤‰æ›ï¼ˆODTï¼‰ã¨ã„ã†ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã®æœ€çµ‚ã‚¯ãƒ©ã‚¹ã«ã¤ã„ã¦ã‚‚èª¬æ˜ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã¯é€šå¸¸ã€çŠ¶æ…‹ã‚’æŒãŸãªã„å¤‰æ›é–¢æ•°ã§ã™ã€‚

Then, we finish the chapter with unit testing of transformation functions with pytest. 
æœ€å¾Œã«ã€pytestã‚’ä½¿ç”¨ã—ãŸå¤‰æ›é–¢æ•°ã®å˜ä½“ãƒ†ã‚¹ãƒˆã§ç« ã‚’ç· ã‚ããã‚Šã¾ã™ã€‚

###### Feature Transformations
###### ç‰¹å¾´å¤‰æ›

Feature transformations can enhance the performance and convergence of various types of ML models. 
ç‰¹å¾´å¤‰æ›ã¯ã€ã•ã¾ã–ã¾ãªã‚¿ã‚¤ãƒ—ã®MLãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨åæŸã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

For example, most ML algorithms cannot accept strings as input, and they need to be transformed into a numerical format. 
ä¾‹ãˆã°ã€ã»ã¨ã‚“ã©ã®MLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯æ–‡å­—åˆ—ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å…¥ã‚Œã‚‹ã“ã¨ãŒã§ããšã€æ•°å€¤å½¢å¼ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

The final input to an ML model is typically a numeric array. 
MLãƒ¢ãƒ‡ãƒ«ã¸ã®æœ€çµ‚å…¥åŠ›ã¯é€šå¸¸ã€æ•°å€¤é…åˆ—ã§ã™ã€‚

Similarly, deep learning models often require numerical features to be normalized or transformed to follow a normal distribution to help ensure proper convergence. 
åŒæ§˜ã«ã€æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¯ã€æ•°å€¤ç‰¹å¾´ãŒæ­£è¦åˆ†å¸ƒã«å¾“ã†ã‚ˆã†ã«æ­£è¦åŒ–ã¾ãŸã¯å¤‰æ›ã•ã‚Œã‚‹ã“ã¨ã‚’å¿…è¦ã¨ã—ã€é©åˆ‡ãªåæŸã‚’ç¢ºä¿ã—ã¾ã™ã€‚

Different feature transformations are performed on a specific feature type (categorical or numerical). 
ç•°ãªã‚‹ç‰¹å¾´å¤‰æ›ã¯ã€ç‰¹å®šã®ç‰¹å¾´ã‚¿ã‚¤ãƒ—ï¼ˆã‚«ãƒ†ã‚´ãƒªã¾ãŸã¯æ•°å€¤ï¼‰ã«å¯¾ã—ã¦å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

The feature type helps identify which feature transformation is appropriate. 
ç‰¹å¾´ã‚¿ã‚¤ãƒ—ã¯ã€ã©ã®ç‰¹å¾´å¤‰æ›ãŒé©åˆ‡ã§ã‚ã‚‹ã‹ã‚’ç‰¹å®šã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚

For example, encoding is used to convert categorical variables into a numerical format, while scaling adjusts the range or distribution of numerical variables. 
ä¾‹ãˆã°ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¯ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’æ•°å€¤å½¢å¼ã«å¤‰æ›ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¯æ•°å€¤å¤‰æ•°ã®ç¯„å›²ã‚„åˆ†å¸ƒã‚’èª¿æ•´ã—ã¾ã™ã€‚

These transformations are often parameterized by properties of the training data, such as the set of categories or descriptive statistics (min, max, mean, standard deviation, or mode). 
ã“ã‚Œã‚‰ã®å¤‰æ›ã¯ã€ã‚«ãƒ†ã‚´ãƒªã®ã‚»ãƒƒãƒˆã‚„è¨˜è¿°çµ±è¨ˆï¼ˆæœ€å°å€¤ã€æœ€å¤§å€¤ã€å¹³å‡ã€æ¨™æº–åå·®ã€ã¾ãŸã¯æœ€é »å€¤ï¼‰ãªã©ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã«ã‚ˆã£ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚Œã‚‹ã“ã¨ãŒã‚ˆãã‚ã‚Šã¾ã™ã€‚

For example, when you one-hot encode a categorical variable, you first enumerate all of the categories in the training data, before you can encode the string as a binary vector. 
ä¾‹ãˆã°ã€ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹å ´åˆã€ã¾ãšãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿å†…ã®ã™ã¹ã¦ã®ã‚«ãƒ†ã‚´ãƒªã‚’åˆ—æŒ™ã—ã€ãã®å¾Œã«æ–‡å­—åˆ—ã‚’ãƒã‚¤ãƒŠãƒªãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

Similarly, when applying standardization (also called z-score normalization) to numerical variables, the mean and standard deviation must first be computed from the training data and then used to consistently scale all feature values in the dataset. 
åŒæ§˜ã«ã€æ•°å€¤å¤‰æ•°ã«æ¨™æº–åŒ–ï¼ˆzã‚¹ã‚³ã‚¢æ­£è¦åŒ–ã¨ã‚‚å‘¼ã°ã‚Œã‚‹ï¼‰ã‚’é©ç”¨ã™ã‚‹å ´åˆã€å¹³å‡ã¨æ¨™æº–åå·®ã¯ã¾ãšãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨ˆç®—ã•ã‚Œã€ãã®å¾Œã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ã™ã¹ã¦ã®ç‰¹å¾´å€¤ã‚’ä¸€è²«ã—ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

###### Encoding Categorical Variables
###### ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

In feature-encoding algorithms, the set of categories may change over time, and to handle this, you should include a special category (called â€œunknownâ€ or â€œotherâ€) for any new categories that appear during inference. 
ç‰¹å¾´ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã¯ã€ã‚«ãƒ†ã‚´ãƒªã®ã‚»ãƒƒãƒˆã¯æ™‚é–“ã¨ã¨ã‚‚ã«å¤‰åŒ–ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€ã“ã‚Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€æ¨è«–ä¸­ã«ç¾ã‚Œã‚‹æ–°ã—ã„ã‚«ãƒ†ã‚´ãƒªã®ãŸã‚ã«ç‰¹åˆ¥ãªã‚«ãƒ†ã‚´ãƒªï¼ˆã€Œä¸æ˜ã€ã¾ãŸã¯ã€Œãã®ä»–ã€ã¨å‘¼ã°ã‚Œã‚‹ï¼‰ã‚’å«ã‚ã‚‹ã¹ãã§ã™ã€‚

For example, the merchant category code given for a credit card payment is important for many bonus rewards programs that give points for a specific type of spending, such as travel. 
ä¾‹ãˆã°ã€ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰æ”¯æ‰•ã„ã«å¯¾ã—ã¦ä¸ãˆã‚‰ã‚Œã‚‹å•†æ¥­ã‚«ãƒ†ã‚´ãƒªã‚³ãƒ¼ãƒ‰ã¯ã€æ—…è¡Œãªã©ã®ç‰¹å®šã®æ”¯å‡ºã‚¿ã‚¤ãƒ—ã«å¯¾ã—ã¦ãƒã‚¤ãƒ³ãƒˆã‚’ä»˜ä¸ã™ã‚‹å¤šãã®ãƒœãƒ¼ãƒŠã‚¹å ±é…¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«ã¨ã£ã¦é‡è¦ã§ã™ã€‚

Each merchant typically has a single category that is added to a credit card payment. 
å„å•†æ¥­è€…ã¯é€šå¸¸ã€ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰æ”¯æ‰•ã„ã«è¿½åŠ ã•ã‚Œã‚‹å˜ä¸€ã®ã‚«ãƒ†ã‚´ãƒªã‚’æŒã£ã¦ã„ã¾ã™ã€‚

In Table 7-1, we one-hot encode the categories. 
è¡¨7-1ã§ã¯ã€ã‚«ãƒ†ã‚´ãƒªã‚’ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã—ã¾ã™ã€‚

For simplicity, I only show four categories, whereas in reality, there are hundreds. 
ç°¡å˜ã®ãŸã‚ã«ã€ç§ã¯4ã¤ã®ã‚«ãƒ†ã‚´ãƒªã®ã¿ã‚’ç¤ºã—ã¾ã™ãŒã€å®Ÿéš›ã«ã¯æ•°ç™¾ã‚ã‚Šã¾ã™ã€‚

Each one-hot-encoded array represents a category with a 1 in the categoryâ€™s position in the array and a 0 in all other positions. 
å„ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸé…åˆ—ã¯ã€é…åˆ—å†…ã®ã‚«ãƒ†ã‚´ãƒªã®ä½ç½®ã«1ã‚’æŒã¡ã€ä»–ã®ã™ã¹ã¦ã®ä½ç½®ã«0ã‚’æŒã¤ã‚«ãƒ†ã‚´ãƒªã‚’è¡¨ã—ã¾ã™ã€‚

_Table 7-1. One-hot encoding of the merchant category for a credit card payment_
**è¡¨7-1. ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰æ”¯æ‰•ã„ã®ãŸã‚ã®å•†æ¥­ã‚«ãƒ†ã‚´ãƒªã®ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**
**Merchant category** **One-hot encoded**
**å•†æ¥­ã‚«ãƒ†ã‚´ãƒª** **ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**
Airlines [1,0,0,0]  
Eating places and restaurants [0,1,0,0]  
Car rental [0,0,1,0]  
Hotels, motels, and resorts [0,0,0,1]  

One-hot encoding is not recommended when there is _high cardinality (i.e., a large_ number of categories), as each category adds a new dimension, increasing memory usage. 
é«˜ã„ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ï¼ˆã™ãªã‚ã¡ã€å¤šãã®ã‚«ãƒ†ã‚´ãƒªï¼‰ãŒã‚ã‚‹å ´åˆã€ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¯æ¨å¥¨ã•ã‚Œã¾ã›ã‚“ã€‚å„ã‚«ãƒ†ã‚´ãƒªãŒæ–°ã—ã„æ¬¡å…ƒã‚’è¿½åŠ ã—ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—åŠ ã™ã‚‹ãŸã‚ã§ã™ã€‚

It is also unsuitable when there is an ordinal relationship between categories, as it does not preserve order, as shown in Table 7-2. 
ã¾ãŸã€ã‚«ãƒ†ã‚´ãƒªé–“ã«é †åºé–¢ä¿‚ãŒã‚ã‚‹å ´åˆã«ã‚‚ä¸é©åˆ‡ã§ã™ã€‚è¡¨7-2ã«ç¤ºã™ã‚ˆã†ã«ã€é †åºã‚’ä¿æŒã—ãªã„ãŸã‚ã§ã™ã€‚

If there is an ordinal relationship between the variables, then the ordinal encoder preserves ordering in the transformed categories. 
å¤‰æ•°é–“ã«é †åºé–¢ä¿‚ãŒã‚ã‚‹å ´åˆã€é †åºã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯å¤‰æ›ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã®é †åºã‚’ä¿æŒã—ã¾ã™ã€‚

_Table 7-2. Popular algorithms for encoding categorical feature data_
**è¡¨7-2. ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«é–¢ã™ã‚‹ä¸€èˆ¬çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **
**Algorithm** **Purpose** **Use case**  
**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ** **ç›®çš„** **ä½¿ç”¨ä¾‹**  
One-hot encoder Transforms categorical data into one-hot-encoded vectors (an array of bytes, with each category representing one bit)  
ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯ã€ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ï¼ˆå„ã‚«ãƒ†ã‚´ãƒªãŒ1ãƒ“ãƒƒãƒˆã‚’è¡¨ã™ãƒã‚¤ãƒˆã®é…åˆ—ï¼‰ã«å¤‰æ›ã—ã¾ã™ã€‚  
Transforming to one-hot encoder when there is no ordinal relationship and low to medium cardinality  
é †åºé–¢ä¿‚ãŒãªãã€ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ãŒä½ã„ã‹ã‚‰ä¸­ç¨‹åº¦ã®ã¨ãã«ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã«å¤‰æ›ã—ã¾ã™ã€‚  
Ordinal encoder Transforms categorical data into an integer  
é †åºã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯ã€ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ã‚’æ•´æ•°ã«å¤‰æ›ã—ã¾ã™ã€‚  
Encoding features that have an ordinal relationship  
é †åºé–¢ä¿‚ã®ã‚ã‚‹ç‰¹å¾´ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¾ã™ã€‚  
Feature hasher Uses the hashing trick to transform categorical data into a fixed-size vector  
ç‰¹å¾´ãƒãƒƒã‚·ãƒ£ãƒ¼ã¯ã€ãƒãƒƒã‚·ãƒ³ã‚°ãƒˆãƒªãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¦ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ã‚’å›ºå®šã‚µã‚¤ã‚ºã®ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã—ã¾ã™ã€‚  
High-dimensional data with many unique categories  
å¤šãã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚«ãƒ†ã‚´ãƒªã‚’æŒã¤é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿  

For features with a very large number of categories, feature hashing (the _feature_ _hasher encoding algorithm) reduces dimensionality by mapping categories to a fixed-size hash table, though this introduces the risk of hash collisions (that is, different catâ€ egories mapping to the same value). 
éå¸¸ã«å¤šãã®ã‚«ãƒ†ã‚´ãƒªã‚’æŒã¤ç‰¹å¾´ã«å¯¾ã—ã¦ã€ç‰¹å¾´ãƒãƒƒã‚·ãƒ³ã‚°ï¼ˆ_ç‰¹å¾´_ _ãƒãƒƒã‚·ãƒ£ãƒ¼ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰ã¯ã€ã‚«ãƒ†ã‚´ãƒªã‚’å›ºå®šã‚µã‚¤ã‚ºã®ãƒãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦æ¬¡å…ƒã‚’å‰Šæ¸›ã—ã¾ã™ãŒã€ã“ã‚Œã«ã‚ˆã‚Šãƒãƒƒã‚·ãƒ¥è¡çªï¼ˆç•°ãªã‚‹ã‚«ãƒ†ã‚´ãƒªãŒåŒã˜å€¤ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã‚‹ãƒªã‚¹ã‚¯ï¼‰ãŒç”Ÿã˜ã¾ã™ã€‚

Be sure that your ML algorithm can tolerate possible hash collisions if you use a feature hasher. 
ç‰¹å¾´ãƒãƒƒã‚·ãƒ£ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€MLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒå¯èƒ½ãªãƒãƒƒã‚·ãƒ¥è¡çªã«è€ãˆã‚‰ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

Finally, label encoding is often used for encoding the target/label variable as integers, thus preserving ordering. 
æœ€å¾Œã«ã€ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¯ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ/ãƒ©ãƒ™ãƒ«å¤‰æ•°ã‚’æ•´æ•°ã¨ã—ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã«ã—ã°ã—ã°ä½¿ç”¨ã•ã‚Œã€ã“ã‚Œã«ã‚ˆã‚Šé †åºãŒä¿æŒã•ã‚Œã¾ã™ã€‚

Many ML algorithms, such as Scikit-Learnâ€™s logistic regression and XGBoostâ€™s multiclass classification, require labels (target variables) to be integer encoded. 
Scikit-Learnã®ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚„XGBoostã®å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ãªã©ã€å¤šãã®MLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€ãƒ©ãƒ™ãƒ«ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼‰ãŒæ•´æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã“ã¨ã‚’è¦æ±‚ã—ã¾ã™ã€‚

Note that for some tree-based algorithms, such as _[CatBoost, you do not need to](https://catboost.ai)_ encode categorical variables. 
_[CatBoost](https://catboost.ai)ãªã©ã®ä¸€éƒ¨ã®æœ¨ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã¯ã€ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚_

CatBoost can handle categorical variables with high cardinality, and it preserves ordinal informationâ€”without the need to spend CPU cycles encoding the categorical data. 
CatBoostã¯é«˜ã„ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã‚’æŒã¤ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’å‡¦ç†ã§ãã€ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã«CPUã‚µã‚¤ã‚¯ãƒ«ã‚’è²»ã‚„ã™å¿…è¦ãªãã€é †åºæƒ…å ±ã‚’ä¿æŒã—ã¾ã™ã€‚

CatBoost can also train models with lots of categorical variables with better performance than XGBoost, for example, through automatically extracting complex interactions between categorical features and by reducing overfitting. 
CatBoostã¯ã€ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é–“ã®è¤‡é›‘ãªç›¸äº’ä½œç”¨ã‚’è‡ªå‹•çš„ã«æŠ½å‡ºã—ã€éå‰°é©åˆã‚’æ¸›å°‘ã•ã›ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€XGBoostã‚ˆã‚Šã‚‚å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã§å¤šãã®ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

###### Distributions of Numerical Variables
###### æ•°å€¤å¤‰æ•°ã®åˆ†å¸ƒ

Many ML algorithms only work well when a numerical feature follows a particular data distribution. 
å¤šãã®MLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€æ•°å€¤ç‰¹å¾´ãŒç‰¹å®šã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«å¾“ã†å ´åˆã«ã®ã¿ã†ã¾ãæ©Ÿèƒ½ã—ã¾ã™ã€‚

For example, if the distribution of your numerical feature data is skewed and your ML algorithm is based on gradient descent (such as neural networks or linear regression), you should standardize the data. 
ä¾‹ãˆã°ã€æ•°å€¤ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒãŒæ­ªã‚“ã§ã„ã¦ã€MLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒå‹¾é…é™ä¸‹æ³•ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚„ç·šå½¢å›å¸°ãªã©ï¼‰ã«åŸºã¥ã„ã¦ã„ã‚‹å ´åˆã€ãƒ‡ãƒ¼ã‚¿ã‚’æ¨™æº–åŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

Standardization transforms a numerical variableâ€™s distribution to have a mean of zero and a unit variance (standard deviation) of one. 
æ¨™æº–åŒ–ã¯ã€æ•°å€¤å¤‰æ•°ã®åˆ†å¸ƒã‚’å¹³å‡0ã€åˆ†æ•£ï¼ˆæ¨™æº–åå·®ï¼‰1ã«å¤‰æ›ã—ã¾ã™ã€‚

This will improve gradient descentâ€™s convergence speed and subsequent model stability. 
ã“ã‚Œã«ã‚ˆã‚Šã€å‹¾é…é™ä¸‹æ³•ã®åæŸé€Ÿåº¦ã¨ãã®å¾Œã®ãƒ¢ãƒ‡ãƒ«ã®å®‰å®šæ€§ãŒå‘ä¸Šã—ã¾ã™ã€‚

Figure 7-1 shows some of the most common distributions for numerical variables. 
å›³7-1ã¯ã€æ•°å€¤å¤‰æ•°ã®æœ€ã‚‚ä¸€èˆ¬çš„ãªåˆ†å¸ƒã®ã„ãã¤ã‹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

It is good practice to identify the distribution of each numerical variable, so that when you use an ML algorithm with that feature, you know which transformation algorithm, if any, to apply to the feature data. 
å„æ•°å€¤å¤‰æ•°ã®åˆ†å¸ƒã‚’ç‰¹å®šã™ã‚‹ã“ã¨ã¯è‰¯ã„ç¿’æ…£ã§ã‚ã‚Šã€ãã®ç‰¹å¾´ã‚’æŒã¤MLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã™ã‚‹éš›ã«ã€ã©ã®å¤‰æ›ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã«é©ç”¨ã™ã¹ãã‹ã‚’çŸ¥ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

_Figure 7-1. An illustrative guide to some common numerical feature distributions. The_ _log-normal distribution has a longer tail than the exponential distribution and is not a_ _max at 0 on the x-axis._  
_å›³7-1. ä¸€éƒ¨ã®ä¸€èˆ¬çš„ãªæ•°å€¤ç‰¹å¾´åˆ†å¸ƒã«é–¢ã™ã‚‹èª¬æ˜ã‚¬ã‚¤ãƒ‰ã€‚å¯¾æ•°æ­£è¦åˆ†å¸ƒã¯æŒ‡æ•°åˆ†å¸ƒã‚ˆã‚Šã‚‚é•·ã„å°¾ã‚’æŒã¡ã€xè»¸ã®0ã§æœ€å¤§ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚_

Returning to our credit card fraud system, we give examples of these distributions for credit card transactions: 
ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰è©æ¬ºã‚·ã‚¹ãƒ†ãƒ ã«æˆ»ã‚‹ã¨ã€ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰å–å¼•ã®ã“ã‚Œã‚‰ã®åˆ†å¸ƒã®ä¾‹ã‚’ç¤ºã—ã¾ã™ã€‚

- The `credit_rating for a bank typically follows a` _normal distribution, with a_ small number of banks having the highest and lowest ratings and most banks clustered around the mean rating. 
- éŠ€è¡Œã®`credit_ratingã¯é€šå¸¸ã€_æ­£è¦åˆ†å¸ƒã«å¾“ã„ã€æœ€ã‚‚é«˜ã„ãŠã‚ˆã³æœ€ã‚‚ä½ã„è©•ä¾¡ã‚’æŒã¤éŠ€è¡Œã¯å°‘æ•°ã§ã‚ã‚Šã€ã»ã¨ã‚“ã©ã®éŠ€è¡Œã¯å¹³å‡è©•ä¾¡ã®å‘¨ã‚Šã«é›†ã¾ã£ã¦ã„ã¾ã™ã€‚_

- A _uniform distribution means each possible value has an equal probability of_ occurring. 
- _ä¸€æ§˜åˆ†å¸ƒã¯ã€å„å¯èƒ½ãªå€¤ãŒç™ºç”Ÿã™ã‚‹ç¢ºç‡ãŒç­‰ã—ã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚_

None of our features in the credit card model are truly uniform. 
ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´ã®ä¸­ã«ã¯ã€çœŸã«ä¸€æ§˜ãªã‚‚ã®ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

Often, variables may start with a uniform distribution, but through grouping or transformation, you can extract new features that have more informative, nonâ€ uniform distributions. 
ã—ã°ã—ã°ã€å¤‰æ•°ã¯ä¸€æ§˜åˆ†å¸ƒã‹ã‚‰å§‹ã¾ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ãŒã€ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã‚„å¤‰æ›ã‚’é€šã˜ã¦ã€ã‚ˆã‚Šæƒ…å ±é‡ã®å¤šã„éä¸€æ§˜åˆ†å¸ƒã‚’æŒã¤æ–°ã—ã„ç‰¹å¾´ã‚’æŠ½å‡ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

- The binomial distribution models discrete outcomes (success/failure) over multiple independent trials. 
- _äºŒé …åˆ†å¸ƒã¯ã€è¤‡æ•°ã®ç‹¬ç«‹ã—ãŸè©¦è¡Œã«ãŠã‘ã‚‹é›¢æ•£çš„ãªçµæœï¼ˆæˆåŠŸ/å¤±æ•—ï¼‰ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ã€‚_

Although not a feature in our credit card model, the probability that a merchant terminal will work or not could be represented as a binomial distribution with a reliability probability of, say, 0.98; that is, 98% of transactions would be successfully processed without errors. 
ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´ã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€å•†æ¥­ç«¯æœ«ãŒæ©Ÿèƒ½ã™ã‚‹ã‹ã©ã†ã‹ã®ç¢ºç‡ã¯ã€ä¿¡é ¼æ€§ç¢ºç‡ãŒ0.98ã§ã‚ã‚‹äºŒé …åˆ†å¸ƒã¨ã—ã¦è¡¨ã™ã“ã¨ãŒã§ãã¾ã™ã€‚ã¤ã¾ã‚Šã€98%ã®å–å¼•ãŒã‚¨ãƒ©ãƒ¼ãªã—ã§æ­£å¸¸ã«å‡¦ç†ã•ã‚Œã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚

- The _Poisson distribution models the number of times independent events occur_ within a fixed interval of time. 
- _ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒã¯ã€å›ºå®šã•ã‚ŒãŸæ™‚é–“é–“éš”å†…ã§ç‹¬ç«‹ã—ãŸã‚¤ãƒ™ãƒ³ãƒˆãŒç™ºç”Ÿã™ã‚‹å›æ•°ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ã€‚_

For example, we could model how many credit card fraud detections occur on average per day as a Poisson distribution. 
ä¾‹ãˆã°ã€1æ—¥ã«å¹³å‡ã—ã¦ä½•ä»¶ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰è©æ¬ºæ¤œå‡ºãŒç™ºç”Ÿã™ã‚‹ã‹ã‚’ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒã¨ã—ã¦ãƒ¢ãƒ‡ãƒ«åŒ–ã§ãã¾ã™ã€‚

The model can decide when to generate alerts if the number of credit card fraud detections is deemed to be anomalous. 
ãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰è©æ¬ºæ¤œå‡ºã®æ•°ãŒç•°å¸¸ã§ã‚ã‚‹ã¨è¦‹ãªã•ã‚Œã‚‹å ´åˆã«ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’æ±ºå®šã§ãã¾ã™ã€‚

- The _exponential distribution can model the time between independent transacâ€_ tions, when events occur continuously and independently at a constant average rate. 
- _æŒ‡æ•°åˆ†å¸ƒã¯ã€ã‚¤ãƒ™ãƒ³ãƒˆãŒä¸€å®šã®å¹³å‡ãƒ¬ãƒ¼ãƒˆã§é€£ç¶šçš„ã‹ã¤ç‹¬ç«‹ã«ç™ºç”Ÿã™ã‚‹å ´åˆã®ç‹¬ç«‹ã—ãŸå–å¼•é–“ã®æ™‚é–“ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã§ãã¾ã™ã€‚_

For example, the average waiting time between card transactions is three hours, meaning short intervals (minutes) are common and much longer waits (days) are less frequent. 
ä¾‹ãˆã°ã€ã‚«ãƒ¼ãƒ‰å–å¼•é–“ã®å¹³å‡å¾…æ©Ÿæ™‚é–“ã¯3æ™‚é–“ã§ã‚ã‚Šã€çŸ­ã„é–“éš”ï¼ˆæ•°åˆ†ï¼‰ãŒä¸€èˆ¬çš„ã§ã€ã¯ã‚‹ã‹ã«é•·ã„å¾…æ©Ÿæ™‚é–“ï¼ˆæ•°æ—¥ï¼‰ã¯å°‘ãªã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚

- The amount spent in a credit card transaction follows a skewed distribution, with a large number of small amounts and a small number of large amounts. 
- ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰å–å¼•ã§ã®æ”¯å‡ºé¡ã¯æ­ªã‚“ã åˆ†å¸ƒã«å¾“ã„ã€å¤šãã®å°é¡ã¨å°‘æ•°ã®å¤§é¡ãŒã‚ã‚Šã¾ã™ã€‚

- The bimodal distribution can help us model the amount spent by each customer on a holiday using two different subgroupsâ€”each of which follows a normal distribution. 
- ãƒã‚¤ãƒ¢ãƒ¼ãƒ€ãƒ«åˆ†å¸ƒã¯ã€ç•°ãªã‚‹2ã¤ã®ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ç”¨ã—ã¦ã€ä¼‘æ—¥ã«å„é¡§å®¢ãŒæ”¯å‡ºã—ãŸé‡‘é¡ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚å„ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ã¯æ­£è¦åˆ†å¸ƒã«å¾“ã„ã¾ã™ã€‚

Regular shoppers spend a mean of $200 (the first peak) and holiday shoppers spend a mean of $800 (the second peak). 
é€šå¸¸ã®è²·ã„ç‰©å®¢ã¯å¹³å‡200ãƒ‰ãƒ«ï¼ˆæœ€åˆã®ãƒ”ãƒ¼ã‚¯ï¼‰ã‚’æ”¯å‡ºã—ã€ä¼‘æ—¥ã®è²·ã„ç‰©å®¢ã¯å¹³å‡800ãƒ‰ãƒ«ï¼ˆ2ç•ªç›®ã®ãƒ”ãƒ¼ã‚¯ï¼‰ã‚’æ”¯å‡ºã—ã¾ã™ã€‚

- Finally, the amount spent in individual credit card transactions typically follows a type of skewed distribution called the log-normal distribution. 
- æœ€å¾Œã«ã€å€‹ã€…ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰å–å¼•ã§ã®æ”¯å‡ºé¡ã¯ã€é€šå¸¸ã€å¯¾æ•°æ­£è¦åˆ†å¸ƒã¨å‘¼ã°ã‚Œã‚‹ã‚¿ã‚¤ãƒ—ã®æ­ªã‚“ã åˆ†å¸ƒã«å¾“ã„ã¾ã™ã€‚

Its characteristics are that the amounts are nonnegative and it is positively skewed to the right (most payments are small, with fewer large payments). 
ãã®ç‰¹å¾´ã¯ã€é‡‘é¡ãŒéè² ã§ã‚ã‚Šã€å³ã«æ­£ã®æ­ªã¿ãŒã‚ã‚‹ã“ã¨ã§ã™ï¼ˆã»ã¨ã‚“ã©ã®æ”¯æ‰•ã„ã¯å°é¡ã§ã€å¤§ããªæ”¯æ‰•ã„ã¯å°‘ãªã„ï¼‰ã€‚



. Its characteristics are that the amounts are nonnegative and it is positively skewed to the right (most payments are small, with fewer large payments).  
ãã®ç‰¹å¾´ã¯ã€é‡‘é¡ãŒéè² ã§ã‚ã‚Šã€å³ã«æ­£ã®æ­ªã¿ãŒã‚ã‚‹ã“ã¨ã§ã™ï¼ˆã»ã¨ã‚“ã©ã®æ”¯æ‰•ã„ã¯å°é¡ã§ã€å¤§ããªæ”¯æ‰•ã„ã¯å°‘ãªã„ã§ã™ï¼‰ã€‚  

###### Transforming Numerical Variables
###### æ•°å€¤å¤‰æ•°ã®å¤‰æ›

Standardizing numerical feature distributions is a common transformation that should be performed on many ML algorithmsâ€”not just the gradient descent mentioned earlier but also kNN and support vector machines (SVMs).  
æ•°å€¤ç‰¹å¾´ã®åˆ†å¸ƒã‚’æ¨™æº–åŒ–ã™ã‚‹ã“ã¨ã¯ã€å¤šãã®æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§è¡Œã†ã¹ãä¸€èˆ¬çš„ãªå¤‰æ›ã§ã™ã€‚ã“ã‚Œã¯ã€å‰è¿°ã®å‹¾é…é™ä¸‹æ³•ã ã‘ã§ãªãã€kNNã‚„ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ï¼ˆSVMï¼‰ã«ã‚‚å½“ã¦ã¯ã¾ã‚Šã¾ã™ã€‚

An alternative to standardization is _normalization (also known as_ _min-max scaling), which similarly_ improves model convergence speed but does so by only scaling the range of values.  
æ¨™æº–åŒ–ã®ä»£æ›¿æ‰‹æ®µã¯ã€_æ­£è¦åŒ–ï¼ˆmin-maxã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¨ã‚‚å‘¼ã°ã‚Œã‚‹ï¼‰_ã§ã‚ã‚Šã€åŒæ§˜ã«ãƒ¢ãƒ‡ãƒ«ã®åæŸé€Ÿåº¦ã‚’æ”¹å–„ã—ã¾ã™ãŒã€å€¤ã®ç¯„å›²ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã ã‘ã§è¡Œã„ã¾ã™ã€‚

Normalization rescales values to a fixed range, such as 0 to 1, while preserving their original distribution shape.  
æ­£è¦åŒ–ã¯ã€å€¤ã‚’0ã‹ã‚‰1ã®ã‚ˆã†ãªå›ºå®šç¯„å›²ã«å†ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ã€å…ƒã®åˆ†å¸ƒã®å½¢çŠ¶ã‚’ä¿æŒã—ã¾ã™ã€‚

Standardization, in contrast, also transforms the distribution shape.  
å¯¾ç…§çš„ã«ã€æ¨™æº–åŒ–ã¯åˆ†å¸ƒã®å½¢çŠ¶ã‚‚å¤‰æ›ã—ã¾ã™ã€‚

For example, credit card transaction amounts can range from $0.01 to $10,000, and account balances can range from $0 to millions of dollars.  
ä¾‹ãˆã°ã€ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã®å–å¼•é‡‘é¡ã¯$0.01ã‹ã‚‰$10,000ã¾ã§ã®ç¯„å›²ã§ã‚ã‚Šã€å£åº§æ®‹é«˜ã¯$0ã‹ã‚‰æ•°ç™¾ä¸‡ãƒ‰ãƒ«ã¾ã§ã®ç¯„å›²ã§ã™ã€‚

If you donâ€™t standardize or normalize the amounts and balances, gradient descent can produce large, erratic updates during training.  
é‡‘é¡ã‚„æ®‹é«˜ã‚’æ¨™æº–åŒ–ã¾ãŸã¯æ­£è¦åŒ–ã—ãªã„ã¨ã€å‹¾é…é™ä¸‹æ³•ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«å¤§ããä¸è¦å‰‡ãªæ›´æ–°ã‚’ç”Ÿæˆã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

Clustering algorithms, like kNN and SVMs, rely on distance values and also benefit from standardization or normalization, as do probabilistic models, like Gaussian Naive Bayes.  
kNNã‚„SVMã®ã‚ˆã†ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯è·é›¢å€¤ã«ä¾å­˜ã—ã€æ¨™æº–åŒ–ã‚„æ­£è¦åŒ–ã®æ©æµã‚’å—ã‘ã¾ã™ã€‚ã‚¬ã‚¦ã‚¹ãƒŠã‚¤ãƒ¼ãƒ–ãƒ™ã‚¤ã‚ºã®ã‚ˆã†ãªç¢ºç‡ãƒ¢ãƒ‡ãƒ«ã‚‚åŒæ§˜ã§ã™ã€‚

In such models, without standardization or normalization, an amount or account balance with a large range of values can dominate other features in a model.  
ãã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ã§ã¯ã€æ¨™æº–åŒ–ã‚„æ­£è¦åŒ–ãŒãªã„ã¨ã€å¤§ããªç¯„å›²ã®å€¤ã‚’æŒã¤é‡‘é¡ã‚„å£åº§æ®‹é«˜ãŒãƒ¢ãƒ‡ãƒ«å†…ã®ä»–ã®ç‰¹å¾´ã‚’æ”¯é…ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

So when should you choose normalization over standardization?  
ã§ã¯ã€ã„ã¤æ­£è¦åŒ–ã‚’æ¨™æº–åŒ–ã®ä»£ã‚ã‚Šã«é¸ã¶ã¹ãã§ã—ã‚‡ã†ã‹ï¼Ÿ

Here are two rules of thumb:  
ä»¥ä¸‹ã«2ã¤ã®çµŒé¨“å‰‡ã‚’ç¤ºã—ã¾ã™ï¼š

- Normalization is often a good fit for neural networks and when the original feature distribution is important.  
  æ­£è¦åŒ–ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚„å…ƒã®ç‰¹å¾´åˆ†å¸ƒãŒé‡è¦ãªå ´åˆã«é©ã—ã¦ã„ã‚‹ã“ã¨ãŒå¤šã„ã§ã™ã€‚

- For example, if outliers in your data are meaningful and not anomalies, normalization may be preferred because it preserves the original shape of the distribution.  
  ä¾‹ãˆã°ã€ãƒ‡ãƒ¼ã‚¿å†…ã®å¤–ã‚Œå€¤ãŒæ„å‘³ã®ã‚ã‚‹ã‚‚ã®ã§ã‚ã‚Šç•°å¸¸ã§ãªã„å ´åˆã€æ­£è¦åŒ–ã¯å…ƒã®åˆ†å¸ƒã®å½¢çŠ¶ã‚’ä¿æŒã™ã‚‹ãŸã‚å¥½ã¾ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

- Standardization is usually preferred for linear models, distance-based algorithms, and when you assume features should be normally distributed.  
  æ¨™æº–åŒ–ã¯é€šå¸¸ã€ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã€è·é›¢ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ãŠã‚ˆã³ç‰¹å¾´ãŒæ­£è¦åˆ†å¸ƒã§ã‚ã‚‹ã¹ãã¨ä»®å®šã™ã‚‹å ´åˆã«å¥½ã¾ã‚Œã¾ã™ã€‚

Ultimately, the best choice depends on your data and model, so you may need to experiment with both approaches.  
æœ€çµ‚çš„ã«ã¯ã€æœ€é©ãªé¸æŠã¯ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã«ä¾å­˜ã™ã‚‹ãŸã‚ã€ä¸¡æ–¹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è©¦ã™å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚

Another important class of transformation is _log transformations.  
ã‚‚ã†ä¸€ã¤ã®é‡è¦ãªå¤‰æ›ã®ã‚¯ãƒ©ã‚¹ã¯ã€_å¯¾æ•°å¤‰æ›_ã§ã™ã€‚

Highly skewed numerical variables, such as transaction amounts, can negatively impact model performance, especially when outliers dominate the data.  
å–å¼•é‡‘é¡ã®ã‚ˆã†ãªé«˜åº¦ã«æ­ªã‚“ã æ•°å€¤å¤‰æ•°ã¯ã€ç‰¹ã«å¤–ã‚Œå€¤ãŒãƒ‡ãƒ¼ã‚¿ã‚’æ”¯é…ã™ã‚‹å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«æ‚ªå½±éŸ¿ã‚’åŠã¼ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

Log transformations help reduce skewness and compress the range of values, making the distribution closer to normal and reducing the influence of extreme values.  
å¯¾æ•°å¤‰æ›ã¯æ­ªã¿ã‚’æ¸›å°‘ã•ã›ã€å€¤ã®ç¯„å›²ã‚’åœ§ç¸®ã—ã€åˆ†å¸ƒã‚’æ­£è¦ã«è¿‘ã¥ã‘ã€æ¥µç«¯ãªå€¤ã®å½±éŸ¿ã‚’æ¸›å°‘ã•ã›ã¾ã™ã€‚

Log transformations are especially effective for right-skewed data.  
å¯¾æ•°å¤‰æ›ã¯ç‰¹ã«å³ã«æ­ªã‚“ã ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦åŠ¹æœçš„ã§ã™ã€‚

However, your data should not contain zeros or negative values, since the logarithm is undefined for those cases.  
ãŸã ã—ã€ãƒ‡ãƒ¼ã‚¿ã«ã¯ã‚¼ãƒ­ã‚„è² ã®å€¤ãŒå«ã¾ã‚Œã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚ãªãœãªã‚‰ã€å¯¾æ•°ã¯ãã®å ´åˆã«å®šç¾©ã•ã‚Œãªã„ã‹ã‚‰ã§ã™ã€‚

If your data does include zeros, you can use a modified transformation such as log 1 + x .  
ãƒ‡ãƒ¼ã‚¿ã«ã‚¼ãƒ­ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€log 1 + xã®ã‚ˆã†ãªä¿®æ­£ã•ã‚ŒãŸå¤‰æ›ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚

Not all ML algorithms require transformation of numerical features, though.  
ãŸã ã—ã€ã™ã¹ã¦ã®æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒæ•°å€¤ç‰¹å¾´ã®å¤‰æ›ã‚’å¿…è¦ã¨ã™ã‚‹ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

There is no need to transform numerical features for tree-based models, such as gradient-boosted decision trees and random forests, since they are unaffected by the scale of features when splitting nodes.  
å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒˆæ±ºå®šæœ¨ã‚„ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ã‚ˆã†ãªæœ¨ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ãƒãƒ¼ãƒ‰ã‚’åˆ†å‰²ã™ã‚‹éš›ã«ç‰¹å¾´ã®ã‚¹ã‚±ãƒ¼ãƒ«ã«å½±éŸ¿ã•ã‚Œãªã„ãŸã‚ã€æ•°å€¤ç‰¹å¾´ã‚’å¤‰æ›ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

However, certain transformations, such as reducing extreme skewness or simplifying feature interactions, improve tree model performance.  
ãŸã ã—ã€æ¥µç«¯ãªæ­ªã¿ã‚’æ¸›å°‘ã•ã›ãŸã‚Šã€ç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’å˜ç´”åŒ–ã—ãŸã‚Šã™ã‚‹ã‚ˆã†ãªç‰¹å®šã®å¤‰æ›ã¯ã€æœ¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚

For example, log-transforming a highly skewed variable can help balance splits and allow the model to better capture patterns across the data range.  
ä¾‹ãˆã°ã€é«˜åº¦ã«æ­ªã‚“ã å¤‰æ•°ã‚’å¯¾æ•°å¤‰æ›ã™ã‚‹ã“ã¨ã§ã€åˆ†å‰²ã‚’ãƒãƒ©ãƒ³ã‚¹ã•ã›ã€ãƒ¢ãƒ‡ãƒ«ãŒãƒ‡ãƒ¼ã‚¿ç¯„å›²å…¨ä½“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚ˆã‚Šã‚ˆãæ‰ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

When youâ€™re computing transformations, you must first make a full pass of the feature values of some of them to compute descriptive statistics, such as the mean, standard deviation, minimum, and maximum values.  
å¤‰æ›ã‚’è¨ˆç®—ã™ã‚‹éš›ã¯ã€ã¾ãšã„ãã¤ã‹ã®ç‰¹å¾´å€¤ã®å®Œå…¨ãªãƒ‘ã‚¹ã‚’å®Ÿè¡Œã—ã¦ã€å¹³å‡ã€æ¨™æº–åå·®ã€æœ€å°å€¤ã€æœ€å¤§å€¤ãªã©ã®è¨˜è¿°çµ±è¨ˆã‚’è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

The second pass can then update each data point by applying the transformation.  
æ¬¡ã®ãƒ‘ã‚¹ã§ã¯ã€å¤‰æ›ã‚’é©ç”¨ã—ã¦å„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’æ›´æ–°ã§ãã¾ã™ã€‚

Here are examples of how common transformations are computed:  
ä»¥ä¸‹ã¯ã€ä¸€èˆ¬çš„ãªå¤‰æ›ãŒã©ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã‚‹ã‹ã®ä¾‹ã§ã™ï¼š

- Normalization involves adjusting the range of feature values so that they fit within a specific range, typically between zero and one.  
  æ­£è¦åŒ–ã¯ã€ç‰¹å¾´å€¤ã®ç¯„å›²ã‚’èª¿æ•´ã—ã¦ç‰¹å®šã®ç¯„å›²ï¼ˆé€šå¸¸ã¯0ã‹ã‚‰1ã®é–“ï¼‰ã«åã‚ã‚‹ã“ã¨ã‚’å«ã¿ã¾ã™ã€‚

- The most common method of normalization is min-max scaling, where, for each data point, you subtract the minimum value and divide by the maximum value minus the minimum value:  
  æ­£è¦åŒ–ã®æœ€ã‚‚ä¸€èˆ¬çš„ãªæ–¹æ³•ã¯min-maxã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã§ã‚ã‚Šã€å„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã«ã¤ã„ã¦æœ€å°å€¤ã‚’å¼•ãã€æœ€å¤§å€¤ã‹ã‚‰æœ€å°å€¤ã‚’å¼•ã„ãŸå€¤ã§å‰²ã‚Šã¾ã™ï¼š  
  $$  
  x_{\text{normalized}} = \frac{x - x_{\text{min}}}{x_{\text{max}} - x_{\text{min}}}  
  $$  

- Standardization involves subtracting the mean and dividing by the standard deviation for every data point.  
  æ¨™æº–åŒ–ã¯ã€å„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã«ã¤ã„ã¦å¹³å‡ã‚’å¼•ãã€æ¨™æº–åå·®ã§å‰²ã‚‹ã“ã¨ã‚’å«ã¿ã¾ã™ã€‚

- It centers the data around zero and scales it based on the standard deviation:  
  ã“ã‚Œã¯ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¼ãƒ­ã®å‘¨ã‚Šã«ä¸­å¿ƒåŒ–ã—ã€æ¨™æº–åå·®ã«åŸºã¥ã„ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ã¾ã™ï¼š  
  $$  
  x_{\text{standardized}} = \frac{x - \mu}{\sigma}  
  $$  
  ã“ã“ã§ã€$\sigma$ã¯æ¨™æº–åå·®ã€$\mu$ã¯å¹³å‡ã§ã™ã€‚

- Log transformations apply a logarithmic function to each data point, typically base 10 or base e (denoted as ln):  
  å¯¾æ•°å¤‰æ›ã¯ã€å„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã«å¯¾æ•°é–¢æ•°ã‚’é©ç”¨ã—ã¾ã™ã€‚é€šå¸¸ã¯åº•10ã¾ãŸã¯åº•eï¼ˆlnã¨ã—ã¦ç¤ºã•ã‚Œã‚‹ï¼‰ã§ã™ï¼š  
  $$  
  x_{\text{log}} = \ln x  
  $$  

- Reciprocal transformation takes the reciprocal (i.e., the inverse) of each value.  
  é€†æ•°å¤‰æ›ã¯ã€å„å€¤ã®é€†æ•°ï¼ˆã™ãªã‚ã¡é€†ï¼‰ã‚’å–ã‚Šã¾ã™ã€‚

- The reciprocal of a number x is 1/x.  
  æ•°å­—$x$ã®é€†æ•°ã¯$1/x$ã§ã™ã€‚

- It can help reduce the skewness of a dataset and stabilize its variance:  
  ã“ã‚Œã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ­ªã¿ã‚’æ¸›å°‘ã•ã›ã€åˆ†æ•£ã‚’å®‰å®šã•ã›ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ï¼š  
  $$  
  x_{\text{reciprocal}} = \frac{1}{x}  
  $$  

- Exponential transformation of a numerical variable x involves applying an exponential function.  
  æ•°å€¤å¤‰æ•°$x$ã®æŒ‡æ•°å¤‰æ›ã¯ã€æŒ‡æ•°é–¢æ•°ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã‚’å«ã¿ã¾ã™ã€‚

- It can linearize relationships between variables when dealing with exponential growth or decay patterns, or it can give greater weight to larger values in a dataset:  
  ã“ã‚Œã¯ã€æŒ‡æ•°çš„æˆé•·ã¾ãŸã¯æ¸›è¡°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ‰±ã†éš›ã«å¤‰æ•°é–“ã®é–¢ä¿‚ã‚’ç·šå½¢åŒ–ã—ãŸã‚Šã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®å¤§ããªå€¤ã«ã‚ˆã‚Šå¤§ããªé‡ã¿ã‚’ä¸ãˆãŸã‚Šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š  
  $$  
  x_{\text{exp}} = a \cdot e^{b \cdot x}  
  $$  
  ã“ã“ã§ã€$a$ã¯ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ã€$b$ã¯æˆé•·ç‡ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚

- Box-Cox transformation stabilizes the variance in a numerical variable, making it more closely approximate a normal distribution.  
  Box-Coxå¤‰æ›ã¯æ•°å€¤å¤‰æ•°ã®åˆ†æ•£ã‚’å®‰å®šã•ã›ã€æ­£è¦åˆ†å¸ƒã«ã‚ˆã‚Šè¿‘ã¥ã‘ã¾ã™ã€‚

- A good value for the hyperparameter, _Î», can be estimated using maximum likelihood estimation, such that it_ minimizes the skewness of the transformed data, making it as close to normal as possible.  
  ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿_Î»_ã®è‰¯ã„å€¤ã¯ã€æœ€å¤§å°¤åº¦æ¨å®šã‚’ä½¿ç”¨ã—ã¦æ¨å®šã§ãã€å¤‰æ›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®æ­ªã¿ã‚’æœ€å°åŒ–ã—ã€ã§ãã‚‹ã ã‘æ­£è¦ã«è¿‘ã¥ã‘ã¾ã™ã€‚

- When ğœ† = 0, the Box-Cox transformation becomes the natural log:  
  ğœ† = 0ã®ã¨ãã€Box-Coxå¤‰æ›ã¯è‡ªç„¶å¯¾æ•°ã«ãªã‚Šã¾ã™ï¼š  
  $$  
  x_{\text{box-cox}}^{\lambda} = \frac{x^{\lambda} - 1}{\lambda}  
  $$  

###### Storing Transformed Feature Data in a Feature Group
###### ç‰¹å¾´ã‚°ãƒ«ãƒ¼ãƒ—ã«å¤‰æ›ã•ã‚ŒãŸç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹

In general, you should not store transformed feature data in feature groups, as it precludes feature reuse by models and introduces write amplification when new data is written to a feature group.  
ä¸€èˆ¬çš„ã«ã€å¤‰æ›ã•ã‚ŒãŸç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å¾´ã‚°ãƒ«ãƒ¼ãƒ—ã«ä¿å­˜ã™ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ãªãœãªã‚‰ã€ãã‚Œã¯ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ç‰¹å¾´ã®å†åˆ©ç”¨ã‚’å¦¨ã’ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒç‰¹å¾´ã‚°ãƒ«ãƒ¼ãƒ—ã«æ›¸ãè¾¼ã¾ã‚Œã‚‹ã¨ãã«æ›¸ãè¾¼ã¿ã®å¢—å¹…ã‚’å¼•ãèµ·ã“ã™ã‹ã‚‰ã§ã™ã€‚

However, in a case where you require the lowest possible latency in a real-time ML system, precomputing as much as possible can help shave off microseconds or milliseconds from prediction request latency.  
ãŸã ã—ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã§å¯èƒ½ãªé™ã‚Šä½ã„ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’è¦æ±‚ã™ã‚‹å ´åˆã€ã§ãã‚‹ã ã‘å¤šãã‚’äº‹å‰è¨ˆç®—ã™ã‚‹ã“ã¨ã§ã€äºˆæ¸¬ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‹ã‚‰ãƒã‚¤ã‚¯ãƒ­ç§’ã¾ãŸã¯ãƒŸãƒªç§’ã‚’å‰Šæ¸›ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚

Milliseconds can be worth millions for some companies.  
ãƒŸãƒªç§’ã¯ã€ã„ãã¤ã‹ã®ä¼æ¥­ã«ã¨ã£ã¦ä½•ç™¾ä¸‡ãƒ‰ãƒ«ã®ä¾¡å€¤ãŒã‚ã‚Šã¾ã™ã€‚

If you absolutely have to apply your feature transformations before the feature store, you can create a separate online-only feature group for your model, including its own dedicated feature pipeline.  
ã‚‚ã—ã€ã©ã†ã—ã¦ã‚‚ç‰¹å¾´ã‚¹ãƒˆã‚¢ã®å‰ã«ç‰¹å¾´å¤‰æ›ã‚’é©ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚‹å ´åˆã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã«å°‚ç”¨ã®ç‰¹å¾´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å«ã‚€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å°‚ç”¨ã®ç‰¹å¾´ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆã§ãã¾ã™ã€‚

The feature pipeline should use the training dataset statistics for your model to apply feature transformations.  
ç‰¹å¾´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆã‚’ä½¿ç”¨ã—ã¦ç‰¹å¾´å¤‰æ›ã‚’é©ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

This â€œtransformedâ€ feature group should be online only, so it will only store the latest feature values and you will not need to recompute existing feature data for every write.  
ã“ã®ã€Œå¤‰æ›ã•ã‚ŒãŸã€ç‰¹å¾´ã‚°ãƒ«ãƒ¼ãƒ—ã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å°‚ç”¨ã§ã‚ã‚‹ã¹ãã§ã€æœ€æ–°ã®ç‰¹å¾´å€¤ã®ã¿ã‚’ä¿å­˜ã—ã€ã™ã¹ã¦ã®æ›¸ãè¾¼ã¿ã«å¯¾ã—ã¦æ—¢å­˜ã®ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚’å†è¨ˆç®—ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

If some of the features are reused in other models, you should update your feature pipeline to first compute the untransformed features and write them to the shared, untransformed feature group.  
ä»–ã®ãƒ¢ãƒ‡ãƒ«ã§ä¸€éƒ¨ã®ç‰¹å¾´ãŒå†åˆ©ç”¨ã•ã‚Œã‚‹å ´åˆã¯ã€æœ€åˆã«æœªå¤‰æ›ã®ç‰¹å¾´ã‚’è¨ˆç®—ã—ã€ãã‚Œã‚’å…±æœ‰ã®æœªå¤‰æ›ç‰¹å¾´ã‚°ãƒ«ãƒ¼ãƒ—ã«æ›¸ãè¾¼ã‚€ã‚ˆã†ã«ç‰¹å¾´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ›´æ–°ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

Then, after applying the feature transformations, you should write the transformed features to the transformed online feature group.  
ãã®å¾Œã€ç‰¹å¾´å¤‰æ›ã‚’é©ç”¨ã—ãŸå¾Œã€å¤‰æ›ã•ã‚ŒãŸç‰¹å¾´ã‚’å¤‰æ›ã•ã‚ŒãŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç‰¹å¾´ã‚°ãƒ«ãƒ¼ãƒ—ã«æ›¸ãè¾¼ã‚€å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

This works for both batch and streaming feature pipelines.  
ã“ã‚Œã¯ã€ãƒãƒƒãƒãŠã‚ˆã³ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã®ç‰¹å¾´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¸¡æ–¹ã§æ©Ÿèƒ½ã—ã¾ã™ã€‚

###### Model-Specific Transformations
###### ãƒ¢ãƒ‡ãƒ«ç‰¹æœ‰ã®å¤‰æ›

_Model-specific transformations are a catchall for any data transformation that is not a_ feature transformation but is specific to one model.  
_ãƒ¢ãƒ‡ãƒ«ç‰¹æœ‰ã®å¤‰æ›ã¯ã€ç‰¹å¾´å¤‰æ›ã§ã¯ãªãã€ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã«ç‰¹æœ‰ã®ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã®ç·ç§°ã§ã™ã€‚

We will look at a couple of examples of such transformations.  
ã“ã®ã‚ˆã†ãªå¤‰æ›ã®ã„ãã¤ã‹ã®ä¾‹ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚

For example, a popular way to impute missing inference data is to first compute the mean/median/mode for features in the training data and replace the missing values with one of the computed values.  
ä¾‹ãˆã°ã€æ¬ æã—ãŸæ¨è«–ãƒ‡ãƒ¼ã‚¿ã‚’è£œå®Œã™ã‚‹ä¸€èˆ¬çš„ãªæ–¹æ³•ã¯ã€ã¾ãšãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´ã®å¹³å‡/ä¸­å¤®å€¤/æœ€é »å€¤ã‚’è¨ˆç®—ã—ã€æ¬ æå€¤ã‚’è¨ˆç®—ã•ã‚ŒãŸå€¤ã®1ã¤ã§ç½®ãæ›ãˆã‚‹ã“ã¨ã§ã™ã€‚

Another example, which does not require training data statistics, is determining how to transform timestamps for features so that they are aligned with the timestamps for targets/labels.  
ã‚‚ã†ä¸€ã¤ã®ä¾‹ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆã‚’å¿…è¦ã¨ã›ãšã€ç‰¹å¾´ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ/ãƒ©ãƒ™ãƒ«ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«åˆã‚ã›ã‚‹ãŸã‚ã«ã©ã®ã‚ˆã†ã«å¤‰æ›ã™ã‚‹ã‹ã‚’æ±ºå®šã™ã‚‹ã“ã¨ã§ã™ã€‚

This transformation enables you to create training data with a more efficient `INNER JOIN` instead of an ASOF LEFT JOIN.  
ã“ã®å¤‰æ›ã«ã‚ˆã‚Šã€ASOF LEFT JOINã®ä»£ã‚ã‚Šã«ã‚ˆã‚ŠåŠ¹ç‡çš„ãª`INNER JOIN`ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã§ãã¾ã™ã€‚

###### Outlier Handling Methods
###### å¤–ã‚Œå€¤å‡¦ç†æ–¹æ³•

_Outlier detection identifies and handles anomalous data points that can skew model_ training and lead to poor predictions.  
_å¤–ã‚Œå€¤æ¤œå‡ºã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ­ªã‚ã€äºˆæ¸¬ã®è³ªã‚’ä½ä¸‹ã•ã›ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ç•°å¸¸ãªãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’ç‰¹å®šã—å‡¦ç†ã—ã¾ã™ã€‚

Where possible, it is preferable to not ingest anomalous data points into a feature group, for example, by using Great Expectations to identify and remove them in feature pipelines.  
å¯èƒ½ãªé™ã‚Šã€å¤–ã‚Œå€¤ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’ç‰¹å¾´ã‚°ãƒ«ãƒ¼ãƒ—ã«å–ã‚Šè¾¼ã¾ãªã„æ–¹ãŒæœ›ã¾ã—ã„ã§ã™ã€‚ä¾‹ãˆã°ã€Great Expectationsã‚’ä½¿ç”¨ã—ã¦ç‰¹å¾´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ãã‚Œã‚‰ã‚’ç‰¹å®šã—å‰Šé™¤ã—ã¾ã™ã€‚

Sometimes, however, feature groups can contain anomalous data, and youâ€™ll then have to perform outlier detection as MDTs.  
ãŸã ã—ã€å ´åˆã«ã‚ˆã£ã¦ã¯ã€ç‰¹å¾´ã‚°ãƒ«ãƒ¼ãƒ—ã«ç•°å¸¸ãªãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã€ãã®å ´åˆã¯MDTã¨ã—ã¦å¤–ã‚Œå€¤æ¤œå‡ºã‚’å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

Scikit-Learn has good support for both univariate (one-feature) and multivariate (multiple-feature) approaches.  
Scikit-Learnã¯ã€å˜å¤‰é‡ï¼ˆ1ã¤ã®ç‰¹å¾´ï¼‰ãŠã‚ˆã³å¤šå¤‰é‡ï¼ˆè¤‡æ•°ã®ç‰¹å¾´ï¼‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ä¸¡æ–¹ã«å¯¾ã—ã¦è‰¯å¥½ãªã‚µãƒãƒ¼ãƒˆã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚

For univariate data, it includes statistical techniques such as the z-score and the interquartile range (IQR) method.  
å˜å¤‰é‡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€zã‚¹ã‚³ã‚¢ã‚„å››åˆ†ä½ç¯„å›²ï¼ˆIQRï¼‰æ³•ãªã©ã®çµ±è¨ˆçš„æ‰‹æ³•ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

For multivariate data, it provides algorithms like the Isolation Forest and Local Outlier Factor (LOF).  
å¤šå¤‰é‡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€Isolation Forestã‚„Local Outlier Factorï¼ˆLOFï¼‰ãªã©ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚

Here is an example that removes small outlier payments (the bottom 0.2% of amounts) in credit card transactions:  
ä»¥ä¸‹ã¯ã€ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰å–å¼•ã«ãŠã‘ã‚‹å°ã•ãªå¤–ã‚Œå€¤ã®æ”¯æ‰•ã„ï¼ˆé‡‘é¡ã®ä¸‹ä½0.2%ï¼‰ã‚’å‰Šé™¤ã™ã‚‹ä¾‹ã§ã™ï¼š  
```  
Q1 = df['amount'].quantile(0.002)  
outliers = df[(df['amount'] < Q1)]  
```  

If large outlier payments remain, a log transformation can help reduce their influence by compressing high values.  
å¤§ããªå¤–ã‚Œå€¤ã®æ”¯æ‰•ã„ãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆã€å¯¾æ•°å¤‰æ›ã¯é«˜ã„å€¤ã‚’åœ§ç¸®ã™ã‚‹ã“ã¨ã§ãã®å½±éŸ¿ã‚’æ¸›å°‘ã•ã›ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚

Generally, you should perform outlier removal before log transformations, and remember, log transformations do not help with small or negative outliers.  
ä¸€èˆ¬çš„ã«ã€å¯¾æ•°å¤‰æ›ã®å‰ã«å¤–ã‚Œå€¤ã®å‰Šé™¤ã‚’è¡Œã†ã¹ãã§ã‚ã‚Šã€å¯¾æ•°å¤‰æ›ã¯å°ã•ãªå¤–ã‚Œå€¤ã‚„è² ã®å¤–ã‚Œå€¤ã«ã¯åŠ¹æœãŒãªã„ã“ã¨ã‚’è¦šãˆã¦ãŠã„ã¦ãã ã•ã„ã€‚

###### Imputing Missing Values
###### æ¬ æå€¤ã®è£œå®Œ

Missing values can sometimes be identified in EDA and handled by not including features in a feature view.  
æ¬ æå€¤ã¯ã€æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰ã§ç‰¹å®šã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã€ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ã«ç‰¹å¾´ã‚’å«ã‚ãªã„ã“ã¨ã§å‡¦ç†ã•ã‚Œã¾ã™ã€‚

For example, you may not select a feature for a model because it has too many missing values.  
ä¾‹ãˆã°ã€æ¬ æå€¤ãŒå¤šã™ãã‚‹ãŸã‚ã«ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã«ç‰¹å¾´ã‚’é¸æŠã—ãªã„ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

In a production feature pipeline, a missing value in a row may be so important that it invalidates all of the other values in that rowâ€”in which case the entire row is dropped.  
ç”Ÿç”£ç’°å¢ƒã®ç‰¹å¾´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã¯ã€è¡Œå†…ã®æ¬ æå€¤ãŒéå¸¸ã«é‡è¦ã§ã‚ã‚Šã€ãã®è¡Œå†…ã®ä»–ã®ã™ã¹ã¦ã®å€¤ã‚’ç„¡åŠ¹ã«ã™ã‚‹å ´åˆã€ãã®è¡Œå…¨ä½“ãŒå‰Šé™¤ã•ã‚Œã¾ã™ã€‚

Often, however, we choose to deal with missing values by imputing them in training and inference pipelines.  
ã—ã‹ã—ã€ã—ã°ã—ã°ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§æ¬ æå€¤ã‚’è£œå®Œã™ã‚‹ã“ã¨ã§å¯¾å‡¦ã™ã‚‹ã“ã¨ã‚’é¸æŠã—ã¾ã™ã€‚

A list of popular techniques for imputing missing data is shown in Figure 7-2.i  
æ¬ æãƒ‡ãƒ¼ã‚¿ã‚’è£œå®Œã™ã‚‹ãŸã‚ã®ä¸€èˆ¬çš„ãªæ‰‹æ³•ã®ãƒªã‚¹ãƒˆã¯ã€å›³7-2.iã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚



_Figure 7-2. Different techniques for the imputation of missing data in training and inference pipelines, based on whether the data is time-series data or not. For non-time-series data, we can use descriptive statistics computed from the training dataset to impute missing values._
_Figure 7-2. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ãŠã‘ã‚‹æ¬ æãƒ‡ãƒ¼ã‚¿ã®è£œå®Œã®ãŸã‚ã®ç•°ãªã‚‹æ‰‹æ³•ã€‚ãƒ‡ãƒ¼ã‚¿ãŒæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã‚ã‚‹ã‹ã©ã†ã‹ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚éæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰è¨ˆç®—ã•ã‚ŒãŸè¨˜è¿°çµ±è¨ˆã‚’ä½¿ç”¨ã—ã¦æ¬ æå€¤ã‚’è£œå®Œã§ãã¾ã™ã€‚_

_In Pandas, we can impute missing time-series data using forward filling as follows:_
_æ¬¡ã®ã‚ˆã†ã«ã€Pandasã‚’ä½¿ç”¨ã—ã¦æ¬ æã—ãŸæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’å‰æ–¹è£œå®Œã§è£œå®Œã§ãã¾ã™ï¼š_

```  
df_forward_filled = df.sort_values("event_time").groupby("cc_num")["amount"].ffill()
```  
```  
df_forward_filled = df.sort_values("event_time").groupby("cc_num")["amount"].ffill()
```  
_Forward filling takes the last valid (nonmissing) value, uses it to fill in the missing values forward for all columns in the DataFrame, and stores the output in a new DataFrame._
_å‰æ–¹è£œå®Œã¯ã€æœ€å¾Œã®æœ‰åŠ¹ï¼ˆæ¬ æã§ãªã„ï¼‰å€¤ã‚’å–ã‚Šã€ãã‚Œã‚’ä½¿ç”¨ã—ã¦DataFrameå†…ã®ã™ã¹ã¦ã®åˆ—ã®æ¬ æå€¤ã‚’å‰æ–¹ã«è£œå®Œã—ã€å‡ºåŠ›ã‚’æ–°ã—ã„DataFrameã«ä¿å­˜ã—ã¾ã™ã€‚_

_It is also possible to impute missing values with backward filling that takes the next valid (nonmissing) value and uses it to fill in the missing values backward._
_æ¬¡ã®æœ‰åŠ¹ï¼ˆæ¬ æã§ãªã„ï¼‰å€¤ã‚’å–ã‚Šã€ãã‚Œã‚’ä½¿ç”¨ã—ã¦æ¬ æå€¤ã‚’å¾Œæ–¹ã«è£œå®Œã™ã‚‹ãƒãƒƒã‚¯ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒªãƒ³ã‚°ã§ã‚‚æ¬ æå€¤ã‚’è£œå®Œã§ãã¾ã™ã€‚_

_In this Pandas operation, we only backfill the `amount` column and update the same DataFrame:_
_ã“ã®Pandasæ“ä½œã§ã¯ã€`amount`åˆ—ã®ã¿ã‚’å¾Œæ–¹è£œå®Œã—ã€åŒã˜DataFrameã‚’æ›´æ–°ã—ã¾ã™ï¼š_

```  
df["amount"] = df.sort_values("event_time").groupby("cc_num")["amount"].bfill()
```  
```  
df["amount"] = df.sort_values("event_time").groupby("cc_num")["amount"].bfill()
```  
_What happens if you have large volumes of data (10s of GBs or more) that Pandas cannot scale to process?_
_ã‚‚ã—PandasãŒå‡¦ç†ã§ããªã„ã»ã©ã®å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ï¼ˆæ•°åGBä»¥ä¸Šï¼‰ãŒã‚ã‚‹å ´åˆã¯ã©ã†ãªã‚Šã¾ã™ã‹ï¼Ÿ_

_You could use PySpark instead of Pandas. PySpark doesnâ€™t have native library support, but you can use a window function (unboundedPreceding or unboundedFollowing) to implement forward and backward filling, respectively, for a specific column._
_Pandasã®ä»£ã‚ã‚Šã«PySparkã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚PySparkã«ã¯ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚µãƒãƒ¼ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é–¢æ•°ï¼ˆunboundedPrecedingã¾ãŸã¯unboundedFollowingï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€ç‰¹å®šã®åˆ—ã«å¯¾ã—ã¦å‰æ–¹ãŠã‚ˆã³å¾Œæ–¹è£œå®Œã‚’å®Ÿè£…ã§ãã¾ã™ã€‚_

_Here, we forward-fill `amount` and specify the primary key as the orderBy column:_
_ã“ã“ã§ã¯ã€`amount`ã‚’å‰æ–¹è£œå®Œã—ã€ä¸»ã‚­ãƒ¼ã‚’orderByåˆ—ã¨ã—ã¦æŒ‡å®šã—ã¾ã™ï¼š_

```  
window_spec = Window.partitionBy("cc_num").orderBy("event_time")     
.rowsBetween(Window.unboundedPreceding, Window.currentRow)   
# Forward fill the 'amount' column with missing values   
df_forward_filled = df.withColumn(     
"filled_amount", F.last("amount", ignoreNulls=True).over(window_spec)   
)
```  
```  
window_spec = Window.partitionBy("cc_num").orderBy("event_time")     
.rowsBetween(Window.unboundedPreceding, Window.currentRow)   
# æ¬ æå€¤ã‚’æŒã¤'amount'åˆ—ã‚’å‰æ–¹è£œå®Œã—ã¾ã™   
df_forward_filled = df.withColumn(     
"filled_amount", F.last("amount", ignoreNulls=True).over(window_spec)   
)
```  
_This will sort the data by `event_time` within each `cc_num. So if there is a missing `amount, it will be replaced by the most recent credit card amount on that card._
_ã“ã‚Œã«ã‚ˆã‚Šã€å„`cc_num`å†…ã§`event_time`ã«ã‚ˆã£ã¦ãƒ‡ãƒ¼ã‚¿ãŒã‚½ãƒ¼ãƒˆã•ã‚Œã¾ã™ã€‚ã—ãŸãŒã£ã¦ã€æ¬ æã—ãŸ`amount`ãŒã‚ã‚‹å ´åˆã€ãã‚Œã¯ãã®ã‚«ãƒ¼ãƒ‰ã®æœ€æ–°ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã®é‡‘é¡ã«ç½®ãæ›ãˆã‚‰ã‚Œã¾ã™ã€‚_

_Here is the same example for backward-filling missing values:_
_æ¬ æå€¤ã‚’å¾Œæ–¹è£œå®Œã™ã‚‹ãŸã‚ã®åŒã˜ä¾‹ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ï¼š_

```  
window_spec_back = Window.partitionBy("cc_num").orderBy("event_time")     
.rowsBetween(Window.currentRow, Window.unboundedFollowing)   
# Backward fill the 'amount' column with missing values   
df_backward_filled = df.withColumn(     
"filled_amount", F.first("amount", ignoreNulls=True).over(window_spec_back))
```  
```  
window_spec_back = Window.partitionBy("cc_num").orderBy("event_time")     
.rowsBetween(Window.currentRow, Window.unboundedFollowing)   
# æ¬ æå€¤ã‚’æŒã¤'amount'åˆ—ã‚’å¾Œæ–¹è£œå®Œã—ã¾ã™   
df_backward_filled = df.withColumn(     
"filled_amount", F.first("amount", ignoreNulls=True).over(window_spec_back))
```  
_Note that these operations are expensive in Spark and require shuffling and sorting the data over all workers._
_ã“ã‚Œã‚‰ã®æ“ä½œã¯Sparkã§ã¯é«˜ã‚³ã‚¹ãƒˆã§ã‚ã‚Šã€ã™ã¹ã¦ã®ãƒ¯ãƒ¼ã‚«ãƒ¼é–“ã§ãƒ‡ãƒ¼ã‚¿ã®ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã¨ã‚½ãƒ¼ãƒˆãŒå¿…è¦ã§ã™ã€‚_

_To scale window functions in PySpark, you need to set a partition key and make sure partition sizes are balanced (if there is a skew in the partition sizes, performance will be negatively impacted)._
_PySparkã§ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é–¢æ•°ã‚’ã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹ã«ã¯ã€ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚­ãƒ¼ã‚’è¨­å®šã—ã€ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºãŒå‡ç­‰ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã«åã‚ŠãŒã‚ã‚‹å ´åˆã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«æ‚ªå½±éŸ¿ã‚’åŠã¼ã—ã¾ã™ï¼‰ã€‚_

_In contrast, sorting in Pandas is a relatively cheap in-memory operation._
_å¯¾ç…§çš„ã«ã€Pandasã§ã®ã‚½ãƒ¼ãƒˆã¯æ¯”è¼ƒçš„å®‰ä¾¡ãªãƒ¡ãƒ¢ãƒªå†…æ“ä½œã§ã™ã€‚_

_What about filling non-time-series data using imputation?_
_æ¬ æå€¤è£œå®Œã‚’ä½¿ç”¨ã—ã¦éæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’è£œå®Œã™ã‚‹å ´åˆã¯ã©ã†ãªã‚Šã¾ã™ã‹ï¼Ÿ_

_In Scikit-Learn pipelines, we can impute missing values using classes in their `impute module, such as the`_
_Scikit-Learnã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã¯ã€`impute`ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å†…ã®ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ã¦æ¬ æå€¤ã‚’è£œå®Œã§ãã¾ã™ã€‚ä¾‹ãˆã°ã€_

```  
from sklearn.impute import SimpleImputer   
from sklearn.pipeline import Pipeline   
pipeline = Pipeline(steps=[     
('imputer', SimpleImputer(strategy='mean'))   
])   
df_imputed = pd.DataFrame(     
pipeline.fit_transform(df[["amount"]]),     
columns=["amount"]   
)
```  
```  
from sklearn.impute import SimpleImputer   
from sklearn.pipeline import Pipeline   
pipeline = Pipeline(steps=[     
('imputer', SimpleImputer(strategy='mean'))   
])   
df_imputed = pd.DataFrame(     
pipeline.fit_transform(df[["amount"]]),     
columns=["amount"]   
)
```  
_This code replaces all missing values with the mean value computed over the selected columns in your DataFrame._
_ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€DataFrameå†…ã®é¸æŠã•ã‚ŒãŸåˆ—ã«å¯¾ã—ã¦è¨ˆç®—ã•ã‚ŒãŸå¹³å‡å€¤ã§ã€ã™ã¹ã¦ã®æ¬ æå€¤ã‚’ç½®ãæ›ãˆã¾ã™ã€‚_

_If the DataFrame stores the training set, this works well._
_ã‚‚ã—DataFrameãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã‚’ä¿å­˜ã—ã¦ã„ã‚‹å ´åˆã€ã“ã‚Œã¯ã†ã¾ãæ©Ÿèƒ½ã—ã¾ã™ã€‚_

_Pipeline objects can be stored with their embedded model in the model registry._
_ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªå†…ã®åŸ‹ã‚è¾¼ã¾ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¨å…±ã«ä¿å­˜ã§ãã¾ã™ã€‚_

_This enables the same Scikit-Learn pipeline object to be downloaded to an inference pipeline, applying the same imputation transformations during inference and thus ensuring no training-serving skew._
_ã“ã‚Œã«ã‚ˆã‚Šã€åŒã˜Scikit-Learnãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã€æ¨è«–ä¸­ã«åŒã˜è£œå®Œå¤‰æ›ã‚’é©ç”¨ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã‚µãƒ¼ãƒ“ã‚¹ã®åã‚ŠãŒãªã„ã“ã¨ã‚’ä¿è¨¼ã—ã¾ã™ã€‚_

_Again, what happens if your data is too large to fit on a single machine?_
_å†åº¦ã€ã‚‚ã—ãƒ‡ãƒ¼ã‚¿ãŒå˜ä¸€ã®ãƒã‚·ãƒ³ã«åã¾ã‚‰ãªã„ã»ã©å¤§ãã„å ´åˆã¯ã©ã†ãªã‚Šã¾ã™ã‹ï¼Ÿ_

_Scikit-Learn pipelines only work on a single machine, so in this case, you can use declarative MDTs on feature views in Hopsworks._
_Scikit-Learnã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯å˜ä¸€ã®ãƒã‚·ãƒ³ã§ã®ã¿æ©Ÿèƒ½ã™ã‚‹ãŸã‚ã€ã“ã®å ´åˆã¯Hopsworksã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ“ãƒ¥ãƒ¼ã§å®£è¨€çš„MDTã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚_

_Hopsworks can use either Pandas or Spark as a backend for creating training datasets with feature views, so this solution scales to very large-sized (TBs or larger) training datasets._
_Hopsworksã¯ã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ“ãƒ¥ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹ãŸã‚ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã—ã¦Pandasã¾ãŸã¯Sparkã®ã„ãšã‚Œã‹ã‚’ä½¿ç”¨ã§ãã‚‹ãŸã‚ã€ã“ã®ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã¯éå¸¸ã«å¤§ããªã‚µã‚¤ã‚ºï¼ˆTBä»¥ä¸Šï¼‰ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¾ã™ã€‚_

_In this example, we min_max_scale the amount feature when we create training data using the feature view object:_
_ã“ã®ä¾‹ã§ã¯ã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ“ãƒ¥ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹éš›ã«ã€amountãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚’min_max_scaleã—ã¾ã™ï¼š_

```  
from hopsworks.hsfs.builtin_transformations import min_max_scaler   
feature_view = fs.create_feature_view(     
name='transactions_view',     
query=query,     
labels=["fraud_label"],     
transformation_functions = [min_max_scaler("amount")]   
)   
# missing values will be imputed during training data creation   
feature_view.create_training_data(test_size=0.2)
```  
```  
from hopsworks.hsfs.builtin_transformations import min_max_scaler   
feature_view = fs.create_feature_view(     
name='transactions_view',     
query=query,     
labels=["fraud_label"],     
transformation_functions = [min_max_scaler("amount")]   
)   
# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ä½œæˆä¸­ã«æ¬ æå€¤ãŒè£œå®Œã•ã‚Œã¾ã™   
feature_view.create_training_data(test_size=0.2)
```  
_For more advanced use cases, you can try model-based imputation that uses statistical models to estimate and fill in missing values._
_ã‚ˆã‚Šé«˜åº¦ãªãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã§ã¯ã€çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦æ¬ æå€¤ã‚’æ¨å®šã—è£œå®Œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®è£œå®Œã‚’è©¦ã™ã“ã¨ãŒã§ãã¾ã™ã€‚_

_See Statistical Analysis with Missing Data by Roderick Little and Donald Rubin (Wiley) for details._
_è©³ç´°ã«ã¤ã„ã¦ã¯ã€Roderick Littleã¨Donald Rubinï¼ˆWileyï¼‰ã®ã€Œæ¬ æãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆåˆ†æã€ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚_

###### Data Cleaning as Model-Based Transformations
###### ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®å¤‰æ›ã¨ã—ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°

_Data cleaning can be guided by heuristics, training data statistics, or a model trained on the data._
_ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆã€ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ã‚¬ã‚¤ãƒ‰ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚_

_Model-based cleaning is most effective when the features and their distributions remain relatively stable between training and inference._
_ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€ç‰¹å¾´ã¨ãã®åˆ†å¸ƒãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã®é–“ã§æ¯”è¼ƒçš„å®‰å®šã—ã¦ã„ã‚‹å ´åˆã«æœ€ã‚‚åŠ¹æœçš„ã§ã™ã€‚_

_An example of data cleaning is the preprocessing done by Meta to clean text data before pretraining LLMs._
_ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã®ä¾‹ã¨ã—ã¦ã€MetaãŒLLMã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å‰ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹ãŸã‚ã«è¡Œã£ãŸå‰å‡¦ç†ãŒã‚ã‚Šã¾ã™ã€‚_

_Pretraining benefits from removing noise from low-quality tokens._
_äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€ä½å“è³ªã®ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ãƒã‚¤ã‚ºã‚’é™¤å»ã™ã‚‹ã“ã¨ã§åˆ©ç›Šã‚’å¾—ã¾ã™ã€‚_

_Meta states that when they are training Llama 3.1, â€œWe use a token-distribution Kullback-Leibler divergence to filter out documents containing excessive numbers of outlier tokens compared to the training corpus distributionâ€¦we developed a series of data-filtering pipelinesâ€¦using heuristic filters, NSFW (not safe for work) filters, semantic deduplication approaches, and text classifiers to predict data quality.â€_
_Metaã¯ã€Llama 3.1ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã„ã‚‹ã¨ãã«ã€ã€Œãƒˆãƒ¼ã‚¯ãƒ³åˆ†å¸ƒã®Kullback-Leiblerãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’ä½¿ç”¨ã—ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒ‘ã‚¹åˆ†å¸ƒã¨æ¯”è¼ƒã—ã¦å¤–ã‚Œå€¤ãƒˆãƒ¼ã‚¯ãƒ³ã®æ•°ãŒéå‰°ãªæ–‡æ›¸ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã™â€¦ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ãƒ•ã‚£ãƒ«ã‚¿ã€NSFWï¼ˆä½œæ¥­ã«é©ã•ãªã„ï¼‰ãƒ•ã‚£ãƒ«ã‚¿ã€æ„å‘³çš„é‡è¤‡æ’é™¤ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡å™¨ã‚’ä½¿ç”¨ã—ã¦ã€ä¸€é€£ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é–‹ç™ºã—ã¾ã—ãŸã€‚ã€ã¨è¿°ã¹ã¦ã„ã¾ã™ã€‚_

_This sounds like a chicken-and-egg problem._
_ã“ã‚Œã¯é¶ã¨åµã®å•é¡Œã®ã‚ˆã†ã«èã“ãˆã¾ã™ã€‚_

_How do you know what the training corpus distribution is when you are trying to create a clean training corpus?_
_ã‚¯ãƒªãƒ¼ãƒ³ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½œæˆã—ã‚ˆã†ã¨ã—ã¦ã„ã‚‹ã¨ãã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒ‘ã‚¹ã®åˆ†å¸ƒãŒä½•ã§ã‚ã‚‹ã‹ã‚’ã©ã†ã‚„ã£ã¦çŸ¥ã‚‹ã“ã¨ãŒã§ãã¾ã™ã‹ï¼Ÿ_

_Their solution was, â€œWe used Llama 2 to generate the training data for the text-quality classifiers that are powering Llama 3.â€_
_å½¼ã‚‰ã®è§£æ±ºç­–ã¯ã€ã€ŒLlama 2ã‚’ä½¿ç”¨ã—ã¦Llama 3ã‚’æ”¯ãˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆå“è³ªåˆ†é¡å™¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚ã€ã§ã—ãŸã€‚_

_That is, they assumed that the text for pretraining LLMs follows a stable distribution from version 2 to version 3._
_ã¤ã¾ã‚Šã€å½¼ã‚‰ã¯LLMã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆãŒãƒãƒ¼ã‚¸ãƒ§ãƒ³2ã‹ã‚‰ãƒãƒ¼ã‚¸ãƒ§ãƒ³3ã¾ã§ã®å®‰å®šã—ãŸåˆ†å¸ƒã«å¾“ã†ã¨ä»®å®šã—ã¾ã—ãŸã€‚_

_So training data for Llama 3.1 could also be used to train text-quality classifiers for Llama 4, and so on._
_ã—ãŸãŒã£ã¦ã€Llama 3.1ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¯Llama 4ã®ãƒ†ã‚­ã‚¹ãƒˆå“è³ªåˆ†é¡å™¨ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ã‚‚ä½¿ç”¨ã§ãã‚‹ã§ã—ã‚‡ã†ã€‚_

_Note that the LLMâ€™s text-quality classifiers only run in the training dataset (or feature) pipeline._
_æ³¨æ„ã™ã¹ãã¯ã€LLMã®ãƒ†ã‚­ã‚¹ãƒˆå“è³ªåˆ†é¡å™¨ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆã¾ãŸã¯ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ï¼‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã®ã¿å®Ÿè¡Œã•ã‚Œã‚‹ã¨ã„ã†ã“ã¨ã§ã™ã€‚_

_They are not MDTs that run in both training and inference pipelines._
_å½¼ã‚‰ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã®ä¸¡æ–¹ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å®Ÿè¡Œã•ã‚Œã‚‹MDTã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚_

_Data cleaning is needed before training, but you make predictions on unclean data, so you shouldnâ€™t apply data cleaning transformations during inference._
_ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å‰ã«ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¿…è¦ã§ã™ãŒã€æ±šã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦äºˆæ¸¬ã‚’è¡Œã†ãŸã‚ã€æ¨è«–ä¸­ã«ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¤‰æ›ã‚’é©ç”¨ã™ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚_

_There are many good open source libraries that can be used for model-based data cleaning._
_ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã§ãã‚‹å„ªã‚ŒãŸã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¤šæ•°ã‚ã‚Šã¾ã™ã€‚_

_For example, Cleanlab is a Python package that identifies and corrects label errors in training datasets, providing confidence estimates for the correctness of each label._
_ä¾‹ãˆã°ã€Cleanlabã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ãƒ©ãƒ™ãƒ«ã‚¨ãƒ©ãƒ¼ã‚’ç‰¹å®šã—ä¿®æ­£ã™ã‚‹Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã§ã‚ã‚Šã€å„ãƒ©ãƒ™ãƒ«ã®æ­£ç¢ºæ€§ã«å¯¾ã™ã‚‹ä¿¡é ¼åº¦ã‚’æä¾›ã—ã¾ã™ã€‚_

_[Lightly is an open library for computer vision that creates image embeddings](https://oreil.ly/DKg48) and then uses clustering and similarity search to help select, prioritize, or pseudo-label samples without full manual annotation._
_[Lightlyã¯ã€ç”»åƒåŸ‹ã‚è¾¼ã¿ã‚’ä½œæˆã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ç”¨ã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã‚ã‚Š](https://oreil.ly/DKg48)ã€ãã®å¾Œã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¨é¡ä¼¼æ€§æ¤œç´¢ã‚’ä½¿ç”¨ã—ã¦ã€å®Œå…¨ãªæ‰‹å‹•æ³¨é‡ˆãªã—ã§ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠã€å„ªå…ˆé †ä½ä»˜ã‘ã€ã¾ãŸã¯æ“¬ä¼¼ãƒ©ãƒ™ãƒ«ä»˜ã‘ã‚’æ”¯æ´ã—ã¾ã™ã€‚_

_This makes Lightly useful in image tasks where acquiring labeled data is challenging or expensive._
_ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã®å–å¾—ãŒå›°é›£ã¾ãŸã¯é«˜ä¾¡ãªç”»åƒã‚¿ã‚¹ã‚¯ã§LightlyãŒå½¹ç«‹ã¡ã¾ã™ã€‚_

_Cleanlab is more widely used on tabular datasets where it can identify and correct label errors, although it can also be used on text and image datasets._
_Cleanlabã¯ã€ãƒ©ãƒ™ãƒ«ã‚¨ãƒ©ãƒ¼ã‚’ç‰¹å®šã—ä¿®æ­£ã§ãã‚‹ãŸã‚ã€è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ˆã‚Šåºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ãŒã€ãƒ†ã‚­ã‚¹ãƒˆã‚„ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚‚ä½¿ç”¨ã§ãã¾ã™ã€‚_

###### Target-/Label-Dependent Transformations
###### ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ/ãƒ©ãƒ™ãƒ«ä¾å­˜ã®å¤‰æ›

_There are some data transformations that are parameterized by properties of the label/target, such as its timestamp._
_ãƒ©ãƒ™ãƒ«/ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ï¼ˆä¾‹ãˆã°ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼‰ã«ã‚ˆã£ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãŒã‚ã‚Šã¾ã™ã€‚_

_Sometimes, you can delay computing features until the label and its properties become known._
_æ™‚ã«ã¯ã€ãƒ©ãƒ™ãƒ«ã¨ãã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãŒçŸ¥ã‚‰ã‚Œã‚‹ã¾ã§ç‰¹å¾´ã®è¨ˆç®—ã‚’é…ã‚‰ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚_

_This enables you to compute these features only when needed._
_ã“ã‚Œã«ã‚ˆã‚Šã€å¿…è¦ãªã¨ãã«ã®ã¿ã“ã‚Œã‚‰ã®ç‰¹å¾´ã‚’è¨ˆç®—ã§ãã¾ã™ã€‚_

_A good example of a label-dependent transformation in the context of credit card fraud detection is time_since_last_transaction, which is calculated relative to the current transactionâ€™s timestamp and the timestamp for the most recent previous transaction:_
_ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰è©æ¬ºæ¤œå‡ºã®æ–‡è„ˆã«ãŠã‘ã‚‹ãƒ©ãƒ™ãƒ«ä¾å­˜ã®å¤‰æ›ã®è‰¯ã„ä¾‹ã¯ã€time_since_last_transactionã§ã‚ã‚Šã€ã“ã‚Œã¯ç¾åœ¨ã®ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨æœ€ã‚‚æœ€è¿‘ã®å‰ã®ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«å¯¾ã—ã¦ç›¸å¯¾çš„ã«è¨ˆç®—ã•ã‚Œã¾ã™ï¼š_

```  
def time_since_last_transaction(event_time, prev_ts_transaction):     
return event_time - prev_ts_transaction
```  
```  
def time_since_last_transaction(event_time, prev_ts_transaction):     
return event_time - prev_ts_transaction
```  
###### Expensive Features Are Computed When Needed
###### é«˜ã‚³ã‚¹ãƒˆã®ç‰¹å¾´ã¯å¿…è¦ãªã¨ãã«è¨ˆç®—ã•ã‚Œã‚‹

_Sometimes it is too expensive to precompute features for all entities in feature pipelines._
_æ™‚ã«ã¯ã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å†…ã®ã™ã¹ã¦ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ç‰¹å¾´ã‚’äº‹å‰ã«è¨ˆç®—ã™ã‚‹ã®ã¯é«˜ã‚³ã‚¹ãƒˆã™ãã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚_

_If your AI system will not consume all of the features that have been precomputed, you can compute them as MDTs._
_ã‚‚ã—ã‚ãªãŸã®AIã‚·ã‚¹ãƒ†ãƒ ãŒäº‹å‰ã«è¨ˆç®—ã•ã‚ŒãŸã™ã¹ã¦ã®ç‰¹å¾´ã‚’æ¶ˆè²»ã—ãªã„å ´åˆã€ãã‚Œã‚‰ã‚’MDTã¨ã—ã¦è¨ˆç®—ã§ãã¾ã™ã€‚_

_For example, imagine you write a batch feature pipeline that runs daily to compute `days_since_bank_cr_changed. But your` (re)training pipeline only runs monthly, and the batch inference pipeline using the feature only runs weekly._
_ä¾‹ãˆã°ã€æ¯æ—¥å®Ÿè¡Œã•ã‚Œã‚‹ãƒãƒƒãƒãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ›¸ã„ãŸã¨æƒ³åƒã—ã¦ãã ã•ã„ã€‚`days_since_bank_cr_changed`ã‚’è¨ˆç®—ã—ã¾ã™ã€‚ã—ã‹ã—ã€ã‚ãªãŸã®ï¼ˆå†ï¼‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯æœˆã«1å›ã—ã‹å®Ÿè¡Œã•ã‚Œãšã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ãƒãƒƒãƒæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯é€±ã«1å›ã—ã‹å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“ã€‚_

_Then you have to recompute `days_since_bank_cr_changed` 7 times before it is used for inference and 30 times before it is used for training._
_ãã®ãŸã‚ã€æ¨è«–ã«ä½¿ç”¨ã•ã‚Œã‚‹å‰ã«`days_since_bank_cr_changed`ã‚’7å›ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã•ã‚Œã‚‹å‰ã«30å›å†è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚_

_That is a lot of wasteful computation._
_ã“ã‚Œã¯éå¸¸ã«ç„¡é§„ãªè¨ˆç®—ã§ã™ã€‚_

_Instead, your training pipeline can compute `days_since_bank_cr_changed` as a MDT in training and batch inference pipelines._
_ãã®ä»£ã‚ã‚Šã«ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³ãƒãƒƒãƒæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§MDTã¨ã—ã¦`days_since_bank_cr_changed`ã‚’è¨ˆç®—ã§ãã¾ã™ã€‚_

_If all of your features can be implemented as MDTs, you may even be able to eliminate your feature pipelines and thus reduce your operational burden._
_ã‚‚ã—ã™ã¹ã¦ã®ç‰¹å¾´ã‚’MDTã¨ã—ã¦å®Ÿè£…ã§ãã‚‹ãªã‚‰ã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ’é™¤ã—ã€é‹ç”¨è² æ‹…ã‚’è»½æ¸›ã§ãã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚_

###### Tokenizers and Chat Templates for LLMs
###### LLMã®ãŸã‚ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

_When you pass text to an LLM for training or for inference, that text needs to be first transformed into tokens by the LLMâ€™s tokenizer before it is fed into the LLM._
_ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¾ãŸã¯æ¨è«–ã®ãŸã‚ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’LLMã«æ¸¡ã™ã¨ãã€ãã®ãƒ†ã‚­ã‚¹ãƒˆã¯æœ€åˆã«LLMã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›ã•ã‚Œã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãã®å¾Œã€LLMã«ä¾›çµ¦ã•ã‚Œã¾ã™ã€‚_

_Every LLM has its own tokenizer, and the process is known as tokenization._
_ã™ã¹ã¦ã®LLMã«ã¯ç‹¬è‡ªã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒã‚ã‚Šã€ã“ã®ãƒ—ãƒ­ã‚»ã‚¹ã¯ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨ã—ã¦çŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚_

_For example, Llama 3â€™s tokenizer, on average, tokenizes one word into two to three tokensâ€”each token is, on average, four characters long._
_ä¾‹ãˆã°ã€Llama 3ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¯ã€å¹³å‡ã—ã¦1ã¤ã®å˜èªã‚’2ã€œ3ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¾ã™â€”å„ãƒˆãƒ¼ã‚¯ãƒ³ã¯å¹³å‡ã—ã¦4æ–‡å­—ã®é•·ã•ã§ã™ã€‚_

_Llama 3 has a tokenization dictionary with a vocabulary of 128K tokens._
_Llama 3ã«ã¯ã€128Kãƒˆãƒ¼ã‚¯ãƒ³ã®èªå½™ã‚’æŒã¤ãƒˆãƒ¼ã‚¯ãƒ³åŒ–è¾æ›¸ãŒã‚ã‚Šã¾ã™ã€‚_

_Tokenization is an MDT, as it is tightly coupled to the version of your LLM._
_ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¯MDTã§ã‚ã‚Šã€ã‚ãªãŸã®LLMã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«å¯†æ¥ã«çµã³ã¤ã„ã¦ã„ã¾ã™ã€‚_

_For example, Llama 3 tokenized text cannot be fed into a Llama 2 or Llama 4 model._
_ä¾‹ãˆã°ã€Llama 3ã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã¯Llama 2ã¾ãŸã¯Llama 4ãƒ¢ãƒ‡ãƒ«ã«ä¾›çµ¦ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚_

_A common problem I have seen among practitioners who fine-tune LLMs is that they encounter skew between training and inference time, due to different versions of_
_ç§ãŒLLMã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å®Ÿå‹™è€…ã®é–“ã§è¦‹ãŸä¸€èˆ¬çš„ãªå•é¡Œã¯ã€ç•°ãªã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãŸã‚ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã®æ™‚é–“ã«åã‚ŠãŒç”Ÿã˜ã‚‹ã“ã¨ã§ã™ã€‚_



tokenizers in their training pipeline and online inference pipeline. 
ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

A solution is to use the Hugging Face (HF) chat template. 
è§£æ±ºç­–ã¯ã€Hugging Face (HF) ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚

HF chat templates are tightly coupled with the tokenizer, and they define a conversation as a single string that can be tokenized in the format expected by the model: 
HFãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨å¯†æ¥ã«çµã³ã¤ã„ã¦ãŠã‚Šã€ä¼šè©±ã‚’ãƒ¢ãƒ‡ãƒ«ãŒæœŸå¾…ã™ã‚‹å½¢å¼ã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã§ãã‚‹å˜ä¸€ã®æ–‡å­—åˆ—ã¨ã—ã¦å®šç¾©ã—ã¾ã™ï¼š

```   
from transformers import AutoTokenizer   
tokenizer=AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B")   
chat = [     
    {"role": "user", "content": "How do I prevent training/inference skew for tokenization in LLMs?"},     
    {"role": "assistant", "content": "A chat template can help"}   
]   
tokenized_prompt = tokenizer.apply_chat_template(chat, tokenize=True)
``` 
ã“ã®HFãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã«ã‚ˆã‚‹æ­ªã¿ã‚’é˜²ããŸã‚ã«ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã§åŒã˜ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ã ã‘ã§æ¸ˆã¿ã¾ã™ã€‚

_Text chunking for LLMs for fine-tuning and RAG breaks documents into pieces (pages, paragraphs, sentences, etc.) and is an MIT performed in a feature pipeline._ 
LLMsã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨RAGã®ãŸã‚ã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã¯ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’éƒ¨åˆ†ï¼ˆãƒšãƒ¼ã‚¸ã€æ®µè½ã€æ–‡ãªã©ï¼‰ã«åˆ†å‰²ã—ã€ç‰¹å¾´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å®Ÿè¡Œã•ã‚Œã‚‹MITã§ã™ã€‚

The chunked text can then be reused at inference time with RAG. 
ãƒãƒ£ãƒ³ã‚¯åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã¯ã€æ¨è«–æ™‚ã«RAGã§å†åˆ©ç”¨ã§ãã¾ã™ã€‚

_Text tokenization, however, is model dependent and, therefore, performed in training and inference pipelines._ 
ãŸã ã—ã€ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¯ãƒ¢ãƒ‡ãƒ«ä¾å­˜ã§ã‚ã‚‹ãŸã‚ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

You should not couple text chunking with text tokenization if you want to index reusable chunked text for LLMs in a vector index. 
ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§LLMsã®å†åˆ©ç”¨å¯èƒ½ãªãƒãƒ£ãƒ³ã‚¯åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã—ãŸã„å ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã¨ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚’çµã³ã¤ã‘ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

###### Transformations in Scikit-Learn Pipelines
###### Scikit-Learnãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ãŠã‘ã‚‹å¤‰æ›

Scikit-Learn provides a library of transformers that can implement MDTs in both training and inference pipelines without skew. 
Scikit-Learnã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¸¡æ–¹ã§æ­ªã¿ãªãMDTã‚’å®Ÿè£…ã§ãã‚‹ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’æä¾›ã—ã¾ã™ã€‚

Scikit-Learn also provides a pipeline object to manage both a sequence of transformers and the model. 
Scikit-Learnã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨ãƒ¢ãƒ‡ãƒ«ã®ä¸¡æ–¹ã‚’ç®¡ç†ã™ã‚‹ãŸã‚ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚‚æä¾›ã—ã¾ã™ã€‚

You can pickle and save the pipeline object in a model registry, instead of just saving the model. 
ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã ã‘ã§ãªãã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ãƒ”ã‚¯ãƒ«åŒ–ã—ã¦ä¿å­˜ã§ãã¾ã™ã€‚

The pipeline object includes both the transformers and the model, as well as any training data parameters (mean, min, max, and encoding maps) needed to apply the feature transformations. 
ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®ä¸¡æ–¹ã€ãªã‚‰ã³ã«ç‰¹å¾´å¤‰æ›ã‚’é©ç”¨ã™ã‚‹ãŸã‚ã«å¿…è¦ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå¹³å‡ã€æœ€å°ã€æœ€å¤§ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒãƒƒãƒ—ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

Then, in an inference pipeline, you download the pipeline object (not the model) and use it to apply MDTs and make predictions in a single method call. 
æ¬¡ã«ã€æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã¯ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆãƒ¢ãƒ‡ãƒ«ã§ã¯ãªãï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ãã‚Œã‚’ä½¿ç”¨ã—ã¦MDTã‚’é©ç”¨ã—ã€å˜ä¸€ã®ãƒ¡ã‚½ãƒƒãƒ‰å‘¼ã³å‡ºã—ã§äºˆæ¸¬ã‚’è¡Œã„ã¾ã™ã€‚

In the training pipeline, you create and use the pipeline as follows: 
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã¯ã€æ¬¡ã®ã‚ˆã†ã«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆã—ã¦ä½¿ç”¨ã—ã¾ã™ï¼š

```   
import joblib   
X_train, X_test, y_train, y_test = fv.train_test_split(test_size=0.2)   
categorical_features = \     
    [ col for col in X_train.columns if X_train[col].dtype == object ]   
numerical_features = \     
    [ col for col in X_train.columns if X_train[col].dtype != object ]   
numeric_transformer = Pipeline(     
    steps=[
        ("imputer", SimpleImputer(strategy="median")),       
        ("scaler", StandardScaler()),     
    ]   
)   
categorical_transformer = Pipeline(     
    steps=[       
        ("encoder", OneHotEncoder(handle_unknown="ignore")),     
    ]   
)   
preprocessor = ColumnTransformer(     
    transformers=[       
        ("num", numeric_transformer, numerical_features),       
        ("cat", categorical_transformer, categorical_features),     
    ]   
)   
clf = Pipeline(     
    steps=[       
        ("preprocessor", preprocessor),       
        ("classifier", LogisticRegression()),     
    ]   
)   
clf.fit(X_train, y_train)   
joblib.dump(clf, "cc_fraud/cc_fraud.pkl")   
mr_model = mr.register_sklearn_model(name=â€cc_fraudâ€, feature_view=fv,..)   
mr_model.save("cc_fraud")
``` 
ç§ãŸã¡ã¯ã€Scikit-Learnãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ä¸€èˆ¬çš„ã«é­é‡ã™ã‚‹å¤§ããªNumPyé…åˆ—ã‚’ä¿å­˜/èª­ã¿è¾¼ã‚€éš›ã«ã€Pythonã®ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ”ã‚¯ãƒ«ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚ˆã‚Šã‚‚åŠ¹ç‡çš„ã§ã‚ã‚‹ãŸã‚ã€joblibã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

In batch inference, we read a batch of feature values to score from the feature store, download the pipeline object (including the transformers and the model), and make predictions: 
ãƒãƒƒãƒæ¨è«–ã§ã¯ã€ç‰¹å¾´ã‚¹ãƒˆã‚¢ã‹ã‚‰ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ç‰¹å¾´å€¤ã®ãƒãƒƒãƒã‚’èª­ã¿è¾¼ã¿ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã‚’å«ã‚€ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€äºˆæ¸¬ã‚’è¡Œã„ã¾ã™ï¼š

```   
model_dir = mr.download_model(name="cc_fraud", version=1)   
clf = joblib.load(os.path.join(model_dir, "cc_fraud.pkl"))   
# Get feature data arrived since yesterday for scoring   
df = fv.get_batch_data(start_time=datetime.now()-timedelta(days=1))   
df["predicted_fraud"] = clf.predict(df)
``` 
The model.predict() method applies all of the pipeline transformations before calling predict on the model. 
model.predict()ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®predictã‚’å‘¼ã³å‡ºã™å‰ã«ã€ã™ã¹ã¦ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¤‰æ›ã‚’é©ç”¨ã—ã¾ã™ã€‚

You need to be careful to use the same version of joblib when building the containers for your training and inference pipelines; otherwise, you may have problems deserializing the pipeline. 
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚³ãƒ³ãƒ†ãƒŠã‚’æ§‹ç¯‰ã™ã‚‹éš›ã«ã¯ã€åŒã˜ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®joblibã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚ã•ã‚‚ãªã‘ã‚Œã°ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã«å•é¡ŒãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚



Scikit-Learn has a number of built-in transformations that may be useful in your training and inference pipelines. 
Scikit-Learnã«ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å½¹ç«‹ã¤å¯èƒ½æ€§ã®ã‚ã‚‹å¤šãã®çµ„ã¿è¾¼ã¿å¤‰æ›ãŒã‚ã‚Šã¾ã™ã€‚

For imputing values, Scikit-Learn transformers can replace missing values, NaNs (â€œnot a numberâ€), or other placeholders with either default values or computed values. 
å€¤ã‚’è£œå®Œã™ã‚‹ãŸã‚ã«ã€Scikit-Learnã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€æ¬ æå€¤ã€NaNï¼ˆã€Œæ•°å€¤ã§ã¯ãªã„ã€ï¼‰ã€ã¾ãŸã¯ä»–ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¾ãŸã¯è¨ˆç®—ã•ã‚ŒãŸå€¤ã§ç½®ãæ›ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

The SimpleImputer is a univariate algorithm that imputes missing values for a feature using only nonmissing values for that feature. 
SimpleImputerã¯ã€ç‰¹å®šã®ç‰¹å¾´ã®éæ¬ æå€¤ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ã€ãã®ç‰¹å¾´ã®æ¬ æå€¤ã‚’è£œå®Œã™ã‚‹å˜å¤‰é‡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚

You can define what a missing value is with the `missing_values` parameter (the default is np.nan). 
æ¬ æå€¤ãŒä½•ã§ã‚ã‚‹ã‹ã¯ã€`missing_values`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯np.nanï¼‰ã§å®šç¾©ã§ãã¾ã™ã€‚

The available SimpleImputer strategies are mean, median, constant (also set the fill_value parameter to the default value to replace the missing value with), and most_frequent, the mode of that feature. 
åˆ©ç”¨å¯èƒ½ãªSimpleImputerã®æˆ¦ç•¥ã¯ã€å¹³å‡ã€ä¸­å¤®å€¤ã€å®šæ•°ï¼ˆæ¬ æå€¤ã‚’ç½®ãæ›ãˆã‚‹ãŸã‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã«fill_valueãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹ã“ã¨ã‚‚å«ã‚€ï¼‰ã€ãŠã‚ˆã³ãã®ç‰¹å¾´ã®æœ€é »å€¤ï¼ˆmost_frequentï¼‰ã§ã™ã€‚

In contrast, the IterativeImputer implements model-based imputation and uses all features to estimate a missing value (it is a multivariate algorithm). 
å¯¾ç…§çš„ã«ã€IterativeImputerã¯ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®è£œå®Œã‚’å®Ÿè£…ã—ã€ã™ã¹ã¦ã®ç‰¹å¾´ã‚’ä½¿ç”¨ã—ã¦æ¬ æå€¤ã‚’æ¨å®šã—ã¾ã™ï¼ˆã“ã‚Œã¯å¤šå¤‰é‡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ï¼‰ã€‚

Another more sophisticated technique is to generate multiple imputations and apply an analysis pipeline to the imputations. 
ã‚‚ã†ä¸€ã¤ã®ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸæŠ€è¡“ã¯ã€è¤‡æ•°ã®è£œå®Œã‚’ç”Ÿæˆã—ã€ãã‚Œã«åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚

For categorical variables, Scikit-Learn supports the OneHotEncoder, which is suitable for categorical variables with a low or medium cardinality. 
ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã«å¯¾ã—ã¦ã€Scikit-Learnã¯OneHotEncoderã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ãŠã‚Šã€ã“ã‚Œã¯ä½ã¾ãŸã¯ä¸­ç¨‹åº¦ã®ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã‚’æŒã¤ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã«é©ã—ã¦ã„ã¾ã™ã€‚

You can exclude infrequent categories with the min_frequency parameter, which removes categories with a cardinality smaller than min_frequency. 
min_frequencyãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã€é »åº¦ã®ä½ã„ã‚«ãƒ†ã‚´ãƒªã‚’é™¤å¤–ã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ãŒmin_frequencyã‚ˆã‚Šå°ã•ã„ã‚«ãƒ†ã‚´ãƒªãŒå‰Šé™¤ã•ã‚Œã¾ã™ã€‚

You can also specify a default category called infrequent by setting the handle_unknown parameter to 'infrequent_if_exist', which will set the category for any new category encountered in inference to infrequent. 
handle_unknownãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’'infrequent_if_exist'ã«è¨­å®šã™ã‚‹ã“ã¨ã§ã€infrequentã¨ã„ã†ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚«ãƒ†ã‚´ãƒªã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚‚ã§ãã€æ¨è«–ä¸­ã«é­é‡ã—ãŸæ–°ã—ã„ã‚«ãƒ†ã‚´ãƒªã®ã‚«ãƒ†ã‚´ãƒªã‚’infrequentã«è¨­å®šã—ã¾ã™ã€‚

You can also set handle_unknown to ignore, which will produce a one-hot encoded array with zeros for all columns. 
handle_unknownã‚’ignoreã«è¨­å®šã™ã‚‹ã¨ã€ã™ã¹ã¦ã®åˆ—ã«å¯¾ã—ã¦ã‚¼ãƒ­ã®å€¤ã‚’æŒã¤ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸé…åˆ—ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚

The default for handle_unknown is to raise an error if a new category is encountered during inference. 
handle_unknownã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã€æ¨è«–ä¸­ã«æ–°ã—ã„ã‚«ãƒ†ã‚´ãƒªãŒé­é‡ã—ãŸå ´åˆã«ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹ã“ã¨ã§ã™ã€‚

Scikit-Learn also supports an OrdinalEncoder for categories with a natural ordering and a TargetEncoder for encoding unordered categories with high cardinality, for example, a zip code in the United States (US). 
Scikit-Learnã¯ã€è‡ªç„¶ãªé †åºã‚’æŒã¤ã‚«ãƒ†ã‚´ãƒªç”¨ã®OrdinalEncoderã¨ã€é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã®é †åºãªã—ã‚«ãƒ†ã‚´ãƒªã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã®TargetEncoderã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚ä¾‹ãˆã°ã€ã‚¢ãƒ¡ãƒªã‚«åˆè¡†å›½ã®éƒµä¾¿ç•ªå·ãªã©ã§ã™ã€‚

For numerical variables, Scikit-Learn provides a number of classes in the sklearn.preprocessing package. 
æ•°å€¤å¤‰æ•°ã«å¯¾ã—ã¦ã€Scikit-Learnã¯sklearn.preprocessingãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å†…ã«ã„ãã¤ã‹ã®ã‚¯ãƒ©ã‚¹ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚

The StandardScaler class standardizes a numerical feature, and it implements Scikit-Learnâ€™s Transformer API to compute the mean and standard deviation of a training set (X_train), which are then saved in the Pipeline object. 
StandardScalerã‚¯ãƒ©ã‚¹ã¯æ•°å€¤ç‰¹å¾´ã‚’æ¨™æº–åŒ–ã—ã€Scikit-Learnã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼APIã‚’å®Ÿè£…ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆï¼ˆX_trainï¼‰ã®å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—ã—ã€ãã‚Œã‚‰ã¯Pipelineã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚

The MinMaxScaler scales features to lie between zero and one (or some other minimum and maximum), preserving the shape of the distribution. 
MinMaxScalerã¯ç‰¹å¾´ã‚’ã‚¼ãƒ­ã¨ä¸€ã®é–“ï¼ˆã¾ãŸã¯ä»–ã®æœ€å°å€¤ã¨æœ€å¤§å€¤ã®é–“ï¼‰ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ã€åˆ†å¸ƒã®å½¢çŠ¶ã‚’ä¿æŒã—ã¾ã™ã€‚

MaxAbsScaler is better at preserving sparsity than MinMaxScaler. 
MaxAbsScalerã¯MinMaxScalerã‚ˆã‚Šã‚‚ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã‚’ä¿æŒã™ã‚‹ã®ã«å„ªã‚Œã¦ã„ã¾ã™ã€‚

Other important numerical transformations are quantile and power transforms that perform monotonic transformations to approximate the Gaussian, preserving the rank order of the data. 
ä»–ã®é‡è¦ãªæ•°å€¤å¤‰æ›ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®é †ä½ã‚’ä¿æŒã—ãªãŒã‚‰ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‚’è¿‘ä¼¼ã™ã‚‹å˜èª¿å¤‰æ›ã‚’è¡Œã†åˆ†ä½æ•°å¤‰æ›ã¨ãƒ‘ãƒ¯ãƒ¼å¤‰æ›ãŒã‚ã‚Šã¾ã™ã€‚

They can both map feature data from any distribution to a distribution that approximates the Gaussian distribution. 
ã“ã‚Œã‚‰ã¯ã©ã¡ã‚‰ã‚‚ã€ä»»æ„ã®åˆ†å¸ƒã‹ã‚‰ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‚’è¿‘ä¼¼ã™ã‚‹åˆ†å¸ƒã«ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒ”ãƒ³ã‚°ã§ãã¾ã™ã€‚

From the power transforms, Scikit-Learn supports both the Box-Cox and Yeo-Johnson algorithms. 
ãƒ‘ãƒ¯ãƒ¼å¤‰æ›ã‹ã‚‰ã€Scikit-Learnã¯Box-CoxãŠã‚ˆã³Yeo-Johnsonã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚

In Scikit-Learn, you can normalize a NumPy array (or Pandas DataFrame backed by a NumPy array) by applying the `preprocessing.normalize` function to specify one of the available norms: l1, l2 (default), or max. 
Scikit-Learnã§ã¯ã€`preprocessing.normalize`é–¢æ•°ã‚’é©ç”¨ã—ã¦ã€åˆ©ç”¨å¯èƒ½ãªãƒãƒ«ãƒ ã®1ã¤ï¼ˆl1ã€l2ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ã€ã¾ãŸã¯maxï¼‰ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€NumPyé…åˆ—ï¼ˆã¾ãŸã¯NumPyé…åˆ—ã«åŸºã¥ãPandas DataFrameï¼‰ã‚’æ­£è¦åŒ–ã§ãã¾ã™ã€‚

The l1 norm updates (scales) the values so that the sum of the absolute values is one, the l2 norm scales the values so that the sum of the squares of the values is equal to one, and the max norm scales the values so that the largest absolute value within each sample is 1. 
l1ãƒãƒ«ãƒ ã¯å€¤ã‚’æ›´æ–°ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰ã—ã¦çµ¶å¯¾å€¤ã®åˆè¨ˆãŒ1ã«ãªã‚‹ã‚ˆã†ã«ã—ã€l2ãƒãƒ«ãƒ ã¯å€¤ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ã¦å€¤ã®äºŒä¹—ã®åˆè¨ˆãŒ1ã«ãªã‚‹ã‚ˆã†ã«ã—ã€maxãƒãƒ«ãƒ ã¯å„ã‚µãƒ³ãƒ—ãƒ«å†…ã®æœ€å¤§çµ¶å¯¾å€¤ãŒ1ã«ãªã‚‹ã‚ˆã†ã«å€¤ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚

For example, with the l2 norm, the array of values [3, 4, 0] would be normalized to [0.6, 0.8, 0]. 
ä¾‹ãˆã°ã€l2ãƒãƒ«ãƒ ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€å€¤ã®é…åˆ—[3, 4, 0]ã¯[0.6, 0.8, 0]ã«æ­£è¦åŒ–ã•ã‚Œã¾ã™ã€‚

As of 2025, the transformation algorithms in Scikit-Learnâ€™s preprocessing package operate on NumPy arrays and do not natively support Arrow-backed DataFrames. 
2025å¹´ç¾åœ¨ã€Scikit-Learnã®å‰å‡¦ç†ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®å¤‰æ›ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯NumPyé…åˆ—ã§å‹•ä½œã—ã€Arrowãƒãƒƒã‚¯ã®DataFrameã‚’ãƒã‚¤ãƒ†ã‚£ãƒ–ã«ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚

Arrow-backed DataFrames, such as those in PySpark and Pandas, are more scalable for large datasets. 
PySparkã‚„Pandasã®ã‚ˆã†ãªArrowãƒãƒƒã‚¯ã®DataFrameã¯ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ã‚ˆã‚Šã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã§ã™ã€‚

In the next section, we will introduce feature transformations for Hopsworks Feature Views that work with Arrow-backed DataFrames. 
æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€Arrowãƒãƒƒã‚¯ã®DataFrameã§å‹•ä½œã™ã‚‹Hopsworks Feature Viewsã®ãŸã‚ã®ç‰¹å¾´å¤‰æ›ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

###### Transformations in Feature Views
###### ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ã«ãŠã‘ã‚‹å¤‰æ›

Feature views in Hopsworks support the execution of transformation functions when reading features from the feature store. 
Hopsworksã®ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ã¯ã€ç‰¹å¾´ã‚¹ãƒˆã‚¢ã‹ã‚‰ç‰¹å¾´ã‚’èª­ã¿è¾¼ã‚€éš›ã«å¤‰æ›é–¢æ•°ã®å®Ÿè¡Œã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚

There are built-in transformation functionsâ€”such as one_hot_encoder, min_max_scalar, and label_encoderâ€”that can be defined as part of a feature view. 
one_hot_encoderã€min_max_scalarã€label_encoderãªã©ã®çµ„ã¿è¾¼ã¿å¤‰æ›é–¢æ•°ãŒã‚ã‚Šã€ã“ã‚Œã‚‰ã¯ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ã®ä¸€éƒ¨ã¨ã—ã¦å®šç¾©ã§ãã¾ã™ã€‚

They take features in the feature view as input parameters and return one or more transformed feature values. 
ã“ã‚Œã‚‰ã¯ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼å†…ã®ç‰¹å¾´ã‚’å…¥åŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦å—ã‘å–ã‚Šã€1ã¤ä»¥ä¸Šã®å¤‰æ›ã•ã‚ŒãŸç‰¹å¾´å€¤ã‚’è¿”ã—ã¾ã™ã€‚

You can also write your own user-defined (custom) transformation functions for features in a feature view. 
ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼å†…ã®ç‰¹å¾´ã«å¯¾ã—ã¦ç‹¬è‡ªã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©ï¼ˆã‚«ã‚¹ã‚¿ãƒ ï¼‰å¤‰æ›é–¢æ•°ã‚’æ›¸ãã“ã¨ã‚‚ã§ãã¾ã™ã€‚

Transformation functions are executed in the Hopsworks client after it has read data with a feature view but before it returns the feature data. 
å¤‰æ›é–¢æ•°ã¯ã€Hopsworksã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã å¾Œã€ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™å‰ã«å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

Feature view transformations are MDTs that guarantee no skew between training and inference. 
ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ã®å¤‰æ›ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã®é–“ã«åã‚ŠãŒãªã„ã“ã¨ã‚’ä¿è¨¼ã™ã‚‹MDTã§ã™ã€‚

Any training data parameters (mean, min, max, and encoding maps) needed to apply feature transformations are stored in training dataset objects that are saved in the model registry, along with the model and the feature view used to create the training data. 
ç‰¹å¾´å¤‰æ›ã‚’é©ç”¨ã™ã‚‹ãŸã‚ã«å¿…è¦ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå¹³å‡ã€æœ€å°ã€æœ€å¤§ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒãƒƒãƒ—ï¼‰ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŠã‚ˆã³ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ã¨ã¨ã‚‚ã«ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚

Then in an inference pipeline, the model, along with its feature view and training data object, is downloaded, and its feature view retrieves feature data and applies MDTs to create feature vectors used for model prediction. 
ãã®å¾Œã€æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¨ãã®ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ãŠã‚ˆã³ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ãŒç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€MDTã‚’é©ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã«ä½¿ç”¨ã•ã‚Œã‚‹ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚

In the following code snippet, we define a feature view over credit card transaction features and declaratively apply three built-in feature transformations to three different featuresâ€”min_max_scaler to the amount feature, one_hot_encoder to the category feature, and label_encoder to the fraud label. 
æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆã§ã¯ã€ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰å–å¼•ã®ç‰¹å¾´ã«å¯¾ã™ã‚‹ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ã‚’å®šç¾©ã—ã€amountç‰¹å¾´ã«min_max_scalerã€categoryç‰¹å¾´ã«one_hot_encoderã€fraudãƒ©ãƒ™ãƒ«ã«label_encoderã¨ã„ã†3ã¤ã®ç•°ãªã‚‹ç‰¹å¾´ã«3ã¤ã®çµ„ã¿è¾¼ã¿ç‰¹å¾´å¤‰æ›ã‚’å®£è¨€çš„ã«é©ç”¨ã—ã¾ã™ã€‚

```python
from hopsworks.hsfs.builtin_transformations import min_max_scaler, label_encoder, one_hot_encoder
fv = fs.create_feature_view(
    name='transactions',
    query=fg_credit_card.select_features(),
    labels=["fraud"],
    transformation_functions = [
        one_hot_encoder("category"),
        min_max_scaler("amount"),
        label_encoder("fraud")
    ]
)
```

When you create a feature view, the transformation_functions list specifies transformations that are applied to named features in the feature view. 
ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ã‚’ä½œæˆã™ã‚‹ã¨ãã€transformation_functionsãƒªã‚¹ãƒˆã¯ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼å†…ã®åå‰ä»˜ãç‰¹å¾´ã«é©ç”¨ã•ã‚Œã‚‹å¤‰æ›ã‚’æŒ‡å®šã—ã¾ã™ã€‚

Each entry in the list contains the name of the transformation function and the names of features from the feature view as input parameters. 
ãƒªã‚¹ãƒˆå†…ã®å„ã‚¨ãƒ³ãƒˆãƒªã«ã¯ã€å¤‰æ›é–¢æ•°ã®åå‰ã¨ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰ã®ç‰¹å¾´ã®åå‰ãŒå…¥åŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

You can also include index columns or helper columns as parameters to a transformation function. 
ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åˆ—ã‚„ãƒ˜ãƒ«ãƒ‘ãƒ¼åˆ—ã‚’å¤‰æ›é–¢æ•°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦å«ã‚ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

In the above example, the transformation functions are univariate (one-to-one) functions that take a single feature as input and return a transformed value as output. 
ä¸Šè¨˜ã®ä¾‹ã§ã¯ã€å¤‰æ›é–¢æ•°ã¯å˜å¤‰é‡ï¼ˆ1å¯¾1ï¼‰é–¢æ•°ã§ã‚ã‚Šã€å˜ä¸€ã®ç‰¹å¾´ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€å¤‰æ›ã•ã‚ŒãŸå€¤ã‚’å‡ºåŠ›ã¨ã—ã¦è¿”ã—ã¾ã™ã€‚

You can also write custom multivariate functions that can take one to many features as input and return one to many transformed features as output. 
1ã¤ä»¥ä¸Šã®ç‰¹å¾´ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€1ã¤ä»¥ä¸Šã®å¤‰æ›ã•ã‚ŒãŸç‰¹å¾´ã‚’å‡ºåŠ›ã¨ã—ã¦è¿”ã™ã‚«ã‚¹ã‚¿ãƒ å¤šå¤‰é‡é–¢æ•°ã‚’æ›¸ãã“ã¨ã‚‚ã§ãã¾ã™ã€‚

If no feature names are provided explicitly in the transformation_functions list, the transformation function will default to using the feature name(s) in the feature view that matches the name of the parameter(s) in the transformation function definition. 
transformation_functionsãƒªã‚¹ãƒˆã«ç‰¹å¾´åãŒæ˜ç¤ºçš„ã«æä¾›ã•ã‚Œã¦ã„ãªã„å ´åˆã€å¤‰æ›é–¢æ•°ã¯å¤‰æ›é–¢æ•°å®šç¾©å†…ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã¨ä¸€è‡´ã™ã‚‹ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼å†…ã®ç‰¹å¾´åã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ãªã‚Šã¾ã™ã€‚

This works well with user-defined transformations, but not with built-in transformations. 
ã“ã‚Œã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©ã®å¤‰æ›ã«ã¯ã†ã¾ãæ©Ÿèƒ½ã—ã¾ã™ãŒã€çµ„ã¿è¾¼ã¿ã®å¤‰æ›ã«ã¯æ©Ÿèƒ½ã—ã¾ã›ã‚“ã€‚

Itâ€™s good practice to be explicit in the feature view definition and provide feature names so that developers can see what transformations are applied to which features. 
ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ã®å®šç¾©ã§æ˜ç¤ºçš„ã«ç‰¹å¾´åã‚’æä¾›ã™ã‚‹ã“ã¨ã¯è‰¯ã„ç¿’æ…£ã§ã‚ã‚Šã€é–‹ç™ºè€…ãŒã©ã®ç‰¹å¾´ã«ã©ã®å¤‰æ›ãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

Letâ€™s look at how transformation functions for feature views work in practice. 
ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ã®å¤‰æ›é–¢æ•°ãŒå®Ÿéš›ã«ã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

In the following code snippet, we use a feature view to read DataFrames containing the features and labels in the training and test sets. 
æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆã§ã¯ã€ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®ç‰¹å¾´ã¨ãƒ©ãƒ™ãƒ«ã‚’å«ã‚€DataFrameã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚

By default, the transformation functions are executed inside the train_test_split method and the returned DataFrames contain the transformed feature data: 
ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€å¤‰æ›é–¢æ•°ã¯train_test_splitãƒ¡ã‚½ãƒƒãƒ‰å†…ã§å®Ÿè¡Œã•ã‚Œã€è¿”ã•ã‚Œã‚‹DataFrameã«ã¯å¤‰æ›ã•ã‚ŒãŸç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¾ã™ã€‚

```python
X_train, X_test, y_train, y_test = fv.train_test_split(test_size=0.1)
```

Similarly, when we read a batch of inference data, it will, by default, return transformed feature data. 
åŒæ§˜ã«ã€æ¨è«–ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒã‚’èª­ã¿è¾¼ã‚€ã¨ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å¤‰æ›ã•ã‚ŒãŸç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ãŒè¿”ã•ã‚Œã¾ã™ã€‚

Here, however, we read untransformed inference data with the feature view by setting Transformed=False: 
ãŸã ã—ã€ã“ã“ã§ã¯Transformed=Falseã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã€ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ã‚’ä½¿ç”¨ã—ã¦å¤‰æ›ã•ã‚Œã¦ã„ãªã„æ¨è«–ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚

```python
features = fv.get_batch_data(
    start_date=(datetime.now() - timedelta(1)), transformed=False
)
```

For the feature viewâ€™s online APIs, when you read feature vectors, the transformation functions are, again, executed transparently in the client by default (transformed=True is default): 
ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³APIã§ã¯ã€ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’èª­ã¿è¾¼ã‚€ã¨ãã€å¤‰æ›é–¢æ•°ã¯å†ã³ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå†…ã§é€æ˜ã«å®Ÿè¡Œã•ã‚Œã¾ã™ï¼ˆtransformed=TrueãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã™ï¼‰ã€‚

```python
features = fv.get_feature_vector(serving_keys={"cc_num": "1234 0432 0122 9833"})
```

Transformation functions can change the schema of the feature data read from the feature view, as they can return more or fewer columns than there are features in the feature view. 
å¤‰æ›é–¢æ•°ã¯ã€ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰èª­ã¿è¾¼ã¾ã‚ŒãŸç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚­ãƒ¼ãƒã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ãŒã§ãã€ç‰¹å¾´ãƒ“ãƒ¥ãƒ¼ã«ã‚ã‚‹ç‰¹å¾´ã®æ•°ã‚ˆã‚Šã‚‚å¤šãã¾ãŸã¯å°‘ãªã„åˆ—ã‚’è¿”ã™ã“ã¨ãŒã§ãã¾ã™ã€‚

For example, one_hot_encoding can transform a string column into
ä¾‹ãˆã°ã€one_hot_encodingã¯æ–‡å­—åˆ—åˆ—ã‚’å¤‰æ›ã§ãã¾ã™ã€‚



hundreds of columns in a returned DataFrame (one column for each category). 
è¿”ã•ã‚ŒãŸDataFrameã«ã¯æ•°ç™¾ã®åˆ—ãŒã‚ã‚Šï¼ˆå„ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«1åˆ—ï¼‰ã€

The feature view, however, ensures that the number and order of columns in the returned data will be consistent when reading training and inference data. 
ã—ã‹ã—ã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ“ãƒ¥ãƒ¼ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨æ¨è«–ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€éš›ã«ã€è¿”ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã®åˆ—ã®æ•°ã¨é †åºãŒä¸€è²«ã—ã¦ã„ã‚‹ã“ã¨ã‚’ä¿è¨¼ã—ã¾ã™ã€‚

As a developer, you only need to work with the modelâ€™s feature view and the training/inference data created by it. 
é–‹ç™ºè€…ã¨ã—ã¦ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ“ãƒ¥ãƒ¼ã¨ãã‚Œã«ã‚ˆã£ã¦ä½œæˆã•ã‚ŒãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°/æ¨è«–ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æ‰±ãˆã°ã‚ˆã„ã§ã™ã€‚

You generally do not work with the model signatureâ€”the schema of the DataFrame input to the model. 
ä¸€èˆ¬çš„ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ã‚·ã‚°ãƒãƒãƒ£ï¼ˆãƒ¢ãƒ‡ãƒ«ã¸ã®DataFrameå…¥åŠ›ã®ã‚¹ã‚­ãƒ¼ãƒï¼‰ã‚’æ‰±ã†ã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

The feature view is responsible for mapping its features to and from the model signature. 
ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ“ãƒ¥ãƒ¼ã¯ã€ãã®ç‰¹å¾´ã‚’ãƒ¢ãƒ‡ãƒ«ã®ã‚·ã‚°ãƒãƒãƒ£ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹è²¬ä»»ãŒã‚ã‚Šã¾ã™ã€‚

This means, for example, that when working with categorical features, you only work with the string column (in the feature view), not with the one-hot encoded columns (in the training/inference data). 
ä¾‹ãˆã°ã€ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚’æ‰±ã†å ´åˆã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ“ãƒ¥ãƒ¼å†…ã®æ–‡å­—åˆ—åˆ—ã®ã¿ã‚’æ‰±ã„ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°/æ¨è«–ãƒ‡ãƒ¼ã‚¿å†…ã®ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸåˆ—ã¯æ‰±ã„ã¾ã›ã‚“ã€‚

You can also define your own custom transformations for feature views as user-defined transformation functions. 
ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ“ãƒ¥ãƒ¼ã«å¯¾ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©ã®å¤‰æ›é–¢æ•°ã¨ã—ã¦ç‹¬è‡ªã®ã‚«ã‚¹ã‚¿ãƒ å¤‰æ›ã‚’å®šç¾©ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

A user-defined transformation function is a Python or Pandas UDF with the @hopsworks.udf annotation. 
ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©ã®å¤‰æ›é–¢æ•°ã¯ã€@hopsworks.udfã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æŒã¤Pythonã¾ãŸã¯Pandas UDFã§ã™ã€‚

Pandas UDFs can be scaled to process large volumes of data, in either Pandas or PySpark, while Python UDFs do not scale well. 
Pandas UDFã¯ã€Pandasã¾ãŸã¯PySparkã®ã„ãšã‚Œã‹ã§å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã«ã‚¹ã‚±ãƒ¼ãƒ«ã§ãã¾ã™ãŒã€Python UDFã¯ã†ã¾ãã‚¹ã‚±ãƒ¼ãƒ«ã—ã¾ã›ã‚“ã€‚

Python UDFs, however, have lower latency in online inference pipelines than Pandas UDFs. 
ãŸã ã—ã€Python UDFã¯Pandas UDFã‚ˆã‚Šã‚‚ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒä½ããªã‚Šã¾ã™ã€‚

For this reason, when possible, the best practice is to write transformation functions as Python functions that can be executed as either a Pandas UDF (in a feature/training/batch-inference pipeline) or a Python UDF (in an online inference pipeline). 
ã“ã®ãŸã‚ã€å¯èƒ½ãªé™ã‚Šã€å¤‰æ›é–¢æ•°ã¯Pandas UDFï¼ˆãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼/ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°/ãƒãƒƒãƒæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å†…ï¼‰ã¾ãŸã¯Python UDFï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å†…ï¼‰ã¨ã—ã¦å®Ÿè¡Œã§ãã‚‹Pythoné–¢æ•°ã¨ã—ã¦è¨˜è¿°ã™ã‚‹ã®ãŒãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã§ã™ã€‚

We call these types of transformation functions _mixed-mode_ UDFs, as they can run as either Pandas UDFs or Python UDFs, depending on the context. 
ã“ã‚Œã‚‰ã®ã‚¿ã‚¤ãƒ—ã®å¤‰æ›é–¢æ•°ã‚’_mixed-mode_ UDFã¨å‘¼ã³ã¾ã™ã€‚ã“ã‚Œã¯ã€æ–‡è„ˆã«å¿œã˜ã¦Pandas UDFã¾ãŸã¯Python UDFã¨ã—ã¦å®Ÿè¡Œã§ãã‚‹ãŸã‚ã§ã™ã€‚

In general, only simple UDFs can be written as mixed-mode UDFs. 
ä¸€èˆ¬çš„ã«ã€å˜ç´”ãªUDFã®ã¿ãŒmixed-mode UDFã¨ã—ã¦è¨˜è¿°ã§ãã¾ã™ã€‚

Here is an example of a mixed-mode transformation function that encodes information about how much a transaction deviates from the mean transaction amount from the training dataset. 
ä»¥ä¸‹ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¹³å‡å–å¼•é¡ã‹ã‚‰ã®å–å¼•ã®åå·®ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹mixed-modeå¤‰æ›é–¢æ•°ã®ä¾‹ã§ã™ã€‚

Hopsworks automatically fills in statistics for the training dataset in the stats object: 
Hopsworksã¯ã€statsã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå†…ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆã‚’è‡ªå‹•çš„ã«åŸ‹ã‚è¾¼ã¿ã¾ã™ï¼š

```   
stats = TransformationStatistics("amount")   
@hopsworks.udf(float)   
def transaction_amount_deviation(amount, statistics=stats):     
    return amount / statistics.amount.mean
```

In a training pipeline, `amount is a` `pd.Series and` `statistics.amount.mean is a` scalar, so it executes as a vectorized function in Pandas. 
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã¯ã€`amountã¯` `pd.Seriesã§ã‚ã‚Šã€` `statistics.amount.meanã¯` ã‚¹ã‚«ãƒ©ãƒ¼ã§ã‚ã‚‹ãŸã‚ã€Pandaså†…ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸé–¢æ•°ã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

However, in online inference, ``` amount is a float, so the function executes as a low-latency Python UDF. 
ãŸã ã—ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¨è«–ã§ã¯ã€``` amountã¯floatã§ã‚ã‚‹ãŸã‚ã€é–¢æ•°ã¯ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®Python UDFã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

We can also explicitly define a user-defined transformation function to run in Pandas _mode, in both training and inference. 
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã®ä¸¡æ–¹ã§Pandas _modeã§å®Ÿè¡Œã•ã‚Œã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©ã®å¤‰æ›é–¢æ•°ã‚’æ˜ç¤ºçš„ã«å®šç¾©ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

This can be executed as a Pandas UDF by_ PySpark. 
ã“ã‚Œã¯ã€PySparkã«ã‚ˆã£ã¦Pandas UDFã¨ã—ã¦å®Ÿè¡Œã§ãã¾ã™ã€‚

Here, we compute days_to_card_expiry in a transformation function that takes as inputs two features from a feature view, the `cc_expiry_date and` ``` event_time, that it expects are pd.Series containing dates. 
ã“ã“ã§ã¯ã€`cc_expiry_date`ã¨``` event_timeã®2ã¤ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€æ—¥ä»˜ã‚’å«ã‚€pd.Seriesã§ã‚ã‚‹ã“ã¨ã‚’æœŸå¾…ã™ã‚‹å¤‰æ›é–¢æ•°å†…ã§days_to_card_expiryã‚’è¨ˆç®—ã—ã¾ã™ã€‚

It computes and returns days_to_card_expiry with int value for each input:   
@hopsworks.udf(return_type=int, mode="pandas")   
def days_to_card_expiry(cc_expiry_date, event_time):     
    return (cc_expiry_date - event_time).dt.days
```

In online inference, this transformation function will also take a Pandas DataFrame as input, which can add a few hundreds of microseconds of additional latency compared with Python UDFs. 
ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¨è«–ã§ã¯ã€ã“ã®å¤‰æ›é–¢æ•°ã‚‚Pandas DataFrameã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€Python UDFã¨æ¯”è¼ƒã—ã¦æ•°ç™¾ãƒã‚¤ã‚¯ãƒ­ç§’ã®è¿½åŠ ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’åŠ ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

As this transformation function does not include training data statistics, it can also be used as ODT in feature/online inference pipelines in Hopsworks (see the next section). 
ã“ã®å¤‰æ›é–¢æ•°ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆã‚’å«ã¾ãªã„ãŸã‚ã€Hopsworksã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼/ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ODTã¨ã—ã¦ã‚‚ä½¿ç”¨ã§ãã¾ã™ï¼ˆæ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‚ç…§ï¼‰ã€‚

Sometimes features can be implemented as either an MIT or an MDT. 
æ™‚ã«ã¯ã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã¯MITã¾ãŸã¯MDTã®ã„ãšã‚Œã‹ã¨ã—ã¦å®Ÿè£…ã§ãã¾ã™ã€‚

For example, in Chapter 6 we described how to compute days_to_card_expiry with an MIT in a feature pipeline. 
ä¾‹ãˆã°ã€ç¬¬6ç« ã§ã¯ã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§MITã‚’ä½¿ç”¨ã—ã¦days_to_card_expiryã‚’è¨ˆç®—ã™ã‚‹æ–¹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã—ãŸã€‚

The feature pipeline, however, will have to run daily to ensure ``` days_to_card_expiry is correct. 
ãŸã ã—ã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ã€``` days_to_card_expiryãŒæ­£ã—ã„ã“ã¨ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã«ã€æ¯æ—¥å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

If the feature pipeline fails to run on a given day (or runs at any time other than midnight), then clients risk reading incorrect feature data. 
ç‰¹å®šã®æ—¥ã«ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒå®Ÿè¡Œã•ã‚Œãªã‹ã£ãŸå ´åˆï¼ˆã¾ãŸã¯çœŸå¤œä¸­ä»¥å¤–ã®æ™‚é–“ã«å®Ÿè¡Œã•ã‚ŒãŸå ´åˆï¼‰ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¯ä¸æ­£ç¢ºãªãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚‹ãƒªã‚¹ã‚¯ãŒã‚ã‚Šã¾ã™ã€‚

There is also the operational overhead of operating the feature pipeline, which you donâ€™t have with the MDT that is only run when needed in training and inference pipelines. 
ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é‹ç”¨ã™ã‚‹ãŸã‚ã®é‹ç”¨ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚‚ã‚ã‚Šã¾ã™ãŒã€ã“ã‚Œã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å¿…è¦ãªã¨ãã«ã®ã¿å®Ÿè¡Œã•ã‚Œã‚‹MDTã«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

Figure 7-3 shows flowcharts that help guide you in how to implement ``` days_to_card_expiry: as an MIT, MDT, or ODT. 
å›³7-3ã¯ã€``` days_to_card_expiryã‚’MITã€MDTã€ã¾ãŸã¯ODTã¨ã—ã¦å®Ÿè£…ã™ã‚‹æ–¹æ³•ã‚’ã‚¬ã‚¤ãƒ‰ã™ã‚‹ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

_Figure 7-3. These flowcharts guide you on how to implement the days_to_card_expiry_ _feature, depending on whether it will be (a) used by batch ML systems or (b) computed_ _at real time._ 
_å›³7-3. ã“ã‚Œã‚‰ã®ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã¯ã€days_to_card_expiryãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚’å®Ÿè£…ã™ã‚‹æ–¹æ³•ã‚’ã‚¬ã‚¤ãƒ‰ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ï¼ˆaï¼‰ãƒãƒƒãƒMLã‚·ã‚¹ãƒ†ãƒ ã§ä½¿ç”¨ã•ã‚Œã‚‹ã‹ã€ï¼ˆbï¼‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¨ˆç®—ã•ã‚Œã‚‹ã‹ã«ã‚ˆã‚Šã¾ã™ã€‚_

If the feature will be used by a batch ML system, you should implement the feature as an MDT if you will not reuse the computed feature or if you donâ€™t want the overhead of the feature pipeline. 
ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãŒãƒãƒƒãƒMLã‚·ã‚¹ãƒ†ãƒ ã§ä½¿ç”¨ã•ã‚Œã‚‹å ´åˆã€è¨ˆç®—ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚’å†åˆ©ç”¨ã—ãªã„å ´åˆã‚„ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’é¿ã‘ãŸã„å ´åˆã¯ã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚’MDTã¨ã—ã¦å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

Otherwise, it should be an MIT. 
ãã†ã§ãªã‘ã‚Œã°ã€ãã‚Œã¯MITã§ã‚ã‚‹ã¹ãã§ã™ã€‚

If days_to_card_expiry is a real-time feature that requires at least one request time parameter to be computed, you should implement it as an MDT if you do not want to be able to precompute the feature using historical data and save it in the feature store for use by many models. 
days_to_card_expiryãŒã€è¨ˆç®—ã«å°‘ãªãã¨ã‚‚1ã¤ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚é–“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¿…è¦ã¨ã™ã‚‹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã§ã‚ã‚‹å ´åˆã€æ­´å²çš„ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚’äº‹å‰è¨ˆç®—ã—ã€å¤šãã®ãƒ¢ãƒ‡ãƒ«ã§ä½¿ç”¨ã™ã‚‹ãŸã‚ã«ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢ã«ä¿å­˜ã—ãŸããªã„å ´åˆã¯ã€MDTã¨ã—ã¦å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

Otherwise, it should be an ODT. 
ãã†ã§ãªã‘ã‚Œã°ã€ãã‚Œã¯ODTã§ã‚ã‚‹ã¹ãã§ã™ã€‚

In our other example user-defined transformation, transaction_amount_deviation has to be an MDT as it takes amount as a request time parameter and a training data statistic (amount.mean) as a parameter. 
åˆ¥ã®ä¾‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©å¤‰æ›ã§ã‚ã‚‹transaction_amount_deviationã¯ã€amountã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚é–“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦å—ã‘å–ã‚Šã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆï¼ˆamount.meanï¼‰ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦å—ã‘å–ã‚‹ãŸã‚ã€MDTã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚

ODTs do not have training data statistics as parameters, as they are computed offline in feature pipelines (where there is no training data, only reusable feature data). 
ODTã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦æŒãŸãšã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§è¨ˆç®—ã•ã‚Œã¾ã™ï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¯ãªãã€å†åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ã¿ãŒã‚ã‚Šã¾ã™ï¼‰ã€‚

User-defined transformation functions are attached to feature views in the same way as built-in transformation functions are: 
ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©ã®å¤‰æ›é–¢æ•°ã¯ã€çµ„ã¿è¾¼ã¿ã®å¤‰æ›é–¢æ•°ã¨åŒæ§˜ã«ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ“ãƒ¥ãƒ¼ã«æ·»ä»˜ã•ã‚Œã¾ã™ï¼š

```   
fv = fs.create_feature_view(     
    ... 
    transformation_functions = \       
        [ days_to_card_expiry("cc_expiry_date", "event_time")     ]   
)
```

You can read the preceding syntax as follows: the days_to_card_expiry transformaâ€ tion function is applied to the `cc_expiry_date and` `event_time features in the feaâ€` ture view. 
å‰è¿°ã®æ§‹æ–‡ã¯æ¬¡ã®ã‚ˆã†ã«èª­ã‚€ã“ã¨ãŒã§ãã¾ã™ï¼šdays_to_card_expiryå¤‰æ›é–¢æ•°ã¯ã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ“ãƒ¥ãƒ¼å†…ã®`cc_expiry_date`ã¨`event_time`ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã«é©ç”¨ã•ã‚Œã¾ã™ã€‚

There is no days_to_card_expiry feature defined in the feature view, just the transformation function to create it. 
ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ“ãƒ¥ãƒ¼ã«ã¯days_to_card_expiryãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã¯å®šç¾©ã•ã‚Œã¦ãŠã‚‰ãšã€ãã‚Œã‚’ä½œæˆã™ã‚‹ãŸã‚ã®å¤‰æ›é–¢æ•°ã®ã¿ãŒã‚ã‚Šã¾ã™ã€‚

The days_to_card_expiry function is run as a Pandas UDF in a training pipeline and a batch inference pipeline. 
days_to_card_expiryé–¢æ•°ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ãƒãƒƒãƒæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§Pandas UDFã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

If you need to create large volumes of training data, you should write a training dataset pipeline in PySpark that uses one of the `fv.create_train*(..) methods to save the training` data as files. 
å¤§é‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚‹å ´åˆã¯ã€`fv.create_train*(..)`ãƒ¡ã‚½ãƒƒãƒ‰ã®1ã¤ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã™ã‚‹PySparkã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è¨˜è¿°ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

PySpark will partition the DataFrame across many workers and execute the transformation function as a Pandas UDF at each worker, with the workers indeâ€ pendently saving the training data they create as files. 
PySparkã¯DataFrameã‚’å¤šãã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã«åˆ†å‰²ã—ã€å„ãƒ¯ãƒ¼ã‚«ãƒ¼ã§Pandas UDFã¨ã—ã¦å¤‰æ›é–¢æ•°ã‚’å®Ÿè¡Œã—ã€ãƒ¯ãƒ¼ã‚«ãƒ¼ã¯ç‹¬ç«‹ã—ã¦ä½œæˆã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚

###### On-Demand Transformations
åŒã˜å¤‰æ›é–¢æ•°ã¯ã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ“ãƒ¥ãƒ¼ã§ä½¿ç”¨ã•ã‚Œã‚‹ã®ã¨åŒæ§˜ã«ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦å«ã¾ãªã„é™ã‚Šã€Hopsworksã§ODTã¨ã—ã¦ä½¿ç”¨ã§ãã¾ã™ã€‚

The same transformation functions used in feature views can be used as ODTs in Hopsworks as long as they do not include training data statistics as a parameter. 
ODTã¯ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚é–“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ“ãƒ¥ãƒ¼ã§èª­ã¿å–ã‚‰ã‚Œã‚‹äº‹å‰è¨ˆç®—ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã®çµ„ã¿åˆã‚ã›ã‚’æŒã¤ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

ODTs may have a combination of request-time parameters and precomputed features read with the feature view. 
æ™‚ã«ã¯ã€æ¨è«–ãƒ˜ãƒ«ãƒ‘ãƒ¼åˆ—ã‚’ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ“ãƒ¥ãƒ¼ã«è¿½åŠ ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ODTã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹äº‹å‰è¨ˆç®—ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒæä¾›ã•ã‚Œã¾ã™ã€‚

Sometimes you add inference helper columns to the feature view, as they provide precomputed feature data that is used to compute an ODT. 
ODTs differ from MDTs in where they are registered. 
ODTã¯ã€MDTã¨ã¯ç•°ãªã‚Šã€ã©ã“ã«ç™»éŒ²ã•ã‚Œã‚‹ã‹ãŒç•°ãªã‚Šã¾ã™ã€‚

You register ODTs with a feature group rather than with a feature view, as ODTs can be executed in feature pipelines. 
ODTã¯ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å®Ÿè¡Œã§ãã‚‹ãŸã‚ã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ“ãƒ¥ãƒ¼ã§ã¯ãªããƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã«ç™»éŒ²ã—ã¾ã™ã€‚

Feature views know which of their features are computed as ODTs and compute them in online inference pipelines. 
ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ“ãƒ¥ãƒ¼ã¯ã€ã©ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãŒODTã¨ã—ã¦è¨ˆç®—ã•ã‚Œã‚‹ã‹ã‚’çŸ¥ã£ã¦ãŠã‚Šã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ãã‚Œã‚‰ã‚’è¨ˆç®—ã—ã¾ã™ã€‚

ODTs can also be univariate or multivariate functions. 
ODTã¯ã€å˜å¤‰é‡ã¾ãŸã¯å¤šå¤‰é‡é–¢æ•°ã§ã‚ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

In the following code, a real-time feature, days_to_card_expiry, is defined for ``` cc_trans_fg: 
ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã§ã¯ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã§ã‚ã‚‹days_to_card_expiryãŒ``` cc_trans_fgã®ãŸã‚ã«å®šç¾©ã•ã‚Œã¦ã„ã¾ã™ï¼š

```   
fg = feature_store.create_feature_group(name="cc_trans_fg",           
    version=1,           
    description="Transaction Features",           
    online_enabled=True,           
    primary_key=['id'],           
    event_time='event_time'           
    transformation_functions=             
        [days_to_card_expiry("cc_expiry_date", "event_time")]           
)   
fg.insert(df) # transformation functions are run on insertion
```

The ODT is executed in this feature pipeline when you call `fg.insert(df). 
ODTã¯ã€`fg.insert(df)`ã‚’å‘¼ã³å‡ºã™ã¨ã€ã“ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

The names of the parameters for the `days_to_card_expiry function need to match the` names of columns in df; otherwise, you will get an error. 
`days_to_card_expiry`é–¢æ•°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã¯ã€dfå†…ã®åˆ—åã¨ä¸€è‡´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãã†ã§ãªã‘ã‚Œã°ã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ã€‚

Sometimes a df can contain columns used to compute the ODT, but those columns are not features in the feature group. 
æ™‚ã«ã¯ã€dfãŒODTã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹åˆ—ã‚’å«ã‚€ã“ã¨ãŒã‚ã‚Šã¾ã™ãŒã€ãã‚Œã‚‰ã®åˆ—ã¯ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

In this case, you can tell the ODT to `drop those columns from` `df after the` feature has been computed: 
ã“ã®å ´åˆã€ODTã«å¯¾ã—ã¦ã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãŒè¨ˆç®—ã•ã‚ŒãŸå¾Œã«`dfã‹ã‚‰ã“ã‚Œã‚‰ã®åˆ—ã‚’å‰Šé™¤ã™ã‚‹`ã‚ˆã†ã«æŒ‡ç¤ºã§ãã¾ã™ï¼š

```   
@hopsworks.udf(return_type=float, drop=["cc_expiry_date"])
```

MDTs can also use the same `drop syntax to drop columns. 
MDTã‚‚åŒã˜`dropæ§‹æ–‡ã‚’ä½¿ç”¨ã—ã¦åˆ—ã‚’å‰Šé™¤ã§ãã¾ã™ã€‚

In Chapter 11, we will look at how both ODTs and MDTs are executed in online inference pipelines. 
ç¬¬11ç« ã§ã¯ã€ODTã¨MDTã®ä¸¡æ–¹ãŒã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã©ã®ã‚ˆã†ã«å®Ÿè¡Œã•ã‚Œã‚‹ã‹ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚

###### PyTorch Transformations
We switch tracks now to look at transformations on unstructured data (image, audio, video, or text data). 
ã“ã“ã§ã€éæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆç”»åƒã€éŸ³å£°ã€å‹•ç”»ã€ã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰ã«å¯¾ã™ã‚‹å¤‰æ›ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚

ML systems trained with unstructured data typically use deep learning algorithms and transform the data into tensors for model input. 
éæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸMLã‚·ã‚¹ãƒ†ãƒ ã¯ã€é€šå¸¸ã€æ·±å±¤å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ã€ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ç”¨ã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã¾ã™ã€‚

_Convoluâ€_ _tional neural networks (CNNs) and_ _transformer architectures (transformers) are the_ most popular deep learning model architectures. 
_ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆCNNï¼‰ã¨_ _ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ï¼‰ã¯ã€æœ€ã‚‚äººæ°—ã®ã‚ã‚‹æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚

PyTorch is the most popular frameâ€ work for deep learning, with alternatives including TensorFlow and JAX. 
PyTorchã¯æ·±å±¤å­¦ç¿’ã®ãŸã‚ã®æœ€ã‚‚äººæ°—ã®ã‚ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚Šã€TensorFlowã‚„JAXãªã©ã®ä»£æ›¿æ‰‹æ®µã‚‚ã‚ã‚Šã¾ã™ã€‚

In ML systems built with PyTorch, we can also benefit from refactoring our data transformation code into MITs, MDTs, and ODTs in FTI pipelines. 
PyTorchã§æ§‹ç¯‰ã•ã‚ŒãŸMLã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€FTIãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å†…ã§ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚³ãƒ¼ãƒ‰ã‚’MITã€MDTã€ãŠã‚ˆã³ODTã«ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§åˆ©ç›Šã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

These data transâ€ formations will, however, output tensors or work with tensorsâ€”up to now, we have only looked at MITs, MDTs, and ODTs that work with tabular data. 
ãŸã ã—ã€ã“ã‚Œã‚‰ã®ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã¯ãƒ†ãƒ³ã‚½ãƒ«ã‚’å‡ºåŠ›ã™ã‚‹ã‹ã€ãƒ†ãƒ³ã‚½ãƒ«ã§ä½œæ¥­ã—ã¾ã™ã€‚ã“ã‚Œã¾ã§ã€ç§ãŸã¡ã¯è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã§å‹•ä½œã™ã‚‹MITã€MDTã€ãŠã‚ˆã³ODTã®ã¿ã‚’è¦‹ã¦ãã¾ã—ãŸã€‚



We will look at PyTorch transformations from the context of an example ML system that predicts your celebrity twin using an image classification model.[1] 
ç§ãŸã¡ã¯ã€ç”»åƒåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚ãªãŸã®æœ‰åäººã®åŒå­ã‚’äºˆæ¸¬ã™ã‚‹ä¾‹ã®MLã‚·ã‚¹ãƒ†ãƒ ã®æ–‡è„ˆã‹ã‚‰PyTorchã®å¤‰æ›ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚[1]

Figure 7-4 shows a real-time ML system based on the FTI architecture. 
å›³7-4ã¯ã€FTIã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ããƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ MLã‚·ã‚¹ãƒ†ãƒ ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

The training pipeline fine-tunes a ResNet model using the CelebA dataset. 
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ã€CelebAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ResNetãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚

The online inference pipeline takes an uploaded image of a person as input, the image is transformed into an input tensor, and the model predicts the closest-matching celebrity by using the input tensor. 
ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸäººç‰©ã®ç”»åƒã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€ãã®ç”»åƒã‚’å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã€ãƒ¢ãƒ‡ãƒ«ã¯å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½¿ç”¨ã—ã¦æœ€ã‚‚ä¸€è‡´ã™ã‚‹æœ‰åäººã‚’äºˆæ¸¬ã—ã¾ã™ã€‚

The source code for this example is found in the bookâ€™s GitHub repository. 
ã“ã®ä¾‹ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã¯ã€æœ¬ã®GitHubãƒªãƒã‚¸ãƒˆãƒªã«ã‚ã‚Šã¾ã™ã€‚

_Figure 7-4. A real-time ML system that predicts your celebrity twin using image classification. It uses PyTorch and Torchvision. Some image preprocessing is offloaded to the feature pipeline and executed in ODTs and image augmentation. Other image preprocessing tasks are executed as MDTs in both the training and online inference pipelines._ 
_å›³7-4. ç”»åƒåˆ†é¡ã‚’ä½¿ç”¨ã—ã¦ã‚ãªãŸã®æœ‰åäººã®åŒå­ã‚’äºˆæ¸¬ã™ã‚‹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ MLã‚·ã‚¹ãƒ†ãƒ ã€‚PyTorchã¨Torchvisionã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ä¸€éƒ¨ã®ç”»åƒå‰å‡¦ç†ã¯ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€ODTsãŠã‚ˆã³ç”»åƒæ‹¡å¼µã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚ä»–ã®ç”»åƒå‰å‡¦ç†ã‚¿ã‚¹ã‚¯ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¸¡æ–¹ã§MDTsã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚_

The benefit of the FTI architecture in this example is that it shifts image transformations from the training pipeline to the feature pipeline. 
ã“ã®ä¾‹ã«ãŠã‘ã‚‹FTIã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åˆ©ç‚¹ã¯ã€ç”»åƒå¤‰æ›ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ç§»ã™ã“ã¨ã§ã™ã€‚

This reduces the number of image transformations that are performed on CPUs in the training pipeline, before the input tensors are passed to the GPU for model training. 
ã“ã‚Œã«ã‚ˆã‚Šã€å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ãŒãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã«GPUã«æ¸¡ã•ã‚Œã‚‹å‰ã«ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§CPUä¸Šã§å®Ÿè¡Œã•ã‚Œã‚‹ç”»åƒå¤‰æ›ã®æ•°ãŒæ¸›ã‚Šã¾ã™ã€‚

If training is bottlenecked on high CPU load due to a large amount of image preprocessing, offloading transformations to the feature pipeline will increase GPU utilization during training. 
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¤§é‡ã®ç”»åƒå‰å‡¦ç†ã«ã‚ˆã‚‹é«˜ã„CPUè² è·ã§ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã£ã¦ã„ã‚‹å ´åˆã€å¤‰æ›ã‚’ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã®GPUã®åˆ©ç”¨ç‡ãŒå‘ä¸Šã—ã¾ã™ã€‚

The feature pipeline performs the following tasks: image resizing, image centering, jitter control, and image augmentation. 
ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ã€æ¬¡ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¾ã™ï¼šç”»åƒã®ãƒªã‚µã‚¤ã‚ºã€ç”»åƒã®ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°ã€ã‚¸ãƒƒã‚¿ãƒ¼åˆ¶å¾¡ã€ãŠã‚ˆã³ç”»åƒæ‹¡å¼µã€‚

Image augmentation occurs when you create many variations on the same input image for training dataâ€”you can flip an image, change its colors, or erase part of an image randomly (for self-supervised learning with transformers). 
ç”»åƒæ‹¡å¼µã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ãŸã‚ã«åŒã˜å…¥åŠ›ç”»åƒã®å¤šãã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹éš›ã«ç™ºç”Ÿã—ã¾ã™ã€‚ç”»åƒã‚’åè»¢ã•ã›ãŸã‚Šã€è‰²ã‚’å¤‰æ›´ã—ãŸã‚Šã€ç”»åƒã®ä¸€éƒ¨ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«æ¶ˆå»ã—ãŸã‚Šã§ãã¾ã™ï¼ˆãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ã—ãŸè‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã®ãŸã‚ã«ï¼‰ã€‚

Image augmentation helps CNNs generalize better, as the different variations of the same image prevent the model from overfitting on a single image by learning features that are invariant to transformations. 
ç”»åƒæ‹¡å¼µã¯ã€åŒã˜ç”»åƒã®ç•°ãªã‚‹ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒã€å¤‰æ›ã«å¯¾ã—ã¦ä¸å¤‰ãªç‰¹å¾´ã‚’å­¦ã¶ã“ã¨ã«ã‚ˆã£ã¦ãƒ¢ãƒ‡ãƒ«ãŒå˜ä¸€ã®ç”»åƒã«éå‰°é©åˆã™ã‚‹ã®ã‚’é˜²ããŸã‚ã€CNNãŒã‚ˆã‚Šè‰¯ãä¸€èˆ¬åŒ–ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚

Image augmentation happens after we resize, center crop, and color jitter images. 
ç”»åƒæ‹¡å¼µã¯ã€ç”»åƒã‚’ãƒªã‚µã‚¤ã‚ºã—ã€ã‚»ãƒ³ã‚¿ãƒ¼ã‚¯ãƒ­ãƒƒãƒ—ã—ã€è‰²ã®ã‚¸ãƒƒã‚¿ãƒ¼ã‚’é©ç”¨ã—ãŸå¾Œã«è¡Œã‚ã‚Œã¾ã™ã€‚

So if we want to migrate `ImageAugmentation from the training pipeline to the feature` pipeline, we also need to migrate `Resize,` `CenterCrop, and` `ColorJitter to the feature pipeline to run as ODTs. 
ã—ãŸãŒã£ã¦ã€`ImageAugmentationã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ç§»è¡Œã—ãŸã„å ´åˆã€`Resizeã€` `CenterCropã€ãŠã‚ˆã³` `ColorJitterã‚’ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ç§»è¡Œã—ã¦ODTsã¨ã—ã¦å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

We will also need to run those transformations in the online inference pipeline on uploaded images. 
ã¾ãŸã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã«å¯¾ã—ã¦ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ãã‚Œã‚‰ã®å¤‰æ›ã‚’å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

The feature pipeline will output transformed and augmented images as PNG files. 
ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ã€å¤‰æ›ã•ã‚ŒãŸæ‹¡å¼µç”»åƒã‚’PNGãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚

In both training and online inference, we need to convert the PNG files to tensors, which we perform in MDTs. 
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¨è«–ã®ä¸¡æ–¹ã§ã€PNGãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€ã“ã‚Œã¯MDTsã§å®Ÿè¡Œã—ã¾ã™ã€‚

PyTorch provides a library for image transformations called Torchvision v2, and it supports built-in transformations for images. 
PyTorchã¯ã€Torchvision v2ã¨ã„ã†ç”»åƒå¤‰æ›ç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’æä¾›ã—ã¦ãŠã‚Šã€ç”»åƒã®ãŸã‚ã®çµ„ã¿è¾¼ã¿å¤‰æ›ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚

The following code snippet shows how to define a custom ImageAugmentation transformation by composing transformation functions: 
ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆã¯ã€å¤‰æ›é–¢æ•°ã‚’çµ„ã¿åˆã‚ã›ã¦ã‚«ã‚¹ã‚¿ãƒ ImageAugmentationå¤‰æ›ã‚’å®šç¾©ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼š

```  
import torchvision.transforms.v2 as v2  
class ImageAugmentation(nn.Module):     
    def __init__(self, flip_prob=0.5, rotation_range=(-30, 30)):       
        self.flip_prob = flip_prob       
        self.rotation_range = rotation_range     
    def forward(self, img):       
        â€¦   
on_demand_transforms = v2.Compose([     
    v2.Resize(...),     
    v2.CenterCrop(...),   
])   
model_independent_transforms = v2.Compose([     
    v2.Resize(...),     
    v2.CenterCrop(...),     
    ImageAugmentation(...)   
])   
model_dependent_transforms = v2.Compose([     
    v2.ToImage(...),     
    v2.ToDtype(...),     
    v2.Normalize(...)   
])
```

PyTorch provides datasets as a data structure to store your features and labels. 
PyTorchã¯ã€ç‰¹å¾´ã¨ãƒ©ãƒ™ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æä¾›ã—ã¾ã™ã€‚

There are pre-created datasets, and you can create your own custom datasets using the provided base classes. 
äº‹å‰ã«ä½œæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒã‚ã‚Šã€æä¾›ã•ã‚ŒãŸãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ã¦ç‹¬è‡ªã®ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã§ãã¾ã™ã€‚

You can apply the transformations to a dataset in PyTorch before training a model, as shown here:  
PyTorchã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å‰ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¤‰æ›ã‚’é©ç”¨ã§ãã¾ã™ã€‚ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ï¼š

```  
dataset = datasets.ImageFolder(root='images/train',     
    transform=model_independent_transforms )   
dataloader = DataLoader(dataset, batch_size=32, num_workers=4)   
for images, labels in dataloader:     
    # Your training code goes here
```

From this example PyTorch system, you can see the benefits of the FTI pipeline architecture in improved code modularity and preprocessing images using feature pipelines. 
ã“ã®ä¾‹ã®PyTorchã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã€FTIãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åˆ©ç‚¹ãŒã€ã‚³ãƒ¼ãƒ‰ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ€§ã®å‘ä¸Šã¨ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨ã—ãŸç”»åƒã®å‰å‡¦ç†ã«ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

###### Using pytest
Transformation functions and feature functions from feature pipelines create features. 
ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‹ã‚‰ã®å¤‰æ›é–¢æ•°ã¨ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼é–¢æ•°ã¯ã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚’ä½œæˆã—ã¾ã™ã€‚

Once a feature has been created and is used by downstream training or inference pipelines, then between the function that creates the feature and the user of the feature, there is an implicit agreement that the feature logic should not change unexpectedly. 
ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãŒä½œæˆã•ã‚Œã€ä¸‹æµã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¾ãŸã¯æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ä½¿ç”¨ã•ã‚Œã‚‹ã¨ã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚’ä½œæˆã™ã‚‹é–¢æ•°ã¨ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é–“ã«ã¯ã€ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ­ã‚¸ãƒƒã‚¯ãŒäºˆæœŸã›ãšå¤‰æ›´ã•ã‚Œãªã„ã¨ã„ã†æš—é»™ã®åˆæ„ãŒã‚ã‚Šã¾ã™ã€‚

Changes in how a feature is computed can break clients. 
ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã®è¨ˆç®—æ–¹æ³•ã®å¤‰æ›´ã¯ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å£Šã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

Unit tests help ensure that developers do not make unexpected changes to how features are computed, and that helps developers make safe, incremental upgrades to their ML pipelines. 
ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã¯ã€é–‹ç™ºè€…ãŒãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã®è¨ˆç®—æ–¹æ³•ã«äºˆæœŸã—ãªã„å¤‰æ›´ã‚’åŠ ãˆãªã„ã‚ˆã†ã«ã—ã€é–‹ç™ºè€…ãŒMLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«å®‰å…¨ã§æ®µéšçš„ãªã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’è¡Œã†ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚

As much of the focus of this book is on Python, we will look in detail at the most popular unit testing framework in Python, pytest, and how we can use it to test transformation functions and, later, feature pipelines. 
ã“ã®æœ¬ã®å¤šãã®ç„¦ç‚¹ãŒPythonã«ã‚ã‚‹ãŸã‚ã€Pythonã§æœ€ã‚‚äººæ°—ã®ã‚ã‚‹ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚‹pytestã‚’è©³ç´°ã«è¦‹ã¦ã„ãã€å¤‰æ›é–¢æ•°ã‚„å¾Œã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ãŸã‚ã«ã©ã®ã‚ˆã†ã«ä½¿ç”¨ã§ãã‚‹ã‹ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚

If you write feature pipelines in another language, such as SQL or Java/Spark, then you can use other testing frameworks, such as unit testing with dbt and JUnit, respectively. 
SQLã‚„Java/Sparkãªã©ã®åˆ¥ã®è¨€èªã§ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ›¸ãå ´åˆã¯ã€ãã‚Œãã‚Œdbtã‚„JUnitã‚’ä½¿ç”¨ã—ãŸãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆãªã©ã€ä»–ã®ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚

###### Unit Tests
Letâ€™s look at our example feature, days_to_card_expiry, and how and why we would test it: 
ä¾‹ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã§ã‚ã‚‹days_to_card_expiryã‚’è¦‹ã¦ã€ãã®ãƒ†ã‚¹ãƒˆæ–¹æ³•ã¨ç†ç”±ã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ï¼š

```  
def days_to_card_expiry(cc_expiry_date, event_time):     
    return (cc_expiry_date - event_time).dt.days
```

This is a straightforward but undocumented function. 
ã“ã‚Œã¯ç°¡å˜ã§ã™ãŒã€æ–‡æ›¸åŒ–ã•ã‚Œã¦ã„ãªã„é–¢æ•°ã§ã™ã€‚

A junior developer discovered that the function would not work with a log transformation if the card expired on the same day as it was used. 
ã‚¸ãƒ¥ãƒ‹ã‚¢é–‹ç™ºè€…ã¯ã€ã‚«ãƒ¼ãƒ‰ãŒä½¿ç”¨ã•ã‚ŒãŸåŒã˜æ—¥ã«æœŸé™åˆ‡ã‚Œã«ãªã£ãŸå ´åˆã€é–¢æ•°ãŒãƒ­ã‚°å¤‰æ›ã§æ©Ÿèƒ½ã—ãªã„ã“ã¨ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚

Log transformations are undefined if the value is zero or negative. 
å€¤ãŒã‚¼ãƒ­ã¾ãŸã¯è² ã®å ´åˆã€ãƒ­ã‚°å¤‰æ›ã¯æœªå®šç¾©ã§ã™ã€‚

So the developer changed the code to return 1 rather than a negative number: 
ãã“ã§ã€é–‹ç™ºè€…ã¯è² ã®æ•°ã§ã¯ãªã1ã‚’è¿”ã™ã‚ˆã†ã«ã‚³ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã—ã¾ã—ãŸï¼š

```  
def days_to_card_expiry(cc_expiry_date, event_time):     
    days_remaining = (cc_expiry_date - event_time).dt.days     
    return max(days_remaining, 1)
```

A senior developer, stressed from their current project, performs a cursory review, approves the code, and lets it go into production. 
ç¾åœ¨ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã‚¹ãƒˆãƒ¬ã‚¹ã‚’æ„Ÿã˜ã¦ã„ã‚‹ã‚·ãƒ‹ã‚¢é–‹ç™ºè€…ã¯ã€ã–ã£ã¨ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡Œã„ã€ã‚³ãƒ¼ãƒ‰ã‚’æ‰¿èªã—ã€ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ã«æŠ•å…¥ã—ã¾ã™ã€‚

Suddenly, the credit card fraud detection model performance degrades. 
çªç„¶ã€ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰è©æ¬ºæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒä½ä¸‹ã—ã¾ã™ã€‚

The senior developer reverts the change to the transformation function and removes the log transformation, resolving the bug for now. 
ã‚·ãƒ‹ã‚¢é–‹ç™ºè€…ã¯å¤‰æ›é–¢æ•°ã®å¤‰æ›´ã‚’å…ƒã«æˆ»ã—ã€ãƒ­ã‚°å¤‰æ›ã‚’å‰Šé™¤ã—ã¦ã€ä»Šã®ã¨ã“ã‚ãƒã‚°ã‚’è§£æ±ºã—ã¾ã™ã€‚

How could we have identified this problem before it rolled out? 
ã“ã®å•é¡Œã‚’å±•é–‹å‰ã«ã©ã®ã‚ˆã†ã«ç‰¹å®šã§ããŸã§ã—ã‚‡ã†ã‹ï¼Ÿ

Studies have shown that code reviews and documentation are not very effective in finding many bugs. 
ç ”ç©¶ã«ã‚ˆã‚‹ã¨ã€ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚„æ–‡æ›¸åŒ–ã¯ã€å¤šãã®ãƒã‚°ã‚’è¦‹ã¤ã‘ã‚‹ã®ã«ã‚ã¾ã‚ŠåŠ¹æœçš„ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

Performing unit tests is a more structured way of finding bugs earlierâ€”before code review. 
ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’å®Ÿæ–½ã™ã‚‹ã“ã¨ã¯ã€ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®å‰ã«ãƒã‚°ã‚’æ—©æœŸã«è¦‹ã¤ã‘ã‚‹ãŸã‚ã®ã‚ˆã‚Šæ§‹é€ åŒ–ã•ã‚ŒãŸæ–¹æ³•ã§ã™ã€‚

Here are a few unit tests for days_to_card_expiry. 
days_to_card_expiryã®ã„ãã¤ã‹ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ã€‚

The test_days_to_today_expiry test would have failed as a result of the junior developerâ€™s changes, and the change would never have made it to production: 
test_days_to_today_expiryãƒ†ã‚¹ãƒˆã¯ã€ã‚¸ãƒ¥ãƒ‹ã‚¢é–‹ç™ºè€…ã®å¤‰æ›´ã®çµæœã¨ã—ã¦å¤±æ•—ã—ã¦ãŠã‚Šã€ãã®å¤‰æ›´ã¯æ±ºã—ã¦ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ°é”ã—ãªã‹ã£ãŸã§ã—ã‚‡ã†ï¼š

```  
import pytest   
def test_days_to_future_expiry():     
    future_date = datetime.date.today() + datetime.timedelta(days=30)     
    assert days_to_card_expiry(future_date, datetime.date.today()) == 30   
def test_days_to_today_expiry():     
    today_date = datetime.date.today()     
    assert days_to_card_expiry(today_date, today_date) == 0   
def test_expired_card():     
    past_date = datetime.date.today() - datetime.timedelta(days=10)     
    with pytest.raises(ValueError, match="Credit card is expired."):       
        days_to_card_expiry(past_date, datetime.date.today())
```

These unit tests were suggested to me by an LLMâ€”I copied in the function and asked it to write some pytest unit tests for me. 
ã“ã‚Œã‚‰ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã¯ã€LLMã«ã‚ˆã£ã¦ææ¡ˆã•ã‚Œã¾ã—ãŸã€‚ç§ã¯é–¢æ•°ã‚’ã‚³ãƒ”ãƒ¼ã—ã€ã„ãã¤ã‹ã®pytestãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’æ›¸ãã‚ˆã†ã«é ¼ã¿ã¾ã—ãŸã€‚

The unit tests cover the following potential error cases: 
ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã¯ã€ä»¥ä¸‹ã®æ½œåœ¨çš„ãªã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã™ï¼š

``` 
test_days_to_future_expiry
``` 
ã“ã‚Œã¯ã€ã‚«ãƒ¼ãƒ‰ãŒæœªæ¥ã®æ•°æ—¥å¾Œã«æœŸé™åˆ‡ã‚Œã«ãªã‚‹ã€Œé€šå¸¸ã®ã€ã‚±ãƒ¼ã‚¹ã§ã™ï¼ˆLLMã¯30æ—¥ã‚’åˆç†çš„ãªæœªæ¥ã®æ—¥ä»˜ã¨ã—ã¦é¸ã³ã¾ã—ãŸï¼‰ã€‚ã“ã‚Œã¯10æ—¥ã€40æ—¥ã€80æ—¥ã§ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚ãŠãã‚‰ã10,000æ—¥ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å®Ÿéš›ã€ã“ã“ã«ã¯æœªæ¥ã®æ—¥æ•°ãŒå¤šã™ãã‚‹å ´åˆã®ãƒ†ã‚¹ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã“ã‚Œã¯æ¼”ç¿’ã¨ã—ã¦è¿½åŠ ã§ãã¾ã™ã€‚

``` 
test_days_to_today_expiry
``` 
ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ç§‘å­¦è€…ã¯ã‚¼ãƒ­ã‹ã‚‰æ•°ãˆå§‹ã‚ã¾ã™ãŒã€ä¸€èˆ¬ã®äººã€…ã¯1ã‹ã‚‰æ•°ãˆå§‹ã‚ã‚‹ãŸã‚ã€ã‚ªãƒ•ãƒã‚¤ãƒ¯ãƒ³ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ãŒã‚ˆãã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯è‰¯ã„ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆã§ã™ã€‚

``` 
test_expired_card
``` 
days_to_card_expiryã®æ–°ã—ã„å®Ÿè£…ã¯ã€cc_expiry_dateãŒå–å¼•æ—¥ã‚ˆã‚Šå‰ã§ã‚ã‚‹å ´åˆã«ValueErrorãŒã‚¹ãƒ­ãƒ¼ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚

The LLM worked reasonably well at generating the unit tests for our function, as its function name, parameter names, and variable names are human readable. 
LLMã¯ã€é–¢æ•°åã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã€ãŠã‚ˆã³å¤‰æ•°åãŒäººé–“ã«ã¨ã£ã¦èª­ã¿ã‚„ã™ã„ãŸã‚ã€ç§ãŸã¡ã®é–¢æ•°ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã®ã«ã‹ãªã‚Šã†ã¾ãæ©Ÿèƒ½ã—ã¾ã—ãŸã€‚

The LLM understood the semantics of the functionâ€”what the function is supposed to do. 
LLMã¯é–¢æ•°ã®æ„å‘³ã‚’ç†è§£ã—ã¦ãŠã‚Šã€é–¢æ•°ãŒä½•ã‚’ã™ã‚‹ã¹ãã‹ã‚’æŠŠæ¡ã—ã¦ã„ã¾ã™ã€‚

Naturally, I did a code review of LLM-generated unit tests, and I was happy with them. 
å½“ç„¶ã®ã“ã¨ãªãŒã‚‰ã€ç§ã¯LLMãŒç”Ÿæˆã—ãŸãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã®ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡Œã„ã€æº€è¶³ã—ã¾ã—ãŸã€‚

If you want more complicated feature functions, you will probably have to write them yourselfâ€”or at least handle some edge cases yourself. 
ã‚ˆã‚Šè¤‡é›‘ãªãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼é–¢æ•°ãŒå¿…è¦ãªå ´åˆã¯ã€ãŠãã‚‰ãè‡ªåˆ†ã§æ›¸ãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã‚ã‚‹ã„ã¯ã€å°‘ãªãã¨ã‚‚ã„ãã¤ã‹ã®ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã‚’è‡ªåˆ†ã§å‡¦ç†ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

Donâ€™t just blindly trust LLMs to generate correct unit tests. 
LLMãŒæ­£ã—ã„ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚’ç›²ç›®çš„ã«ä¿¡é ¼ã—ãªã„ã§ãã ã•ã„ã€‚

Trust is good, but validation is better. 
ä¿¡é ¼ã¯è‰¯ã„ã§ã™ãŒã€æ¤œè¨¼ã¯ã•ã‚‰ã«è‰¯ã„ã§ã™ã€‚



A failure to introduce automated testing is what brought global IT infrastructure to its knees in mid-2024, when a bug was introduced into the Windows kernel by the security company CrowdStrike, causing Windows to crash. 
è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚’å°å…¥ã—ãªã‹ã£ãŸã“ã¨ãŒã€2024å¹´ä¸­é ƒã«ä¸–ç•Œã®ITã‚¤ãƒ³ãƒ•ãƒ©ã‚’éº»ç—ºã•ã›ãŸåŸå› ã§ã™ã€‚ã“ã‚Œã¯ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¼šç¤¾CrowdStrikeã«ã‚ˆã£ã¦Windowsã‚«ãƒ¼ãƒãƒ«ã«ãƒã‚°ãŒå°å…¥ã•ã‚Œã€WindowsãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ãŸã“ã¨ã«ã‚ˆã‚‹ã‚‚ã®ã§ã™ã€‚

The bug was that a developer did not check whether an element in a struct was null before using it. 
ãã®ãƒã‚°ã¯ã€é–‹ç™ºè€…ãŒæ§‹é€ ä½“å†…ã®è¦ç´ ãŒnullã§ã‚ã‚‹ã‹ã©ã†ã‹ã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ç¢ºèªã—ãªã‹ã£ãŸã“ã¨ã«èµ·å› ã—ã¦ã„ã¾ã™ã€‚

They admitted that they hadnâ€™t tested the code change that was rolled out to servers worldwide, causing widespread delays at airports and railways and problems at many retailers and other internet companies. 
å½¼ã‚‰ã¯ã€ä¸–ç•Œä¸­ã®ã‚µãƒ¼ãƒã«å±•é–‹ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰å¤‰æ›´ã‚’ãƒ†ã‚¹ãƒˆã—ã¦ã„ãªã‹ã£ãŸã“ã¨ã‚’èªã‚ã¦ãŠã‚Šã€ãã®çµæœã€ç©ºæ¸¯ã‚„é‰„é“ã§ã®åºƒç¯„ãªé…å»¶ã‚„ã€å¤šãã®å°å£²æ¥­è€…ã‚„ä»–ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆä¼æ¥­ã§ã®å•é¡Œã‚’å¼•ãèµ·ã“ã—ã¾ã—ãŸã€‚

I wouldnâ€™t have wanted to be that junior developer, but they werenâ€™t the main culprit. 
ç§ã¯ãã®ã‚¸ãƒ¥ãƒ‹ã‚¢é–‹ç™ºè€…ã«ãªã‚ŠãŸãã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸãŒã€å½¼ã‚‰ãŒä¸»ãªåŸå› ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚

Engineering leaders didnâ€™t introduce automated testing, a fundamental software engineering practice that would have detected the bug before it was rolled out into production. 
ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãƒªãƒ¼ãƒ€ãƒ¼ã¯ã€è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚’å°å…¥ã—ã¾ã›ã‚“ã§ã—ãŸã€‚ã“ã‚Œã¯ã€ãƒã‚°ãŒæœ¬ç•ªç’°å¢ƒã«å±•é–‹ã•ã‚Œã‚‹å‰ã«æ¤œå‡ºã§ããŸåŸºæœ¬çš„ãªã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å®Ÿè·µã§ã™ã€‚

###### Implementing pytest unit tests
###### pytestãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã®å®Ÿè£…

Unit tests are defined on Python functions. 
ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã¯Pythoné–¢æ•°ã«å¯¾ã—ã¦å®šç¾©ã•ã‚Œã¾ã™ã€‚

If you want to unit-test individual features, you should factor your code so that each feature is computed by a single function. 
å€‹ã€…ã®æ©Ÿèƒ½ã‚’ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã—ãŸã„å ´åˆã¯ã€å„æ©Ÿèƒ½ãŒå˜ä¸€ã®é–¢æ•°ã«ã‚ˆã£ã¦è¨ˆç®—ã•ã‚Œã‚‹ã‚ˆã†ã«ã‚³ãƒ¼ãƒ‰ã‚’æ•´ç†ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

As we use Python functions to implement the feature logic, we can use a unit test to validate that the code that computes a feature correctly follows a specification defined by the unit test itself. 
ç§ãŸã¡ã¯Pythoné–¢æ•°ã‚’ä½¿ç”¨ã—ã¦æ©Ÿèƒ½ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…ã™ã‚‹ãŸã‚ã€ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€æ©Ÿèƒ½ã‚’è¨ˆç®—ã™ã‚‹ã‚³ãƒ¼ãƒ‰ãŒãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆè‡ªä½“ã«ã‚ˆã£ã¦å®šç¾©ã•ã‚ŒãŸä»•æ§˜ã«æ­£ã—ãå¾“ã£ã¦ã„ã‚‹ã“ã¨ã‚’æ¤œè¨¼ã§ãã¾ã™ã€‚

That is, the unit test is a specification of the invariants, _preconditions, and postconditions for the feature logic:_ 
ã¤ã¾ã‚Šã€ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã¯æ©Ÿèƒ½ãƒ­ã‚¸ãƒƒã‚¯ã®ä¸å¤‰æ¡ä»¶ã€_å‰ææ¡ä»¶ã€åŠã³å¾Œç¶šæ¡ä»¶ã®ä»•æ§˜ã§ã™ï¼š

_Invariant_ A condition that remains true throughout the lifetime of the functionâ€”it is true before and after the function call and also within the scope of the function. 
_ä¸å¤‰æ¡ä»¶_ é–¢æ•°ã®ãƒ©ã‚¤ãƒ•ã‚¿ã‚¤ãƒ å…¨ä½“ã«ã‚ãŸã£ã¦çœŸã§ã‚ã‚‹æ¡ä»¶ã§ã™ã€‚é–¢æ•°å‘¼ã³å‡ºã—ã®å‰å¾ŒãŠã‚ˆã³é–¢æ•°ã®ã‚¹ã‚³ãƒ¼ãƒ—å†…ã§ã‚‚çœŸã§ã™ã€‚

Invariants are more applicable to stateful objects, where certain properties need to hold true across multiple function calls. 
ä¸å¤‰æ¡ä»¶ã¯ã€ç‰¹å®šã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãŒè¤‡æ•°ã®é–¢æ•°å‘¼ã³å‡ºã—ã«ã‚ãŸã£ã¦çœŸã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹çŠ¶æ…‹ã‚’æŒã¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ã‚ˆã‚Šé©ç”¨ã•ã‚Œã¾ã™ã€‚

_Precondition_ Must be true before a function can be executed correctly. 
_å‰ææ¡ä»¶_ é–¢æ•°ãŒæ­£ã—ãå®Ÿè¡Œã•ã‚Œã‚‹å‰ã«çœŸã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚

It defines a valid input and/or state for the function to be executed without error. 
ãã‚Œã¯ã€é–¢æ•°ãŒã‚¨ãƒ©ãƒ¼ãªã—ã«å®Ÿè¡Œã•ã‚Œã‚‹ãŸã‚ã®æœ‰åŠ¹ãªå…¥åŠ›ãŠã‚ˆã³/ã¾ãŸã¯çŠ¶æ…‹ã‚’å®šç¾©ã—ã¾ã™ã€‚

_Postcondition_ A condition or set of conditions that must hold true after a function or method completes its execution. 
_å¾Œç¶šæ¡ä»¶_ é–¢æ•°ã¾ãŸã¯ãƒ¡ã‚½ãƒƒãƒ‰ãŒå®Ÿè¡Œã‚’å®Œäº†ã—ãŸå¾Œã«çœŸã§ãªã‘ã‚Œã°ãªã‚‰ãªã„æ¡ä»¶ã¾ãŸã¯æ¡ä»¶ã®ã‚»ãƒƒãƒˆã§ã™ã€‚

Often, they are related to stateful functionsâ€”functions that modify external stateâ€”but you can also validate the output of stateless functions. 
ã—ã°ã—ã°ã€ã“ã‚Œã‚‰ã¯å¤–éƒ¨çŠ¶æ…‹ã‚’å¤‰æ›´ã™ã‚‹çŠ¶æ…‹ã‚’æŒã¤é–¢æ•°ã«é–¢é€£ã—ã¦ã„ã¾ã™ãŒã€ç„¡çŠ¶æ…‹é–¢æ•°ã®å‡ºåŠ›ã‚’æ¤œè¨¼ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

In our days_to_card_expiry function, we can see examples of our conditions: 
ç§ãŸã¡ã®days_to_card_expiryé–¢æ•°ã§ã¯ã€æ¡ä»¶ã®ä¾‹ã‚’è¦‹ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

_Precondition_ The cc_expiry_date cannot be earlier than the transaction_date. 
_å‰ææ¡ä»¶_ cc_expiry_dateã¯transaction_dateã‚ˆã‚Šã‚‚æ—©ãã¦ã¯ãªã‚Šã¾ã›ã‚“ã€‚

_Postcondition_ Our function is stateless (it depends only on its input arguments), but we can still validate a postconditionâ€”if it doesnâ€™t throw an exception, it should return either zero or a positive integer value. 
_å¾Œç¶šæ¡ä»¶_ ç§ãŸã¡ã®é–¢æ•°ã¯ç„¡çŠ¶æ…‹ã§ã™ï¼ˆå…¥åŠ›å¼•æ•°ã®ã¿ã«ä¾å­˜ã—ã¾ã™ï¼‰ãŒã€å¾Œç¶šæ¡ä»¶ã‚’æ¤œè¨¼ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã™ã€‚ä¾‹å¤–ã‚’ã‚¹ãƒ­ãƒ¼ã—ãªã„å ´åˆã€ã‚¼ãƒ­ã¾ãŸã¯æ­£ã®æ•´æ•°å€¤ã‚’è¿”ã™ã¹ãã§ã™ã€‚

_Invariant_ There are no invariants tested in our preceding unit tests, mostly because it is a stateless function call we are testing. 
_ä¸å¤‰æ¡ä»¶_ å‰ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã§ã¯ä¸å¤‰æ¡ä»¶ã¯ãƒ†ã‚¹ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä¸»ã«ã€ç§ãŸã¡ãŒãƒ†ã‚¹ãƒˆã—ã¦ã„ã‚‹ã®ã¯ç„¡çŠ¶æ…‹ã®é–¢æ•°å‘¼ã³å‡ºã—ã ã‹ã‚‰ã§ã™ã€‚

You need to understand three additional concepts to write unit tests in pytest: _test_ _functions, assertions, and test setup. 
pytestã§ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’æ›¸ããŸã‚ã«ã¯ã€3ã¤ã®è¿½åŠ ã®æ¦‚å¿µã‚’ç†è§£ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š_ãƒ†ã‚¹ãƒˆ_ _é–¢æ•°ã€ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³ã€ãŠã‚ˆã³ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã€‚

Unit tests may be written either as functions (as in the preceding example) or as methods in classes. 
ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã¯ã€é–¢æ•°ï¼ˆå‰ã®ä¾‹ã®ã‚ˆã†ã«ï¼‰ã¾ãŸã¯ã‚¯ãƒ©ã‚¹å†…ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¨ã—ã¦æ›¸ãã“ã¨ãŒã§ãã¾ã™ã€‚

Also, pytest has a naming convention to automatically discover test modules/classes/functions. 
ã¾ãŸã€pytestã«ã¯ãƒ†ã‚¹ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«/ã‚¯ãƒ©ã‚¹/é–¢æ•°ã‚’è‡ªå‹•çš„ã«ç™ºè¦‹ã™ã‚‹ãŸã‚ã®å‘½åè¦å‰‡ãŒã‚ã‚Šã¾ã™ã€‚

A test class must be named Test*, and test functions or methods must be named test_* (as in the preceding example). 
ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã¯Test*ã¨ã„ã†åå‰ã§ãªã‘ã‚Œã°ãªã‚‰ãšã€ãƒ†ã‚¹ãƒˆé–¢æ•°ã¾ãŸã¯ãƒ¡ã‚½ãƒƒãƒ‰ã¯test_*ã¨ã„ã†åå‰ã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ï¼ˆå‰ã®ä¾‹ã®ã‚ˆã†ã«ï¼‰ã€‚

[In Figure 7-5, we can see that pytest is run during development as offline testsâ€”not](https://oreil.ly/Qy5aN) when pipelines have been deployed to production (as online tests). 
[å›³7-5ã§ã¯ã€pytestãŒé–‹ç™ºä¸­ã«ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã‚‹æ§˜å­ãŒè¦‹ãˆã¾ã™ã€‚ã“ã‚Œã¯ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒæœ¬ç•ªç’°å¢ƒã«å±•é–‹ã•ã‚ŒãŸã¨ãï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆã¨ã—ã¦ï¼‰ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚]

_Figure 7-5. Diagram showing pytest running unit tests offline. They should run with zero friction during development._ 
_å›³7-5. pytestãŒã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¦ã„ã‚‹æ§˜å­ã‚’ç¤ºã™å›³ã€‚é–‹ç™ºä¸­ã¯æ‘©æ“¦ãªãå®Ÿè¡Œã•ã‚Œã‚‹ã¹ãã§ã™ã€‚_

You typically run unit tests in your development environment before you create a pull request (PR). 
é€šå¸¸ã€ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆPRï¼‰ã‚’ä½œæˆã™ã‚‹å‰ã«ã€é–‹ç™ºç’°å¢ƒã§ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

When you submit your PR to a staging branch, a CI/CD environment should also run the unit tests and ask you to fix your code and resubmit your PR if any of the unit tests are failing. 
PRã‚’ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ãƒ–ãƒ©ãƒ³ãƒã«æå‡ºã™ã‚‹ã¨ã€CI/CDç’°å¢ƒã‚‚ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã€ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ãŸå ´åˆã¯ã‚³ãƒ¼ãƒ‰ã‚’ä¿®æ­£ã—ã¦PRã‚’å†æå‡ºã™ã‚‹ã‚ˆã†ã«æ±‚ã‚ã‚‹ã¹ãã§ã™ã€‚

With our directory structure from Chapter 6 (you depend on the default Python behavior of putting the current directory in sys.path), you can run your unit tests in your development environment from the root directory of the credit card projectâ€™s directory in the source code repository: 
ç¬¬6ç« ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½¿ç”¨ã™ã‚‹ã¨ï¼ˆç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’sys.pathã«ç½®ãã¨ã„ã†Pythonã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å‹•ä½œã«ä¾å­˜ã—ã¾ã™ï¼‰ã€ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ãƒªãƒã‚¸ãƒˆãƒªå†…ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰é–‹ç™ºç’°å¢ƒã§ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã§ãã¾ã™ï¼š

```   
python -m pytest
```

You only need to install the pytest library during development or when automated tests are run after you commit code to GitHub. 
pytestãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ã€é–‹ç™ºä¸­ã¾ãŸã¯ã‚³ãƒ¼ãƒ‰ã‚’GitHubã«ã‚³ãƒŸãƒƒãƒˆã—ãŸå¾Œã«è‡ªå‹•ãƒ†ã‚¹ãƒˆãŒå®Ÿè¡Œã•ã‚Œã‚‹ã¨ãã«ã®ã¿ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

You donâ€™t need pytest installed in your production pipelines. 
æœ¬ç•ªç’°å¢ƒã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ã¯pytestã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

###### Running pytest as part of a GitHub Action
###### GitHub Actionã®ä¸€éƒ¨ã¨ã—ã¦pytestã‚’å®Ÿè¡Œã™ã‚‹

You can define a GitHub Action that will run the pytest unit tests whenever code is pushed to the main branch or whenever a pull request is created for the main branch: 
ã‚³ãƒ¼ãƒ‰ãŒãƒ¡ã‚¤ãƒ³ãƒ–ãƒ©ãƒ³ãƒã«ãƒ—ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ãŸã³ã€ã¾ãŸã¯ãƒ¡ã‚¤ãƒ³ãƒ–ãƒ©ãƒ³ãƒã«ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒä½œæˆã•ã‚Œã‚‹ãŸã³ã«pytestãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹GitHub Actionã‚’å®šç¾©ã§ãã¾ã™ï¼š

```   
name: Credit Card Fraud Test   
on:    
  push:     
    branches:      
      - main    
  pull_request:     
    branches:      
      - main   
jobs:    
  test:     
    runs-on: ubuntu-latest     
    steps:     
      - name: Check out repository code      
        uses: actions/checkout@v3     
      - name: Set up Python      
        uses: actions/setup-python@v5      
        with:       
          python-version: '3.12'     
      - name: Install dependencies      
        run: |       
          cd ccfraud       
          python -m pip install --upgrade pip       
          pip install -r requirements.txt     
      - name: Run tests      
        run: |       
          pytest
```

You can click on failed actions in GitHub to see the logs for why a unit test failed. 
GitHubã§å¤±æ•—ã—ãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ãŸç†ç”±ã®ãƒ­ã‚°ã‚’è¦‹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

Finally, when the test passes and after a code review, you want to merge the new PR to the main branch. 
æœ€å¾Œã«ã€ãƒ†ã‚¹ãƒˆãŒåˆæ ¼ã—ã€ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒå®Œäº†ã—ãŸã‚‰ã€æ–°ã—ã„PRã‚’ãƒ¡ã‚¤ãƒ³ãƒ–ãƒ©ãƒ³ãƒã«ãƒãƒ¼ã‚¸ã—ãŸã„ã¨æ€ã„ã¾ã™ã€‚

When you merge the PR, you should squash your commits (turn all your commits into one big commit) to get rid of your messy trail of commits. 
PRã‚’ãƒãƒ¼ã‚¸ã™ã‚‹ã¨ãã¯ã€ã‚³ãƒŸãƒƒãƒˆã‚’ã‚¹ã‚¯ãƒ¯ãƒƒã‚·ãƒ¥ã™ã‚‹ï¼ˆã™ã¹ã¦ã®ã‚³ãƒŸãƒƒãƒˆã‚’1ã¤ã®å¤§ããªã‚³ãƒŸãƒƒãƒˆã«å¤‰æ›ã™ã‚‹ï¼‰ã¹ãã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ··ä¹±ã—ãŸã‚³ãƒŸãƒƒãƒˆã®å±¥æ­´ã‚’å–ã‚Šé™¤ãã“ã¨ãŒã§ãã¾ã™ã€‚

In the long run, it pays to keep your house tidy! 
é•·æœŸçš„ã«ã¯ã€æ•´ç†æ•´é “ã‚’ä¿ã¤ã“ã¨ãŒé‡è¦ã§ã™ï¼

###### A Testing Methodology
###### ãƒ†ã‚¹ãƒˆæ–¹æ³•è«–

After covering all that tactical work on defining unit tests, running tests, and automating tests, we need to consider how we write tests and what we should test. 
ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã®å®šç¾©ã€ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œã€è‡ªå‹•åŒ–ã«é–¢ã™ã‚‹ã™ã¹ã¦ã®æˆ¦è¡“çš„ä½œæ¥­ã‚’ã‚«ãƒãƒ¼ã—ãŸå¾Œã€ç§ãŸã¡ã¯ãƒ†ã‚¹ãƒˆã‚’æ›¸ãæ–¹æ³•ã¨ä½•ã‚’ãƒ†ã‚¹ãƒˆã™ã¹ãã‹ã‚’è€ƒæ…®ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

For that, we need a methodology for structuring test cases. 
ãã®ãŸã‚ã«ã¯ã€ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’æ§‹é€ åŒ–ã™ã‚‹ãŸã‚ã®æ–¹æ³•è«–ãŒå¿…è¦ã§ã™ã€‚

I recommend using [the](https://oreil.ly/Tjokv) _[arrange, act, assert pattern that arranges the inputs and targets, acts on the target](https://oreil.ly/Tjokv)_ behavior, and asserts expected outcomes. 
ç§ã¯ã€å…¥åŠ›ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’æ•´ç†ã—ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«å¯¾ã—ã¦è¡Œå‹•ã—ã€æœŸå¾…ã•ã‚Œã‚‹çµæœã‚’ä¸»å¼µã™ã‚‹_ [arrange, act, assertãƒ‘ã‚¿ãƒ¼ãƒ³](https://oreil.ly/Tjokv)ã®ä½¿ç”¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

This is the structure we use in the examples here. 
ã“ã‚Œã¯ã€ã“ã“ã§ã®ä¾‹ã§ä½¿ç”¨ã™ã‚‹æ§‹é€ ã§ã™ã€‚

However, how do you know what to test and how to test it? 
ã—ã‹ã—ã€ä½•ã‚’ãƒ†ã‚¹ãƒˆã™ã¹ãã‹ã€ã©ã®ã‚ˆã†ã«ãƒ†ã‚¹ãƒˆã™ã¹ãã‹ã‚’ã©ã†ã‚„ã£ã¦çŸ¥ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ

Testing is not always required for all features. 
ãƒ†ã‚¹ãƒˆã¯ã™ã¹ã¦ã®æ©Ÿèƒ½ã«å¯¾ã—ã¦å¸¸ã«å¿…è¦ãªã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

If the feature is a revenue driver at your company, then you probably should test it thoroughly, but if your feature is experimental, then maybe it requires minimal or no testing for now. 
ãã®æ©Ÿèƒ½ãŒä¼šç¤¾ã®åç›Šã‚’ç”Ÿã‚€ã‚‚ã®ã§ã‚ã‚Œã°ã€å¾¹åº•çš„ã«ãƒ†ã‚¹ãƒˆã™ã‚‹ã¹ãã§ã™ãŒã€æ©Ÿèƒ½ãŒå®Ÿé¨“çš„ã§ã‚ã‚Œã°ã€ä»Šã®ã¨ã“ã‚æœ€å°é™ã®ãƒ†ã‚¹ãƒˆã¾ãŸã¯ãƒ†ã‚¹ãƒˆãªã—ã§æ¸ˆã‚€ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚

That said, our preferred testing methodology for features is a simple recipe: 
ã¨ã¯ã„ãˆã€æ©Ÿèƒ½ã«å¯¾ã™ã‚‹ç§ãŸã¡ã®æ¨å¥¨ã™ã‚‹ãƒ†ã‚¹ãƒˆæ–¹æ³•è«–ã¯ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¬ã‚·ãƒ”ã§ã™ï¼š

1. Write unit tests for all feature and transformation functions (MITs, MDTs, and ODTs) and check your test code coverage (what percentage of the code paths are covered by unit tests). 
1. ã™ã¹ã¦ã®æ©Ÿèƒ½ãŠã‚ˆã³å¤‰æ›é–¢æ•°ï¼ˆMITã€MDTã€ODTï¼‰ã«å¯¾ã—ã¦ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’æ›¸ãã€ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼ˆãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã§ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ã‚‹ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¹ã®å‰²åˆï¼‰ã‚’ç¢ºèªã—ã¾ã™ã€‚

2. Test feature pipelines, training pipelines, and batch inference pipelines with end-to-end tests. 
2. ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€æ©Ÿèƒ½ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ãŠã‚ˆã³ãƒãƒƒãƒæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚

3. Write unit tests for utility functions and other important untested code paths. 
3. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã‚„ä»–ã®é‡è¦ãªæœªãƒ†ã‚¹ãƒˆã®ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¹ã«å¯¾ã—ã¦ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’æ›¸ãã¾ã™ã€‚

This methodology will help get you started, but it is not a panacea. 
ã“ã®æ–¹æ³•è«–ã¯ã€ã‚ãªãŸã‚’å§‹ã‚ã‚‹æ‰‹åŠ©ã‘ã‚’ã—ã¾ã™ãŒã€ä¸‡èƒ½ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

For example, imagine you write a feature to compute monthly aggregations but forget to include code handling the leap year. 
ä¾‹ãˆã°ã€æœˆæ¬¡é›†è¨ˆã‚’è¨ˆç®—ã™ã‚‹æ©Ÿèƒ½ã‚’æ›¸ã„ãŸãŒã€ã†ã‚‹ã†å¹´ã‚’å‡¦ç†ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚ã‚‹ã®ã‚’å¿˜ã‚ŒãŸã¨ã—ã¾ã™ã€‚

With this methodology, you would not see that the leap year code path was not covered in test code coverage. 
ã“ã®æ–¹æ³•è«–ã§ã¯ã€ã†ã‚‹ã†å¹´ã®ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¹ãŒãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ã‚«ãƒãƒ¬ãƒƒã‚¸ã«ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„ã“ã¨ã«æ°—ã¥ã‹ãªã„ã§ã—ã‚‡ã†ã€‚

Only when you first discover the bug will you fix it, and then you should write a unit test to ensure that you donâ€™t have a regression where the leap year bug appears again. 
æœ€åˆã«ãƒã‚°ã‚’ç™ºè¦‹ã—ãŸã¨ãã«ä¿®æ­£ã—ã€ãã®å¾Œã€ã†ã‚‹ã†å¹´ã®ãƒã‚°ãŒå†ç™ºã—ãªã„ã“ã¨ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’æ›¸ãã¹ãã§ã™ã€‚

What will help is testing with more edge cases in your input data and anticipating edge cases. 
å½¹ç«‹ã¤ã®ã¯ã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã§ã‚ˆã‚Šå¤šãã®ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã‚’ãƒ†ã‚¹ãƒˆã—ã€ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã§ã™ã€‚

You should use LLMs to help suggest edge cases for testing. 
ãƒ†ã‚¹ãƒˆã®ãŸã‚ã®ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã‚’ææ¡ˆã™ã‚‹ãŸã‚ã«LLMã‚’ä½¿ç”¨ã™ã¹ãã§ã™ã€‚

Although there are different schools of thought regarding test-driven development, we do not think that test-first development is productive when you are experimenting. 
ãƒ†ã‚¹ãƒˆé§†å‹•é–‹ç™ºã«é–¢ã™ã‚‹ç•°ãªã‚‹è€ƒãˆæ–¹ãŒã‚ã‚‹ã‚‚ã®ã®ã€å®Ÿé¨“ä¸­ã«ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆé–‹ç™ºãŒç”Ÿç”£çš„ã§ã‚ã‚‹ã¨ã¯è€ƒãˆã¦ã„ã¾ã›ã‚“ã€‚

A good way to start is to list out what you want to test. 
å§‹ã‚ã‚‹è‰¯ã„æ–¹æ³•ã¯ã€ãƒ†ã‚¹ãƒˆã—ãŸã„ã“ã¨ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã“ã¨ã§ã™ã€‚

Then decide what you should test offline using pytest and what to test at runtime with data validation checks, A/B tests, and feature/model monitoring. 
æ¬¡ã«ã€pytestã‚’ä½¿ç”¨ã—ã¦ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ãƒ†ã‚¹ãƒˆã™ã¹ãã“ã¨ã¨ã€ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒã‚§ãƒƒã‚¯ã€A/Bãƒ†ã‚¹ãƒˆã€æ©Ÿèƒ½/ãƒ¢ãƒ‡ãƒ«ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã§å®Ÿè¡Œæ™‚ã«ãƒ†ã‚¹ãƒˆã™ã¹ãã“ã¨ã‚’æ±ºå®šã—ã¾ã™ã€‚

###### Summary and Exercises
###### ã¾ã¨ã‚ã¨æ¼”ç¿’

In this chapter, we looked at MDTs and ODTs from both a data science perspective and an engineering perspective. 
ã“ã®ç« ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®è¦–ç‚¹ã¨ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®è¦–ç‚¹ã®ä¸¡æ–¹ã‹ã‚‰MDTã¨ODTã‚’è¦‹ã¦ãã¾ã—ãŸã€‚

We presented why and how you transform both categorical variables and numerical features into numerical representations. 
ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã¨æ•°å€¤ç‰¹å¾´ã‚’æ•°å€¤è¡¨ç¾ã«å¤‰æ›ã™ã‚‹ç†ç”±ã¨æ–¹æ³•ã‚’ç¤ºã—ã¾ã—ãŸã€‚

We looked at different frameworks for implementing MDTs without any skew between training and inference pipelines. 
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®é–“ã«æ­ªã¿ãŒãªã„MDTã‚’å®Ÿè£…ã™ã‚‹ãŸã‚ã®ã•ã¾ã–ã¾ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’è¦‹ã¦ãã¾ã—ãŸã€‚

We introduced pipelines and transformers in Scikit-Learn, which work well with smaller data volumes in NumPy arrays. 
NumPyé…åˆ—ã®å°ã•ãªãƒ‡ãƒ¼ã‚¿ãƒœãƒªãƒ¥ãƒ¼ãƒ ã§ã†ã¾ãæ©Ÿèƒ½ã™ã‚‹Scikit-Learnã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ç´¹ä»‹ã—ã¾ã—ãŸã€‚

We looked at transformation functions in Hopsworks, how they scale to handle large data volumes with Pandas UDFs, and how they can be used to implement both MDTs and ODTs. 
Hopsworksã®å¤‰æ›é–¢æ•°ã€Pandas UDFã‚’ä½¿ç”¨ã—ã¦å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ãƒœãƒªãƒ¥ãƒ¼ãƒ ã‚’å‡¦ç†ã™ã‚‹æ–¹æ³•ã€ãã—ã¦ãã‚Œã‚‰ãŒã©ã®ã‚ˆã†ã«MDTã¨ODTã®ä¸¡æ–¹ã‚’å®Ÿè£…ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã‚‹ã‹ã‚’è¦‹ã¦ãã¾ã—ãŸã€‚

We then looked at how to organize transformations in FTI pipelines using an example PyTorch system. 
æ¬¡ã«ã€ä¾‹ã¨ã—ã¦PyTorchã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¦FTIãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å¤‰æ›ã‚’æ•´ç†ã™ã‚‹æ–¹æ³•ã‚’è¦‹ã¦ãã¾ã—ãŸã€‚

This included writing different MITs, MDTs, and ODTs for images and tensor data. 
ã“ã‚Œã«ã¯ã€ç”»åƒã‚„ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ãŸã‚ã®ã•ã¾ã–ã¾ãªMITã€MDTã€ODTã‚’æ›¸ãã“ã¨ãŒå«ã¾ã‚Œã¾ã™ã€‚

Finally, we concluded with an introduction to pytest and how it can be used to unit-test transformation functions. 
æœ€å¾Œã«ã€pytestã®ç´¹ä»‹ã¨ã€ãã‚ŒãŒå¤‰æ›é–¢æ•°ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã«ã©ã®ã‚ˆã†ã«ä½¿ç”¨ã§ãã‚‹ã‹ã‚’çµè«–ä»˜ã‘ã¾ã—ãŸã€‚

Now that we have covered the MITs, MDTs, and ODTs for creating features, we can look at how we write pipelines to run them. 
æ©Ÿèƒ½ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®MITã€MDTã€ODTã‚’ã‚«ãƒãƒ¼ã—ãŸã®ã§ã€ã“ã‚Œã‚‰ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ›¸ãæ–¹æ³•ã‚’è¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚

The following exercises will help you learn how to design your own MDTs and ODTs: 
ä»¥ä¸‹ã®æ¼”ç¿’ã¯ã€ç‹¬è‡ªã®MDTã¨ODTã‚’è¨­è¨ˆã™ã‚‹æ–¹æ³•ã‚’å­¦ã¶ã®ã«å½¹ç«‹ã¡ã¾ã™ï¼š

- I have a feature I would like to implement that is specific to one model but is quite computationally complex. 
- ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã«ç‰¹æœ‰ã§ã€è¨ˆç®—çš„ã«è¤‡é›‘ãªæ©Ÿèƒ½ã‚’å®Ÿè£…ã—ãŸã„ã¨æ€ã£ã¦ã„ã¾ã™ã€‚

I want to minimize online latency for retrieving or computing it. 
ãã‚Œã‚’å–å¾—ã¾ãŸã¯è¨ˆç®—ã™ã‚‹éš›ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’æœ€å°é™ã«æŠ‘ãˆãŸã„ã§ã™ã€‚

Should I implement it as an MIT, MDT, or ODT? 
ãã‚Œã‚’MITã€MDTã€ã¾ãŸã¯ODTã¨ã—ã¦å®Ÿè£…ã™ã¹ãã§ã™ã‹ï¼Ÿ

- I am building a batch ML system that requires daily retraining and makes daily predictions. 
- æ¯æ—¥å†ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å¿…è¦ã¨ã—ã€æ¯æ—¥äºˆæ¸¬ã‚’è¡Œã†ãƒãƒƒãƒMLã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™ã€‚

Can I implement it as a single monolithic pipeline with MITs or MDTs? 
ãã‚Œã‚’MITã¾ãŸã¯MDTã‚’ä½¿ç”¨ã—ã¦å˜ä¸€ã®ãƒ¢ãƒãƒªã‚·ãƒƒã‚¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ã—ã¦å®Ÿè£…ã§ãã¾ã™ã‹ï¼Ÿ



