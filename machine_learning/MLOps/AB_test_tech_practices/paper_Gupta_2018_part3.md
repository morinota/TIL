# 8. Computation of Experiment Analysis and Metrics

## 8.1. Problem

When 100s of experiments are running simultaneously on millions of users each, having an automated, reliable and efficient way to compute metrics for these experiments at scale is crucial to create a culture where OCEs are the norm. The system to compute experiment results can be viewed as a pipeline. It starts with the product collecting telemetry data points instrumented to measure user response, like clicks on a particular part of the product. The product uploads telemetry to a cloud store. This telemetry is seldom used in raw form for any analysis. Further data processing, commonly called cooking, joins this data with other data logs, like experiment assignment logs, and organizes it in a set of logs in standard format, called a cooked log. Most reporting and experiment analysis occur on top of the cooked log. For running experiments at scale, it is important to have a system for defining metrics of interest on top of these logs and actually computing metrics for each experiment running over millions of users. In addition, the system must support further ad hoc analysis of experiments so that data scientists can try different metrics and methods to find better ways of analyzing results. There are a few key properties of a good system that help in running experiments at scale. Each part of the system must be efficient and fast to scale to 100s of experiments over millions of users each. It must be decentralized so that many people in the organization can configure and use the system to fulfill their needs. It must also have some level of quality control to ensure that the results are trustworthy. Finally, it must be flexible enough to support the diverse needs of feature teams who are constantly working on adding new features and new telemetry, and data scientists working on new metrics and methodologies to extract insights from these experiments. This system forms the core of experimentation analysis for any product. If done well, it empowers feature teams to run 100s of experiments smoothly and get trustworthy insights in an automated and timely manner. It helps them understand if the treatment succeeded or failed in moving the key OEC metric and gives insight into why it happened. These insights are crucial in taking next steps on an experiment: investigating a failure or investing further in successful areas. Conversely, if this system does not have the desired properties mentioned above, it often becomes a bottleneck for scaling experimentation operations and getting value from experiments.

## 8.2. Common Solutions and Challenges

### 8.2.1. Data Management and Schema

The structure and schema of cooked logs affect how data is processed in downstream data pipelines, such as metric definitions and experiment analysis. There is a clear tradeoff between reliability and flexibility. If the rules and constraints are strict, the data will be reliable and can be consumed consistently across different use cases. At the same time, having too strict constraints can slow down the implementation of the logging, and thus decelerate experimentation and product development. Different companies have different ways of solving this issue. At Netflix, there is a single cooked log where each row is a JSON array containing all data collected. JSON structure allows flexibility and extensibility. There is a risk that the log may keep quickly changing. This must be managed by development practices to ensure that key telemetry is not lost due to a code change. A similar approach is used by MSN and Bing at Microsoft. The bring-your-own-data approach is followed at LinkedIn, Airbnb, and Facebook. Each product team is responsible for creating data streams and metrics for each experiment unit every day. These streams follow certain guidelines that enable any experiment to use these streams to compute metrics for that experiment. Products, like Microsoft Office, have an event-view schema, where each event is on a separate row. This format is also extensible with a more structured schema. Another approach followed by some products is to have a fixed-set of key columns required to compute key metrics, and a propertybag column that contains all other information. This allows stability for key columns and flexibility to add new telemetry to the log.

### 8.2.2. Timely and Trustworthy Experiment Analysis

Many companies track hundreds of metrics in experiments to understand the impact of a new feature across multiple business units, and new metrics are added all the time. Computing metrics and providing analysis of an experiment on time is a big challenge for experimentation platforms. As previously mentioned, in many companies, like LinkedIn, Facebook and Airbnb, the metrics framework and experimentation platform are separate, so that each product team or business unit own their metrics and is responsible for them. The experimentation platform is only responsible for the computation of metrics for experiment analysis. In other companies, like Microsoft, Google, Booking.com and Lyft, the metric computation is usually done by the experimentation team right from telemetry or cooked logs. Individual metrics and segments can have data quality issues, delays or be computationally expensive. To resolve these issues, companies segment metrics in various ways. Having ‘tiers’ or metrics so that high-tier metrics are prioritized and thoroughly tested is a way to consume reliable experiment results. Also, if not all metrics have to be pre-computed, experimentation platforms can offer an on-demand calculation of the metrics to save computation resources. Telemetry data from apps may have large delay getting uploaded from a section of devices. It is important to incorporate this latearriving data in experiment analysis to avoid selection bias. Some companies like Facebook leave a placeholder for these metric values and fill it in once enough data arrives. In other companies, like LinkedIn and Microsoft, these metric values are computed with the data received at the time and then recomputed later to update the results. Usually there is a definite waiting period after which the metric value is no longer updated. A few companies put additional steps to ensure that metrics are good quality. Some companies like LinkedIn have a committee to approve adding new metrics or modifying existing metrics to ensure metric quality. At a few companies, the metrics must be tested to ensure that they are sensitive enough to detect a meaningful difference between treatment groups. To save computational resources, the experimentation platform can require a minimum statistical power on the metrics or place metrics in specific formats. Booking.com has an automated process to detect data and metric quality issues which includes having two separate data and metric computation pipelines and process to compare the final results from both [41].

### 8.2.3. Metric ownership

Metrics often have an implicit or explicit owner who cares about the impact on that metric. In a large organization running 100s of experiments every day, scalable solutions ensure that these metric owners know about the experiments that move their metric, and that experiment owners know who to talk with when a particular metric moves. In many cases, it is easy to view the results of any experiment, and metric owners look for experiments that impact their metrics. Team organization structure also helps in this case. If there is a special performance team in the organization, it becomes clear to experiment owners to talk with that team when performance metrics start degrading. Some companies like Microsoft built automated systems for informing both experiment owners and metric owners when large movements are seen in a particular metric. Some teams, like performance teams, may have additional tools to search through multiple experiments to find ones that impact their metrics.

### 8.2.4. Supporting Exploratory and Advanced Experiment Analysis Pipelines

Very often, an experiment requires additional ad hoc analysis that cannot be supported by the regular computation pipeline. It is important that data scientists can easily conduct ad hoc analysis for experiments. Some ad hoc analyses may quickly find application in many more experiments. It is a challenge for experimentation platforms to keep up with supporting new ways of analyzing experiments while maintaining reliability and trustworthiness. While there was no common solution to solving this problem across different companies, there are some common considerations for supporting a new analysis method: • Is the new analysis method reliable and generalizable for all metrics and experiments? • Is the benefit from the new method worth the additional complexity and computation? • Which result should we rely on if the results of the experiment are different between various methods? • How can we share the guideline so that the results are interpreted correctly?

# 9. Dealinig with Client Bloat

## 9.1. Problem

Many experiments are run on client software (e.g., desktop and mobile). In these experiments, a new feature is coded behind a flag switched off by default. During an experiment, the client downloads a configuration, that may turn the feature flag on for that device. As more and more experiments are run over time, the configuration files that need to be sent keep growing larger and increase client bloat. This eventually starts to affect the performance of the client.

## 9.2. Common Solutions

While it may seem that if feature F is successful it will need the flag set to ON forever, that’s not the case if the experimentation system is aware of versions and which versions expect a setting for F. A key observation is that at some point when feature F is successful, it is integrated into the codebase, and from that point on, the configuration of F is NOT needed. Here is a description of this scenario: V10.1: Feature F is in code but not finished. - Default (in code) = Off. - Config: No F V10.2 (experiment): Feature F is done. - Default (in code) = Off - Config: F is on/off at 50/50 If the idea fails, stop sending config for F. If the idea succeeds, Config: F=On. The key observation is that the config system must send F=On for every release that needs F as config by default, 10.2 and higher V10.3 – Other features are evaluated. - Config: F=On, G=On… V10.4 – Code is cleaned. - F=On in code. No need for F in config Config system should stop sending F for V10.4 and higher. Every feature then has [Min version] and after cleanup [Min Version, Max version]. If we assume every release has 100 new features driven by config and 1/3 of these features are successful, the number of configuration features on the server grows at 100/3 ~ 33 per release, but only successful features should be maintained. The number of features sent to the client is bounded by those that must be experimented and those not cleaned. Assuming three releases are needed to experiment and clean, there are 100 features in config for experiments and 100 (33 \* 3 releases) maintained waiting for cleanup. This means that the total configurations are about 200, and that does not grow.

# 10. Network Interactions

## 10.1. Problem

Network interactions are a significant concern in A/B testing. Traditional A/B test assume a stable user treatment value (SUTVA) to accurately analyze the treatment effect. SUTVA implies that the response of an experiment unit (user) is independent of the response of another experiment unit under treatment [73]. A network interaction can occur when a user’s behavior is influenced by another user’s, so that users in the control group are influenced by actions taken by members in the treatment group. As a result, the control group is only a control group in name and no longer reflect outcomes that would be observed if the treatment did not exist. If you ignore network interactions, you get a biased estimate of the treatment effect.

## 10.2. Common Solutions and Challenges

These network interactions are an inherent outcome of the products and scenarios where changes are being tested. There does not seem to be one single method that can mitigate the impact of network interactions on the accuracy of the estimated treatment effect. Here are some common cases and the methods to deal with them.

### 10.2.1. Producer and Consumer Model

At LinkedIn, there is a meaningful producer/consumer distinction between user roles for a feature. For instance, there are producers and consumers of the hashtags feature for the main feed on LinkedIn. In these cases, LinkedIn typically uses two-sided randomization. Two orthogonal experiments are run together: one controlling the production experience and one controlling the consumption experience. For the hashtags example, this implies that the production experiment allows users in treatment to add hashtags to their posts, and the consumption experiment allows users in treatment to see hashtags on their feed. The production experience starts at a low ramp percentage with consumption one at a high percentage, and then gradually ramping the production experience. If we do a simple A/B test lumping both features together, then things go wrong: The producer effect is underestimated because there are too few potential consumers. For our example, if a user in treatment in the production experiment can post hashtags but not everybody can see them, then the user is likely to engage less with the platform. The consumer effect is underestimated because there are too few potential producers. Being able to see hashtags may make users more engaged, but not if too few people (i.e. only treated members) use them. Using two sided randomization helps: when 95% of consumers can see the produced content, then the effect of producers (say at 50% ramp) is more accurate; when 95% of producers are “enabled,” then the consumer test (say 50% ramp) is more accurate. This method may not account for competition effects between producers, in which case we typically use a 95% ramp over 50% ramp if enough power is available. Further, it may not be possible to disentangle consumption from production in a feature. For instance, if a user mentions another user using ‘@ mention’ feature, then the consumer of the feature must be notified about being mentioned.

### 10.2.2. Known Influence Network Model

In many products at LinkedIn and Facebook, the network over which users can influence each other is known. This information is helpful for designing better controlled experiments. LinkedIn typically uses its egoClusters method, creating about 200,000 ego-networks, comprised of an “ego” (the individual whose metrics are measured) and “alters,” who receive treatments but whose metrics are not of interest. Clusters are designed to have egos representative of LinkedIn users and their networks, and treatment is allocated as follows: in all clusters, egos are treated. In “treated” clusters, all alters are treated. In control clusters, all alters remain in control. A simple two-sample t-test between egos of treated clusters and egos of control clusters gives the approximate first-order effect of having all their connections treated versus none. Facebook and Google employ similar cluster based randomization techniques [20, 26]. These designs are the subject of recent academic papers [9].

### 10.2.3. One-to-One Communication

When the feature being tested is one-to-one communication, LinkedIn typically uses model-based approaches when analyzing one-to-one messaging experiments, counting messages explicitly according to four categories: those that stay within the treatment group, those that stay within the control group, and those that cross (one way or the other). The total number of messages of these categories are contrasted with the help of a model and permutation testing to measure the impact of network interactions. At Skype, some experiments related to call quality are randomized at the call level, where each call has an equal probability of being treatment or control. Note that a single user may make multiple calls during the experiment. This approach does not account for within-user effect from a treatment but tends to have much greater statistical power for detecting the treatment effect on the call metrics.

### 10.2.4. Market Effects

In a two-sided marketplace, different users’ behavior is correlated with each other due to a demand-and-supply curve. If we look at a ride service, when a driver is matched to a passenger, it lowers the probability that other drivers in vicinity are matched. Simple randomization of passengers or drivers into Treatment and Control groups causes changes in market conditions, therefore biases the estimated Treatment effect. To reduce the network interactions between users, Lyft conducts cluster sampling by randomizing across spatial regions or time intervals of varying size, ensuring similarity in market conditions between variants. The coarser the experimental units are, the less interference bias persists, although it comes with the cost of increased variance in the estimate [29]. Uber has tried introducing the treatment to a random set of markets and have a synthetic control to predict the counterfactual [1, 34]. Similar market effects also affect online ads. In this hypothetical example, assume that all budget for a set of advertisers is being spent. For the experiment, the treatment increases ad load from these advertisers therefore increasing ad consumption. In this experiment, you would observe that revenue in the treatment group goes up. But the treatment group is stealing budget from the control group, and there will be no increase in revenue when the treatment ships to all users. One way to prevent budget stealing is to split the ad budget of all ad providers in proportion to the percentage of user traffic exposed to the treatment and control groups. While this addresses the problem of budget stealing, it does not help us understand if the treatment will cause an increase in revenue. Higher use of budgets not being entirely spent or an increase in budget from advertisers spending their entire budget may be a better indicator of increase in revenue.

### 10.2.5. Multiple Identities for the Same Person

Similar statistical issues arise when the same user has several accounts or cookies. Instead of spillover occurring from one user to another, it may occur from one account to another, within the same user. A natural level of randomization is user. However, this requires knowing which accounts belong to the same user. If this is unknown or imperfectly known, randomization at the account-level may be the only alternative. Account-level randomization generally tends to suffer from attenuation bias. Studies in Facebook have indicated that cookie level randomization can underestimate person level effects by a factor of 2 or 3 [15]. Attenuation bias is also one of the main pitfalls in running long-term experiments because the chances of within-user spillover increases with time [23].

# 11. Interactions between Multiple Experiments

## 11.1. Problem

If there are non-independent treatment effects in two experiments, then those experiments are said to be interacting: 𝐴𝑇𝐸(𝑇1 ) + 𝐴𝑇𝐸(𝑇2 ) ≠ 𝐴𝑇𝐸(𝑇1𝑇2) A textbook example of interaction between two experiments is where the treatment in the first experiment changes the foreground color to blue and the treatment in the second experiment changes the background color to blue. In this example let us assume that there are positives for each experiment in isolation, but the impact of both treatments is catastrophic. A user who experiences both treatments at the same time sees a blue screen. In products where 100s of experiments run concurrently this can be a serious issue. Ideally you want to prevent contamination where the treatment effect measured in one experiment may become biased because that experiment interacts with another experiment. At the same time, you need to make a joint ship decision for interacting experiments. As in the case of the text book example above, individually both treatments are good ship candidates but jointly you can only ship one.

## 11.2. Common Solutions and Challenges

From our experience, it is rare that two interacting experiments cause enough contamination that it changes the ship decision. Most products are well architected and small teams work independently of most other teams working on different areas of the product. The chances of interaction between two experiments are highest when both experiments are being run by the same sub team who are changing the same part of the product. To prevent interaction between these types of experiments, the Microsoft and Google experimentation platforms have the concept of numberlines or layers [46, 68]. Experiments that run on the same numberline or layer are guaranteed to get an exclusive random sample of the user population, so no user is exposed to two experiments being run concurrently in the same layer or numberline. This limits the number of users who can be part of an experiment. If the first experiment is exposed to half of all users, then the second experiment cannot be exposed to more than remaining half of the user base. Small teams manage a group of numberlines or layers. Based on their understanding of the treatments in different experiments, the teams can decide whether to run the experiments in the same numberline/layer. To detect interactions between two experiments running in two different layers, Microsoft runs a daily job that tests each pair of experiments for additivity of their treatment effects: 𝜇(𝑇1𝐶2 ) − 𝜇(𝐶1𝐶2 ) ≠ 𝜇(𝑇1𝑇2 ) − 𝜇(𝐶1𝑇2 ). It is rare to detect interactions between two experiments as experiment owners already try to isolate experiments that may conflict by running them on the same numberline or layer. To address the problem of joint decision making, you can run both experiments on different numberlines or layers—if we know that the combination of two experiments cannot lead to a catastrophic result. In this case, you can analyze the factorial combination of both experiments to understand the effect of treatment from each experiment individually and the effect of treatments from both experiments.

# 12. Conclusion

This is the first paper that brings together the top practical challenges in running OCEs at scale from thirty-four experts in thirteen different organizations with experience in testing more than one hundred thousand treatments last year alone. These challenges broadly fall into four categories: analysis of experiments, culture and engineering, deviations from traditional A/B tests, and data quality. In Sections 3-5, we discussed the problem that while most experiments run for a short period of time, we want to estimate the long term impact of a treatment and define an overall evaluation criteria (OEC) to make ship decisions for all experiments in a consistent and objective manner while taking into account the heterogenous treatment effects across different product and user segments. In sections 6-9, we discussed the importance of culture and engineering systems in running OCEs at scale. We discussed common challenges and approaches in making OCEs the default method for testing any product change and scaling OCE expertise across the company. We also discussed some common challenges and solutions for computation of experiment analysis and metrics, and client bloat due to configurations from a large number of OCEs. In Sections 10 and 11, we discussed problems and challenges arising from some common deviations from traditional OCEs due to inherent network interactions in different product scenarios and interactions between experiments. There are many more issues of great importance like privacy, fairness and ethics that are handled in each company individually and often form the underlying subtext of the analysis methods and best practices including expert supervision and review described in this paper. We hope to discuss these topics in more detail in future summits/meetups. We hope this paper sparks further research and cooperation in academia and industry on these problems.
