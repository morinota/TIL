# 5. Heterogeniety in Treatment Effects(HTE)

## 5.1. Problem

Without loss of generality, we consider the case that there is only one treatment and one control. Under the potential outcome framework, (𝑌(1), 𝑌(0)) is the potential outcome pairs and 𝜏 = 𝑌(1) − 𝑌(0) is the individual treatment effect. The primary goal of an A/B test is to understand the average treatment effect (ATE), 𝐸(𝜏). Although it is obvious that knowing individual effect is ideal, it is also impossible as we cannot observe the counterfactual. The closest thing is the conditional average treatment effect (CATE) [74], 𝐸(𝜏|𝑋), where 𝑋 is some attribute or side information about each individual that is not affected by the treatment. This makes CATE the best regression prediction of individual treatment effect 𝜏 based on 𝑋. Attributes 𝑋 can be either discrete/categorical or continuous. Categorical 𝑋 segments the whole population into subpopulations, or segments. In practice, the industry almost entirely uses categorical attributes. Even continuous attributes are made discrete and considered ordered categorical segments. Perhaps the most interesting cases are when treatment moves the same metric in different directions, or when the same metric has statistically significant movement in one segment but not in another segment. Assume, for a given segment, say market, a metric moves positively for some markets but negatively for another, both highly statistically significant. Making the same ship decision for all segments would be sub-optimal. Such cases uncover key insights about the differences between segments. Further investigation is needed to understand why the treatment was not appreciated in some markets and identify opportunities for improvement. In some cases adaptive models can be used to fit different treatments on different types of users [6, 52, 53, 77]. However, most common cases of HTE only show difference in magnitude, not direction. Knowledge of these differences can be valuable for detecting outlier segments that may be indicative of bugs affecting a segment, or for encouraging further investment into different segments based on results.

## 5.2. Common Solutions and Challenges

### 5.2.1. Common Segments

It is a very common practice to define key segments based on product and user knowledge. Where possible, it is preferred to define segments so that the treatment does not interact with the segment definition to avoid bias. Here are some of commonly defined segments for many software products and services: 1. Market/country: Market is commonly used by all companies with global presence who are running experiments and shipping features across different markets. When there are too many markets, it is useful to put them into larger categories or buckets like markets already with high penetration and growing markets or markets clustered by language. 2. User activity level: Classifying users based on their activity level into heavy, light and new users can show interesting HTE. It is important to have this classification based on data before the experiment started to avoid any bias. 3. Device and platform: Today most products have both desktop and mobile application. We can test most backend server-side features across devices and platforms. With device and platform fragmentation, it is getting harder to eliminate bugs for all devices and platforms. Using device and platform segments in A/B testing is essential to flag potential bugs using live traffic. For example, in a recent experiment, a feature of the Outlook mobile app was moving key metrics on all Android devices except a few versions, which indicated further investigation was needed. Device and platforms also represent different demographics. Many studies show a difference between iOS users and Android users. 4. Time and day of week: Another common segment used is time. Plotting the effects delta or percent delta by day can show interesting patterns, such as the weekday and weekend effect, reveal a novelty effect [13], and help flag data quality issues. 5. Product specific segments: LinkedIn segmented users by normal user and recruiter. On Twitter, some handles can belong to a single user, so it is useful to segment Twitter handles by primary or secondary account. For Netflix, network speed and device types have proved to be good segments. Airbnb has found that segments of customers based on whether they have booked before and based on from where they first arrived on Airbnb site are useful.

### 5.2.2. Methodology and Computation

Our community recognizes a lot of recent work from both academia and industry. The most common mental model is the linear model with a first-order interaction term between treatment assignment and covariates 𝑋: 𝑌 = 𝜃 + 𝛿𝑇 + 𝛽 × 𝑇 × 𝑋 + 𝜖 . Most useful segments used by the community are categorical, so the linear model suffices. There is consensus that the first-order treatment effect adjustment by a single covariate, such as a segment of one categorical variable, is the most actionable. One active area of research is adapting more MLMs for identifying HTE [74]. Nevertheless, there are a lot of outstanding challenges: 1. Computation scale: Because A/B tests routinely analyze hundreds or thousands of metrics on millions of experiment units (users), the resources and time spent on an automatically scheduled analysis cannot be too much to ensure that results are not delayed and are not too expensive to generate. There is a desire to use a simple algorithm directly formulated using sufficient statistics, instead of using individual-unit level data. 2. Low Signal Noise Ratio (SNR): A/B testing is already dealing with low power to estimate the average treatment effect. Learning HTE is even harder than learning ATE because of the reduced sample sizes in each subpopulation. 3. Multiple Testing Problem [66]: There is a severe multiple testing problem when looking at many metrics, and many possible ways to segment the population. This issue, along with low SNR further complicates HTE estimations. 4. Interpretable and memorable results: Most experimenters are not experts in statistics or machine learning. You must have concise and memorable result summaries to facilitate experimenters to act. 5. Absolute vs. Relative: While determining the HTE, you must decide whether you will use absolute CATE or relative CATE (as a percentage of average value of the metric in control). In many cases it makes sense to use the relative CATE as the baseline or the average value of a control metric can be very different for different segments, like different countries. Use a relative CATE to normalize the treatment effect in different segments. To tackle these challenges, there are common approaches companies take. 1. Separate on-demand and scheduled analysis. For ondemand analysis, people are willing to spend more resources and wait longer to get results. For this kind of one-off analysis, linear regression with sparsity (L1 and elastic net) and tree-based algorithms, like causal tree, are very popular. Double ML also gained a lot of attention recently [14]. 2. Because of the challenge of low SNR and multiple testing, sparse modeling is a must. Even if the ground truth is not sparse, there are limited resources that experimenters can spend on learning and taking actions based on HTE. Sparse modeling forces concise results. 3. To make results memorable, when certain segment has many values, markets might have a lot of values, it is desired to merge those values based on a common effect. For instance, the effect might be different for Asian markets compared to rest of the world. Instead of reporting market HTE and list treatment effect estimates for individual markets, it is better to merge Asian markets and the rest of the world, and report only two different effect estimates. Algorithms that can perform regression and clustering is preferred in these cases, including Fused Lasso [69] and Total Variation Regularization.

### 5.2.3. Correlation is not Causation

Another difficulty in acting based on HTE results is more fundamental: HTE results are not causal, only correlational. HTE is a regression to predict individual treatment effect based on covariates 𝑋. There is no guarantee that predictor 𝑋 explains the root cause of the HTE. In fact, when covariates 𝑋 are correlated, there might be even issues like collinearity. For example, we may find HTE in devices showing iOS users and Android users have different effect. Do we know if device is the reason why the treatment effects are different? Of course not. iOS and Android users are different in many ways. To help experimenters investigate the difference, an HTE model that can adjust the contribution of devices by other factors would be more useful. Historical patterns and knowledge about whether investigating a segment 𝑋 helped to understand HTE of a metric 𝑀 could provide extra side information.

# 6. Developing Experimentation Culture

## 6.1. Problem

Culture is the tacit social order of an organization. It shapes attitudes and behaviors in wide-ranging and durable ways. Cultural norms define what is encouraged, discouraged, accepted, or rejected within a group [35]. There is a big challenge in creating an experiment-driven product development culture in an organization. Cultural change involves transformation of an organization through multiple phases. There may be hubris at first, where every idea of the team is considered a winner. Then there may be introduction of some skepticism as the team begins experimentation and its intuition gets challenged. Finally, a culture develops where there is humility about our value judgement of different ideas, and better understanding of the product and customers [3]. It is well known that our intuition is a poor judge for the value of ideas. Case studies at Microsoft showed a third of all ideas tested through an OCE succeed in showing statistically significant improvements in key metrics of interest, and a third showed statistically significant regressions. Similar results have been noted by many major software companies [3, 17, 28, 47, 56, 60]. Yet it can be hard to subject your idea to an OCE and receive negative feedback, especially when you have spent a lot of time working on implementing it and selling it to your team. This phenomenon is not unique to the software industry. It is generally referred to as Semmelweis Reflex, based on the story of the long and hard transition of mindset among doctors about the importance of hygiene and having clean hands and scrubs before visiting a patient [65]. It takes a while to transition from a point where negative experiment results feel like someone telling you that your baby is ugly. You must enact a paradigm shift to put your customers and business in focus and listen to customer responses. At that point, negative experiment results are celebrated as saving customers and your business from harm. Note that not only bad ideas (including bloodletting [11]) appear as great ideas to a human mind, we are also likely to discount the value of great ideas (including good hand hygiene for doctors [65]). There are cases where an idea that languished in the product backlog for months as no one thought it was valuable turns out to be one of the best ideas for the product in its history [51]. A culture of working together towards the common goal of improving products through OCEs amplifies the benefits of controlled experimentation at scale [32]. This paves the way for frictionless integration of OCEs into the development process, and makes it easy to run an OCE to test an idea, get automated and trustworthy analysis of this experiment quickly, and interpret the results to take the next step: ship the feature, iterate, or discard the idea. A strong experimentation culture ensures that all changes to the product are tested using OCEs and teams benefit from OCEs discovering valuable improvements while not degrading product quality. It allows you to streamline product development discussions so everyone understands the OEC for the product and can take an objective decision to ship a feature based on the impact on the OEC metric. This gives developers freedom to build and test different ideas with minimum viable improvements without having to sell the entire team on the idea beforehand. And allows the team to make future decisions to invest in a product area based on changes to the OEC metric due to features seen in that area.

## 6.2. Common Solutions and Challenges

There are many cultural aspects to adoption of OCEs at scale to have a trustworthy estimate of the impact of every change made to a product.

### 6.2.1. Experimentation Platform and Tools

First, we need to make sure that the experimentation platform has the right set of capabilities to support the team. It must be able to test the hypothesis of interest to the product team. To do that one of the of the most important things required is a set of trustworthy and easily interpretable metrics to evaluate a change made to the product. In addition, it’s useful if there are easy tools to manage multiple experiments and clearly communicate results from these experiments.

### 6.2.2. Practices, Policies and Capabilities

The second aspect deals with creating right set of practices, policies, and capabilities to encourage teams to test every change made to their product using OCEs. The following are strategies that different companies use to achieve this goal. High Touch: Once per quarter, the LinkedIn experimentation team handpicks a few business-critical teams, prioritizes these teams, and then works closely with them on their needs. At the end of the quarter the team agrees they’ll use that experiment platform going forward, and the experimentation team continues to monitor them. Over several years a data-driven culture is built. Managers and directors now rely on development teams running experiments before features launch. The Microsoft experimentation team selects product teams to onboard based on factors indicative of the impact experimentation has on the product. The experimentation team works very closely with product teams over multiple years to advance the adoption of experimentation and its maturity over time. The downside of the High Touch approach is the large overhead in having a deep engagement with every team, and it may become a bottleneck for scaling. Top down buy in: It can help if there is a buy-in into experimentation by leadership and they expect every change tested in a controlled experiment. Further they can set team goals based on moving a metric in controlled experiments. This creates a culture where all ship decisions are talked about in terms of their impact on key metrics. The product teams celebrate shipping changes that improve key metrics, and equally importantly, celebrate not shipping changes that would cause a regression in key metrics. It is important that the team’s key metrics are determined beforehand and agreed upon by the team. It is prudent to be cautious about preventing the gaming of metrics or over fitting metric flaws, where the metrics of interest move but are not indicative of improvement in the product. At Netflix a long-standing culture of peer review of experiment results is organized around frequent “Product Strategy” forums where results are summarized and debated amongst experimenters, product managers, and leadership teams before an experiment is “rolled out”. Negative and positive case studies: Stories about surprising negative results where a feature that is widely acclaimed as a positive causes a large regression in key metrics, or a surprising positive incident where a small change no one believed would be of consequence causes a large improvement in a metric were great drivers for cultural change. These cases drive home a humbling point that our intuition is not a good judge of the value of ideas. There are some documented examples the best OCEs with surprising outcomes [4]. For instance, an engineer at Bing had the idea to make ad titles longer for ads with very short titles. The change was a simple and cheap, but it was not developed for many months as neither the developer nor the team had much confidence in the idea. When it was finally tested, it caused one of the biggest increases in Bing revenue in history [51]. Safe Rollout: It is easier to get a team to adopt experimentation when it fits into their existing processes and makes them better. Some teams at Microsoft and Google began using experimentation as a way to do safe feature rollouts to all users, where an A/B test runs automatically during deployment as the feature is gradually turned on for a portion of users (Treatment) and others (Control) don’t have the feature turned on. During this controlled feature rollout, the feature’s impact estimate on key reliability and userbehavior metrics helped find bugs. This method helps gain a toe hold in the feature team’s development process. Over time, as the feature team started seeing value in experimentation, they looked forward to using experimentation to test more hypotheses. Report cards and Gamification: Microsoft found that they encourage the adoption of OCEs in a set of teams by having a report card for each team that assesses their experimentation maturity level [31]. This report card gives the team a way to think about the potential of using experiments to improve the product. It gives the team a measure of its status and relative status among other teams and helps highlight key areas where they can invest to further improve. Booking.com is experimenting with gamification in their experimentation platform where users of the platform can receive badges to encourage the adoption of good practices. Twitter and Microsoft also use mascots, like duck [70] and HiPPO [37] to spread awareness about experimentation in their companies. Education and support: When a company tests thousands of experiments a year, it is impossible for experimentation teams to monitor each experiment to ensure that experiment analysis is trustworthy. It is important that each team has subject matter experts to help them run experiments and ensure that they obtain reliable and trustworthy results. Educating team members on how to use OCEs to test hypotheses and how to avoid common pitfalls is critical in scaling experimentation adoption. We will discuss this important point in detail in section 7.

# 7. Training Others in the Organisation to scale Experimentation

## 7.1. Problem

While the concept of an A/B test is simple, there can be complex practical issues in designing an experiment to test a particular feature and analyzing the results of the experiment. Product teams need custom support when running experiments, because they often have very specific questions that cannot be answered with a simple set of frequently answered questions. A centralized support function does not scale very well. Central teams end up spending too much time on support and not enough on other things. Additionally, specific product domain knowledge is often required to provide support. A centralized support function requires deep knowledge of all supported products, which is often not feasible. Conversely, anyone providing support needs fundamental experimentation knowledge, which might be easier to scale. Such democratization of knowledge and expertise enables a better experimentation culture.

## 7.2. Common Solutions and Challenges

Across different companies, there are a few key practical challenges in spreading the expertise about OCEs that enable experimentation at scale. • How do we set up a community to support experimenters? • How do we incorporate them in the experiment lifecycle? • How do we incentivize these people? • How do we quantify their impact? • How do we train them? • How do we maintain quality standards? Here are examples from several companies on how they tried to solve these challenges.

### 7.2.1. Yandex: “Experts on Experiment”

At Yandex, a program called “Experts on Experiment” exists to scale support. These Experts are handpicked from product teams by the central experimentation group. Any experiments must be approved by an Expert before they are allowed to ship. Experts are motivated because their product needs approval before shipping, so they voluntarily sign up to be an Expert. Their application is then reviewed by the central experimentation group. Experts are motivated by the status provided by being an Expert. They get a digital badge in internal staff systems, so their status is visible to others. There are no clear KPIs for the program. There is a checklist of minimum experience and an informal interview process involved in becoming an expert.

### 7.2.2. Amazon: “Weblab Bar Raisers”

Weblab is Amazon’s experimentation platform. In 2013, Amazon’s Personalization team piloted a “Weblab Bar Raisers” program in their local organization with the intention of raising the overall quality of experimental design, analysis, and decision making. The initial Bar Raisers were selected to be high-judgment, experienced experimenters, with an ability to teach and influence. Expectations for the role were clearly defined and documented and, after a few iterations, the program was expanded company wide. Bar Raiser review is not mandatory for all organizations; often because not enough Bar Raisers are available. Bar Raisers spend about 2–4 hours per week providing OCE support. Incentives rely on Bar Raisers buying into the mission of the program, which contributes to their personal growth and status within the company. A mentorship program, where existing Bar Raisers train new ones, exists to ensure that new Bar Raisers are brought up to speed quickly.

### 7.2.3. Twitter: “Experiment Shepherds”

At Twitter, the “Experiment Shepherds” program, founded three years ago by a product group including the current CTO, now has approximately 50 shepherds. Most of these are engineers with experience running experiments. There are strict entry requirements. Experiment owners implicitly opt-in for review: either pre-test or pre-launch. Shepherds have on-call duty one week a year to triage incoming requests. Incentives include feelings of responsibility for the product and acknowledgement of contribution during performance review. There are no clear impact KPIs, but qualitatively impact seems to exist. There is a structured training program consisting of one hour of classroom training per week for two months. These classes cover seven topics (e.g. dev cycle, ethics, metrics, stats 101). There are also case study-based discussions.

### 7.2.4. Booking.com: “Experimentation Ambassadors”

At Booking.com, the “Experimentation Ambassadors” program started about six months ago. The central experimentation organization handpicked people (~15) with experimentation experience and interest in providing support in product organizations that seemed to need the most support. Ambassadors form the first line of support with a clear escalation path and priority support from the central organization. Ambassadors are hooked into the central support ticketing system so that they are aware of other open support questions and can pick up tickets as they see fit. They are included in the experimentation organization’s internal communications, to keep them aware of current developments or issues. There is a monthly meeting to discuss product needs and concerns. Incentives for Ambassadors include feeling responsible for the product, getting priority support from the central organization, and acknowledgement on their performance review. There are no clear impact or quality KPIs, but there are plans to include these as the program scales. There is no specific training for Ambassadors, but there is extensive general experiment training for all experimenters, including Ambassadors.

### 7.2.5. Booking.com: “Peer-Review Program”

Booking.com also has a separate “Peer-Review Program” aimed at getting people involved in providing pro-active feedback to experimenters. Anyone in the company can opt-in to the program. Every week participants are paired with a random counterpart. Currently approximately 80 people participate. Each pair picks a random experiment to review. The experiment platform includes a “give me a random experiment” button for this purpose. The platform also supports built-in commenting and threading as part of the reporting interface. Incentives to participate include making new friends, learning new things, and reward badges displayed on the platform interface. There are KPIs defined around reviews and comments. Newcomers are paired with experienced users the first few times to ensure that they are brought up to speed. A one-page guide for writing good reviews is also available [33].

### 7.2.6. Microsoft: Center of Excellence Model

At Microsoft, a data scientist or two from the central experimentation platform team (Analysis & Experimentation) work very closely with a product team. At first, the data scientists from the experimentation platform handle almost all support needs for the product and gain good insight into the product, business, customers, technology, and data. At the same time, the data scientists work on transferring knowledge and expertise to champions in the product team. The expectation is that over time, as more experiments are run, the product team will become more self-sufficient in running trustworthy experiments, and the person from the central experimentation platform team helps with a smaller and smaller percentage of experiments—those that are unique or have issues. The data scientists from the central team and champions from the product team usually conduct further training to educate the entire product team on best practices and processes for running experiments. The experimentation team maintains a monthly scorecard to measure the goals of each product onboarding for running trustworthy experiments at scale. These goals are set at the beginning of every year. Every six weeks, the data scientists and champions review the experimentation operations in the product where successes and failures from the past are highlighted along with a plan to address gaps and opportunities. The incentives for data scientists and champions are partially tied to the success of experimentation in their respective products. The central experimentation team holds a weekly experiment review, where any experiment owner can share their experiment and request feedback from the data scientists. The central experimentation team also conducts a monthly Introduction to Experimentation class and Experiment Analysis lab open to everyone at Microsoft. In addition, twice a year the team hosts a meeting focused on experiments and discusses the best controlled experiments. This provides product teams an opportunity to showcase their strengths in experimentation and learn from other teams.

### 7.2.7. Google: Just-in-time Education Model

Google has used a variety of approaches, but one of the most successful relies heavily on just-in-time education [67]. For example, for experiment design, they have a checklist that asks experimenters a series of questions, ranging from “what is your hypothesis?” to “how will you measure success?” and “how big of a change do you need to detect?” Google has an “experiment council” of experts who review the checklists, and have found consistently that the first time through, an experimenter needs handholding. But on subsequent experiments, less handholding is needed, and the experimenter starts teaching their team members. As they become more experienced, some experimenters can become experts and perform reviews. Some teams have sufficient expertise that they can retire the entire checklist process. For analysis, Google has an experiment review similar to Microsoft. The advantage is both just-in-time education to experimenters about interpreting experiment results and metaanalysis by experts to find the larger patterns.

# 8. Computation of Experiment Analysis and Metrics

## 8.1. Problem

When 100s of experiments are running simultaneously on millions of users each, having an automated, reliable and efficient way to compute metrics for these experiments at scale is crucial to create a culture where OCEs are the norm. The system to compute experiment results can be viewed as a pipeline. It starts with the product collecting telemetry data points instrumented to measure user response, like clicks on a particular part of the product. The product uploads telemetry to a cloud store. This telemetry is seldom used in raw form for any analysis. Further data processing, commonly called cooking, joins this data with other data logs, like experiment assignment logs, and organizes it in a set of logs in standard format, called a cooked log. Most reporting and experiment analysis occur on top of the cooked log. For running experiments at scale, it is important to have a system for defining metrics of interest on top of these logs and actually computing metrics for each experiment running over millions of users. In addition, the system must support further ad hoc analysis of experiments so that data scientists can try different metrics and methods to find better ways of analyzing results. There are a few key properties of a good system that help in running experiments at scale. Each part of the system must be efficient and fast to scale to 100s of experiments over millions of users each. It must be decentralized so that many people in the organization can configure and use the system to fulfill their needs. It must also have some level of quality control to ensure that the results are trustworthy. Finally, it must be flexible enough to support the diverse needs of feature teams who are constantly working on adding new features and new telemetry, and data scientists working on new metrics and methodologies to extract insights from these experiments. This system forms the core of experimentation analysis for any product. If done well, it empowers feature teams to run 100s of experiments smoothly and get trustworthy insights in an automated and timely manner. It helps them understand if the treatment succeeded or failed in moving the key OEC metric and gives insight into why it happened. These insights are crucial in taking next steps on an experiment: investigating a failure or investing further in successful areas. Conversely, if this system does not have the desired properties mentioned above, it often becomes a bottleneck for scaling experimentation operations and getting value from experiments.

## 8.2. Common Solutions and Challenges

### 8.2.1. Data Management and Schema

The structure and schema of cooked logs affect how data is processed in downstream data pipelines, such as metric definitions and experiment analysis. There is a clear tradeoff between reliability and flexibility. If the rules and constraints are strict, the data will be reliable and can be consumed consistently across different use cases. At the same time, having too strict constraints can slow down the implementation of the logging, and thus decelerate experimentation and product development. Different companies have different ways of solving this issue. At Netflix, there is a single cooked log where each row is a JSON array containing all data collected. JSON structure allows flexibility and extensibility. There is a risk that the log may keep quickly changing. This must be managed by development practices to ensure that key telemetry is not lost due to a code change. A similar approach is used by MSN and Bing at Microsoft. The bring-your-own-data approach is followed at LinkedIn, Airbnb, and Facebook. Each product team is responsible for creating data streams and metrics for each experiment unit every day. These streams follow certain guidelines that enable any experiment to use these streams to compute metrics for that experiment. Products, like Microsoft Office, have an event-view schema, where each event is on a separate row. This format is also extensible with a more structured schema. Another approach followed by some products is to have a fixed-set of key columns required to compute key metrics, and a propertybag column that contains all other information. This allows stability for key columns and flexibility to add new telemetry to the log.

### 8.2.2. Timely and Trustworthy Experiment Analysis

Many companies track hundreds of metrics in experiments to understand the impact of a new feature across multiple business units, and new metrics are added all the time. Computing metrics and providing analysis of an experiment on time is a big challenge for experimentation platforms. As previously mentioned, in many companies, like LinkedIn, Facebook and Airbnb, the metrics framework and experimentation platform are separate, so that each product team or business unit own their metrics and is responsible for them. The experimentation platform is only responsible for the computation of metrics for experiment analysis. In other companies, like Microsoft, Google, Booking.com and Lyft, the metric computation is usually done by the experimentation team right from telemetry or cooked logs. Individual metrics and segments can have data quality issues, delays or be computationally expensive. To resolve these issues, companies segment metrics in various ways. Having ‘tiers’ or metrics so that high-tier metrics are prioritized and thoroughly tested is a way to consume reliable experiment results. Also, if not all metrics have to be pre-computed, experimentation platforms can offer an on-demand calculation of the metrics to save computation resources. Telemetry data from apps may have large delay getting uploaded from a section of devices. It is important to incorporate this latearriving data in experiment analysis to avoid selection bias. Some companies like Facebook leave a placeholder for these metric values and fill it in once enough data arrives. In other companies, like LinkedIn and Microsoft, these metric values are computed with the data received at the time and then recomputed later to update the results. Usually there is a definite waiting period after which the metric value is no longer updated. A few companies put additional steps to ensure that metrics are good quality. Some companies like LinkedIn have a committee to approve adding new metrics or modifying existing metrics to ensure metric quality. At a few companies, the metrics must be tested to ensure that they are sensitive enough to detect a meaningful difference between treatment groups. To save computational resources, the experimentation platform can require a minimum statistical power on the metrics or place metrics in specific formats. Booking.com has an automated process to detect data and metric quality issues which includes having two separate data and metric computation pipelines and process to compare the final results from both [41].

### 8.2.3. Metric ownership

Metrics often have an implicit or explicit owner who cares about the impact on that metric. In a large organization running 100s of experiments every day, scalable solutions ensure that these metric owners know about the experiments that move their metric, and that experiment owners know who to talk with when a particular metric moves. In many cases, it is easy to view the results of any experiment, and metric owners look for experiments that impact their metrics. Team organization structure also helps in this case. If there is a special performance team in the organization, it becomes clear to experiment owners to talk with that team when performance metrics start degrading. Some companies like Microsoft built automated systems for informing both experiment owners and metric owners when large movements are seen in a particular metric. Some teams, like performance teams, may have additional tools to search through multiple experiments to find ones that impact their metrics.

### 8.2.4. Supporting Exploratory and Advanced Experiment Analysis Pipelines

Very often, an experiment requires additional ad hoc analysis that cannot be supported by the regular computation pipeline. It is important that data scientists can easily conduct ad hoc analysis for experiments. Some ad hoc analyses may quickly find application in many more experiments. It is a challenge for experimentation platforms to keep up with supporting new ways of analyzing experiments while maintaining reliability and trustworthiness. While there was no common solution to solving this problem across different companies, there are some common considerations for supporting a new analysis method: • Is the new analysis method reliable and generalizable for all metrics and experiments? • Is the benefit from the new method worth the additional complexity and computation? • Which result should we rely on if the results of the experiment are different between various methods? • How can we share the guideline so that the results are interpreted correctly?

# 9. Dealinig with Client Bloat

## 9.1. Problem

Many experiments are run on client software (e.g., desktop and mobile). In these experiments, a new feature is coded behind a flag switched off by default. During an experiment, the client downloads a configuration, that may turn the feature flag on for that device. As more and more experiments are run over time, the configuration files that need to be sent keep growing larger and increase client bloat. This eventually starts to affect the performance of the client.

## 9.2. Common Solutions

While it may seem that if feature F is successful it will need the flag set to ON forever, that’s not the case if the experimentation system is aware of versions and which versions expect a setting for F. A key observation is that at some point when feature F is successful, it is integrated into the codebase, and from that point on, the configuration of F is NOT needed. Here is a description of this scenario: V10.1: Feature F is in code but not finished. - Default (in code) = Off. - Config: No F V10.2 (experiment): Feature F is done. - Default (in code) = Off - Config: F is on/off at 50/50 If the idea fails, stop sending config for F. If the idea succeeds, Config: F=On. The key observation is that the config system must send F=On for every release that needs F as config by default, 10.2 and higher V10.3 – Other features are evaluated. - Config: F=On, G=On… V10.4 – Code is cleaned. - F=On in code. No need for F in config Config system should stop sending F for V10.4 and higher. Every feature then has [Min version] and after cleanup [Min Version, Max version]. If we assume every release has 100 new features driven by config and 1/3 of these features are successful, the number of configuration features on the server grows at 100/3 ~ 33 per release, but only successful features should be maintained. The number of features sent to the client is bounded by those that must be experimented and those not cleaned. Assuming three releases are needed to experiment and clean, there are 100 features in config for experiments and 100 (33 \* 3 releases) maintained waiting for cleanup. This means that the total configurations are about 200, and that does not grow.

# 10. Network Interactions

## 10.1. Problem

Network interactions are a significant concern in A/B testing. Traditional A/B test assume a stable user treatment value (SUTVA) to accurately analyze the treatment effect. SUTVA implies that the response of an experiment unit (user) is independent of the response of another experiment unit under treatment [73]. A network interaction can occur when a user’s behavior is influenced by another user’s, so that users in the control group are influenced by actions taken by members in the treatment group. As a result, the control group is only a control group in name and no longer reflect outcomes that would be observed if the treatment did not exist. If you ignore network interactions, you get a biased estimate of the treatment effect.

## 10.2. Common Solutions and Challenges

These network interactions are an inherent outcome of the products and scenarios where changes are being tested. There does not seem to be one single method that can mitigate the impact of network interactions on the accuracy of the estimated treatment effect. Here are some common cases and the methods to deal with them.

### 10.2.1. Producer and Consumer Model

At LinkedIn, there is a meaningful producer/consumer distinction between user roles for a feature. For instance, there are producers and consumers of the hashtags feature for the main feed on LinkedIn. In these cases, LinkedIn typically uses two-sided randomization. Two orthogonal experiments are run together: one controlling the production experience and one controlling the consumption experience. For the hashtags example, this implies that the production experiment allows users in treatment to add hashtags to their posts, and the consumption experiment allows users in treatment to see hashtags on their feed. The production experience starts at a low ramp percentage with consumption one at a high percentage, and then gradually ramping the production experience. If we do a simple A/B test lumping both features together, then things go wrong: The producer effect is underestimated because there are too few potential consumers. For our example, if a user in treatment in the production experiment can post hashtags but not everybody can see them, then the user is likely to engage less with the platform. The consumer effect is underestimated because there are too few potential producers. Being able to see hashtags may make users more engaged, but not if too few people (i.e. only treated members) use them. Using two sided randomization helps: when 95% of consumers can see the produced content, then the effect of producers (say at 50% ramp) is more accurate; when 95% of producers are “enabled,” then the consumer test (say 50% ramp) is more accurate. This method may not account for competition effects between producers, in which case we typically use a 95% ramp over 50% ramp if enough power is available. Further, it may not be possible to disentangle consumption from production in a feature. For instance, if a user mentions another user using ‘@ mention’ feature, then the consumer of the feature must be notified about being mentioned.

### 10.2.2. Known Influence Network Model

In many products at LinkedIn and Facebook, the network over which users can influence each other is known. This information is helpful for designing better controlled experiments. LinkedIn typically uses its egoClusters method, creating about 200,000 ego-networks, comprised of an “ego” (the individual whose metrics are measured) and “alters,” who receive treatments but whose metrics are not of interest. Clusters are designed to have egos representative of LinkedIn users and their networks, and treatment is allocated as follows: in all clusters, egos are treated. In “treated” clusters, all alters are treated. In control clusters, all alters remain in control. A simple two-sample t-test between egos of treated clusters and egos of control clusters gives the approximate first-order effect of having all their connections treated versus none. Facebook and Google employ similar cluster based randomization techniques [20, 26]. These designs are the subject of recent academic papers [9].

### 10.2.3. One-to-One Communication

When the feature being tested is one-to-one communication, LinkedIn typically uses model-based approaches when analyzing one-to-one messaging experiments, counting messages explicitly according to four categories: those that stay within the treatment group, those that stay within the control group, and those that cross (one way or the other). The total number of messages of these categories are contrasted with the help of a model and permutation testing to measure the impact of network interactions. At Skype, some experiments related to call quality are randomized at the call level, where each call has an equal probability of being treatment or control. Note that a single user may make multiple calls during the experiment. This approach does not account for within-user effect from a treatment but tends to have much greater statistical power for detecting the treatment effect on the call metrics.

### 10.2.4. Market Effects

In a two-sided marketplace, different users’ behavior is correlated with each other due to a demand-and-supply curve. If we look at a ride service, when a driver is matched to a passenger, it lowers the probability that other drivers in vicinity are matched. Simple randomization of passengers or drivers into Treatment and Control groups causes changes in market conditions, therefore biases the estimated Treatment effect. To reduce the network interactions between users, Lyft conducts cluster sampling by randomizing across spatial regions or time intervals of varying size, ensuring similarity in market conditions between variants. The coarser the experimental units are, the less interference bias persists, although it comes with the cost of increased variance in the estimate [29]. Uber has tried introducing the treatment to a random set of markets and have a synthetic control to predict the counterfactual [1, 34]. Similar market effects also affect online ads. In this hypothetical example, assume that all budget for a set of advertisers is being spent. For the experiment, the treatment increases ad load from these advertisers therefore increasing ad consumption. In this experiment, you would observe that revenue in the treatment group goes up. But the treatment group is stealing budget from the control group, and there will be no increase in revenue when the treatment ships to all users. One way to prevent budget stealing is to split the ad budget of all ad providers in proportion to the percentage of user traffic exposed to the treatment and control groups. While this addresses the problem of budget stealing, it does not help us understand if the treatment will cause an increase in revenue. Higher use of budgets not being entirely spent or an increase in budget from advertisers spending their entire budget may be a better indicator of increase in revenue.

### 10.2.5. Multiple Identities for the Same Person

Similar statistical issues arise when the same user has several accounts or cookies. Instead of spillover occurring from one user to another, it may occur from one account to another, within the same user. A natural level of randomization is user. However, this requires knowing which accounts belong to the same user. If this is unknown or imperfectly known, randomization at the account-level may be the only alternative. Account-level randomization generally tends to suffer from attenuation bias. Studies in Facebook have indicated that cookie level randomization can underestimate person level effects by a factor of 2 or 3 [15]. Attenuation bias is also one of the main pitfalls in running long-term experiments because the chances of within-user spillover increases with time [23].

# 11. Interactions between Multiple Experiments

## 11.1. Problem

If there are non-independent treatment effects in two experiments, then those experiments are said to be interacting: 𝐴𝑇𝐸(𝑇1 ) + 𝐴𝑇𝐸(𝑇2 ) ≠ 𝐴𝑇𝐸(𝑇1𝑇2) A textbook example of interaction between two experiments is where the treatment in the first experiment changes the foreground color to blue and the treatment in the second experiment changes the background color to blue. In this example let us assume that there are positives for each experiment in isolation, but the impact of both treatments is catastrophic. A user who experiences both treatments at the same time sees a blue screen. In products where 100s of experiments run concurrently this can be a serious issue. Ideally you want to prevent contamination where the treatment effect measured in one experiment may become biased because that experiment interacts with another experiment. At the same time, you need to make a joint ship decision for interacting experiments. As in the case of the text book example above, individually both treatments are good ship candidates but jointly you can only ship one.

## 11.2. Common Solutions and Challenges

From our experience, it is rare that two interacting experiments cause enough contamination that it changes the ship decision. Most products are well architected and small teams work independently of most other teams working on different areas of the product. The chances of interaction between two experiments are highest when both experiments are being run by the same sub team who are changing the same part of the product. To prevent interaction between these types of experiments, the Microsoft and Google experimentation platforms have the concept of numberlines or layers [46, 68]. Experiments that run on the same numberline or layer are guaranteed to get an exclusive random sample of the user population, so no user is exposed to two experiments being run concurrently in the same layer or numberline. This limits the number of users who can be part of an experiment. If the first experiment is exposed to half of all users, then the second experiment cannot be exposed to more than remaining half of the user base. Small teams manage a group of numberlines or layers. Based on their understanding of the treatments in different experiments, the teams can decide whether to run the experiments in the same numberline/layer. To detect interactions between two experiments running in two different layers, Microsoft runs a daily job that tests each pair of experiments for additivity of their treatment effects: 𝜇(𝑇1𝐶2 ) − 𝜇(𝐶1𝐶2 ) ≠ 𝜇(𝑇1𝑇2 ) − 𝜇(𝐶1𝑇2 ). It is rare to detect interactions between two experiments as experiment owners already try to isolate experiments that may conflict by running them on the same numberline or layer. To address the problem of joint decision making, you can run both experiments on different numberlines or layers—if we know that the combination of two experiments cannot lead to a catastrophic result. In this case, you can analyze the factorial combination of both experiments to understand the effect of treatment from each experiment individually and the effect of treatments from both experiments.

# 12. Conclusion

This is the first paper that brings together the top practical challenges in running OCEs at scale from thirty-four experts in thirteen different organizations with experience in testing more than one hundred thousand treatments last year alone. These challenges broadly fall into four categories: analysis of experiments, culture and engineering, deviations from traditional A/B tests, and data quality. In Sections 3-5, we discussed the problem that while most experiments run for a short period of time, we want to estimate the long term impact of a treatment and define an overall evaluation criteria (OEC) to make ship decisions for all experiments in a consistent and objective manner while taking into account the heterogenous treatment effects across different product and user segments. In sections 6-9, we discussed the importance of culture and engineering systems in running OCEs at scale. We discussed common challenges and approaches in making OCEs the default method for testing any product change and scaling OCE expertise across the company. We also discussed some common challenges and solutions for computation of experiment analysis and metrics, and client bloat due to configurations from a large number of OCEs. In Sections 10 and 11, we discussed problems and challenges arising from some common deviations from traditional OCEs due to inherent network interactions in different product scenarios and interactions between experiments. There are many more issues of great importance like privacy, fairness and ethics that are handled in each company individually and often form the underlying subtext of the analysis methods and best practices including expert supervision and review described in this paper. We hope to discuss these topics in more detail in future summits/meetups. We hope this paper sparks further research and cooperation in academia and industry on these problems.
