# 方策勾配ベースのランク学習にて、公平性や格差の制約を追加するためのアプローチ「Fair-PG-Rank」の論文を読んだ!

## ざっくりどんな論文??

- 「報酬最大化 + 公平性制約」の両方を考慮してランク学習(LTR)するための手法を提案した論文。
- 

## 本論文のモチベーションは??



## 公平性や格差を考慮したランキング方策の学習

- 本論文の主な目標:
  - ユーザの効用（utility）を最大化するだけの従来のランク学習(LTR)じゃなく、
- 例:
  - 男性5人と女性5人の求職者がいて、男性の関連性確率は0.89、女性の関連性確率は0.88だった場合、メリット最大化だけ考えると男性が上位に表示されて**露出の不公平が生じる**。
  - なので、**メリットに基づいた露出配分**を制約として組み込みたい!
- 満たしたい目標3つ:
  - 1つ目: 露出はメリットに比例すること!
  - 2つ目: そのルール(比例の仕方)を明示的に指定可能であること!
  - 3つ目: でもユーザ効用も犠牲にしないように最適化すること!

### ERM(経験リスク最小化)によるランキング方策学習の定式化

基本設定:
- クエリ分布 $Q$ からサンプリングされるクエリ $q$ がある。
- 各クエリ $q$ には、候補ドキュメント集合 $d^q$ があり、各ドキュメント $d_{i} \in d_q$ にはクエリ $q$ への関連性スコア $rel^q_{i}$がある。
- 各ドキュメントは、特徴量ベクトル $x^{q}_{i} = \Psi(q, d^{i}_{q})$ で表現される。

学習対象:
- 学習したいのは、確率的なランキング方策 $\pi$ (ドキュメントのランキング順列 $r$ の確率分布)。
- 目的は、以下の $\pi^{*}$ を見つけること。


$$
\pi^{*} = \arg\max_{\pi} \mathbb{E}_{q \sim Q} [U(\pi|q)]
$$

- 日本語にすると、「確率分布 $p(q) = Q$ に対する効用 $U(\pi|q)$ の期待値を、最大化するようなランキング方策 $\pi$ を見つけたい!」ってこと。
- ここで、効用(Utility)は以下で表される。
  - $U(\pi|q) = \mathbb{E}_{r \sim \pi(r|q)} [\Delta(r, rel^q)]$
  - 日本語にすると、「あるクエリqに対して、確率分布 $\pi(r|q)$ のランキング方策を稼働させた場合の、なんらかの指標 $\Delta$ の期待値」ってこと。
  - (あ、$U(\pi|q)$ ってqの関数なのか...!:thinking:)
- $Delta$ は例えばnDCGとか...!! 任意のランキングのユーザ体験に関わるmetricで良さそう??:thinking:

### 公平性制約の導入

- 上述の効用(Utility) $U(\pi|q)$ の他に、本手法では $D(\pi|q)$ という「不公平さを表す指標」を定義して、以下のような制約付きの最適化問題を解くようにする。
  - (あ、効用Uだけじゃなくて、この不公平指標Dもqの関数なのか...!:thinking:)

$$
\pi^{*} = \arg\max_{\pi} \mathbb{E}_{q \sim Q} [U(\pi|q)] 
\quad \text{s.t.} \quad D(\pi|q) \leq \epsilon
$$

- ラグランジュ乗数 $\lambda$ を導入して、以下のようにトレードオフを考慮した1つの式にまとめられる:

$$
\pi^{*}_{\lambda} = \arg\max_{\pi} \mathbb{E}_{q \sim Q} [U(\pi|q) - \lambda D(\pi|q)]
$$

### 不公平指標 D の定義について

格差測定Dを定式化するために、まず位置バイアスと露出を定義する必要がある。

- Position Bias (位置バイアス) $v_{j}$
  - ランキング内のある位置 $j$ の位置バイアス $v_{j}$ は、「ランキングにアクセスするユーザのうち、位置 $j$ のアイテムを調べる割合」として定義される。
    - (これってこの論文オリジナルの定義??:thinking:)
- Exposure (露出) $Exposure(d_{i}|\pi)$
  - 特定のクエリ $q$ とあるランキング方策 $\pi(r|q)$ におけるドキュメント $d_{i}$ の露出は、「ドキュメントが受ける注意(attention)の期待値」として定義される。
    - (露出は、qと $\pi$ の関数なのか...!:thinking:)
  - 数式で表すと以下。
    - 日本語でいうと「あるクエリ $q$ に対するランキング方策 $\pi$ によるドキュメント $d_i$ の露出は、ドキュメント $d_i$ の表示位置 $r(d_i)$ における位置バイアス $v_{r(d_i)}$ の、ランキングの確率分布に関する期待値」って感じ...!
    - ここで、$r(d_i)$ は、あるランキング $r$ におけるドキュメント $d_i$ の位置を表す。

$$
Exposure(d_{i}|\pi) = v_{\pi}(d_i) = \mathbb{E}_{r \sim \pi(r|q)} [v_{r(d_i)}]
$$

### メリットに基づく露出の割り当て制約について

- 基本的なアイデア:
  - メリットが高いドキュメントは、低いドキュメントと比較してより多くの露出を受けるべき。
  - ちなみに...
    - **ここでいう「メリット」は、各アプリケーションに依って異なる**。
    - 特に本論文では、主に情報検索の文脈なので、あるドキュメントのメリットを、クエリ $q$ に対する関連性の関数として定義してる。(ex. $rel_{i}, rel_{i}^2, \sqrt{rel_{i}}$ など)
- 上記のアイデアに基づく完全な比例制約を数式に表すと以下。
  - 日本語にすると「全てのドキュメント $d_i$ について、露出 $Exposure(d_i|\pi)$ は、その関連性スコア $rel_i$ のメリット $M(rel_i)$ に比例するべき」ってこと。

$$
% 全てのドキュメントにおいて、露出はメリットに比例すべき。
\forall d_i \in d^{q}, Exposure(d_i|\pi) \propto M(rel_i)
$$
  
- しかし、多くの場合、この完全な比例制約は満たせない。
  - 情報検索タスクなどでは、メリットが低いドキュメントも、どうしてもある程度の露出を受ける必要があるから。
- なので、**不等式を使った少し緩めの比例制約**を考える!
  - 日本語にすると「任意のドキュメントのペア $d_i, d_j$ に対して、メリットが大きいドキュメント $d_j$ の露出は、メリットが小さいドキュメント $d_i$ の露出よりも大きいか等しければOK!」ってこと!

$$
\frac{Exposure(d_i|\pi)}{M(rel_i)} \leq \frac{Exposure(d_j|\pi)}{M(rel_j)}
\\
\forall d_i, d_j \in d^{q}, M(rel_i) \geq M(rel_j) > 0
$$

### 個々のドキュメント粒度の公平性(Individual Fairness)

- 前述の緩めの比例制約を満たすようにランキング方策を制御するには、「より高いメリットのアイテムが、そのメリット分を超える露出を得ないようにすること」を強制することが重要!
  - なんでこっちが大事??
    - 逆の方向 (i.e. 「より低いメリットのアイテムが過剰に露出されてしまう」) は、すでに効用最大化(i.e. 累積報酬最大化)を目指すことで防げるので...!!
- なので上記の方向の制約を課したいことを踏まえると、以下のような不公平指標 $D(\pi|q)$ を定義できる。

$$
D_{ind}(\pi|q) = \frac{1}{|H_q|} \sum_{(i,j) \in H_q} \max(0, \frac{Exposure(d_i|\pi)}{M(rel_i)} - \frac{Exposure(d_j|\pi)}{M(rel_j)})
\tag{3}
$$

- ここで、
  - $H_q = \{(i,j) \mid M(rel_i) \geq M(rel_j) > 0\}$。(日本語にすると、$H_q$ は、メリットが高いドキュメントと低いドキュメントの全てのペアの集合! :thinking:)
- 不公平指標 $D_{ind}(\pi|q)$ は常に非負であり、全てのペアについて制約が完全に満たされている場合にのみ0になる!
  - (値が小さい方が公平!!:thinking:)

### グループ粒度の公平性(Group Fairness)

- 前述の不公平指標 $D_{ind}(\pi|q)$ は、個々のドキュメント粒度の評価だったが、**アプリケーションによってはグループ粒度の公平性を考慮・保証したかったりする!**
  - ex. (まあ確かに。動画ランキングの文脈では、各チャンネルやクリエイター単位の公正性を考えることも多そう...!:thinking:)
- なので、グループ粒度の不公平指標も定義しよう!

$$
D_{group}(\pi|q) = \max(0, \frac{Exposure(G_i|\pi)}{M_{G_i}} - \frac{Exposure(G_j|\pi)}{M_{G_j}})
\tag{4}
$$

- ここで...
  - $G_i$ と $G_j$ は $M_{G_i} \geq M_{G_j}$ である。(i.e. グループ $G_i$ のメリット 大なり等しい グループ $G_j$ のメリット...! :thinking:)
  - $Exposure(G|\pi) = v_{\pi}(G) = \frac{1}{|G|} \sum_{d_i \in G} v_{\pi}(d_i)$ はあるグループ $G$ の平均露出を表す。(i.e. グループ $G$ 内の全ドキュメントの露出の平均...!:thinking:)
  - グループ $G$ のメリットは $M_G = \frac{1}{|G|} \sum_{d_i \in G} M_i$ で表される。(i.e. グループ $G$ 内の全ドキュメントのメリットの平均...!:thinking:)

## 効用最大化 + 公平性維持のランク学習のための方策学習アルゴリズム「Fair-PG-Rank」について

- 前セクションで「効用最大化 + 公平性制約」の指標を定義した。じゃあ**その指標を元にランキング方策を実際にどうやって学習(最適化)するか**、が本セクションの話！
  - 具体的には、方策空間 $\Pi$ の中から、効用を最大化しつつ露出の公平性を担保するような最適な方策 $\pi^{*}$ を学習して見つけたい。

### まずランキング方策の確率分布をどう定式化する?: Plackett-Luceモデルの活用

最初のステップとして、ランキング方策(クエリqを条件づけてランキングrを選択する確率分布関数)を定式化する。
本論文内で定義するランキング方策 $\pi$ は、以下の2つのコンポーネントで構成される:

- **スコアリングモデル**: 
  - ランキングの元となる、各ドキュメントのスコアを計算する。
- **サンプリング手法**: 
  - スコアリングモデルから得られたスコアを元に、ランキングをサンプリングする。
- (あ、やっぱり2段階にはなるのか...一気にランキング選択分布を作っちゃうようなアプローチはあまり現実の運用に耐えられないのかな:thinking:)

スコアリングモデル $h_{\theta}$ について:

- 学習可能なパラメータ $\theta$ を持つ任意の微分可能なMLモデル (ex. 線形モデル、ニューラルネットワークなど) を使う。
- 入力:
  - 候補セットの全てのクエリ-ドキュメントペアの特徴量 $x^{q} = (x^{q}_{1}, x^{q}_{2}, \ldots, x^{q}_{|d^q|})$ 
    - ここで、$x^{q}_{i} = \Psi(q, d^{i}_{q})$ はクエリ $q$ とドキュメント $d^{i}_{q}$ に関する特徴量ベクトル。
- 出力:
  - スコアのベクトル $h_{\theta}(x^{q}) = (h_{\theta}(x^{q}_{1}), h_{\theta}(x^{q}_{2}), \ldots, h_{\theta}(x^{q}_{|d^q|}))$ 
  - このスコアは、各ドキュメントのクエリに対する関連性を表す。

サンプリング手法:

- スコアリングモデルから得られたスコアベクトルに基づいて、Plackett-Luceモデルを用いて各ランキングの選択確率分布を定式化する。(確率質量関数...!:thinking:)
- 具体的にはソフトマックス関数を用いて以下のように定義される。

$$
\pi_{\theta}(r|q) = \prod_{i=1}^{n_q} \frac{\exp(h_{\theta}(x^q_{r(i)}))}{\sum_{j=i}^{n_q} \exp(h_{\theta}(x^q_{r(j)}))}
\tag{5}
$$

- 上記のランキング選択確率分布は効率的に計算できる。
- 注目すべき点: **スコアリングモデル $h_{\theta}$ が微分可能である限り、$\pi_{\theta}(r|q)$ および $\log \pi_{\theta}(r|q)$ の導関数は必ず存在する**。
  - ↑が意味することって...**勾配ベースの学習アプローチの目的関数に組み込める**ってこと...??:thinking:
- 実際にサンプリングしてランキングを得る方法
  - ランキングの1番目(先頭)から初めて、ドキュメントのスコアに対するソフトマックスから得られる確率分布からドキュメントを再帰的に引き出し**、セットが空になるまで(or ランキングの長さに到達するまで)続ける。
    - (うんうん、これぞプラケットルースモデルだ...!!:thinking:)

### 次に　ランキング方策をどう最適化する?: 方策勾配を用いた最適化

次のステップとして、ランキング方策 $\pi_{\theta}$ を、前述の「効用最大化 + 公平性制約」の目的関数を最大化するようなパラメータ $\theta$ を見つける方法を考える。

(「反実仮想機械学習」的な観点から見ると、ざっくり**方策勾配のAVG推定量を定義して、勾配ベースのアプローチ**を使ってる、みたいな感じでした！:thinking:)

以下はざっくり最終的な方策勾配の導出の流れ！

---

①まず最初に、ある方策性能を定義! 1個目は効用(Utility)に関するもの。2個目は不公平指標(Diversity)に関するもの。

$$
U(\pi_{\theta}|q) = \mathbb{E}_{\pi_{\theta}(r|q)} [\Delta(r, rel^q)]
$$

$$
D(\pi_{\theta}|q) = \mathbb{E}_{\pi_{\theta}(r|q)} [D(r, rel^q)]
$$

②次に、上記の方策性能の $\theta$ に関する勾配を定義! とりあえず効用の方！

(なんか、「ログ導関数トリック(log-derivative trick)」って呼ばれる式変形を使うと以下のようにある!:thinking:)

$$
\nabla_{\theta} U(\pi_{\theta}|q) = \nabla_{\theta} E_{\pi_{\theta}(r|q)} [\Delta(r, rel[q])] 
\\
= E_{\pi_{\theta}(r|q)} [\nabla_{\theta} \log \pi_{\theta}(r|q) \Delta(r, rel[q])]
$$

なお、最終的な期待値の値は、式5のPlackett-Luce モデルからのモンテカルロサンプリングを介して近似される。
(あ、てっきり方策勾配の推定値を得るために、クローズドフォームの推定量を定義するのかと思ったけど、乱数生成を元に近似値を得るのか! まあ経験平均っぽい感じだと思うからAVG推定量って思っていいのでは!:thinking:)

③次に、不公平指標の方の $\theta$ に関する勾配も定義する!

$$
\nabla_{\theta} D(\pi_{\theta}|q) = \nabla_{\theta} E_{\pi_{\theta}(r|q)} [D(r, rel[q])]
\\
= E_{\pi_{\theta}(r|q)} [\nabla_{\theta} \log \pi_{\theta}(r|q) D(r, rel[q])]
$$

これも効用の項と同様に、同じモンテカルロアプローチで方策勾配の期待値の近似値を得る。
具体的には、以下の式が方策勾配の推定量になる。

$$
\nabla_{\theta} D_{ind} = \sum_{(i,j) \in H_1} v_r(d_i) - v_r(d_j) \times E_{r \sim \pi_{\theta}(r|q)} [v_r(d_i) - v_r(d_j) \nabla_{\theta} \log \pi_{\theta}(r|q)]
\\
(H = {(i, j) s.t. Mi ≥ _Mj})
$$

こっちはグループ粒度の不公平指標 $D_{group}$ の方!

$$
\nabla_{\theta} D_{group}(\pi|G_0, G_1, q) = \nabla_{\theta} \max(0, \xi_q \text{diff}(\pi|q)) = 1_{\xi_q \text{diff}(\pi|q) > 0} \xi_q \nabla_{\theta} \text{diff}(\pi|q)
$$












