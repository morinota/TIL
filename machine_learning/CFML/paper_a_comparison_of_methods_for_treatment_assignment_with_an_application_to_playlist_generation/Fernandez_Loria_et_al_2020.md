## link

- https://arxiv.org/abs/2004.11532

## title

A Comparison of Methods for Treatment Assignment with an Application to Playlist Generation

[Submitted on 24 Apr 2020 (v1), last revised 30 Apr 2022 (this version, v5)]

## Abstract

This study presents a systematic comparison of methods for individual treatment assignment, a general problem that arises in many applications and has received significant attention from economists, computer scientists, and social scientists. We group the various methods proposed in the literature into three general classes of algorithms (or metalearners): learning models to predict outcomes (the O-learner), learning models to predict causal effects (the E-learner), and learning models to predict optimal treatment assignments (the A-learner). We compare the metalearners in terms of (1) their level of generality and (2) the objective function they use to learn models from data; we then discuss the implications that these characteristics have for modeling and decision making. Notably, we demonstrate analytically and empirically that optimizing for the prediction of outcomes or causal effects is not the same as optimizing for treatment assignments, suggesting that in general the A-learner should lead to better treatment assignments than the other metalearners. We demonstrate the practical implications of our findings in the context of choosing, for each user, the best algorithm for playlist generation in order to optimize engagement. This is the first comparison of the three different metalearners on a real-world application at scale (based on more than half a billion individual treatment assignments). In addition to supporting our analytical findings, the results show how large A/B tests can provide substantial value for learning treatment assignment policies, rather than simply choosing the variant that performs best on average.

# Introduction

Systems that make automated decisions are often deployed with the underlying goal of improving (rather than just predicting) outcomes. For example, when targeting online ads or retention incentives, the goal is often to encourage customers to make a purchase or stay with the company rather than just predicting their behavior. This type of task is more generally known as a treatment assignment problem (Manski 2004), where each possible course of action corresponds to a different ‘treatment’ (e.g., ‘show ad’ vs ‘do not show ad’), and ideally each individual is assigned to the treatment associated with the most beneficial outcome (e.g., the one with the highest profit). Treatment assignment policies may be estimated from data using statistical modeling, allowing decision makers to map individuals to the best treatment according to their characteristics (e.g., preferences, behaviors, history with the firm). However, there are different ways in which one could proceed, and various methods for the estimation of treatment assignment policies from sample data have been proposed across different fields, including econometrics (Manski 2004), data mining (Lo 2002), and multi-armed bandits (Beygelzimer and Langford 2009). This paper gathers these various methods into three general classes of algorithms (or metalearners) for individualized treatment assignment, all of which have been proposed in the literature. Table 1 describes these metalearners in terms of their estimand (i.e., the model that each metalearner intends to estimate) and their resulting treatment assignment policy.1 The first metalearner (Outcome Learner, O-learner) learns a model that predicts the expected outcome (Y ) given individual-level characteristics (X = x) and a specific treatment assignment (T = j). Each individual is then assigned to the treatment with the best (e.g., largest) predicted outcome. The second metalearner (Effect Learner, E-learner) learns a model for heterogeneous causal effect estimation and assigns each individual to the treatment with the largest predicted causal effect. Finally, the third metalearner (Assignment Learner, A-learner) directly learns and assigns individuals to the treatments that are expected to have the best outcome.

At a first glance, the policies estimated by the metalearners in Table 1 may look the same: they all seek to assign the treatment with the best outcome. As a main contribution, we reveal two key distinctions between these learning approaches and the implications that these distinctions have for personalized treatment assignment. The first distinction between metalearners is the level of generality of the tasks that the machinelearned models can perform. For instance, models that predict outcomes (ˆµ in Table 1) may be used to estimate causal effects (ˆτ in Table 1), whereas models that predict causal effects generally cannot predict outcomes. Thus, a more general metalearner may be preferable when the machinelearned models are important for other reasons besides identifying the treatment assignment with the largest outcome, such as when there are decision-making constraints that depend on predicted outcomes or causal effects. The second distinction is in the objective function that each metalearner optimizes when learning models. The first metalearner optimizes for outcome prediction, the second for causal effect prediction, and the third for optimal action (treatment assignment) prediction. It is clear from Table 1 that when models are highly accurate (i.e., ˆµ = µ, ˆτ = τ , and ˆa ∗ = a ∗ ), then all the metalearners produce the same policy. In practice, however, machine-learned models are derived from sample data, so each metalearner may lead to a different treatment assignment policy depending on how its objective function frames the prediction problem. Importantly, and as we discuss in detail in this paper, optimizing models to predict outcomes or causal effects is not the same as optimizing models to predict treatment assignments. We illustrate this with an example in Figure 1, which compares the outcome predictions made by two different models for a single individual. One model has high prediction errors (Figure 1a) and the other has low prediction errors (Figure 1b). The blue (dark) dots correspond to the true conditional expectations (they are the same for both figures), whereas the red dots correspond to the predictions. A larger distance between the blue and the red dots (represented by dashed lines) implies that the model makes worse outcome predictions. In this example, the conditional expectation when T = 1 is larger than when T = 2 (as shown by the blue dots), which implies that T = 1 is a better treatment assignment. Therefore, models make the optimal assignment when ˆµ(x, 1) > µˆ(x, 2). Figure 1a shows that the model with larger prediction errors actually makes the optimal treatment assignment because the rank ordering of the predicted outcomes across treatments is the same as the rank ordering of the true values. On the other hand, the second model depicted in Figure 1b makes a worse assignment, even though its prediction errors are smaller, because the ordering is inverted. Importantly, this can also occur when fitting models for causal effect prediction. Therefore, since better outcome or causal effect prediction can lead to worse treatment assignments, learning models that predict optimal assignments (i.e., using an A-learner) should in principle lead to better assignments than the other two metalearners. As a second main contribution, we empirically assess the practical impact that this analytical finding can have on treatment assignment performance by conducting a massive-scale, experimental comparison of the three metalearners in the context of content selection at Spotify. Our focal application is choosing, for each listener, which playlist generation algorithm (treatment) to apply in order to maximize the number of songs streamed. To our knowledge, this is the first real-world, at-scale comparison of the three metalearners. The experiment shows several things. First, it supports the analytical finding that models specifically trained to predict the best treatment are best for treatment assignment (i.e., the A-learner outperforms the O-learner and the E-learner for treatment assignment). This is the case even with training data consisting of more than half a billion observations, a surprising finding given that, in theory, all metalearners should converge to the same treatment assignment policy with large enough data. This finding also implies that we should reconsider some of the justifications made in prior research for using causal effect prediction methods for personalized treatment assignment. The experiment also reveals interesting findings for the application to content selection in music streaming. Specifically, it shows that (1) using an algorithm assignment policy can substantially improve total streaming compared to the typical approach of applying the same playlist generation algorithm to everyone, and that (2) larger data sets lead to significantly better policies, illustrating the advantages of conducting massive-scale A/B tests for the purpose of learning treatment assignment policies (rather than just for evaluation or for selecting the best variant).

# Treatment assignment problem

Treatment assignment problems correspond to settings where a decision-maker wants to maximize the overall causal effect of decisions on an outcome of interest (e.g., deciding what playlist generation algorithm to use for each listener to maximize the number of streams). Each possible alternative corresponds to a different treatment, and the goal is to assign individuals to the treatment that maximizes their outcome. We formalize the problem in this section. We consider settings in which decisions are independent and the treatment assignment policy is learned from historical data on previous decisions made at random. This implies that each decision affects a single unit (or instance) and there is no selection bias in the data. In the causal inference literature, the first assumption is also known as the Stable Unit Treatment Value Assumption (SUTVA) (Cox 1958). The second assumption is known as unconfoundedness and implies there are no systematic differences between users assigned to different treatments. Unconfoundedness also goes by other names in different fields, including ignorability (Rosenbaum and Rubin 1983), the back-door criterion (Pearl 2009), and exogeneity (Wooldridge 2015). Practically speaking, these assumptions hold when we use a carefully designed randomized A/B test to gather data. Let T be the treatment assignment variable and Y be the observed outcome. We use potential outcomes to frame causality (Rubin 1974) and define Y (j) as the outcome we would observe if we were to assign treatment j (out of k possible treatment alternatives), so that Y = Y (j) if T = j. Then, the treatment assignment that would lead to the best outcome on average is:

$$
\tag{1}
$$

and can be estimated using the sample mean (Eˆ) for each treatment:

$$
\tag{2}
$$

Equation 2 describes a standard A/B test approach that compares multiple treatments across a predefined population. This approach, however, does not address individualized treatment assignments. In our setting, we learn optimal treatments for individuals or subpopulations. Suppose individuals vary with respect to a set of variables (features) X. We can then think of a feature vector x as a subpopulation where X = x and formulate the optimal assignment (given x) as:

$$
\tag{3}
$$

Without the argmax, the right-hand side is essentially the formulation of a predictive model. Applying statistical modeling frees us from specifying in advance what are the particular subpopulations of interest. In Section 3, we present three metalearners that can be used to estimate a ∗ (x) from data, each producing a treatment assignment policy ˆa(x). Treatment assignment policies can be evaluated in terms of their ability to minimize the expected difference between the outcome when optimal assignments are made, Y (a ∗ (X)), and the outcome when the policy is deployed, Y (ˆa(X)). This evaluation measure is also known as expected regret (or just ‘regret’) in decision theory:

$$
\tag{4}
$$

and minimizing regret is the same as maximizing the expected outcome of deploying the policy:

$$
\tag{5}
$$

However, evaluating treatment assignment policies using historical data (as is typical when building standard machine learning models) is challenging because we do not observe all potential outcomes for any given individual; we only observe the single potential outcome for the treatment actually given. Therefore, if (for any given individual) the policy assigns a different treatment from the one that was assigned in the historical data, we do not know the corresponding potential outcome. Fortunately, given a data set of n individuals from a randomized A/B test, we can still obtain an unbiased estimate of Equation 5 (Li et al. 2010):

$$
\tag{6}
$$

where for each individual i, xi is the feature vector, ti is the assigned treatment in the data, yi is the observed outcome, and P(T = ti) is the probability of being assigned to treatment ti in the data (a known quantity if the data was collected through a randomized A/B test). We present a simplified proof of this result in Appendix A (see Li et al. (2010) for a detailed proof).

# Metalearners

The causal inference literature often focuses its attention on the estimation of aggregate causal effects, such as the so-called average treatment (or causal) effect (ATE), which corresponds to the average effect of a treatment across the individuals in some well-defined population. However, estimating the ATE does not help us to target different individuals with different treatments, because it does not discriminate between the individuals in the population at all. A fundamental motivation behind this work is that the population exhibits heterogeneous treatment effects (HTEs), which are defined in terms of the degree to which a treatment may have different effects on different individuals (Imai et al. 2013). One can account for HTEs through the estimation of conditional average treatment effects (CATEs), which correspond to the average causal effect conditioned on a set of available features. Thus, to the extent that individuals in the population differ on their features (and those features are related to causal effects), we may estimate different causal effects for each individual. Of course, treatment effects may still vary among individuals that share the same features (since we may not be accounting for all aspects related to the causal effect). Nevertheless, the estimation of HTEs by using CATEs allows us to make different interventions for different individuals without knowing the relevant subpopulations in advance. As we discuss in more detail in Section 6.1, the ideas behind CATE estimation have been fundamental to the development of methods for learning treatment assignment policies from data. We group such methods into three classes of algorithms (or metalearners), described below and in Table 1. This grouping is meant to highlight the perspective that outcome prediction, causal effect prediction, and treatment assignment are different tasks, which has important implications for modeling and the use of predictive models for decision making.

- 1. Outcome Learner (O-learner): Standard machine learning is used to learn a model that is optimized to predict the outcome under each treatment. The policy assigns individuals to the treatment with the largest predicted outcome.
- 2. Effect Learner (E-learner): a machine learning method specifically designed to estimate CATEs is used to learn a model optimized to predict the differences between the outcome of each treatment and a baseline (or control). The policy assigns individuals to the treatment with the largest predicted difference (i.e., treatment effect).
- 3. Assignment Learner (A-learner): The problem of estimating optimal treatment assignments can be transformed into a weighted classification problem wherein each treatment corresponds to a class and the optimal classifier corresponds to the optimal treatment assignment policy (Zadrozny 2003, Zhang et al. 2012). Weights are often defined using the observed outcome under each treatment condition, so that weights are larger for treatments with larger outcomes. Thus, standard machine learning can be used to learn a weighted classification model optimized to predict the treatment with the largest weight. The resulting classifier can then be used to assign individuals to the class (treatment) that is predicted to have the largest weight (outcome).

Section 6.1 discusses multiple studies that have used or recommended specific instances of these metalearners for treatment assignment. Importantly, the three metalearners converge to optimal treatment assignments with large enough samples, assuming the machine learning procedure that is used to learn the models is a consistent estimator of the estimands presented in Table 1. However, there are two key differences between the metalearners that are critical in practice, as summarized in Table 2 and discussed next.

## Distinction 1: Level of generality

The first key distinction between the three metalearners is their level of generality (the metalearners are listed above from the most general to the least general). O-learners are the most general of the metalearners because they produce models that predict outcomes, and such models may also be used to predict causal effects or optimal treatments. Specifically, causal effect predictions may be obtained by taking the difference between the predicted outcomes of two treatments under consideration, and optimal-treatment predictions may be obtained by selecting the treatment with the largest predicted outcome. Therefore, O-learners may be used for all three different purposes. Models that predict causal effects cannot be used to predict outcomes. For such models, predictions estimate the expected marginal change in the outcome that results from assigning some specific treatment, but the predictions cannot be used to estimate expected outcomes under an arbitrary treatment condition. Therefore, while causal effect predictions may still be used to predict optimal treatments (by selecting the treatment with the largest predicted effect), E-learners are not as general as O-learners. Finally, models trained to predict optimal treatment assignments (i.e., learned with an A-learner) can only be used for that purpose. These models, the least general, cannot predict the outcome or the effect that would result from making those assignments. This distinction implies that more general metalearners (O-learners and E-learners), can be preferable over A-learners when outcome and causal effect predictions are important for other reasons besides their usefulness to determine the treatment with the most beneficial outcome. For example, McFowland et al. (2021) consider treatment assignment settings where there are budget constraints and the decision maker faces costs that are unknown ex-ante. In such settings, quantifying the benefit of each individual decision (e.g., via causal effect prediction) and the cost of each possible course of action (e.g., via outcome prediction) is important to allocate resources in the most profitable way. Feasible treatment assignment rules can also be constrained for ethical, legislative, or political reasons. For example, a public policy maker may want to prioritize the assignment of subsidies to individuals in some protected class unless the predicted effect of the subsidy on annual income is below a certain threshold or the individual is predicted to already have an annual income above certain threshold. Since assessing whether an individual meets these two conditions would require causal effect and outcome predictions, implementing an A-learner in this type of settings may be counterproductive or infeasible. A-learner decision rules may also be more difficult to implement in settings where the models are intended to support (rather than automate) human decision making. For example, for economic policy and medical treatment assignment, decision makers may need to weigh the potential benefit of the treatment alternatives with respect to some other information not available to the model (e.g., how the individuals affected by the treatments feel about the treatment alternatives), so predicting the treatment with the “most beneficial outcome” may not suffice. Additionally, information about causal effects and outcomes can be important for other reasons beyond decision making (e.g., for users to trust the model, to debug the model, to develop more effective treatments in the future). Nonetheless, in settings where outcomes and effects are relevant only for the model to assign individuals to the most beneficial treatment, we should expect A-learners to make better treatment assignments because they are specifically designed for treatment assignment; we elaborate on this premise in detail in the rest of the paper.

## Distinction 2: Learning procedure

The second key distinction is that each metalearner uses a different learning objective (or loss function) for the machine learning. O-learners use a loss function designed to optimize outcome predictions; E-learners use a loss function designed to optimize causal effect predictions, and Alearners use a loss function designed to optimize treatment assignments. This implies that, while all metalearners share the same ultimate goal (optimizing treatment assignments as specified by Equations 4, 5, and 6), they differ with respect to the procedures they use to learn from data. This distinction is important because an improvement in the prediction of outcomes or causal effects does not imply an improvement in treatment assignment (as previously shown in Figure 1). In fact, the improvements may occur at the expense of worse treatment assignments! Thus, we should expect machine learning with loss functions specifically tailored to optimize treatment assignments to produce better models: the A-learner should outperform the other metalearners with finite training data when making treatment assignment decisions. Nevertheless, O-learners and E-learners are much more commonly used among scholars and practitioners in marketing and information systems (IS), even though those metalearners are optimized to minimize prediction errors in outcomes or causal effects rather than decision making errors. One goal of this study is to encourage a more widespread consideration, study, and use of A-learners among management researchers by showing how decisions can be substantially improved when machine learning models are directly optimized for treatment assignment (decision making).

# Choice of objective function

In this section, we compare the three metalearners analytically to illustrate how their choice of objective function may affect their performance in treatment assignments.

## Outcome prediction

As mentioned, the O-learner assigns treatments by learning one or more models that predict the expected outcome of each treatment (ˆµ):

$$
\tag{7}
$$

and then selecting the treatment with the best predicted outcome:2

$$
\tag{8}
$$

A standard approach to fit Equation 7 is to regress outcome Y on features X and treatment assignment T using various machine learning methods designed to minimize the mean squared error for the outcome (MSEµ):

$$
\tag{9}
$$

and then to choose the model(s) with the lowest MSEµ. The premise here is that minimizing MSEµ implies better outcome predictions, and therefore better treatment assignments. However, optimizing for outcome prediction (by minimizing MSEµ or other measures such as mean absolute error or cross-entropy) does not necessarily optimize for treatment assignment. Going back to the earlier example in the introduction, Figure 1a shows that the model with larger prediction errors makes the optimal treatment assignment because the rank ordering of the predicted outcomes is the same as the rank ordering of the true values. The second model makes a worse assignment, even though its prediction errors are smaller, because the ordering is inverted. Therefore, choosing the model with the lower (and thus better) MSEµ leads to a worse treatment assignment. Multiple researchers have noted the potential of overfitting when multiple outcome models are used to estimate treatment effects instead of directly fitting a causal effect model (K¨unzel et al. 2019, Nie and Wager 2017). For example, suppose that features X1 and X2 are predictive of outcomes, but only feature X1 is predictive of effects. This implies that, for the purposes of estimating effects and assigning treatments, segmenting individuals using exclusively X1 is more statistically efficient than segmenting them according to X1 and X2. Hence, by focusing statistical power on features that are predictive of effects, models optimized for treatment effect estimation can achieve lower bias and lower variance than models optimized for outcome prediction. Subsequently, many researchers have proposed methods that directly model effects (rather than outcomes) to make treatment assignments; these are instances of the E-learner. However, as we discuss next, optimizing for causal effects is not the same as optimizing for treatment assignments either.

## Causal effect prediction

The second metalearner, the E-learner, consists of learning one or more models to estimate the CATEs (ˆτ ) directly:

$$
\tag{10}
$$

where T = 0 corresponds to a baseline treatment (e.g., the control in an A/B test setting). The optimal treatment may then be chosen as follows:

$$
\tag{11}
$$

The (sometimes unstated) goal of machine learning methods designed for the estimation of CATEs is to minimize the mean squared error for treatment effects (MSEτ ):

$$
\tag{12}
$$

Therefore, these methods are not optimized to predict outcomes but rather to predict causal effects, which are usually defined as the difference between potential outcomes (i.e., Y (1) − Y (0)). The main challenge is that we only observe one potential outcome for any given individual, so we cannot calculate Equation 12 directly. However, we may use alternative formulations to estimate MSEτ from data (Schuler et al. 2018), allowing us to compare (and optimize) models on the basis of how good they are at predicting causal effects. Unfortunately for our application, and similarly to the previous section, optimizing causal effect predictions (by minimizing MSEτ ) is not the same as optimizing treatment assignments either. We illustrate this in Figure 2, which shows a similar example to the one illustrated in Figure 1, except that it compares the causal effect (rather than outcome) predictions made by two models.3 Therefore, the blue (dark) dots in this example represent the causal effects of the treatments for a specific individual (these dots are the same in both graphs), and the red dots represent the estimation of the effects by the models. As before, the first model has high prediction errors (Figure 2a) but makes a better assignment, while the second has lower prediction errors (Figure 2b) but makes a worse assignment. Thus, the model that makes a better causal effect prediction (i.e., that has lower MSEτ ) makes a worse treatment assignment. Surprisingly, this implies that models that are (relatively) bad at causal effect prediction may be good at making treatment assignments. This result, while seemingly counter-intuitive at first, may be attributed to the bias-variance decomposition of errors. In the machine learning community, it is well known that models that have a good classification performance are not necessarily good at estimating class probabilities, and vice versa (Friedman 1997). A useful analogy in our context is to think about treatment assignment as a classification problem and to think about causal effect estimation as a probability estimation problem; the two tasks are closely related but not exactly the same. Importantly, the bias and variance components of the estimation error in causal effect predictions may combine to influence treatment assignment errors in a very different way than with the squared error of the predictions themselves (Fern´andez-Lor´ıa and Provost 2022). Figure 3 illustrates this in more detail by depicting the sampling distribution of the causal effect estimates previously shown in Figure 2. Specifically, Figure 3a shows that the large prediction errors of the model in Figure 2a are the result of high bias because the sampling distributions are not centered on the causal effect estimands. However, this model works very well for treatment assignment because P(ˆτ (x, 2) < τˆ(x, 1)) ≈ 1 and τ (x, 2) < τ (x, 1). On the other hand, Figure 3b shows that the model in Figure 2b is an unbiased estimator of causal effects and has lower mean squared error. However, this model is more likely to make the incorrect assignment due to sampling errors (variance). Importantly, what matters in this case is not the accuracy of the causal effect estimates but how good they are at discriminating between treatment alternatives. We elaborate more on this in Section 4.4, after discussing the objective function of the A-learner.

## Treatment assignment prediction

The third metalearner, the A-learner, estimates the treatment assignment policy by directly learning the treatment assignments that lead to the best outcomes. As Zhang et al. (2012) describe in detail, the treatment assignment problem can be transformed into a weighted classification problem. The idea is that each treatment alternative can be mapped to a class, and classes are associated with weights that correspond to the cost of not predicting the corresponding class. Weights are generally defined in terms of the potential outcomes associated with each treatment alternative (Beygelzimer and Langford 2009, Zhao et al. 2012, Kitagawa and Tetenov 2018), but more broad definitions exist (Zhang et al. 2012). Thus, the goal is to ‘classify’ individuals into the class (treatment) with the largest weight in order to minimize misclassification costs. The challenge here is that we only observe a single weight for any given individual (the one associated with the treatment assigned in the data), so we do not observe correct classifications at the individual level. Fortunately, samples from treatment assignment problems can be transformed into weighted classification samples so that any importance-weighted classification algorithm can be used to learn treatment assignment policies. For example, given a probability distribution P(T) over the treatment (e.g., the probability that an individual gets assigned to treatment T in the A/B test data), each observation (x, y, t) can be transformed into an importance-weighted example where y/P(t) is the cost of not predicting treatment t given input x (Beygelzimer and Langford 2009, Zhao et al. 2012); if t is predicted, then the cost is zero. The weighted misclassification rate (WMR) of classifier ˆa under this setting is:

$$
\tag{13}
$$

which is directly tied to treatment assignment performance because minimizing the WMR is equivalent to minimizing expected regret (as defined in Equation 4). We present a simplified proof of this result in Appendix B (see Beygelzimer and Langford (2009) for more details). As a result, we should expect WMR to be a better objective function than MSEµ or MSEτ when the goal is to make the best possible treatment assignments.

## Bias-variance tradeoff

In this section, we use a simulated example to illustrate how the A-learner can exploit the biasvariance tradeoff in the learning procedure to make better treatment assignments than the Elearner. The data from the simulated example are shown in Figure 4. There are two treatment alternatives, treat (orange) and not treat (gray), and the goal is to use feature X to learn a treatment assignment policy to discriminate individuals into those that would benefit from the treatment (orange line is above gray line) and those that would not (gray line is above orange line). The lines represent the expected outcomes under both treatment conditions, and the dots represent the available training data. See Appendix C for a description of the data generating process. Similarly to previous studies that have considered models with constrained functional forms for ethical, legislative, or political reasons (Kitagawa and Tetenov 2018, Athey and Wager 2021), suppose we are considering the class of treatment rules that split the population only once (e.g., decision trees with a single split). One alternative is to split individuals according to causal effect heterogeneity and then make treatment assignments according to their estimated causal effects, which corresponds to the E-learner. Figure 5 shows the result of learning a single-split causal tree—a tree that splits individuals according to causal effects (Athey and Imbens 2016). The blue line is the expected treatment effect given X (the estimand in causal effect estimation); the red line is the prediction from the tree that is learned from the data shown in Figure 4; the green line is from the tree that is learned with unlimited data (i.e., the best possible single split that could be made to minimize MSEτ ). Finally, the dashed line corresponds to the actual decision boundary: it is optimal to treat when the expected treatment effect (blue line) is above the boundary. The treatment assignment policy treats when the predicted effect (green or red lines) is above the boundary. Note that a treatment assignment policy based on estimated causal effects (red line) would not treat individuals with X . 0.6 even though most individuals with such values for X would actually benefit from the treatment. This is the result of variance in the estimation procedure. With more data, errors due to variance eventually disappear, and the estimated causal effects (red line) converge to the best-in-class predictor (green line). Unfortunately, the best-in-class predictor does not lead to optimal assignments either, because it estimates that the the treatment is beneficial for everyone even though individuals with a small value for X do not benefit from the treatment. In this case, errors in treatment assignment occur due to bias in the estimation procedure: the causal tree is not complex enough to identify who does not benefit from the treatment. Nevertheless, this does not imply that the class of treatment rules that split the population only once is not complex enough to model treatment assignments. It just implies that learning a tree by splitting the population according to effect heterogeneity (an E-learner) does not lead to optimal assignments. A second alternative is to use an A-learner to split the population according to preferred treatment assignments. In other words, to learn a classification model optimized to minimize Equation 13 (WMR) instead of Equation 12 (MSEτ ). An important challenge, however, is that Equation 13 can be viewed as a weighted version of 0-1 loss, and it is well known in the machine learning literature that minimizing such loss is difficult due to its discontinuity and non-convexity. A common approach to address this challenge is to use a surrogate loss to learn a scoring model, such as the negative log likelihood in logistic regression or the hinge loss in support vector machines (Zhao et al. 2012), and then classify individuals according to their scores. The predictions of the resulting scoring model ( ˆθ) would correspond to:

$$
\tag{14}
$$

where T˜ corresponds to the weighted treatment class, and the scoring model may be used to choose the optimal treatment as follows:

$$
\tag{15}
$$

Nevertheless, in the case of tree-based algorithms, it is tractable to optimize according to 0-1 loss (and hence according to WMR). Figure 6 shows the result of learning such a tree. Similarly to before, the blue line is the target score in the weighted classification task (the estimand); the red line is the scoring model that is learned from the data shown in Figure 4; the green line is the scoring model that is learned with unlimited data (i.e., the best possible single split that could be made to minimize WMR); the dashed line corresponds to the decision boundary. Note that although the estimated scoring model (red line) also suffers from errors due to variance and bias, these errors do not affect decision making (treatment assignment) as much. For example, there is a substantial underestimation of the treatment score for individuals with a small value for X, but this does not affect decision making because the optimal decision for those individuals is not to intervene. Importantly, with more data, the model eventually converges to the best-in-class predictor (green line), which leads to optimal assignments even though the treatment scores are biased due to the simplicity of the treatment rules under consideration. Essentially, this example shows that minimizing errors in causal effect (or outcome) predictions may not imply better treatment assignments because the direction of the errors is critical. For the purposes of decision making, overestimations do not hurt when the treatment is beneficial, and underestimations do not hurt when the treatment is detrimental. This point is important because E-learners may correct such errors at the expense of increasing errors that will hurt decision making (as shown in Figure 3). In contrast, A-learners are specifically designed to minimize errors that hurt decision making; errors that do not affect decisions are essentially ignored. In our example, the E-learner splits the data to minimize the bias in the causal effect predictions, whereas the A-learner splits the data to minimize the bias that negatively affects decisions. Table 3 compares the two metalearners with unlimited training data (resulting in the green lines in Figures 5 and 6). The A-learner split leads to larger bias (and hence larger MSEτ ) than the E-learner split, but that bias does not have negative implications for decision making (regret). Of course, if decisions are based on true causal effects rather than causal effect estimates (i.e., the blue line in Figure 5), then regret is also minimized. So, with more data, one could learn a more complex causal effect model (e.g., a tree with more splits), decrease the modeling bias, and eventually converge to optimal decision making. Nonetheless, as our subsequent empirical analysis shows, the A-learner can outperform the O-learner and the E-learner even when the training sample consists of hundreds of millions of observations.

# Experiments & Results 

sWe now present an empirical comparison of the three metalearners for choosing which playlist generation algorithm to apply for each listener (see Liebman et al. 2019, for an overview of prior playlist generation studies in IS).
