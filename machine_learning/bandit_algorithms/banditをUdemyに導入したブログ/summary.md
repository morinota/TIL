<!-- 仮タイトル: Udemyのユニット推薦にバンディットアルゴリズムを導入するブログを読んだ! -->

## これは何??

- Udemyさんのユニット推薦にMAB(multi-armed bandit)アルゴリズムを導入するテックブログを読んだメモです!
  - [Building a Multi-Armed Bandit System from the Ground Up: A Recommendations and Ranking Case Study, Part I](https://medium.com/udemy-engineering/building-a-multi-armed-bandit-system-from-the-ground-up-a-recommendations-and-ranking-case-study-b598f1f880e1)
  - [Building a Multi-Armed Bandit System from the Ground Up: A Recommendations and Ranking Case Study, Part II](https://medium.com/udemy-engineering/building-a-multi-armed-bandit-system-from-the-ground-up-a-recommendations-and-ranking-case-study-8f09f65d26b6)

## MABをUdemy内のユニット推薦に導入するモチベーション。

- 探索-活用のバランスをとるアプローチが様々なアプリケーションで成功を収めてるため、MAB(multi-armed bandit)アルゴリズムの人気が高まってる。
  - 特に成功してるアプリケーションの一つが、推薦。
- ゼロからMABシステムを本番システムに導入するのって エンジニアリング・運用・データサイエンス全部の観点から大変。
- 今回のテックブログでその実体験をシェアしてくれてる(ありがてぇ...:thinking:)。
  - part1: 理論とデータサイエンス的な話
  - part2: ソフトウェアエンジニアリング的な話

## MABとは?? なぜ推薦にMABを使うのか??

### ざっくりMABの問題設定

- MABは、**不確実性の元での意思決定問題を解くために使用される explore-exploit(探索-活用)アルゴリズム**の一種。
- explore-exploit(探索-活用)アルゴリズムってどんな問題設定を解くためのもの??
  - 例: カジノのスロットマシン (かなりイメージしやすくてわかりやすい例でした!:thinking:)
    - あなたはカジノに入り、選択可能なスロットマシンがいくつか提示される。
    - 各スロットマシンにはレバー(アーム)があり、それを引く(プレイ)と、各スロットマシンの事前に定められた報酬確率分布からサンプリングされたランダムな報酬(支払い)を受け取る。
      - 各スロットマシンは異なる報酬確率分布を持ち、これらの分布は最初にプレイを始めた時のあなたは知らない。
    - あなたの目標は、これらのスロットマシンをプレイすることで得られる勝ち金(累積報酬)を最大化すること。
    - あなたは賢いギャンブラーなので、使うつもりのある固定金額を持ってカジノに来た。なので、アームを引く回数は限られている。
    - **もし各スロットマシンの真の報酬確率分布を知っていれば、長期的に勝ち金を最大化する方法は非常に明確**である。
      - シンプルに、報酬期待値が最も高い1つのスロットマシンのアームを継続的にプレイするだけで良いはず。
    - しかし、**あなたは最初はこれらの報酬分布について何も知らないため、限られたアームを引く回数の一部を使って探索し、各アームの期待報酬についてもっと学ぶ**必要がある。また、**これまで得た学びを活用して最も良さそうなアームを引く**こととのバランスも取るよう必要もある。
      - 探索だけを行うと、各スロットマシンの報酬分布に対する高い信頼性を得ることができる。しかし、最適なアームを引くことを知る前に、アームを引ける回数が尽きてしまう。
      - 活用だけを行うと、最適でないスロットマシンを継続的にプレイする可能性が高くなり、それはそれでより高い報酬を得る機会を逃し得る。
    - これが古典的な「**探索-活用(explore-exploit)のジレンマ**」!
    - MABアルゴリズムをはじめとした探索-活用アルゴリズムは、このジレンマを解決するため(i.e. 探索と活用の最適なバランスをとり累積報酬の最大化を助けるため)に設計されてる。

### MABの定義

- MABは、不確実性の下で意思決定を行うために使用される探索-活用アルゴリズムの一種。
  - これは簡略化された強化学習アルゴリズムとみなせる。
- 標準的なMAB問題の設定は、以下のように定義できる:
  - $k$ 本のアーム（選択肢）のセット $A = \{a_1, a_2, \ldots, a_k\}$ が与えられた時、目標は、これらのアームをプレイする事で得られる累積報酬を最大化するために、最良のアームを決定すること。
  - アーム $a_i$ が時刻 $t$ にプレイされると、そのアームの報酬確率分布からサンプリングされた報酬 $r_{t}$ が得られる。プレイされていないアームに対しては他の報酬は観測されない。このアーム-報酬ペア $(a_i, r_t)$ を**観測(observation)**と呼んで記録する。
  - 我々は、最適でないアームをあまり頻繁に表示することなく、各アームの真の期待報酬についてより多くを学ぶために、**次にどのアームを選択するかを最適に決定する戦略**を考案する。戦略は、時刻 $t-1$ までのアーム-報酬ペアの観測履歴を考慮し、時刻 $t$ において次にプレイするアームを選択する。
- MAB戦略には多くの異なるアプローチがあるが、それぞれ利点と欠点がある。
  - ちなみに、**Udemy内で構築したアプリケーションでは、主にThompson Sampling戦略を使用**してるらしい。

### 推薦にMABを使うこと

- 推薦ドメインは、MABの完全な応用を含んでいる。
  - **一般的な推薦問題において、私たちの目的はユーザに最適なアイテムを推薦することであり、その「最適」とは何らかのユーザフィードバックに基づいて定義される。** 
  - (前述のMABの問題設定っぽい...!:thinking:)
- 従って、**推薦問題は以下のようなMAB問題として定式化**できる:
  - 推薦するための候補アイテム集合は、プレイ可能なアームの集合 (各アイテムはアーム)。
  - 特定の推薦アイテムをユーザに一定の時間表示することは、「そのアームをプレイする」ことに相当する。
  - 推薦アイテムが表示された時のユーザフィードバック(ex. クリック、いいね、Udemyの場合はコース登録など)は、観測された報酬に対応する。(これはすなわち、選ばれたアームの未知の報酬分布からサンプリングされた報酬)
  - 目標は、探索と活用のバランスを取りながら、ユーザにさまざまなアイテムを動的に推薦することによって、できるだけ早く最高の報酬を得られるアイテムを特定すること。
- また、MABには、推薦アプリケーションに優れた特性が多くある。**MABが特に推薦に適してる主な理由は3つ**:
  - 理由1: 推薦タスクにはopportunity cost(機会コスト?)があるから。
    - (もっとより最適なアイテムをおすすめできたのでは...という機会損失のコスト、という解釈...!:thinking:)
    - MABは、最適でないアームをプレイすることによるコストがある問題設定で真価を発揮する。なぜなら、 MABはそもそも累積regretを最小化するように設計されてるから。
  - 理由2: 推薦におけるフィードバックループバイアス問題を克服するのに役立つから。
    - フィードバックループバイアス問題: hoge
    - **MABにおける探索は本質的にこのサイクルを断ち切る**。あまり探索されてないアイテムはMAB的に推薦されやすくなるので、**真に最適でない限り、同じアイテムを繰り返し表示することに陥る可能性が低くなる**
  - 理由3: MABは、コールドスタートアイテム問題に自然に対処できるから。
    - フィードバックループバイアス問題に密接に関連しているのがコールドスタートアイテム問題。
      - フィードバックがほとんどない新しいアイテムは、関連するポジティブなシグナルがほとんどないため、推薦される可能性が低くなる。
    - MABの設計上、報酬期待値の推定に自信を持つまでは新しいアイテムを自然に探索するようになってる。なので、**MABは全ての候補アームに公平な機会が与える**。
      - 探索フェーズにて成功と見なされた場合はそのアイテムを活用し、失敗と見なされた場合は表示を停止する。

## ユニット推薦へのMAB導入に関するデータサイエンス的な話

### ランキング問題をMAB問題として解くこと

- 伝統的なMABの問題設定は、ユーザに一度に1つのアームが提示され単一のポジションに対して最良のアームを見つけることが目標。
  - (ex. バナー最適化、サムネイル最適化、ボタンの色の最適化, etc.)
  - (いわゆる、**Single-Play MAB**って呼ばれるやつ...!:thinking:)
- 一方で、推薦ドメインにはランキング問題もある。
  - **ここでの重要な違いは、ランキング問題では考慮すべき複数のポジションがあり、そのため技術的には複数のアームに対して同時に報酬を収集できる**ということ。
  - (いわゆる、**Multi-Plays MAB**って呼ばれるやつ...!:thinking:)
- Udemyにおけるランキング推薦の一般的な応用の1つは、推薦ユニットランキングである。
  - 推薦ユニットは、特定グループに属するコース達を集めた横カルーセル。(ex. 「Because you Viewed」「Recommended for You」「Popular for ___」など)
  - (つまり、ページ内での横カルーセル単位のランクづけ、みたいな感じか...!:thinking:)。
  - これらの推薦ユニットがユーザのホームページに表示される順序は、ユーザーの視点とビジネスの視点の両方から非常に重要。

今回、Udemyでは推薦ユニットランキング問題を以下のように定式化したとのこと。
(この定式化方法は実運用で参考になりそう! 定式化大事だよなぁ...書籍「反実仮想機械学習」でも最初のステップとして「**定式化がめちゃめちゃ大事だからスキップするな!**」みたいな事が書かれてた気がするし...!:thinking:)

1. 各推薦ユニットは「アーム」である。
2. 「アームをプレイする」ことは、特定のポジションにおいてユーザに推薦ユニットを固定時間(今回の事例では15分)表示することを意味する。各ユーザは、表示されたユニットに対して提供されたFBに基づいて「報酬(reward)」の「観測(obeservation)」を生成する。(ユニットのimpressionが発生した場合にのみ報酬が観測される。)
3. 「報酬(reward)」は、特定のポジションにおけるさまざまなユニットのパフォーマンスを比較するために使用したい指標。今回の事例では、固定時間内にユニット内のコースに対するユーザのクリックと登録の組み合わせを使用する (=確か後述されてたけど、報酬関数はこの2種類のFBの線形結合だった気がする...!:thinking:)

- このMABの定式化は十分にシンプルそう。しかし、**ランキングの複数のポジションにおけるフィードバックの扱い**についてはまだ検討すべき点が残ってる。
  - 伝統的なMAB設定では、単一のアームのみがプレイされる。
  - **複数のアームを同時に引くことができるという利点を活かすために、伝統的なMAB設定を修正**する必要がある。(なるほど、解決すべき問題としては複雑性が増しててデメリットだと思っちゃうけど、利点とも見做せるのか...!:thinking:)
- ランキング形式のフィードバックの扱いをいい感じにするために、今回の事例では2つの異なるフレームワークが検討された。
  - **Per-Position Framework**: 各ポジションを独立したMAB問題として扱う。
  - **Slate Bandit Framework**: 各ポジションを同時にテストする単一のMAB問題として扱う。

#### 検討されたアプローチ1: Per-Position Framework

- 各ポジションを独立したMAB問題として扱う。
  - よって、**ランキングの各ポジションに対して別々のバンディットインスタンスを使用することになる。**
    - この場合の各バンディットの目標は、割り当てられた位置でプレイする最適なアームを見つけること。
    - ex. 位置2専用のバンディットインスタンスは、位置2でユーザにユニットが表示された場合にのみアームを「プレイした」と見なされ、位置2に関する観察のみを収集して学習する。
    - (なるほど。**ポジションの数だけバンディットインスタンスを立てて、各バンディットインスタンスはSingle-play MABを解く**ってことか...!:thinking:)
- 利点:
  - 各ポジションで最適なユニットを学習する設定なので、詳細でspecificなランキング最適化を達成できる。
- 欠点: 
  - **いくつかの実践的な課題(practical challenges)がある**:
    - ポジション間の相互依存関係を考慮できない。
      - (上で同じようなユニットをおすすめしてたらそれを避ける、みたいな多様性の観点も含まれそう:thinking:)
    - ランキングの長さが大きくなるにつれて、この相互依存関係はますます複雑になり得る。
  - 同時に複数の異なるバンディットインスタンスを維持(i.e. 運用)する必要がある。
    - 特にUdemyさんのシステムでは相互に依存するストリーミングアプリケーションとして構築されてる?ので、1つのバンディットインスタンスの障害は、他のバンディットインスタンスに問題を引き起こす可能性がある。
  - 観測される報酬FBは、異なるバンディットインスタンス間で簡単に共有できない。
    - ex. 位置2専用バンディットインスタンスの視点での「ユニットAの報酬」の定義は、「位置2でユニットAが表示された場合に発生するクリックとsubscription」である。よって、位置3でユニットAが表示された場合に観測されたFBは、位置2専用バンディットインスタンスでは使用できない。
    - これにより、バンディットが学習する時間とランキングが収束する時間が増加し得る。
    - (各position専用のバンディットインスタンス達が、個別で頑張って学習する必要があるって話か〜...:thinking:)
- 上記の実践的な課題を考慮して、Udemyさんの事例ではPer-Position Frameworkは採用されなかった。代わりに後述されるSlate Bandit Frameworkが採用されたとのこと。

#### 検討されたアプローチ2: Slate Bandit Framework

- 複数のアームを同時にテストするような、単一のバンディットインスタンスを使用する。
  - このフレームワークでは、**「アームをプレイする」とは、上位 k 位置のいずれかにユニットを表示すること**と定義される。
- 注意点: この場合の**kの値の選択は重要!**
  - なぜなら、**上位 k 位置のいずれかに表示されたアームに対するFBはすべて同等に扱われるから**。
    - ex. 最初の位置でのクリックや登録は、バンディットインスタンス視点では、k番目の位置でのクリックや登録と同等に扱われる。
  - 一般的にkは、ユースケース次第のハイパーパラメータ。使用ケースやページレイアウトによって異なる。
- Udemyさんの特定の(今回の?)ユースケースでは $k=3$が選択されたとのこと。
  - 設定理由: ユーザのログインしたホームページは通常、画面が最大化されるとウィンドウ内に3つの完全なユニットを表示するから。これらの位置にあるユニットに対するFBは、一般的にほぼ等価と見做せるという考え。

### MAB活用におけるDS的な課題

#### 課題1: 収束の達成とその測定

#### 課題2: 方策のオフライン評価

## MAB導入のソフトウェアエンジニアリング的な話

### システムアーキテクチャの全体像

### MAB導入におけるエンジニアリング的な課題

#### 課題1: リアルタイムシステムを開発することの学習コスト

#### 課題2: リアルタイムシステムの運用の大変さ

## 今後の発展の方向性

### 報酬の強化(報酬関数の定義の見直し、拡張みたいな話!)

### Contextual Bandit
