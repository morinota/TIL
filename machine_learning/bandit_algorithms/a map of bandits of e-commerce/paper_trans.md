
# A Map of Bandits for E-commerce Eã‚³ãƒãƒ¼ã‚¹ã®ãŸã‚ã®ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®åœ°å›³  
## Yi Liu[âˆ—]  
## Yi Liu[âˆ—]  
#### yiam@amazon.com Amazon.com Seattle, WA, United States  
#### yiam@amazon.com Amazon.com ã‚·ã‚¢ãƒˆãƒ«, WA, ã‚¢ãƒ¡ãƒªã‚«åˆè¡†å›½  
### ABSTRACT è¦ç´„  
The rich body of Bandit literature not only offers a diverse toolbox of algorithms, but also makes it hard for a practitioner to find the right solution to solve the problem at hand.  
ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«é–¢ã™ã‚‹è±Šå¯Œãªæ–‡çŒ®ã¯ã€å¤šæ§˜ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒ„ãƒ¼ãƒ«ãƒœãƒƒã‚¯ã‚¹ã‚’æä¾›ã™ã‚‹ã ã‘ã§ãªãã€å®Ÿå‹™è€…ãŒç›´é¢ã—ã¦ã„ã‚‹å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®é©åˆ‡ãªè§£æ±ºç­–ã‚’è¦‹ã¤ã‘ã‚‹ã®ã‚’é›£ã—ãã—ã¾ã™ã€‚  
Typical textbooks on Bandits focus on designing and analyzing algorithms, and surveys on applications often present a list of individual applications.  
ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«é–¢ã™ã‚‹å…¸å‹çš„ãªæ•™ç§‘æ›¸ã¯ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è¨­è¨ˆã¨åˆ†æã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãŠã‚Šã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«é–¢ã™ã‚‹èª¿æŸ»ã¯ã—ã°ã—ã°å€‹ã€…ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒªã‚¹ãƒˆã‚’æç¤ºã—ã¾ã™ã€‚  
While these are valuable resources, there exists a gap in mapping applications to appropriate Bandit algorithms.  
ã“ã‚Œã‚‰ã¯è²´é‡ãªãƒªã‚½ãƒ¼ã‚¹ã§ã™ãŒã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é©åˆ‡ãªãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ã‚®ãƒ£ãƒƒãƒ—ãŒå­˜åœ¨ã—ã¾ã™ã€‚  
In this paper, we aim to reduce this gap with a structured map of Bandits to help practitioners navigate to find relevant and practical Bandit algorithms.  
æœ¬è«–æ–‡ã§ã¯ã€å®Ÿå‹™è€…ãŒé–¢é€£ã™ã‚‹å®Ÿç”¨çš„ãªãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã®æ§‹é€ åŒ–ã•ã‚ŒãŸãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®åœ°å›³ã‚’ç”¨ã„ã¦ã€ã“ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’ç¸®å°ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚  
Instead of providing a comprehensive overview, we focus on a small number of key decision points related to reward, action, and features, which often affect how Bandit algorithms are chosen in practice.  
åŒ…æ‹¬çš„ãªæ¦‚è¦ã‚’æä¾›ã™ã‚‹ã®ã§ã¯ãªãã€å ±é…¬ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€ãŠã‚ˆã³ç‰¹å¾´ã«é–¢é€£ã™ã‚‹å°‘æ•°ã®é‡è¦ãªæ„æ€æ±ºå®šãƒã‚¤ãƒ³ãƒˆã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ã€‚ã“ã‚Œã‚‰ã¯å®Ÿéš›ã«ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒé¸æŠã•ã‚Œã‚‹æ–¹æ³•ã«ã—ã°ã—ã°å½±éŸ¿ã‚’ä¸ãˆã¾ã™ã€‚  



### KEYWORDS ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
Bandit, reward, action, E-commerce, recommendation
ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã€å ±é…¬ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€Eã‚³ãƒãƒ¼ã‚¹ã€æ¨è–¦

**ACM Reference Format:** Yi Liu and Lihong Li. 2021. A Map of Bandits for E-commerce. 
**ACMå‚ç…§å½¢å¼:** Yi Liuã¨Lihong Li. 2021. Eã‚³ãƒãƒ¼ã‚¹ã®ãŸã‚ã®ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®åœ°å›³ã€‚ 

In Proceedings _of Marble-KDD 21. Singapore, 5 pages._
Marble-KDD 21ã®è­°äº‹éŒ²ã«ãŠã„ã¦ã€‚ã‚·ãƒ³ã‚¬ãƒãƒ¼ãƒ«ã€5ãƒšãƒ¼ã‚¸ã€‚



### 1 INTRODUCTION AND MOTIVATION 1. ã¯ã˜ã‚ã«ã¨å‹•æ©Ÿ

Bandit is a framework for sequential decision making, where the decision maker (â€œagentâ€) sequentially chooses an action (also known as an â€œarmâ€), potentially based on the current contextual information, and observes a reward signal. 
ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã¯ã€é€æ¬¡çš„ãªæ„æ€æ±ºå®šã®ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚Šã€æ„æ€æ±ºå®šè€…ï¼ˆã€Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ï¼‰ãŒé€æ¬¡çš„ã«ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆã€Œã‚¢ãƒ¼ãƒ ã€ã¨ã‚‚å‘¼ã°ã‚Œã‚‹ï¼‰ã‚’é¸æŠã—ã€ç¾åœ¨ã®æ–‡è„ˆæƒ…å ±ã«åŸºã¥ã„ã¦ã€å ±é…¬ä¿¡å·ã‚’è¦³å¯Ÿã—ã¾ã™ã€‚ 
The typical goal of the agent is to learn an optimal action-selection policy to maximize some function of the observed reward signals. 
ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å…¸å‹çš„ãªç›®æ¨™ã¯ã€è¦³å¯Ÿã•ã‚ŒãŸå ±é…¬ä¿¡å·ã®ã„ãã¤ã‹ã®é–¢æ•°ã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã®æœ€é©ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³é¸æŠãƒãƒªã‚·ãƒ¼ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã™ã€‚ 
This problem is a special case of reinforcement learning (RL) [1], and has been a subject of extensive research in AI. 
ã“ã®å•é¡Œã¯å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã®ç‰¹åˆ¥ãªã‚±ãƒ¼ã‚¹ã§ã‚ã‚Š[1]ã€AIã«ãŠã‘ã‚‹åºƒç¯„ãªç ”ç©¶ã®å¯¾è±¡ã¨ãªã£ã¦ã„ã¾ã™ã€‚

The main reason for extensive research of Bandit in the literature is its wide applications. 
æ–‡çŒ®ã«ãŠã‘ã‚‹ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®åºƒç¯„ãªç ”ç©¶ã®ä¸»ãªç†ç”±ã¯ã€ãã®åºƒç¯„ãªå¿œç”¨ã§ã™ã€‚ 
The paper focuses on one of the most important domains, E-commerce, including online recommendation, dynamic pricing, supply chain optimization, among others [2]. 
æœ¬è«–æ–‡ã¯ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¨è–¦ã€å‹•çš„ä¾¡æ ¼è¨­å®šã€ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³æœ€é©åŒ–ãªã©ã‚’å«ã‚€ã€æœ€ã‚‚é‡è¦ãªé ˜åŸŸã®1ã¤ã§ã‚ã‚‹Eã‚³ãƒãƒ¼ã‚¹ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™[2]ã€‚ 
For instance, Bandit has been used for online recommendation across companies such as Amazon, Google, Netflix, and Yahoo! [3]. 
ä¾‹ãˆã°ã€ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã¯Amazonã€Googleã€Netflixã€Yahoo!ãªã©ã®ä¼æ¥­ã§ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¨è–¦ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™[3]ã€‚ 
Early applications were on optimizing webpage content suggestion such as news articles, advertisement, and marketing messages [4, 5]. 
åˆæœŸã®å¿œç”¨ã¯ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã€åºƒå‘Šã€ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãªã©ã®ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ææ¡ˆã‚’æœ€é©åŒ–ã™ã‚‹ã“ã¨ã«ã‚ã‚Šã¾ã—ãŸ[4, 5]ã€‚ 
Nowadays, its applications have been extended to dynamic pricing [6], revenue management [7], inventory buying [8], as well as recommendation of various contents such as skills through virtual assistants [9]. 
ç¾åœ¨ã§ã¯ã€ãã®å¿œç”¨ã¯å‹•çš„ä¾¡æ ¼è¨­å®š[6]ã€åç›Šç®¡ç†[7]ã€åœ¨åº«è³¼å…¥[8]ã€ãŠã‚ˆã³ãƒãƒ¼ãƒãƒ£ãƒ«ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‚’é€šã˜ãŸã‚¹ã‚­ãƒ«ãªã©ã®ã•ã¾ã–ã¾ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ¨è–¦ã«ã¾ã§æ‹¡å¼µã•ã‚Œã¦ã„ã¾ã™[9]ã€‚ 

The rich body of literature not only offers a diverse toolbox of algorithms, but also makes it hard for a practitioner to find the right solution to solve the problem at hand.  
æ–‡çŒ®ã®è±Šå¯Œãªä½“ã¯ã€å¤šæ§˜ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒ„ãƒ¼ãƒ«ãƒœãƒƒã‚¯ã‚¹ã‚’æä¾›ã™ã‚‹ã ã‘ã§ãªãã€**å®Ÿå‹™è€…ãŒç›´é¢ã—ã¦ã„ã‚‹å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®é©åˆ‡ãªè§£æ±ºç­–ã‚’è¦‹ã¤ã‘ã‚‹ã®ã‚’é›£ã—ãã—ã¾ã™**ã€‚
A main challenge lies in the many choices when formulating a Bandit problem, and the resulting combinatorial explosion of problem space and algorithms.  
ä¸»ãªèª²é¡Œã¯ã€ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã‚’å®šå¼åŒ–ã™ã‚‹éš›ã®å¤šãã®é¸æŠè‚¢ã¨ã€ãã‚Œã«ä¼´ã†å•é¡Œç©ºé–“ã¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®çµ„ã¿åˆã‚ã›çˆ†ç™ºã«ã‚ã‚Šã¾ã™ã€‚  
Typical textbooks on Bandits focus on designing and analyzing algorithms [10, 11], and surveys on applications often present a list of individual applications [e.g., 3].  
ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«é–¢ã™ã‚‹å…¸å‹çš„ãªæ•™ç§‘æ›¸ã¯ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è¨­è¨ˆã¨åˆ†æã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãŠã‚Š[10, 11]ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«é–¢ã™ã‚‹èª¿æŸ»ã¯ã—ã°ã—ã°å€‹ã€…ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒªã‚¹ãƒˆã‚’æç¤ºã—ã¾ã™[e.g., 3]ã€‚  
While they are valuable resources, there is a gap in mapping applications to algorithms.  
ãã‚Œã‚‰ã¯è²´é‡ãªãƒªã‚½ãƒ¼ã‚¹ã§ã™ãŒã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹éš›ã«ã‚®ãƒ£ãƒƒãƒ—ãŒã‚ã‚Šã¾ã™ã€‚  
This paper aims to reduce this gap, by presenting a structured map for the world of Bandits.  
ã“ã®è«–æ–‡ã¯ã€ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ä¸–ç•Œã®ãŸã‚ã®æ§‹é€ åŒ–ã•ã‚ŒãŸãƒãƒƒãƒ—ã‚’æç¤ºã™ã‚‹ã“ã¨ã§ã€ã“ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’ç¸®å°ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚  
The map consists of a few key decision points, to guide practitioners to navigate in the complex world of Bandits to locate proper algorithms.  
ã“ã®ãƒãƒƒãƒ—ã¯ã€å®Ÿå‹™è€…ãŒãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®è¤‡é›‘ãªä¸–ç•Œã‚’ãƒŠãƒ“ã‚²ãƒ¼ãƒˆã—ã€é©åˆ‡ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã®ã„ãã¤ã‹ã®é‡è¦ãªæ„æ€æ±ºå®šãƒã‚¤ãƒ³ãƒˆã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚  
While we use E-commerce as running examples, the map is useful to other applications.  
ç§ãŸã¡ã¯Eã‚³ãƒãƒ¼ã‚¹ã‚’å®Ÿä¾‹ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ãŒã€ã“ã®ãƒãƒƒãƒ—ã¯ä»–ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚‚å½¹ç«‹ã¡ã¾ã™ã€‚  
Furthermore, it is beyond the scope of the paper to provide a comprehensive map.  
ã•ã‚‰ã«ã€åŒ…æ‹¬çš„ãªãƒãƒƒãƒ—ã‚’æä¾›ã™ã‚‹ã“ã¨ã¯ã“ã®è«–æ–‡ã®ç¯„å›²ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚  
Instead, our map only focuses on a small number of factors that often affect how Bandit algorithms are chosen in practice.  
ä»£ã‚ã‚Šã«ã€ç§ãŸã¡ã®ãƒãƒƒãƒ—ã¯ã€å®Ÿéš›ã«ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒã©ã®ã‚ˆã†ã«é¸ã°ã‚Œã‚‹ã‹ã«ã—ã°ã—ã°å½±éŸ¿ã‚’ä¸ãˆã‚‹å°‘æ•°ã®è¦å› ã«ã®ã¿ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚  
The map entry is section 2, which assesses whether Bandit is the right formulation.  
ãƒãƒƒãƒ—ã®ã‚¨ãƒ³ãƒˆãƒªã¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³2ã§ã‚ã‚Šã€ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãŒæ­£ã—ã„å®šå¼åŒ–ã§ã‚ã‚‹ã‹ã©ã†ã‹ã‚’è©•ä¾¡ã—ã¾ã™ã€‚  
Sections 3 and 4 describe the navigational details of the map, to help locate appropriate algorithms by inspecting several properties of rewards and actions of the application at hand.  
ã‚»ã‚¯ã‚·ãƒ§ãƒ³3ã¨4ã§ã¯ã€ãƒãƒƒãƒ—ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã®è©³ç´°ã‚’èª¬æ˜ã—ã€å¯¾è±¡ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å ±é…¬ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã„ãã¤ã‹ã®ç‰¹æ€§ã‚’æ¤œæŸ»ã™ã‚‹ã“ã¨ã§é©åˆ‡ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è¦‹ã¤ã‘ã‚‹æ‰‹åŠ©ã‘ã‚’ã—ã¾ã™ã€‚  
Section 5 complements the map with a discussion of topics that a practitioner often faces.  
ã‚»ã‚¯ã‚·ãƒ§ãƒ³5ã§ã¯ã€å®Ÿå‹™è€…ãŒã—ã°ã—ã°ç›´é¢ã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã®è­°è«–ã§ãƒãƒƒãƒ—ã‚’è£œå®Œã—ã¾ã™ã€‚  
Section 6 concludes the paper.  
ã‚»ã‚¯ã‚·ãƒ§ãƒ³6ã§è«–æ–‡ã‚’ç· ã‚ããã‚Šã¾ã™ã€‚  

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### 2 MAP ENTRY: IS BANDIT THE RIGHT FORMULATION? ãƒãƒ³ãƒ‰itã¯é©åˆ‡ãªå®šå¼åŒ–ã‹ï¼Ÿ

In typical sequential decision making modeled by Bandit, the agent repeats the following in every step: observe contextual information _ğ‘¥ğ‘¡_, take an action ğ‘ğ‘¡ from an action set ğ´ğ‘¡, and receive reward ğ‘Ÿğ‘¡, where ğ‘¡ denotes the step. 
ãƒãƒ³ãƒ‰itã«ã‚ˆã£ã¦ãƒ¢ãƒ‡ãƒ«åŒ–ã•ã‚ŒãŸå…¸å‹çš„ãªé€æ¬¡æ„æ€æ±ºå®šã§ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯å„ã‚¹ãƒ†ãƒƒãƒ—ã§ä»¥ä¸‹ã®ã“ã¨ã‚’ç¹°ã‚Šè¿”ã—ã¾ã™ï¼šæ–‡è„ˆæƒ…å ± _ğ‘¥ğ‘¡_ ã‚’è¦³å¯Ÿã—ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆ ğ´ğ‘¡ ã‹ã‚‰ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ğ‘ğ‘¡ ã‚’é¸æŠã—ã€å ±é…¬ ğ‘Ÿğ‘¡ ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚ã“ã“ã§ã€ğ‘¡ ã¯ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç¤ºã—ã¾ã™ã€‚
The reward depends on ğ‘¥ğ‘¡ and ğ‘ğ‘¡. 
å ±é…¬ã¯ ğ‘¥ğ‘¡ ã¨ ğ‘ğ‘¡ ã«ä¾å­˜ã—ã¾ã™ã€‚
The objective of a Bandit algorithm is to recommend actions for each step to maximize the expected cumulative reward over time: $R[T] = \sum_{t=1}^{T} E[r_t]$ where ğ‘‡ is the total number of steps. 
ãƒãƒ³ãƒ‰itã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç›®çš„ã¯ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¨å¥¨ã—ã€æ™‚é–“ã‚’é€šã˜ã¦æœŸå¾…ã•ã‚Œã‚‹ç´¯ç©å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã§ã™ï¼š$R[T] = \sum_{t=1}^{T} E[r_t]$ ã“ã“ã§ã€ğ‘‡ ã¯ã‚¹ãƒ†ãƒƒãƒ—ã®ç·æ•°ã§ã™ã€‚
Suppose we want to recommend a video (ğ‘ğ‘¡) from recently released ones (ğ´ğ‘¡) to customers, an application seen with Amazon Prime video, Netflix, HBO, etc. 
æœ€è¿‘ãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸå‹•ç”»ï¼ˆğ‘ğ‘¡ï¼‰ã‚’é¡§å®¢ã«æ¨å¥¨ã—ãŸã„ã¨ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€Amazon Prime Videoã€Netflixã€HBOãªã©ã§è¦‹ã‚‰ã‚Œã‚‹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ã€‚
We want to consider customer features to improve the recommendation relevance: film genre (drama, romance, etc.) preference of the customer (ğ‘¥ğ‘¡). 
æ¨å¥¨ã®é–¢é€£æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€é¡§å®¢ã®ç‰¹å¾´ã‚’è€ƒæ…®ã—ãŸã„ã¨æ€ã„ã¾ã™ï¼šé¡§å®¢ã®æ˜ ç”»ã‚¸ãƒ£ãƒ³ãƒ«ï¼ˆãƒ‰ãƒ©ãƒã€ãƒ­ãƒãƒ³ã‚¹ãªã©ï¼‰ã®å¥½ã¿ï¼ˆğ‘¥ğ‘¡ï¼‰ã€‚
The business metric of interest is total video viewers. 
é–¢å¿ƒã®ã‚ã‚‹ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒˆãƒªãƒƒã‚¯ã¯ã€ç·è¦–è´è€…æ•°ã§ã™ã€‚
The observed reward (ğ‘Ÿğ‘¡) can then be defined as 1 if the customer watches the recommended video and 0 otherwise. 
è¦³å¯Ÿã•ã‚ŒãŸå ±é…¬ï¼ˆğ‘Ÿğ‘¡ï¼‰ã¯ã€é¡§å®¢ãŒæ¨å¥¨ã•ã‚ŒãŸå‹•ç”»ã‚’è¦–è´ã—ãŸå ´åˆã¯1ã€ãã†ã§ãªã„å ´åˆã¯0ã¨å®šç¾©ã§ãã¾ã™ã€‚
Assume stationarity and linear structure, we may model the reward as $E[r_t(x_t, a_t)] = g(w_0 + w_1 \cdot a_t + w_2 \cdot x_t - a_t)$ where $w_i$â€™s are weights parameters and $g$ is a link function to map a linear predictor to the mean of the reward. 
å®šå¸¸æ€§ã¨ç·šå½¢æ§‹é€ ã‚’ä»®å®šã™ã‚‹ã¨ã€å ±é…¬ã‚’æ¬¡ã®ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã§ãã¾ã™ï¼š$E[r_t(x_t, a_t)] = g(w_0 + w_1 \cdot a_t + w_2 \cdot x_t - a_t)$ ã“ã“ã§ã€$w_i$ ã¯é‡ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚Šã€$g$ ã¯ç·šå½¢äºˆæ¸¬å­ã‚’å ±é…¬ã®å¹³å‡ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ãƒªãƒ³ã‚¯é–¢æ•°ã§ã™ã€‚
Logit and Probit are common link functions in Bandit applications. 
Logit ã¨ Probit ã¯ã€ãƒãƒ³ãƒ‰itã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ä¸€èˆ¬çš„ãªãƒªãƒ³ã‚¯é–¢æ•°ã§ã™ã€‚

The term â€œBanditâ€ refers to the fact that only the reward of the chosen action is observed. 
ã€Œãƒãƒ³ãƒ‰itã€ã¨ã„ã†ç”¨èªã¯ã€é¸æŠã•ã‚ŒãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å ±é…¬ã®ã¿ãŒè¦³å¯Ÿã•ã‚Œã‚‹ã¨ã„ã†äº‹å®Ÿã‚’æŒ‡ã—ã¾ã™ã€‚
If rewards of all actions are observed, the setting is called â€œfull-informationâ€ [12]. 
ã™ã¹ã¦ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å ±é…¬ãŒè¦³å¯Ÿã•ã‚Œã‚‹å ´åˆã€ãã®è¨­å®šã¯ã€Œãƒ•ãƒ«ã‚¤ãƒ³ãƒ•ã‚©ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã€ã¨å‘¼ã°ã‚Œã¾ã™[12]ã€‚
For example, consider typical multi-class classification, where predicting the label for an instance can be viewed as choosing an action. 
ä¾‹ãˆã°ã€å…¸å‹çš„ãªå¤šã‚¯ãƒ©ã‚¹åˆ†é¡ã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã“ã“ã§ã¯ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ãƒ©ãƒ™ãƒ«ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã¯ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã™ã‚‹ã“ã¨ã¨è¦‹ãªã™ã“ã¨ãŒã§ãã¾ã™ã€‚
It is full-information, because once we know an instanceâ€™s correct label, the rewards of all predictions are known (1 for the correct prediction and 0 otherwise). 
ã“ã‚Œã¯ãƒ•ãƒ«ã‚¤ãƒ³ãƒ•ã‚©ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ã€‚ãªãœãªã‚‰ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®æ­£ã—ã„ãƒ©ãƒ™ãƒ«ãŒã‚ã‹ã‚Œã°ã€ã™ã¹ã¦ã®äºˆæ¸¬ã®å ±é…¬ãŒã‚ã‹ã‚‹ã‹ã‚‰ã§ã™ï¼ˆæ­£ã—ã„äºˆæ¸¬ã«ã¯1ã€ãã†ã§ãªã„å ´åˆã¯0ï¼‰ã€‚
One should follow a different algorithmic path of supervised learning [13, 14] if the application is in the full-information setting. 
**ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒãƒ•ãƒ«ã‚¤ãƒ³ãƒ•ã‚©ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šã«ã‚ã‚‹å ´åˆã€ç•°ãªã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é“ã‚’ãŸã©ã‚‹ã¹ãã§ã™**[13, 14]ã€‚

On the other hand, in the more general RL setting, the next context (â€œstateâ€) may depend on previous contexts and taken actions, so the agent needs to reason with long-term rewards using more complex algorithms like Q-learning [1]. 
ä¸€æ–¹ã€ã‚ˆã‚Šä¸€èˆ¬çš„ãªå¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰è¨­å®šã§ã¯ã€æ¬¡ã®æ–‡è„ˆï¼ˆã€ŒçŠ¶æ…‹ã€ï¼‰ã¯ä»¥å‰ã®æ–‡è„ˆã‚„å–ã‚‰ã‚ŒãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«ä¾å­˜ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯Q-learning [1]ã®ã‚ˆã†ãªã‚ˆã‚Šè¤‡é›‘ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¦é•·æœŸçš„ãªå ±é…¬ã‚’è€ƒæ…®ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
For example, suppose we want to maximize sales from an online shopping website, which has search, (product) detail, add-to-cart, and checkout pages. 
ä¾‹ãˆã°ã€æ¤œç´¢ã€ï¼ˆå•†å“ï¼‰è©³ç´°ã€ã‚«ãƒ¼ãƒˆã«è¿½åŠ ã€ãƒã‚§ãƒƒã‚¯ã‚¢ã‚¦ãƒˆãƒšãƒ¼ã‚¸ã‚’æŒã¤ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‹ã‚‰ã®å£²ä¸Šã‚’æœ€å¤§åŒ–ã—ãŸã„ã¨ã—ã¾ã™ã€‚
The shopperâ€™s state on the detail page is affected by results shown earlier on the search page. 
è©³ç´°ãƒšãƒ¼ã‚¸ã§ã®è²·ã„ç‰©å®¢ã®çŠ¶æ…‹ã¯ã€æ¤œç´¢ãƒšãƒ¼ã‚¸ã§ä»¥å‰ã«è¡¨ç¤ºã•ã‚ŒãŸçµæœã«å½±éŸ¿ã•ã‚Œã¾ã™ã€‚
The revenue at the end of each shopping session depends on information on all pages in the session. 
å„ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æœ€å¾Œã®åç›Šã¯ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã®ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã®æƒ…å ±ã«ä¾å­˜ã—ã¾ã™ã€‚
When long-term impacts are significant, Bandits may not be the best formulation, but can still be a good baseline or starting point [15]. 
é•·æœŸçš„ãªå½±éŸ¿ãŒé‡è¦ãªå ´åˆã€ãƒãƒ³ãƒ‰itã¯æœ€é©ãªå®šå¼åŒ–ã§ã¯ãªã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€ä¾ç„¶ã¨ã—ã¦è‰¯ã„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¾ãŸã¯å‡ºç™ºç‚¹ã¨ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™[15]ã€‚

### 3 NAVIGATION BY REWARD å ±é…¬ã«ã‚ˆã‚‹ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³

The nature of reward signals in an application plays a major role in deciding the right Bandit algorithms.  
ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ãŠã‘ã‚‹å ±é…¬ä¿¡å·ã®æ€§è³ªã¯ã€é©åˆ‡ãªãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ±ºå®šã™ã‚‹ä¸Šã§é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚  
Figure 1 identifies six key properties of rewards that lead to 10 typical use cases in practice.  
å›³1ã¯ã€å®Ÿéš›ã®10ã®å…¸å‹çš„ãªä½¿ç”¨ä¾‹ã«è‡³ã‚‹å ±é…¬ã®6ã¤ã®é‡è¦ãªç‰¹æ€§ã‚’ç‰¹å®šã—ã¦ã„ã¾ã™ã€‚  
The highlighted two paths are perhaps the most common.  
å¼·èª¿ã•ã‚ŒãŸ2ã¤ã®çµŒè·¯ã¯ã€ãŠãã‚‰ãæœ€ã‚‚ä¸€èˆ¬çš„ãªã‚‚ã®ã§ã™ã€‚  
The key reward properties are:  
å ±é…¬ã®ä¸»è¦ãªç‰¹æ€§ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ï¼š

**Dimension The reward can be one-dimension (scalar) or multi-** dimension (vector).  
**æ¬¡å…ƒ å ±é…¬ã¯1æ¬¡å…ƒï¼ˆã‚¹ã‚«ãƒ©ãƒ¼ï¼‰ã¾ãŸã¯å¤šæ¬¡å…ƒï¼ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚**  
In the latter case, the task can be optimizing the vector reward (node 3), or maximizing one reward dimension subject to constraints on remaining dimensions (node 4).  
å¾Œè€…ã®å ´åˆã€ã‚¿ã‚¹ã‚¯ã¯ãƒ™ã‚¯ãƒˆãƒ«å ±é…¬ï¼ˆãƒãƒ¼ãƒ‰3ï¼‰ã‚’æœ€é©åŒ–ã™ã‚‹ã‹ã€æ®‹ã‚Šã®æ¬¡å…ƒã«å¯¾ã™ã‚‹åˆ¶ç´„ã®ä¸‹ã§1ã¤ã®å ±é…¬æ¬¡å…ƒã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼ˆãƒãƒ¼ãƒ‰4ï¼‰ã€‚  

**Distributional assumption In stochastic Bandits, the reward is** drawn from an unknown distribution.  
**åˆ†å¸ƒã®ä»®å®š ç¢ºç‡çš„ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã§ã¯ã€å ±é…¬ã¯æœªçŸ¥ã®åˆ†å¸ƒã‹ã‚‰å¼•ãå‡ºã•ã‚Œã¾ã™ã€‚**  
When the reward distribution may change slowly over time, as in many real-world applications, one can still treat it as a stochastic Bandit by constantly retraining the policy with new data.  
å ±é…¬åˆ†å¸ƒãŒå¤šãã®å®Ÿä¸–ç•Œã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚ˆã†ã«æ™‚é–“ã¨ã¨ã‚‚ã«ã‚†ã£ãã‚Šã¨å¤‰åŒ–ã™ã‚‹å ´åˆã§ã‚‚ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§ãƒãƒªã‚·ãƒ¼ã‚’å¸¸ã«å†è¨“ç·´ã™ã‚‹ã“ã¨ã§ã€ä¾ç„¶ã¨ã—ã¦ç¢ºç‡çš„ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã¨ã—ã¦æ‰±ã†ã“ã¨ãŒã§ãã¾ã™ã€‚  
In adversarial Bandits, there is no probabilistic assumption on the reward (node 7).  
æ•µå¯¾çš„ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã§ã¯ã€å ±é…¬ã«é–¢ã™ã‚‹ç¢ºç‡çš„ãªä»®å®šã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆãƒãƒ¼ãƒ‰7ï¼‰ã€‚  

**Relativity While a reward usually measures how good an action** is, in some problems like ranking one can also work with relative rewards that compare actions, as in dueling Bandits (node 8).  
**ç›¸å¯¾æ€§ å ±é…¬ã¯é€šå¸¸ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®è‰¯ã•ã‚’æ¸¬å®šã—ã¾ã™ãŒã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ã‚ˆã†ãªä¸€éƒ¨ã®å•é¡Œã§ã¯ã€ãƒ‡ãƒ¥ã‚¨ãƒªãƒ³ã‚°ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆï¼ˆãƒãƒ¼ãƒ‰8ï¼‰ã®ã‚ˆã†ã«ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¯”è¼ƒã™ã‚‹ç›¸å¯¾çš„ãªå ±é…¬ã‚’æ‰±ã†ã“ã¨ã‚‚ã§ãã¾ã™ã€‚**  

**Granularity When actions are combinatorial (see section 4), the** reward can be for the entire action (node 9), or can provide signal for subactions that comprise the action (node 10).  
**ç²’åº¦ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒçµ„ã¿åˆã‚ã›çš„ã§ã‚ã‚‹å ´åˆï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³4ã‚’å‚ç…§ï¼‰ã€å ±é…¬ã¯å…¨ä½“ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆãƒãƒ¼ãƒ‰9ï¼‰ã«å¯¾ã—ã¦ã§ã‚ã‚‹ã‹ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ§‹æˆã™ã‚‹ã‚µãƒ–ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«å¯¾ã™ã‚‹ä¿¡å·ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼ˆãƒãƒ¼ãƒ‰10ï¼‰ã€‚**  
The latter setting is also known as semi-bandits.  
å¾Œè€…ã®è¨­å®šã¯ã‚»ãƒŸãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã¨ã—ã¦ã‚‚çŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚  

**Delay Practical limitations like software constraints may prevent** rewards from being observed before the next actions have to be taken.  
**é…å»¶ ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã®åˆ¶ç´„ã®ã‚ˆã†ãªå®Ÿç”¨çš„ãªåˆ¶é™ã«ã‚ˆã‚Šã€æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–ã‚‹å‰ã«å ±é…¬ãŒè¦³å¯Ÿã•ã‚Œã‚‹ã®ã‚’å¦¨ã’ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚**  
There are two types of reward delays: bounded (node 5) and indefinite (node 6), depending on whether we have a reasonably small upper bound for the delay.  
å ±é…¬ã®é…å»¶ã«ã¯2ç¨®é¡ã‚ã‚Šã€åˆ¶ç´„ã•ã‚ŒãŸã‚‚ã®ï¼ˆãƒãƒ¼ãƒ‰5ï¼‰ã¨ç„¡æœŸé™ã®ã‚‚ã®ï¼ˆãƒãƒ¼ãƒ‰6ï¼‰ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€é…å»¶ã«å¯¾ã—ã¦åˆç†çš„ã«å°ã•ãªä¸Šé™ãŒã‚ã‚‹ã‹ã©ã†ã‹ã«ã‚ˆã‚Šã¾ã™ã€‚  

**Value type Reward can be binary (node 1) or numerical (node 2).**  
**å€¤ã®ã‚¿ã‚¤ãƒ— å ±é…¬ã¯ãƒã‚¤ãƒŠãƒªï¼ˆãƒãƒ¼ãƒ‰1ï¼‰ã¾ãŸã¯æ•°å€¤ï¼ˆãƒãƒ¼ãƒ‰2ï¼‰ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚**  
These two leaf nodes are unique as value type must be defined for the other seven nodes to complete the reward formulation.  
ã“ã‚Œã‚‰ã®2ã¤ã®ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ã¯ã€å ±é…¬ã®å®šç¾©ã‚’å®Œæˆã•ã›ã‚‹ãŸã‚ã«ä»–ã®7ã¤ã®ãƒãƒ¼ãƒ‰ã«å¯¾ã—ã¦å€¤ã®ã‚¿ã‚¤ãƒ—ã‚’å®šç¾©ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ã§ã™ã€‚  
We take this into consideration for algorithm recommendation in Table 1.  
ç§ãŸã¡ã¯ã€è¡¨1ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¨å¥¨ã«ã“ã‚Œã‚’è€ƒæ…®ã—ã¾ã™ã€‚  

We design the structure in Figure 1 so that it covers common use cases in E-commerce.  
ç§ãŸã¡ã¯ã€å›³1ã®æ§‹é€ ã‚’è¨­è¨ˆã—ã€Eã‚³ãƒãƒ¼ã‚¹ã«ãŠã‘ã‚‹ä¸€èˆ¬çš„ãªä½¿ç”¨ä¾‹ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã™ã€‚  
Leaf nodes 3â€“10 are not exhaustive as the splits are not mutually exclusive.  
ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰3ã€œ10ã¯ã€åˆ†å‰²ãŒç›¸äº’ã«æ’ä»–çš„ã§ãªã„ãŸã‚ã€ç¶²ç¾…çš„ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚  
For instance, adversarial Bandits can also be a dueling one [30] and there can be delay in reward [22].  
ä¾‹ãˆã°ã€æ•µå¯¾çš„ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã¯ãƒ‡ãƒ¥ã‚¨ãƒªãƒ³ã‚°ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã§ã‚ã‚‹å¯èƒ½æ€§ã‚‚ã‚ã‚Š[30]ã€å ±é…¬ã«é…å»¶ãŒã‚ã‚‹å ´åˆã‚‚ã‚ã‚Šã¾ã™[22]ã€‚  
In practice, however, such combinations appear uncommon.  
ã—ã‹ã—ã€å®Ÿéš›ã«ã¯ãã®ã‚ˆã†ãªçµ„ã¿åˆã‚ã›ã¯ä¸€èˆ¬çš„ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚  

In Table 1, for each leaf note we list example business problems with suggested algorithms.  
è¡¨1ã§ã¯ã€å„ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ã«å¯¾ã—ã¦ã€ææ¡ˆã•ã‚ŒãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æŒã¤ãƒ“ã‚¸ãƒã‚¹å•é¡Œã®ä¾‹ã‚’ãƒªã‚¹ãƒˆã—ã¾ã™ã€‚  
Given this paperâ€™s focus, we recommend algorithms that are empirically validated, especially those that find wide applications in practice.  
æœ¬è«–æ–‡ã®ç„¦ç‚¹ã‚’è€ƒæ…®ã—ã€çµŒé¨“çš„ã«æ¤œè¨¼ã•ã‚ŒãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ç‰¹ã«å®Ÿéš›ã«åºƒãå¿œç”¨ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã‚’æ¨å¥¨ã—ã¾ã™ã€‚  
For leaf nodes 3â€“10, we may have two suggested benchmark papers when binary and numerical reward are both common.  
ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰3ã€œ10ã«ã¤ã„ã¦ã¯ã€ãƒã‚¤ãƒŠãƒªå ±é…¬ã¨æ•°å€¤å ±é…¬ã®ä¸¡æ–¹ãŒä¸€èˆ¬çš„ãªå ´åˆã«ã€2ã¤ã®ææ¡ˆã•ã‚ŒãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è«–æ–‡ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚  

To find Bandit solutions for a given business problem, one can use Figure 1 as a guide to land in the most relevant node, then refer to Table 1 for similar applications and algorithmic suggestions.  
ç‰¹å®šã®ãƒ“ã‚¸ãƒã‚¹å•é¡Œã«å¯¾ã™ã‚‹ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã«ã€å›³1ã‚’ã‚¬ã‚¤ãƒ‰ã¨ã—ã¦ä½¿ç”¨ã—ã¦æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ãƒãƒ¼ãƒ‰ã«åˆ°é”ã—ã€ãã®å¾Œã€è¡¨1ã‚’å‚ç…§ã—ã¦é¡ä¼¼ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ææ¡ˆã‚’ç¢ºèªã§ãã¾ã™ã€‚  
We emphasize that there are no universally best algorithms, but expect the suggested references offer a good starting point for algorithm development and experimentation.  
ç§ãŸã¡ã¯ã€æ™®éçš„ã«æœ€è‰¯ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯å­˜åœ¨ã—ãªã„ã“ã¨ã‚’å¼·èª¿ã—ã¾ã™ãŒã€ææ¡ˆã•ã‚ŒãŸå‚è€ƒæ–‡çŒ®ãŒã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é–‹ç™ºã¨å®Ÿé¨“ã®è‰¯ã„å‡ºç™ºç‚¹ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¦ã„ã¾ã™ã€‚  



### 4 NAVIGATION BY ACTION

**Figure 2: Illustration of three action types**  
**å›³2: 3ã¤ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—ã®å›³è§£**  

The action set ğ´ also plays an important role in determining the right bandit algorithm.  
ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆğ´ã¯ã€é©åˆ‡ãªãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ±ºå®šã™ã‚‹ä¸Šã§é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚ 
Here, we identify two properties: action type and action set size.  
**ã“ã“ã§ã¯ã€2ã¤ã®ç‰¹æ€§ã‚’ç‰¹å®šã—ã¾ã™ï¼šã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã€‚**
The cases based on these properties are not exclusive to those identified in the previous section, but are orthogonal in many scenarios.  
ã“ã‚Œã‚‰ã®ç‰¹æ€§ã«åŸºã¥ãã‚±ãƒ¼ã‚¹ã¯ã€å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ç‰¹å®šã•ã‚ŒãŸã‚‚ã®ã«é™å®šã•ã‚Œã‚‹ã‚ã‘ã§ã¯ãªãã€å¤šãã®ã‚·ãƒŠãƒªã‚ªã§ç›´äº¤ã—ã¦ã„ã¾ã™ã€‚  

Figure 2 shows three common action types: single, slate and combinatorial.  
å›³2ã¯ã€3ã¤ã®ä¸€èˆ¬çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼šã‚·ãƒ³ã‚°ãƒ«ã€ã‚¹ãƒ¬ãƒ¼ãƒˆã€ãã—ã¦ã‚³ãƒ³ãƒ“ãƒŠãƒˆãƒªã‚¢ãƒ«ã€‚  
In the first, the action set is a set of items.  
æœ€åˆã®ã‚¿ã‚¤ãƒ—ã§ã¯ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆã¯ã‚¢ã‚¤ãƒ†ãƒ ã®é›†åˆã§ã™ã€‚  

The set is often small and finite, but can also be large or even infinite.  
ã“ã®ã‚»ãƒƒãƒˆã¯ã—ã°ã—ã°å°ã•ãæœ‰é™ã§ã™ãŒã€å¤§ãã„å ´åˆã‚„ç„¡é™ã®å ´åˆã‚‚ã‚ã‚Šã¾ã™ã€‚  

In the second, the action is a slate consisting of a ranked list of items.  
2ç•ªç›®ã®ã‚¿ã‚¤ãƒ—ã§ã¯ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒ©ãƒ³ã‚¯ä»˜ã‘ã•ã‚ŒãŸãƒªã‚¹ãƒˆã‹ã‚‰ãªã‚‹ã‚¹ãƒ¬ãƒ¼ãƒˆã§ã™ã€‚  
The challenge is the exponentially many possible permutations that require more efficient algorithms.  
èª²é¡Œã¯ã€ã‚ˆã‚ŠåŠ¹ç‡çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å¿…è¦ã¨ã™ã‚‹æŒ‡æ•°é–¢æ•°çš„ã«å¤šãã®å¯èƒ½ãªé †åˆ—ã§ã™ã€‚  
Furthermore, we need to consider two effects: position bias for actions shown at different slots and item diversity in the overall slate.  
ã•ã‚‰ã«ã€ç•°ãªã‚‹ã‚¹ãƒ­ãƒƒãƒˆã§è¡¨ç¤ºã•ã‚Œã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ä½ç½®ãƒã‚¤ã‚¢ã‚¹ã¨ã€å…¨ä½“ã®ã‚¹ãƒ¬ãƒ¼ãƒˆã«ãŠã‘ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®å¤šæ§˜æ€§ã¨ã„ã†2ã¤ã®åŠ¹æœã‚’è€ƒæ…®ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚  

In the third, the action is a combinatorial object (such as content layout on a web page), consisting of sub-actions coming from different sets.  
3ç•ªç›®ã®ã‚¿ã‚¤ãƒ—ã§ã¯ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯ç•°ãªã‚‹ã‚»ãƒƒãƒˆã‹ã‚‰ã®ã‚µãƒ–ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã§æ§‹æˆã•ã‚Œã‚‹çµ„ã¿åˆã‚ã›ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ä¸Šã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãªã©ï¼‰ã§ã™ã€‚  
The algorithmic challenge is often in dealing with combinatorial explosions of actions, and with interaction effects between sub-actions.  
ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®èª²é¡Œã¯ã€ã—ã°ã—ã°ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®çµ„ã¿åˆã‚ã›çˆ†ç™ºã‚„ã‚µãƒ–ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é–“ã®ç›¸äº’ä½œç”¨åŠ¹æœã«å¯¾å‡¦ã™ã‚‹ã“ã¨ã«ã‚ã‚Šã¾ã™ã€‚ 
In the literature, slate bandits are sometimes referred to as combinatorial.  
æ–‡çŒ®ã§ã¯ã€ã‚¹ãƒ¬ãƒ¼ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã¯æ™‚æŠ˜ã‚³ãƒ³ãƒ“ãƒŠãƒˆãƒªã‚¢ãƒ«ã¨å‘¼ã°ã‚Œã¾ã™ã€‚  
Table 2 lists business problems and recommended work by these two types.  
è¡¨2ã¯ã€ã“ã‚Œã‚‰2ã¤ã®ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹ãƒ“ã‚¸ãƒã‚¹å•é¡Œã¨æ¨å¥¨ã•ã‚Œã‚‹ä½œæ¥­ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚  

Another action property is the size of the action set.  
åˆ¥ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ç‰¹æ€§ã¯ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã§ã™ã€‚  
In Figure 3, we list representative business applications with increasing size.  
å›³3ã§ã¯ã€ã‚µã‚¤ã‚ºãŒå¢—åŠ ã™ã‚‹ä»£è¡¨çš„ãªãƒ“ã‚¸ãƒã‚¹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç¤ºã—ã¾ã™ã€‚  
Many Bandit use cases have discrete actions.  
å¤šãã®ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«ã¯é›¢æ•£çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã™ã€‚  
When the action space is small, actions ID can be considered as a categorical value and encoded as a set of binary variables in the reward formulation.  
ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“ãŒå°ã•ã„å ´åˆã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³IDã¯ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å€¤ã¨è¦‹ãªã•ã‚Œã€å ±é…¬ã®å®šå¼åŒ–ã«ãŠã„ã¦ãƒã‚¤ãƒŠãƒªå¤‰æ•°ã®ã‚»ãƒƒãƒˆã¨ã—ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚
When the action space becomes large, we use features to represent actions.  
ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“ãŒå¤§ãããªã‚‹ã¨ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¾ã™ã‚‹ãŸã‚ã«ç‰¹å¾´ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚  
Such action featurization not only reduces the dimension of variables but also enables generalization across actions which mitigates action cold-start problem.  
ã“ã®ã‚ˆã†ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ç‰¹å¾´åŒ–ã¯ã€å¤‰æ•°ã®æ¬¡å…ƒã‚’å‰Šæ¸›ã™ã‚‹ã ã‘ã§ãªãã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é–“ã®ä¸€èˆ¬åŒ–ã‚’å¯èƒ½ã«ã—ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆå•é¡Œã‚’è»½æ¸›ã—ã¾ã™ã€‚ 
While action and contextual features contain different meanings, they can be handled in a similar manner.  
ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ç‰¹å¾´ã¯ç•°ãªã‚‹æ„å‘³ã‚’æŒã¡ã¾ã™ãŒã€åŒæ§˜ã®æ–¹æ³•ã§æ‰±ã†ã“ã¨ãŒã§ãã¾ã™ã€‚  

More discussions are found in section 5.  
è©³ç´°ãªè­°è«–ã¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³5ã«ã‚ã‚Šã¾ã™ã€‚  

It is also common to see continuous actions in practice.  
å®Ÿéš›ã«ã¯é€£ç¶šçš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¦‹ã‚‹ã“ã¨ã‚‚ä¸€èˆ¬çš„ã§ã™ã€‚  

In this case, the continuous action set often has a natural distance metric, so that one can use tools like Gaussian process to solve the Bandit problem.  
ã“ã®å ´åˆã€é€£ç¶šã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆã¯ã—ã°ã—ã°è‡ªç„¶ãªè·é›¢ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’æŒã¡ã€ã‚¬ã‚¦ã‚¹éç¨‹ã®ã‚ˆã†ãªãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã‚’è§£æ±ºã§ãã¾ã™ã€‚  



### 5 OTHER TOPICS ãã®ä»–ã®ãƒˆãƒ”ãƒƒã‚¯

This section discusses feature engineering, offline policy evaluation, and best-arm identification, three important topics in Bandit applications to complement the previous sections.
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒãƒªã‚·ãƒ¼è©•ä¾¡ã€ãŠã‚ˆã³ãƒ™ã‚¹ãƒˆã‚¢ãƒ¼ãƒ ã®ç‰¹å®šã¨ã„ã†ã€ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ãŠã‘ã‚‹é‡è¦ãª3ã¤ã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦è­°è«–ã—ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### 5.1 ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

In many applications, we use features to deal with large context or action sets more efficiently. 
å¤šãã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€å¤§ããªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆã‚’ã‚ˆã‚ŠåŠ¹ç‡çš„ã«æ‰±ã†ãŸã‚ã«ç‰¹å¾´é‡ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ 
The expected reward ğ‘Ÿ can be written as a function of action features ğ“ğ’‚ and contextual features ğ“ğ’™ : ğ¸ [ğ‘Ÿ ] = ğ‘“(ğ“ğ’‚, ğ“ğ’™). 
æœŸå¾…å ±é…¬ $r$ ã¯ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç‰¹å¾´é‡ $\phi_a$ ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ $\phi_x$ ã®é–¢æ•°ã¨ã—ã¦æ›¸ãã“ã¨ãŒã§ãã¾ã™: $E[r] = f(\phi_a, \phi_x)$ã€‚
Feature engineering in Bandit involves selection/preprocessing of ğ“ğ’‚ and ğ“ğ’™ and their interaction terms. 
ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«ãŠã‘ã‚‹ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã¯ã€$\phi_a$ ã¨ $\phi_x$ ã®é¸æŠ/å‰å‡¦ç†ãŠã‚ˆã³ãã‚Œã‚‰ã®ç›¸äº’ä½œç”¨é …ã‚’å«ã¿ã¾ã™ã€‚
Linear Bandits are the most studied in the literature where ğ¸ [ğ‘Ÿ ] is assumed to be linear in the features. 
ç·šå½¢ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã¯ã€æ–‡çŒ®ã§æœ€ã‚‚ç ”ç©¶ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã§ã€$E[r]$ ã¯ç‰¹å¾´é‡ã«å¯¾ã—ã¦ç·šå½¢ã§ã‚ã‚‹ã¨ä»®å®šã•ã‚Œã¦ã„ã¾ã™ã€‚
To model non-linearity especially when the size(s) of ğ“ğ’‚ or/and ğ“ğ’™ is large, we can learn lower-dimension embeddings from the raw features (ğ“ğ’‚ or/and ğ“ğ’™) and put the embeddings in the reward function instead [38, 39]. 
ç‰¹ã« $\phi_a$ ã¾ãŸã¯/ãŠã‚ˆã³ $\phi_x$ ã®ã‚µã‚¤ã‚ºãŒå¤§ãã„å ´åˆã«éç·šå½¢æ€§ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ãŸã‚ã«ã€ç”Ÿã®ç‰¹å¾´é‡ï¼ˆ$\phi_a$ ã¾ãŸã¯/ãŠã‚ˆã³ $\phi_x$ï¼‰ã‹ã‚‰ä½æ¬¡å…ƒã®åŸ‹ã‚è¾¼ã¿ã‚’å­¦ç¿’ã—ã€åŸ‹ã‚è¾¼ã¿ã‚’å ±é…¬é–¢æ•°ã«ä»£ã‚ã‚Šã«å…¥ã‚Œã‚‹ã“ã¨ãŒã§ãã¾ã™ [38, 39]ã€‚
Embedding generation techniques for supervised learning generally apply to Bandits [40]. 
æ•™å¸«ã‚ã‚Šå­¦ç¿’ã®ãŸã‚ã®åŸ‹ã‚è¾¼ã¿ç”ŸæˆæŠ€è¡“ã¯ä¸€èˆ¬çš„ã«ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«é©ç”¨ã•ã‚Œã¾ã™ [40]ã€‚
Another way to relax the linear-reward assumption is to use non-linear Bandits where reward function becomes nonlinear in feature vectors [e.g., 31]. 
ç·šå½¢å ±é…¬ã®ä»®å®šã‚’ç·©å’Œã™ã‚‹åˆ¥ã®æ–¹æ³•ã¯ã€å ±é…¬é–¢æ•°ãŒç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã«å¯¾ã—ã¦éç·šå½¢ã«ãªã‚‹éç·šå½¢ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ [e.g., 31]ã€‚

### 5.2 ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒãƒªã‚·ãƒ¼è©•ä¾¡

Testing a Bandit policy in real user traffic is often expensive, and poses risks on user experiences. 
ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒãƒªã‚·ãƒ¼ã‚’å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã§ãƒ†ã‚¹ãƒˆã™ã‚‹ã“ã¨ã¯ã—ã°ã—ã°é«˜ä¾¡ã§ã‚ã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã«ãƒªã‚¹ã‚¯ã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚
It is common to evaluate a new policy offline before deploying it. 
æ–°ã—ã„ãƒãƒªã‚·ãƒ¼ã‚’å±•é–‹ã™ã‚‹å‰ã«ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§è©•ä¾¡ã™ã‚‹ã“ã¨ã¯ä¸€èˆ¬çš„ã§ã™ã€‚
A key challenge in offline policy evaluation is that we do not know how users would have reacted to actions different from the one in the log data, since the data only have rewards for selected actions. 
**ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒãƒªã‚·ãƒ¼è©•ä¾¡ã®é‡è¦ãªèª²é¡Œã¯ã€ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã«ã‚ã‚‹è¡Œå‹•ã¨ã¯ç•°ãªã‚‹è¡Œå‹•ã«å¯¾ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã©ã®ã‚ˆã†ã«åå¿œã—ãŸã‹ã‚’çŸ¥ã‚‹ã“ã¨ãŒã§ããªã„ã“ã¨**ã§ã™ã€‚ãªãœãªã‚‰ã€ãƒ‡ãƒ¼ã‚¿ã«ã¯é¸æŠã•ã‚ŒãŸè¡Œå‹•ã«å¯¾ã™ã‚‹å ±é…¬ã—ã‹å«ã¾ã‚Œã¦ã„ãªã„ã‹ã‚‰ã§ã™ã€‚
This counterfactual nature makes Bandit offline evaluation similar to causal inference where we want to infer the average reward $E_\pi [r]$ (the causal effect) if policy $\pi$ is used to choose actions. 
ã“ã®åå®Ÿä»®æƒ³çš„ãªæ€§è³ªã«ã‚ˆã‚Šã€ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡ã¯å› æœæ¨è«–ã«ä¼¼ã¦ãŠã‚Šã€ãƒãƒªã‚·ãƒ¼ $\pi$ ãŒè¡Œå‹•ã‚’é¸æŠã™ã‚‹å ´åˆã®å¹³å‡å ±é…¬ $E_\pi [r]$ï¼ˆå› æœåŠ¹æœï¼‰ã‚’æ¨æ¸¬ã—ãŸã„ã¨è€ƒãˆã¾ã™ã€‚
There exist effective approaches to evaluating a stationary policy, including simulation [41], inverse propensity scoring [42, 43], doubly robust evaluation [44], and selfnormalized inverse propensity estimators [45]. 
å®šå¸¸ãƒãƒªã‚·ãƒ¼ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®åŠ¹æœçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã¯ã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ [41]ã€é€†å‚¾å‘ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° [42, 43]ã€äºŒé‡ãƒ­ãƒã‚¹ãƒˆè©•ä¾¡ [44]ã€ãŠã‚ˆã³è‡ªå·±æ­£è¦åŒ–é€†å‚¾å‘æ¨å®šå™¨ [45] ãŒå«ã¾ã‚Œã¾ã™ã€‚
Typically, offline evaluation becomes more challenging with a larger action set. 
**ä¸€èˆ¬çš„ã«ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡ã¯ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆãŒå¤§ãããªã‚‹ã»ã©é›£ã—ããªã‚Šã¾ã™ã€‚**
For a slate Bandit, pseudoinverse estimator is available to account for position bias [46]. 
ã‚¹ãƒ¬ãƒ¼ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®å ´åˆã€ä½ç½®ãƒã‚¤ã‚¢ã‚¹ã‚’è€ƒæ…®ã™ã‚‹ãŸã‚ã®æ“¬ä¼¼é€†æ¨å®šå™¨ãŒåˆ©ç”¨å¯èƒ½ã§ã™ [46]ã€‚
Offline evaluation for non-stationary Bandits remains challenging [47, 48], with opportunities for further research. 
éå®šå¸¸ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡ã¯ä¾ç„¶ã¨ã—ã¦é›£ã—ã [47, 48]ã€ã•ã‚‰ãªã‚‹ç ”ç©¶ã®æ©Ÿä¼šãŒã‚ã‚Šã¾ã™ã€‚

### 5.3 Best-arm Identification æœ€è‰¯ã‚¢ãƒ¼ãƒ ã®ç‰¹å®š

In some bandit applications, our goal is not to maximize reward during an experiment, but to identify the best action (e.g., best marketing campaign strategy) at the end of the experiment. 
ã„ãã¤ã‹ã®ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€ç§ãŸã¡ã®ç›®æ¨™ã¯å®Ÿé¨“ä¸­ã«å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã§ã¯ãªãã€å®Ÿé¨“ã®çµ‚äº†æ™‚ã«æœ€è‰¯ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆä¾‹ï¼šæœ€è‰¯ã®ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³æˆ¦ç•¥ï¼‰ã‚’ç‰¹å®šã™ã‚‹ã“ã¨ã§ã™ã€‚ 
This problem, known as best-arm identification [49â€“51], shares the same goal as conventional A/B/N testing [52], but can be statistically more efficient by adaptively selecting actions during an experiment.
ã“ã®å•é¡Œã¯æœ€è‰¯ã‚¢ãƒ¼ãƒ ã®ç‰¹å®šï¼ˆbest-arm identificationï¼‰ã¨ã—ã¦çŸ¥ã‚‰ã‚Œã€å¾“æ¥ã®A/B/Nãƒ†ã‚¹ãƒˆã¨åŒã˜ç›®æ¨™ã‚’å…±æœ‰ã—ã¦ã„ã¾ã™ãŒã€å®Ÿé¨“ä¸­ã«ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©å¿œçš„ã«é¸æŠã™ã‚‹ã“ã¨ã§çµ±è¨ˆçš„ã«ã‚ˆã‚ŠåŠ¹ç‡çš„ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚



### 6 CONCLUSIONS çµè«–

We presented a structured map for the world of Bandits, with the hope to guide practitioners to navigate to practical Bandit algorithms. 
ç§ãŸã¡ã¯ã€Banditsã®ä¸–ç•Œã®ãŸã‚ã®æ§‹é€ åŒ–ã•ã‚ŒãŸãƒãƒƒãƒ—ã‚’æç¤ºã—ã€å®Ÿå‹™è€…ãŒå®Ÿç”¨çš„ãªBanditã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ãƒŠãƒ“ã‚²ãƒ¼ãƒˆã§ãã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¦ã„ã¾ã™ã€‚
The work is not attempted to provide a comprehensive map. 
ã“ã®ä½œæ¥­ã¯ã€åŒ…æ‹¬çš„ãªãƒãƒƒãƒ—ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã›ã‚“ã€‚
Instead, it focuses on a few key decision points that often affect how Bandit algorithms are chosen. 
ä»£ã‚ã‚Šã«ã€Banditã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é¸æŠã«ã—ã°ã—ã°å½±éŸ¿ã‚’ä¸ãˆã‚‹ã„ãã¤ã‹ã®é‡è¦ãªæ„æ€æ±ºå®šãƒã‚¤ãƒ³ãƒˆã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚
We hope it reduces the gap in connecting applications to appropriate algorithms. 
ç§ãŸã¡ã¯ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨é©åˆ‡ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’çµã³ã¤ã‘ã‚‹ã‚®ãƒ£ãƒƒãƒ—ã‚’æ¸›å°‘ã•ã›ã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¦ã„ã¾ã™ã€‚
