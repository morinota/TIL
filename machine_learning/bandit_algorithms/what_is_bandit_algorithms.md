## 0.1. 参考

- https://blog.albert2005.co.jp/2017/01/23/%E3%83%90%E3%83%B3%E3%83%87%E3%82%A3%E3%83%83%E3%83%88%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0%E3%80%80%E5%9F%BA%E6%9C%AC%E7%B7%A8/

# 1. 基本

## 1.1. bandit algorithmにおける問題設定

- 選択肢はいくつもあるが、どの選択肢が効果が高いのかは事前にはわからない.
- 限られた試行回数でできる限りいい選択肢を選んでいき、トータルの報酬を最大化したい.

一点目の"どの選択肢が良いか、事前に情報がない"という点が重要.
もし各選択肢について情報があるのであれば、それを学習データとして教師あり学習による予測モデルを作る事で事足りる.
bandit algorithmでは、学習データがない状況からどの選択肢が良いかを学びながら、その過程で得られる報酬を最大化する事を目的としている.

## 1.2. A/Bテストとの比較

- A/Bテスト:
  - AかBの2つの選択肢のどちらがより優れているかを統計学的仮説検定を元に判断する
  - 全体を２つのグループに分割し、あるグループにはAという選択肢、もう一方のグループにはBを適用し、その結果からAとBのどちらが優れているかを決定する.
  - また、一度決定した後は優れた選択肢を選びつづけるというのが一般的.
  - つまり**A/Bテストの目的は最適な選択肢を見つける事**.
  - このように**最適な選択肢を見つけ出す枠組み**を"**最適腕識別**"と言い、A/Bテストはこの最適腕識別に分類される.
- bandit algorithms:
  - 目的は、**累積報酬の最大化**.
    - 有限回の試行の中で報酬を最大化するには、優れたarmを多く引き、劣ったarmは引く回数を抑えることが必要となる.
    - （banditの文脈では**選択肢のことをarm**と呼ぶ）
    - しかし、事前に各armの良し悪しはわからないので、**どのarmが良いかを探りつつ（探索）**、**良さそうなarmほど積極的に引いていく（活用）**をバランスさせるのがbandit algorithmの本質.
  - どのarmが良いかについて:
    - **それまでに得られた報酬の標本平均などをarmごとにスコア化、それらを比較することで判断する**.
    - そのため、ある時点で選択したarmの報酬が高ければ選択したarmのスコアが高くなり、報酬が低ければ選択したarmのスコアが低くなる.
    - このように、**随時情報を更新しながらarmの良し悪しを判断**していく
  - A/Bテストと比べた機会損失の度合い?
    - bandit algorithmsは、armを選択した結果得られる報酬を**動的に反映**し、次のarmの選択に活かす.
      - -> A/Bテストに比べて機会損失の低減が見込める.
    - また、**コンテンツの入れ替わりが多いようなケース**ではA/Bテストを都度設計するのは手間だが、bandit ならシステム化できるというメリットもある.

## 1.3. bandit algorithmsの代表的な方策:

### 1.3.1. $\epsilon$-greedy方策

- 最も単純な方策.
- 各ステップ毎に確率$\epsilon$で探索、$1 - \epsilon$で活用を行う.
- 具体的なアプローチは以下.
  - 探索時: 全てのarmをランダムに選択.
  - 活用時: 過去の試行の結果から、報酬の標本平均$\hat{\mu}_i$の最も高かったarmを選択.
- シンプルで分かりやすいが、**最適な探索回数を見つけるのが困難**. (i.e. 探索と活用のバランス調整が難しい)
  - 探索が少ないと...最適なarmを発見できず、活用時に最適でないarmを引き続ける.
  - 探索が多いと...最適でないarmを余分に引いてしまう.

### 1.3.2. UCB(Upper Confidence Base)方策

- 累積報酬を最大化する為には...最適なarmを多く引く事が重要.
- ただ選択回数が少ないarmについては、報酬が正確に推定できてない可能性(arm毎のスコアの不確実性)を考慮する必要がある.
- UCBでは、armを選択する際に毎回以下の式で表されるスコアを算出. 最もスコアの高いarmを引く.
  - $\bar{\mu}_i(t) = \hat{\mu}_i(t) + \sqrt{\frac{\log t}{2 N_i(t)}}$
  - $\bar{\mu}_i(t)$: 時刻tのarm iのスコア.
  - $\hat{\mu}_i(t)$: 時刻tのarm iの(報酬の?)標本平均.
  - $N_i(t)$: 時刻tまでのarm iの選択回数.
  - 標本平均に補正項を加えた値がスコア. 選択回数が少ないarm程、選ばれやすくなる.
- つまり、単純に標本平均の大きなarmが選択される時は**活用**が行われ、標本平均は小さいが、選択回数が少ないarmが選択される時は**探索**が行なわれていると解釈できる.
- UCBでは探索と活用のバランスを上手く取りながらarmの選択を行い、報酬の最大化を目指す.

### 1.3.3. Thompson Sampling方策

- "そのarmが最適なarmである確率"に注目した**確率一致法**、という方法論がある.
- Thompson Samplingはベイズ統計の枠組みを確率一致法に適用した方策.
- arm iの期待値のパラメータ$\mu_i$が何らかの事前分布$\pi_i(\mu_i)$から生成されるとし、時刻tまでの観測$H(t)$が得られた時の期待値$\mu_i$の事後分布を考える.

"arm iが最適"という命題は、「$\mu_i=x$, かつ全ての$j \neq i$で$\mu_j \leq x$」と解釈できる.(ここでxは$\mu_i$の報酬の期待値?)
数式を用いて以下のように表現できる.

$$
\pi(\mu_i = \mu^*|H(t)) = \int \pi(x_i|H(t))(
    \Pi_{j \neq i} \int_{x_j \leq x_i}{\pi_j{x_j|H(t)}}
    ) d x_i
$$

上式を各armについて計算し事後確率を比較すれば良いのだが、**一般にこの式を計算する事は困難な事が知られている**.

しかし実際に事後確率を直接比較する必要はなく、以下の手続きを行うことで **「期待値最大の事後確率」に基づいたarmの選択**を実現することができる(?)

1. 各arm iの期待値 $x_i$の事後分布$\pi(x_i|H(t))$から乱数$\tilde{\mu}_i$を生成.
2. $\tilde{\mu}_i$を最大にするようなarm iを引く.

Thompson Sampling方策はUCB方策に比べて余分な探索が少なくなることが知られていて、有限の試行回数でより良い性能を達成することができる.

#### 連続値の報酬関数の場合のThompson Samplingって??

- Answer: **共役事前分布として"正規分布"もしくは"ガウスガンマ分布"が用いられる!**
  - (binary報酬の場合は、報酬分布パラメータの共役事前分布としてベータ分布が使われてたよね...!:thinking:)
- 詳細:
  - Thompson Samplingは、ベイズ推定スタイル。各アームの報酬分布のパラメータ(ex. 期待値や分散)が未知の時、過去の報酬データからそのパラメータの事後分布を更新していく。
  - 各アームaから得られる報酬rが正規分布に従うと仮定する場合...
    - 報酬分布のパラメータは、平均$\mu_a$と分散$\sigma_a^2$。
    - ここで分散が既知の場合は、平均$\mu_a$の共役事前分布として正規分布が使われる。
    - もし分散も未知の場合は、平均と分散の共役事前分布としてガウス-ガンマ分布が使われる。
