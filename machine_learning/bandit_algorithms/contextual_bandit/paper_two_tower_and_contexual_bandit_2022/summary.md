# Two-Stage Neural Contextual Bandits for Personalised News Recommendation

## ニュース推薦の問題設定

- ユーザにパーソナライズされたニュースを逐次的に推薦するニュース推薦システムを考え、全ユーザの累積報酬を最大化することを目指す。
  - 推薦システムはユーザとのインタラクション履歴から学習し、特定のユーザに対して、候補ニュースセットから選択された複数のニュースを表示する。
  - ニュースドメインの特徴
    - 候補ニュースセットの数が膨大で、かつ時間と共に動的に変化する点。
    - 多くのコールドユーザ（すなわち、履歴を持たないユーザ）が存在し、またユーザの興味は時間とともに変化する可能性がある。
- 上述のような推薦戦略を設計する方法は、contexual banditの分野で研究されている**sequential decision-making problem(逐次的な意思決定問題)として定式化**できる
  - ニュース推薦システムはエージェントとみなされる。
  - ニュースアイテムはアーム(選択肢)。
  - ユーザ及び/またはアイテムの埋め込み表現がコンテキストを形成する。
  - 具体的には...
    - 各試行 $t = 1, \ldots, N$ において、ユーザ $u_t$ と候補アームセット $A_t$ が与えられた場合、すべての $i \in A_t$ に対してアイテム埋め込み $x_i \in \mathbb{R}^{d_1}$ を生成し、ユーザ埋め込み $z_{u_t} \in \mathbb{R}^{d_2}$ をコンテキストとして使用する。
      - 以下では、曖昧さがない場合は $u_t$ の下付き文字tを省略する。
    - エージェントは、コンテキストが与えられた場合に、方策(policy) $\pi$ に従って $m \geq 1$ 個のニュースアイテムリスト $S_{rec}$ を推薦する。
    - その後、エージェントはフィードバック $\{y_{t,1}, \ldots, y_{t,m}\}$ を受け取る。
      - ここで、$y_{t,i} \in \{0, 1\}$ はイテレーション $t$ で推薦した アイテム $i$ に対して得られた報酬を示す。
  - 目標は、期待される累積後悔(cumulative regret)（定義1）を最小化するポリシーを設計することであり、これは期待される累積報酬を最大化することと同等**。

## UCB(upper confidence bound)アルゴリズム

## LinUCBアルゴリズム

## Generalized LinUCBアルゴリズム

- 論文タイトル「Parametric Bandits: The Generalized Linear Case」
- contextual bandit * UCBアルゴリズムの、GLM(一般化線形モデル)版って感じのやつ!
  - i.e. つまり、報酬が従う確率分布が正規分布以外の場合(ex. 報酬がbinaryだったり、カウントデータだったり...!)にも、LinUCBのようなアルゴリズムを適用できるように拡張(一般化)したやつ!
- GLB(Generalized Linear Bandit、一般化線形バンディット)
  - LinUCBなどの線形バンディットだと「報酬 = 文脈ベクトルとパラメータの掛け算」だけど、このモデルだと「報酬 = 文脈ベクトルとパラメータの掛け算の結果を、逆リンク関数で変換したもの」としてモデル化する。ここがポイント。
    - ex.)
      - 報酬がbinaryの場合: 逆リンク関数を $\frac{\exp(x)}{1 + \exp(x)}$ として、報酬をモデル化する。
      - 報酬がカウントデータの場合: 逆リンク関数を $\exp(x)$ として、報酬をモデル化する。
- GLM-UCBアルゴリズムの話
  - 流れ!
    - 推定: 過去の観測データからパラメータ $\hat{\theta}$ を計算する。
    - 探索ボーナス(exploration bonus): 見積もりが足りないアームには追加で「ボーナス」をつけて探索を優先する
    - 選択: 「報酬の期待値の推定値 + 探索ボーナス」が一番高いアームを選ぶ!
  - 特徴
    - 報酬の構造をモデル化するので、無駄に全部のアームを探索しなくて良くなる。
    - 理論的には、GLM-UCBの累積後悔は、「特徴量ベクトルの次元 $d$ 」には依存するけど、「アームの数$K$」には依存しないらしい。

### 探索ボーナス(Exploration Bonus)の算出方法

以下の式で計算される

$$
\beta_{t}[a] = \rho(t) ||m_a||_{M_{t-1}}
$$

ここで

- $\beta_{t}[a]$: アーム $a$ に対する探索ボーナス (タイムステップtによってデータの溜まり度合いが変わり、変化していくはず)
- $\rho(t)$: タイムステップ $t$ に基づく、探索ボーナスのスケーリング係数

- $||m_a||_{M_{t-1}}$: アーム $a$ の特徴量ベクトル $m_a$ に基づいた、「重み付きノルム」
  -

重みつきノルムは、以下のように計算される:

$$
||m_a||_{M_{t-1}} = \sqrt{m_a^T M_{t-1}^{-1} m_a}
$$

ここで、

- $m_a$: アーム $a$ の特徴量ベクトル (特徴量の次元数 $d$)
- $M_{t-1}$: タイムステップ $t-1$ までに得られた特徴量ベクトルの「設計行列」(design matrix)。($d \times d$ の行列)
- $M_{t-1}^{-1}$: 設計行列の逆行列

設計行列は以下のように計算される。
タイムステップ $t-1$ までに収集されたデータを要約する行列。**設計行列は、これまでのタイムステップで選ばれた全てのアームの特徴量ベクトルの外積和を取る**ことで計算される。

$$
M_{t-1} = \sum_{l=1}^{t-1} m_{A_k} m_{A_k}^T
$$

ここで、

- $m_{A_k}$: タイムステップ $k$ で選択されたアーム $A_k$ の特徴量ベクトル(次元数 $d$)
- $m_{A_k}^T$: $m_{A_k}$ の転置行列 (列ベクトルを転置してるので、行ベクトルになる)
- $m_{A_k} m_{A_k}^T$: ベクトル $m_{A_k}$ の外積 ($d \times d$ の行列になる)
  - (内積だと掛け算の順番が逆...!:thinking_face:)
  - 対称行列であることに注意!
  - 行列の要素は、特徴量ベクトルの成分の積で構成される。

## Two-Stage Neural Contextual Bandits
