
## Scalable Neural Contextual Bandit for Recommender Systems  
### Zheqing Zhu ##### Meta AI, Stanford University Menlo Park, CA, USA billzhu@meta.com  

#### Abstract è¦ç´„
High-quality recommender systems ought to deliver both innovative and relevant content through effective and exploratory interactions with users.  
é«˜å“è³ªãªæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®åŠ¹æœçš„ã‹ã¤æ¢ç´¢çš„ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’é€šã˜ã¦ã€é©æ–°çš„ã§é–¢é€£æ€§ã®ã‚ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æä¾›ã™ã‚‹ã¹ãã§ã™ã€‚  
Yet, supervised learning-based neural networks, which form the backbone of many existing recommender systems, only leverage recognized user interests, falling short when it comes to efficiently uncovering unknown user preferences.  
ã—ã‹ã—ã€æ—¢å­˜ã®å¤šãã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®åŸºç›¤ã‚’å½¢æˆã™ã‚‹æ•™å¸«ã‚ã‚Šå­¦ç¿’ã«åŸºã¥ããƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€èªè­˜ã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ã®ã¿ã‚’æ´»ç”¨ã—ã€æœªçŸ¥ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã‚’åŠ¹ç‡çš„ã«æ˜ã‚‰ã‹ã«ã™ã‚‹ã“ã¨ã«ã¯ä¸ååˆ†ã§ã™ã€‚  
While there has been some progress with neural contextual bandit algorithms towards enabling online exploration through neural networks, their onerous computational demands hinder widespread adoption in real-world recommender systems.  
ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’é€šã˜ã¦ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¢ç´¢ã‚’å¯èƒ½ã«ã™ã‚‹ãŸã‚ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«é–¢ã—ã¦ã¯ä¸€å®šã®é€²å±•ãŒã‚ã‚Šã¾ã—ãŸãŒã€ãã®å³ã—ã„è¨ˆç®—è¦æ±‚ã¯ã€å®Ÿä¸–ç•Œã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã§ã®åºƒç¯„ãªæ¡ç”¨ã‚’å¦¨ã’ã¦ã„ã¾ã™ã€‚  
In this work, we propose a scalable sample-efficient neural contextual bandit algorithm for recommender systems.  
æœ¬ç ”ç©¶ã§ã¯ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ãŸã‚ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã§ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã®è‰¯ã„ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆã—ã¾ã™ã€‚  
To do this, we design an epistemic neural network architecture, Epistemic Neural Recommendation (ENR), that enables Thompson sampling at a large scale.  
ã“ã‚Œã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«ã€ç§ãŸã¡ã¯å¤§è¦æ¨¡ãªThompsonã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å¯èƒ½ã«ã™ã‚‹ã‚¨ãƒ”ã‚¹ãƒ†ãƒŸãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€Epistemic Neural Recommendation (ENR)ã‚’è¨­è¨ˆã—ã¾ã—ãŸã€‚  
In two distinct large-scale experiments with real-world tasks, ENR significantly boosts clickthrough rates and user ratings by at least 9% and 6% respectively compared to state-of-the-art neural contextual bandit algorithms.  
å®Ÿä¸–ç•Œã®ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹2ã¤ã®ç•°ãªã‚‹å¤§è¦æ¨¡å®Ÿé¨“ã«ãŠã„ã¦ã€ENRã¯æœ€å…ˆç«¯ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨æ¯”è¼ƒã—ã¦ã€ã‚¯ãƒªãƒƒã‚¯ç‡ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼è©•ä¾¡ã‚’ãã‚Œãã‚Œå°‘ãªãã¨ã‚‚9%ã¨6%å‘ä¸Šã•ã›ã¾ã™ã€‚  
Furthermore, it achieves equivalent performance with at least 29% fewer user interactions compared to the best-performing baseline algorithm.  
ã•ã‚‰ã«ã€ENRã¯ã€æœ€ã‚‚å„ªã‚ŒãŸãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨æ¯”è¼ƒã—ã¦ã€å°‘ãªãã¨ã‚‚29%å°‘ãªã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã§åŒç­‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã¾ã™ã€‚  
Remarkably, while accomplishing these improvements, ENR demands orders of magnitude fewer computational resources than neural contextual bandit baseline algorithms.  
é©šãã¹ãã“ã¨ã«ã€ã“ã‚Œã‚‰ã®æ”¹å–„ã‚’é”æˆã—ãªãŒã‚‰ã€ENRã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚ˆã‚Šã‚‚æ¡é•ã„ã«å°‘ãªã„è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’è¦æ±‚ã—ã¾ã™ã€‚  


```md
### Benjamin Van Roy
##### Stanford University Stanford, CA, USA bvr@stanford.edu
**Figure 1: MIND Experiment Computation-Performance** **Tradeoff. Better Tradeoff towards Top Right. We expect full-** **scale NeuralUCB and NeuralTS to perform similarly to our** **methods, but require orders of magnitude more compute.**  
### ãƒ™ãƒ³ã‚¸ãƒ£ãƒŸãƒ³ãƒ»ãƒ´ã‚¡ãƒ³ãƒ»ãƒ­ã‚¤
##### ã‚¹ã‚¿ãƒ³ãƒ•ã‚©ãƒ¼ãƒ‰å¤§å­¦ ã‚«ãƒªãƒ•ã‚©ãƒ«ãƒ‹ã‚¢å·ã‚¹ã‚¿ãƒ³ãƒ•ã‚©ãƒ¼ãƒ‰ã€ã‚¢ãƒ¡ãƒªã‚« bvr@stanford.edu
**å›³1: MINDå®Ÿé¨“ã®è¨ˆç®—æ€§èƒ½ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã€‚å³ä¸Šã«å‘ã‹ã†ã»ã©è‰¯ã„ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã€‚ç§ãŸã¡ã¯ã€ãƒ•ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ã®NeuralUCBã¨NeuralTSãŒç§ãŸã¡ã®æ‰‹æ³•ã¨åŒæ§˜ã«æ©Ÿèƒ½ã™ã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¦ã„ã¾ã™ãŒã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã¯æ¡é•ã„ã«å¤šãå¿…è¦ã§ã™ã€‚**

#### Keywords
Recommender Systems, Contextual Bandits, Reinforcement Learning, Exploration vs Exploitation, Decision Making under Uncertainty  
#### ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã€å¼·åŒ–å­¦ç¿’ã€æ¢ç´¢ã¨æ´»ç”¨ã€ ä¸ç¢ºå®Ÿæ€§ä¸‹ã§ã®æ„æ€æ±ºå®š

#### 1 Introduction
Recommender systems (RS), paramount in personalizing digital content, critically influence the quality of information accessed via the Internet. Traditionally, these systems have employed supervised learning algorithms, such as Collaborative Filtering [45], which have greatly benefited from advances in deep learning. These algorithms analyze vast quantities of data to discern user preferences; however, they are not designed to strategically probe in order to more quickly learn about user interests. Instead, they learn passively from collected data.  
#### 1 ã¯ã˜ã‚ã«
ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ï¼ˆRSï¼‰ã¯ã€ãƒ‡ã‚¸ã‚¿ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã«ãŠã„ã¦é‡è¦ã§ã‚ã‚Šã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‚’é€šã˜ã¦ã‚¢ã‚¯ã‚»ã‚¹ã•ã‚Œã‚‹æƒ…å ±ã®è³ªã«é‡å¤§ãªå½±éŸ¿ã‚’ä¸ãˆã¾ã™ã€‚å¾“æ¥ã€ã“ã‚Œã‚‰ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ã€å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°[45]ã®ã‚ˆã†ãªæ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¦ãŠã‚Šã€æ·±å±¤å­¦ç¿’ã®é€²æ­©ã‹ã‚‰å¤§ããªæ©æµã‚’å—ã‘ã¦ãã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ãƒ¦ãƒ¼ã‚¶ã®å¥½ã¿ã‚’è­˜åˆ¥ã—ã¾ã™ãŒã€ãƒ¦ãƒ¼ã‚¶ã®èˆˆå‘³ã‚’ã‚ˆã‚Šæ—©ãå­¦ã¶ãŸã‚ã«æˆ¦ç•¥çš„ã«æ¢ã‚‹ã‚ˆã†ã«ã¯è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä»£ã‚ã‚Šã«ã€åé›†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å—å‹•çš„ã«å­¦ç¿’ã—ã¾ã™ã€‚

Current research [24, 32, 49] reveals that deep-learning-driven RS tend to quickly confine their focus to a limited set of suboptimal topics, limiting their scope and hampering their learning capacity.  
ç¾åœ¨ã®ç ”ç©¶[24, 32, 49]ã¯ã€æ·±å±¤å­¦ç¿’ã«åŸºã¥ãRSãŒè¿…é€Ÿã«é™ã‚‰ã‚ŒãŸã‚µãƒ–ã‚ªãƒ—ãƒ†ã‚£ãƒãƒ«ãªãƒˆãƒ”ãƒƒã‚¯ã«ç„¦ç‚¹ã‚’çµã‚‹å‚¾å‘ãŒã‚ã‚Šã€ãã®ç¯„å›²ã‚’åˆ¶é™ã—ã€å­¦ç¿’èƒ½åŠ›ã‚’å¦¨ã’ã¦ã„ã‚‹ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ã¦ã„ã¾ã™ã€‚

This restrictive personalization strategy confines RS to recommend only those topics with which they have established familiarity, thus failing to discover and learn usersâ€™ other potential interests.  
ã“ã®åˆ¶é™çš„ãªãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºæˆ¦ç•¥ã¯ã€RSãŒæ—¢ã«è¦ªã—ã¿ã®ã‚ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã®ã¿ã‚’æ¨å¥¨ã™ã‚‹ã“ã¨ã«åˆ¶ç´„ã•ã‚Œã€ãƒ¦ãƒ¼ã‚¶ã®ä»–ã®æ½œåœ¨çš„ãªèˆˆå‘³ã‚’ç™ºè¦‹ã—å­¦ã¶ã“ã¨ãŒã§ããªããªã‚Šã¾ã™ã€‚

The ability of an RS to identify and learn about userâ€™s unexplored interests is a significant determinant of its long-term performance.  
RSãŒãƒ¦ãƒ¼ã‚¶ã®æœªæ¢ç´¢ã®èˆˆå‘³ã‚’ç‰¹å®šã—å­¦ã¶èƒ½åŠ›ã¯ã€ãã®é•·æœŸçš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®é‡è¦ãªæ±ºå®šè¦å› ã§ã™ã€‚

The concept of exploration in RS draws heavily from bandit learning [51]. In the bandit learning formulation of RS, the system functions as an agent, each user represents a unique context, and each recommendation is an action.  
RSã«ãŠã‘ã‚‹æ¢ç´¢ã®æ¦‚å¿µã¯ã€ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå­¦ç¿’[51]ã‹ã‚‰å¤§ããå½±éŸ¿ã‚’å—ã‘ã¦ã„ã¾ã™ã€‚ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå­¦ç¿’ã®RSã®å®šå¼åŒ–ã§ã¯ã€ã‚·ã‚¹ãƒ†ãƒ ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã—ã¦æ©Ÿèƒ½ã—ã€å„ãƒ¦ãƒ¼ã‚¶ã¯ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ã—ã€å„æ¨å¥¨ã¯ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¨ãªã‚Šã¾ã™ã€‚

Bandit learning algorithms, such as upper confidence bound (UCB) [2, 6] and Thompson sampling [53], provide the groundwork for efficient exploration.  
ä¸Šé™ä¿¡é ¼åŒºé–“ï¼ˆUCBï¼‰[2, 6]ã‚„ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°[53]ãªã©ã®ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€åŠ¹ç‡çš„ãªæ¢ç´¢ã®åŸºç›¤ã‚’æä¾›ã—ã¾ã™ã€‚

Theoretical advances [2, 3, 5, 6, 17, 18] have offered great insight into these methods, and the efficacy of such methods has been demonstrated through experiments with small-scale environments.  
ç†è«–çš„ãªé€²å±•[2, 3, 5, 6, 17, 18]ã¯ã€ã“ã‚Œã‚‰ã®æ‰‹æ³•ã«é–¢ã™ã‚‹å¤§ããªæ´å¯Ÿã‚’æä¾›ã—ã¦ãŠã‚Šã€ãã®æœ‰åŠ¹æ€§ã¯å°è¦æ¨¡ãªç’°å¢ƒã§ã®å®Ÿé¨“ã‚’é€šã˜ã¦å®Ÿè¨¼ã•ã‚Œã¦ã„ã¾ã™ã€‚

While much of this literature has focused on small-scale environments that do not require that an agent generalizes across contexts and actions, the scale of practical RS calls for such generalization.  
ã“ã®æ–‡çŒ®ã®å¤šãã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¨ªæ–­ã—ã¦ä¸€èˆ¬åŒ–ã™ã‚‹å¿…è¦ã®ãªã„å°è¦æ¨¡ãªç’°å¢ƒã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã¾ã—ãŸãŒã€å®Ÿéš›ã®RSã®ã‚¹ã‚±ãƒ¼ãƒ«ã¯ãã®ã‚ˆã†ãªä¸€èˆ¬åŒ–ã‚’å¿…è¦ã¨ã—ã¾ã™ã€‚

Advances in neural bandits offer flexible and scalable approaches to generalization that support more sample-efficient exploration algorithms.  
ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®é€²å±•ã¯ã€ã‚ˆã‚Šã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã®è‰¯ã„æ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ä¸€èˆ¬åŒ–ã«å¯¾ã™ã‚‹æŸ”è»Ÿã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æä¾›ã—ã¾ã™ã€‚

Building on ideas developed for linear UCB and linear Thompson sampling [12, 37], these approaches offer variations of UCB and Thompson sampling that interoperate with deep learning [1, 19, 37, 38, 42, 43, 59, 61, 64].  
ç·šå½¢UCBãŠã‚ˆã³ç·šå½¢ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°[12, 37]ã®ãŸã‚ã«é–‹ç™ºã•ã‚ŒãŸã‚¢ã‚¤ãƒ‡ã‚¢ã«åŸºã¥ã„ã¦ã€ã“ã‚Œã‚‰ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯æ·±å±¤å­¦ç¿’ã¨ç›¸äº’é‹ç”¨å¯èƒ½ãªUCBãŠã‚ˆã³ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æä¾›ã—ã¾ã™[1, 19, 37, 38, 42, 43, 59, 61, 64]ã€‚

While these approaches may be sample-efficient, their computational requirements become onerous at scale.  
ã“ã‚Œã‚‰ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ãŒè‰¯ã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€ã‚¹ã‚±ãƒ¼ãƒ«ãŒå¤§ãããªã‚‹ã¨è¨ˆç®—è¦ä»¶ãŒå³ã—ããªã‚Šã¾ã™ã€‚

This has limited their practical impact.  
ã“ã‚Œã«ã‚ˆã‚Šã€å®Ÿéš›ã®å½±éŸ¿ãŒåˆ¶é™ã•ã‚Œã¦ã„ã¾ã™ã€‚

An obstacle to scaling aforementioned approaches has been in the computation required to maintain and apply epistemic uncertainty estimates.  
å‰è¿°ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã™ã‚‹éš›ã®éšœå®³ã¯ã€ã‚¨ãƒ”ã‚¹ãƒ†ãƒŸãƒƒã‚¯ä¸ç¢ºå®Ÿæ€§ã®æ¨å®šã‚’ç¶­æŒã—é©ç”¨ã™ã‚‹ãŸã‚ã«å¿…è¦ãªè¨ˆç®—ã«ã‚ã‚Šã¾ã™ã€‚

Such estimates allow an agent to know what it does _not know, which is critical for guiding exploration.  
ã“ã®ã‚ˆã†ãªæ¨å®šã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè‡ªåˆ†ãŒä½•ã‚’çŸ¥ã‚‰ãªã„ã‹ã‚’çŸ¥ã‚‹ã“ã¨ã‚’å¯èƒ½ã«ã—ã€æ¢ç´¢ã‚’å°ããŸã‚ã«é‡è¦ã§ã™ã€‚

The EpiNet [41]_ offers a scalable approach to uncertainty estimation, and therefore, a path toward supporting efficient exploration in practical RS.  
EpiNet[41]ã¯ã€ä¸ç¢ºå®Ÿæ€§æ¨å®šã«å¯¾ã™ã‚‹ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æä¾›ã—ã€ã—ãŸãŒã£ã¦ã€å®Ÿéš›ã®RSã«ãŠã‘ã‚‹åŠ¹ç‡çš„ãªæ¢ç´¢ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹é“ã‚’æä¾›ã—ã¾ã™ã€‚

In particular, epinet-enhanced deep learning combined with Thompson sampling, has the potential to greatly improve RSâ€™ exploration capabilities and therefore improve personalization.  
ç‰¹ã«ã€ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨çµ„ã¿åˆã‚ã›ãŸã‚¨ãƒ”ãƒãƒƒãƒˆå¼·åŒ–æ·±å±¤å­¦ç¿’ã¯ã€RSã®æ¢ç´¢èƒ½åŠ›ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã€ã—ãŸãŒã£ã¦ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã‚’æ”¹å–„ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

To this end, we introduce Epistemic Neural Recommendation (ENR), a novel architecture customized for RS.  
ã“ã®ç›®çš„ã®ãŸã‚ã«ã€ç§ãŸã¡ã¯ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã«ç‰¹åŒ–ã—ãŸæ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚ã‚‹ã‚¨ãƒ”ã‚¹ãƒ†ãƒŸãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆENRï¼‰ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

We run a series of experiments using large-scale real-world RS datasets to empirically demonstrate that ENR outperforms state-of-the-art neural contextual bandit algorithms.  
ç§ãŸã¡ã¯ã€å¤§è¦æ¨¡ãªå®Ÿä¸–ç•Œã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ä¸€é€£ã®å®Ÿé¨“ã‚’è¡Œã„ã€ENRãŒæœ€å…ˆç«¯ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã—ã¾ã™ã€‚

ENR greatly enhances personalization, achieving an 9% and 6% improvement in click-through rate and user rating, respectively, across two real-world experiments.  
ENRã¯ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã€2ã¤ã®å®Ÿä¸–ç•Œã®å®Ÿé¨“ã§ãã‚Œãã‚Œã‚¯ãƒªãƒƒã‚¯ç‡ã¨ãƒ¦ãƒ¼ã‚¶è©•ä¾¡ã§9%ãŠã‚ˆã³6%ã®æ”¹å–„ã‚’é”æˆã—ã¾ã—ãŸã€‚

Furthermore, it attains the performance of the best baseline algorithm with at least 29% fewer user interactions.  
ã•ã‚‰ã«ã€ENRã¯ã€å°‘ãªãã¨ã‚‚29%å°‘ãªã„ãƒ¦ãƒ¼ã‚¶ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã§ã€æœ€è‰¯ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã¾ã™ã€‚

Importantly, ENR accomplishes these while requiring orders of magnitude fewer computational resources than neural contextual bandit baseline algorithms, making it a considerably more scalable solution.  
é‡è¦ãªã®ã¯ã€ENRãŒãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚ˆã‚Šã‚‚æ¡é•ã„ã«å°‘ãªã„è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’å¿…è¦ã¨ã—ãªãŒã‚‰ã“ã‚Œã‚’é”æˆã™ã‚‹ãŸã‚ã€ã‹ãªã‚Šã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã¨ãªã‚‹ã“ã¨ã§ã™ã€‚

As a spoiler, please see Figure 1 for a computation-performance tradeoff comparison between our method and baseline methods based on one of our real-world experiments.  
ãƒã‚¿ãƒãƒ¬ã¨ã—ã¦ã€ç§ãŸã¡ã®æ‰‹æ³•ã¨å®Ÿä¸–ç•Œã®å®Ÿé¨“ã®1ã¤ã«åŸºã¥ããƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ‰‹æ³•ã¨ã®è¨ˆç®—æ€§èƒ½ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®æ¯”è¼ƒã«ã¤ã„ã¦ã¯ã€å›³1ã‚’ã”è¦§ãã ã•ã„ã€‚

The remainder of this paper is organized as follows. In Section 2, we formulate RS as a contextual bandit. In Section 3, we review existing online contextual bandit algorithms, current industry adoptions, and their relative merits. In Section 4, we introduce Epistemic Neural Recommendation. In Section 5, we present empirical results.  
ã“ã®è«–æ–‡ã®æ®‹ã‚Šã®éƒ¨åˆ†ã¯æ¬¡ã®ã‚ˆã†ã«æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚»ã‚¯ã‚·ãƒ§ãƒ³2ã§ã¯ã€RSã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã¨ã—ã¦å®šå¼åŒ–ã—ã¾ã™ã€‚ã‚»ã‚¯ã‚·ãƒ§ãƒ³3ã§ã¯ã€æ—¢å­˜ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ç¾åœ¨ã®æ¥­ç•Œã§ã®æ¡ç”¨ã€ãŠã‚ˆã³ãã‚Œã‚‰ã®ç›¸å¯¾çš„ãªåˆ©ç‚¹ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¾ã™ã€‚ã‚»ã‚¯ã‚·ãƒ§ãƒ³4ã§ã¯ã€ã‚¨ãƒ”ã‚¹ãƒ†ãƒŸãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ã‚»ã‚¯ã‚·ãƒ§ãƒ³5ã§ã¯ã€å®Ÿè¨¼çµæœã‚’ç¤ºã—ã¾ã™ã€‚

In Section 6, we summarize our contributions, benefits they afford, and potential future directions for neural contextual bandits.  
ã‚»ã‚¯ã‚·ãƒ§ãƒ³6ã§ã¯ã€ç§ãŸã¡ã®è²¢çŒ®ã€æä¾›ã™ã‚‹åˆ©ç‚¹ã€ãŠã‚ˆã³ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®å°†æ¥ã®æ–¹å‘æ€§ã‚’è¦ç´„ã—ã¾ã™ã€‚

#### 2 Problem Definition
We conceptualize the design of RS as a contextual bandit problem where an agent interacts with a RS environment by taking an action (recommendation) based on a context (user) observed. More formally, the environment is specified by a triple that consists E of an observation space, an action space, and an observation O A probability function ğ‘ƒğ‘‚ .  
#### 2 å•é¡Œå®šç¾©
ç§ãŸã¡ã¯ã€RSã®è¨­è¨ˆã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã¨ã—ã¦æ¦‚å¿µåŒ–ã—ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè¦³å¯Ÿã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒ¦ãƒ¼ã‚¶ï¼‰ã«åŸºã¥ã„ã¦ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæ¨å¥¨ï¼‰ã‚’å–ã‚‹ã“ã¨ã«ã‚ˆã£ã¦RSç’°å¢ƒã¨ç›¸äº’ä½œç”¨ã—ã¾ã™ã€‚ã‚ˆã‚Šæ­£å¼ã«ã¯ã€ç’°å¢ƒã¯è¦³å¯Ÿç©ºé–“ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“ã€ãŠã‚ˆã³è¦³å¯ŸOã®3ã¤ã®è¦ç´ ã‹ã‚‰ãªã‚‹ä¸‰é‡é …Eã«ã‚ˆã£ã¦æŒ‡å®šã•ã‚Œã¾ã™ã€‚ç¢ºç‡é–¢æ•°ã¯$P_O$ã§ã™ã€‚

**Observation space O: In a contextual bandit problem, the ob-** servation generated by the environment offers feedback in response to the previous action taken by the agent as well as a new context the agent needs to execute an action. Formally, O = S Ã— R and at time step ğ‘¡, ğ‘‚ğ‘¡ = (ğ‘†ğ‘¡ _, ğ‘…ğ‘¡_ ). ğ‘†ğ‘¡ âˆˆS is a new context. This context could, for example, provide a userâ€™s demographic information as well as data from their past interactions. We will sometimes refer to ğ‘†ğ‘¡ as a user, since we think of it as providing information that distinguishes users. ğ‘…ğ‘¡ âˆˆ R denotes scalar feedback from the user of the previous time step ğ‘¡ âˆ’ 1 in response to the action ğ´ğ‘¡ âˆ’1. For example, ğ‘…ğ‘¡ could be binary-valued, indicating whether the user clicked in response to a recommendation. We will refer to ğ‘…ğ‘¡ as the reward received by the agent at time ğ‘¡.  
**è¦³å¯Ÿç©ºé–“O: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã«ãŠã„ã¦ã€ç’°å¢ƒã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚Œã‚‹è¦³å¯Ÿã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå–ã£ãŸå‰ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«å¯¾ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¨ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«å¿…è¦ãªæ–°ã—ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚æ­£å¼ã«ã¯ã€$O = S Ã— R$ã§ã‚ã‚Šã€æ™‚åˆ»$t$ã«ãŠã„ã¦ã€$O_t = (S_t, R_t)$ã§ã™ã€‚$S_t âˆˆ S$ã¯æ–°ã—ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚ã“ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¯ã€ä¾‹ãˆã°ã€ãƒ¦ãƒ¼ã‚¶ã®äººå£çµ±è¨ˆæƒ…å ±ã‚„éå»ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ç§ãŸã¡ã¯æ™‚ã€…$S_t$ã‚’ãƒ¦ãƒ¼ã‚¶ã¨å‘¼ã³ã¾ã™ã€‚ãªãœãªã‚‰ã€ã“ã‚Œã¯ãƒ¦ãƒ¼ã‚¶ã‚’åŒºåˆ¥ã™ã‚‹æƒ…å ±ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã¨è€ƒãˆã‚‹ã‹ã‚‰ã§ã™ã€‚$R_t âˆˆ R$ã¯ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³$A_{t-1}$ã«å¯¾ã™ã‚‹å‰ã®æ™‚åˆ»$t - 1$ã®ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®ã‚¹ã‚«ãƒ©ãƒ¼ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç¤ºã—ã¾ã™ã€‚ä¾‹ãˆã°ã€$R_t$ã¯ãƒã‚¤ãƒŠãƒªå€¤ã§ã‚ã‚Šã€ãƒ¦ãƒ¼ã‚¶ãŒæ¨å¥¨ã«å¿œã˜ã¦ã‚¯ãƒªãƒƒã‚¯ã—ãŸã‹ã©ã†ã‹ã‚’ç¤ºã™ã“ã¨ãŒã§ãã¾ã™ã€‚ç§ãŸã¡ã¯$R_t$ã‚’æ™‚åˆ»$t$ã«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå—ã‘å–ã£ãŸå ±é…¬ã¨å‘¼ã³ã¾ã™ã€‚**

**Action space A: At each time ğ‘¡, the agent selects an action** _ğ´ğ‘¡_ âˆˆA, which identifies a content unit for recommendation to the user ğ‘†ğ‘¡ . The action space A specifies the range of possibilities afforded by the universe of available content.  
**ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“A: å„æ™‚åˆ»$t$ã«ãŠã„ã¦ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã‚¢ã‚¯ã‚·ãƒ§ãƒ³$A_t âˆˆ A$ã‚’é¸æŠã—ã€ã“ã‚Œã¯ãƒ¦ãƒ¼ã‚¶$S_t$ã«æ¨å¥¨ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ¦ãƒ‹ãƒƒãƒˆã‚’ç‰¹å®šã—ã¾ã™ã€‚ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“Aã¯ã€åˆ©ç”¨å¯èƒ½ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å®‡å®™ã«ã‚ˆã£ã¦æä¾›ã•ã‚Œã‚‹å¯èƒ½æ€§ã®ç¯„å›²ã‚’æŒ‡å®šã—ã¾ã™ã€‚**

**Observation probability function ğ‘ƒğ‘‚** : The observation probability function assigns a probability ğ‘ƒğ‘‚ (ğ‘‚ğ‘¡ +1|ğ‘†ğ‘¡ _,ğ´ğ‘¡_ ) to each possible outcome ğ‘‚ğ‘¡ +1 = (ğ‘†ğ‘¡ +1, ğ‘…ğ‘¡ +1) from recommending ğ´ğ‘¡ to user ğ‘†ğ‘¡ .  
**è¦³å¯Ÿç¢ºç‡é–¢æ•°$P_O$**: è¦³å¯Ÿç¢ºç‡é–¢æ•°ã¯ã€ãƒ¦ãƒ¼ã‚¶$S_t$ã«$A_t$ã‚’æ¨å¥¨ã™ã‚‹ã“ã¨ã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹å„å¯èƒ½ãªçµæœ$O_{t+1} = (S_{t+1}, R_{t+1})$ã«ç¢ºç‡$P_O(O_{t+1}|S_t, A_t)$ã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚

This probability is a product of two others, specified by functions _ğ‘ƒğ‘†_ and ğ‘ƒğ‘…:  
ã“ã®ç¢ºç‡ã¯ã€é–¢æ•°$P_S$ã¨$P_R$ã«ã‚ˆã£ã¦æŒ‡å®šã•ã‚Œã‚‹ä»–ã®2ã¤ã®ç©ã§ã™ã€‚

$$
P_O(O_{t+1}|S_t, A_t) = P_S(S_{t+1})P_R(R_{t+1}|S_t, A_t).
$$

At time ğ‘¡, the environment samples a new user ğ‘†ğ‘¡ âˆ¼ _ğ‘ƒğ‘†_ . The agent then supplies a recommendation ğ´ğ‘¡, and the environment samples ğ‘…ğ‘¡ +1 âˆ¼ _ğ‘ƒğ‘…_ (Â·|ğ‘†ğ‘¡ _,ğ´ğ‘¡_ ).  
æ™‚åˆ»$t$ã«ãŠã„ã¦ã€ç’°å¢ƒã¯æ–°ã—ã„ãƒ¦ãƒ¼ã‚¶$S_t âˆ¼ P_S$ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯æ¬¡ã«æ¨å¥¨$A_t$ã‚’æä¾›ã—ã€ç’°å¢ƒã¯$R_{t+1} âˆ¼ P_R(Â·|S_t, A_t)$ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚

The overall objective of the agent is to maximize its average reward _ğ‘‡[1]_ ï¿½ğ‘‡ğ‘¡ =1 _[ğ‘…][ğ‘¡]_ [over][ ğ‘‡] [time steps. Note that, though each ac-]  
ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å…¨ä½“çš„ãªç›®çš„ã¯ã€å¹³å‡å ±é…¬$\frac{1}{T} \sum_{t=1}^{T} R_t$ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã§ã™ã€‚å„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³$A_t$ã¯ã€$S_t$ä»¥å¤–ã®ãƒ¦ãƒ¼ã‚¶ã«ç›´æ¥å½±éŸ¿ã‚’ä¸ãˆãªã„ãŒã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯è¦³å¯Ÿ$R_{t+1}$ã‚’é€šã˜ã¦åé›†ã•ã‚Œã‚‹æƒ…å ±ã«å½±éŸ¿ã‚’ä¸ãˆã¾ã™ã€‚ã“ã®ã‚ˆã†ã«ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå­¦ã¶ã“ã¨ã‚’é€šã˜ã¦å¾Œç¶šã®ãƒ¦ãƒ¼ã‚¶ã«é–“æ¥çš„ãªå½±éŸ¿ã‚’ä¸ãˆã¾ã™ã€‚å­¦ç¿’ã®é…å»¶åˆ©ç›Šã‚’æœ€é©åŒ–ã™ã‚‹ã«ã¯ã€æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹æˆ¦ç•¥çš„ãªé€æ¬¡æ±ºå®šãŒå¿…è¦ã§ã™ã€‚

We think of ğ‘†ğ‘¡ and ğ´ğ‘¡ as offering information about users and content units in arbitrary raw formats. Agents, however, often rely on starting with a more structured representations provided by domain experts.  
ç§ãŸã¡ã¯$S_t$ã¨$A_t$ã‚’ã€ãƒ¦ãƒ¼ã‚¶ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ¦ãƒ‹ãƒƒãƒˆã«é–¢ã™ã‚‹æƒ…å ±ã‚’ä»»æ„ã®ç”Ÿã®å½¢å¼ã§æä¾›ã™ã‚‹ã‚‚ã®ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚ã—ã‹ã—ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã—ã°ã—ã°ãƒ‰ãƒ¡ã‚¤ãƒ³å°‚é–€å®¶ã«ã‚ˆã£ã¦æä¾›ã•ã‚Œã‚‹ã‚ˆã‚Šæ§‹é€ åŒ–ã•ã‚ŒãŸè¡¨ç¾ã‹ã‚‰å§‹ã¾ã‚‹ã“ã¨ã«ä¾å­˜ã—ã¾ã™ã€‚

In this work, we also assume availability of feature extractors that map ğ‘†ğ‘¡ and ğ´ğ‘¡ to such representations ğœ™ğ´ğ‘¡ and ğœ“ğ‘†ğ‘¡ .  
æœ¬ç ”ç©¶ã§ã¯ã€$S_t$ã¨$A_t$ã‚’ãã®ã‚ˆã†ãªè¡¨ç¾$\phi_{A_t}$ã¨$\psi_{S_t}$ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ç‰¹å¾´æŠ½å‡ºå™¨ã®åˆ©ç”¨å¯èƒ½æ€§ã‚‚ä»®å®šã—ã¾ã™ã€‚

Note that our problem formulation encompasses only online learning. In a real RS, one would pretrain an agent offline on historical data before engaging in online learning.  
ç§ãŸã¡ã®å•é¡Œå®šç¾©ã¯ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã®ã¿ã‚’å«ã‚€ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚å®Ÿéš›ã®RSã§ã¯ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã«å¾“äº‹ã™ã‚‹å‰ã«ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§æ­´å²ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦äº‹å‰å­¦ç¿’ã•ã›ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚

The methods we develop for online learning can be applied in this workflow post pretraining of an epinet-enhanced model.  
ç§ãŸã¡ãŒã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã®ãŸã‚ã«é–‹ç™ºã™ã‚‹æ‰‹æ³•ã¯ã€ã‚¨ãƒ”ãƒãƒƒãƒˆå¼·åŒ–ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’å¾Œã®ã“ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«é©ç”¨ã§ãã¾ã™ã€‚

However, in order to focus our attention on the problem of exploration, in this paper, we limit our discussions, designs and experiments to the online learning part and leave offline learning for future work.  
ã—ã‹ã—ã€æ¢ç´¢ã®å•é¡Œã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ãŸã‚ã«ã€æœ¬è«–æ–‡ã§ã¯è­°è«–ã€è¨­è¨ˆã€å®Ÿé¨“ã‚’ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’éƒ¨åˆ†ã«åˆ¶é™ã—ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å­¦ç¿’ã¯å°†æ¥ã®ä½œæ¥­ã«æ®‹ã—ã¾ã™ã€‚

#### 3 Related Work and Prerequisites
Among various bandit algorithms, there are two most commonly adopted online bandit strategies. One explores based on the reward estimates sampled from context-action pairsâ€™ posterior distributions, represented by Thompson sampling [53], and the other follows the "optimism in the face of uncertainty" principle, represented by upper confidence bound (UCB) [2, 6].  
#### 3 é–¢é€£ç ”ç©¶ã¨å‰ææ¡ä»¶
ã•ã¾ã–ã¾ãªãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ä¸­ã§ã€æœ€ã‚‚ä¸€èˆ¬çš„ã«æ¡ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆæˆ¦ç•¥ã¯2ã¤ã‚ã‚Šã¾ã™ã€‚1ã¤ã¯ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ-ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒšã‚¢ã®äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸå ±é…¬æ¨å®šã«åŸºã¥ã„ã¦æ¢ç´¢ã™ã‚‹ã‚‚ã®ã§ã€ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°[53]ã§è¡¨ã•ã‚Œã¾ã™ã€‚ã‚‚ã†1ã¤ã¯ã€ã€Œä¸ç¢ºå®Ÿæ€§ã«ç›´é¢ã—ãŸæ¥½è¦³ä¸»ç¾©ã€ã®åŸå‰‡ã«å¾“ã„ã€ä¸Šé™ä¿¡é ¼åŒºé–“ï¼ˆUCBï¼‰[2, 6]ã§è¡¨ã•ã‚Œã¾ã™ã€‚

In this section, we offer a high level introduction to Thompson sampling and UCB algorithms as well as their extensions to contextual settings.  
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨UCBã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ãŠã‚ˆã³ãã‚Œã‚‰ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¨­å®šã¸ã®æ‹¡å¼µã«ã¤ã„ã¦ã®é«˜ãƒ¬ãƒ™ãƒ«ã®ç´¹ä»‹ã‚’æä¾›ã—ã¾ã™ã€‚

Their deep neural network versions of the algorithms are considered baselines for our work.  
ã“ã‚Œã‚‰ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ã€ç§ãŸã¡ã®ä½œæ¥­ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨è¦‹ãªã•ã‚Œã¾ã™ã€‚

We will also review related industry and practical adoptions of these strategies.  
ã¾ãŸã€ã“ã‚Œã‚‰ã®æˆ¦ç•¥ã®é–¢é€£ã™ã‚‹æ¥­ç•ŒãŠã‚ˆã³å®Ÿç”¨çš„ãªæ¡ç”¨ã«ã¤ã„ã¦ã‚‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¾ã™ã€‚

Note that in this section, we limit our discussion to exploration strategy for immediate reward and do not cover optimization approaches for cumulative returns [13, 60, 65].  
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€å³æ™‚å ±é…¬ã®ãŸã‚ã®æ¢ç´¢æˆ¦ç•¥ã«è­°è«–ã‚’åˆ¶é™ã—ã€ç´¯ç©ãƒªã‚¿ãƒ¼ãƒ³ã®æœ€é©åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ[13, 60, 65]ã«ã¤ã„ã¦ã¯æ‰±ã„ã¾ã›ã‚“ã€‚

#### 3.1 Thompson Sampling, Its Extensions and Related Adoptions
Thompson sampling is an exploration strategy that samples reward estimates from context-action pairsâ€™ posterior distributions and then executes greedily with respect to these samples.  
#### 3.1 ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€ãã®æ‹¡å¼µã¨é–¢é€£ã™ã‚‹æ¡ç”¨
ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ-ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒšã‚¢ã®äº‹å¾Œåˆ†å¸ƒã‹ã‚‰å ±é…¬æ¨å®šã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ã“ã‚Œã‚‰ã®ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã—ã¦è²ªæ¬²ã«å®Ÿè¡Œã™ã‚‹æ¢ç´¢æˆ¦ç•¥ã§ã™ã€‚

This strategy is particularly popular due to its simplicity.  
ã“ã®æˆ¦ç•¥ã¯ã€ãã®ã‚·ãƒ³ãƒ—ãƒ«ã•ã‹ã‚‰ç‰¹ã«äººæ°—ãŒã‚ã‚Šã¾ã™ã€‚

Following the definition in Section 2, we first consider a vanilla Thompson sampling algorithm [53] for multi-armed bandits.  
ã‚»ã‚¯ã‚·ãƒ§ãƒ³2ã®å®šç¾©ã«å¾“ã„ã€ã¾ãšã¯ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ãŸã‚ã®ãƒãƒ‹ãƒ©ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ [53]ã‚’è€ƒãˆã¾ã™ã€‚

For a multi-armed bandit, the context ğ‘†ğ‘¡ is always the same and the agent needs to choose _ğ´ğ‘¡_ from A without any representation learning.  
ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®å ´åˆã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ$S_t$ã¯å¸¸ã«åŒã˜ã§ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯è¡¨ç¾å­¦ç¿’ãªã—ã«$A$ã‹ã‚‰$A_t$ã‚’é¸æŠã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

A Thompson sampling agent keeps a reward posterior distribution _ğ‘ƒ[Ë†]ğ‘_ for each arm _ğ‘_ and updates it over time.  
ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€å„ã‚¢ãƒ¼ãƒ $a$ã«å¯¾ã—ã¦å ±é…¬ã®äº‹å¾Œåˆ†å¸ƒ$P_a$ã‚’ä¿æŒã—ã€æ™‚é–“ã¨ã¨ã‚‚ã«æ›´æ–°ã—ã¾ã™ã€‚

At time step ğ‘¡, the agent chooses  
æ™‚åˆ»$t$ã«ãŠã„ã¦ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯æ¬¡ã®ã‚ˆã†ã«é¸æŠã—ã¾ã™ã€‚

$$
A_t = \arg \max_{a \in A} \hat{R}_{t+1,a}, \quad \hat{R}_{t+1,a} \sim P_a.
$$

where $\hat{R}_{t,a}$ is the estimated reward sampled from the agentâ€™s posterior belief.  
ã“ã“ã§ã€$\hat{R}_{t,a}$ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®äº‹å¾Œä¿¡å¿µã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸæ¨å®šå ±é…¬ã§ã™ã€‚

The agent then updates its posterior belief $P_a$ with the latest reward $R_{t+1}$ and selected action $A_t$.  
ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯æ¬¡ã«ã€æœ€æ–°ã®å ±é…¬$R_{t+1}$ã¨é¸æŠã—ãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³$A_t$ã§äº‹å¾Œä¿¡å¿µ$P_a$ã‚’æ›´æ–°ã—ã¾ã™ã€‚

The vanilla Thompson sampling approach is a popular production strategy when it comes to small action space recommendations [31, 50].  
ãƒãƒ‹ãƒ©ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€å°ã•ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“ã®æ¨å¥¨ã«é–¢ã—ã¦äººæ°—ã®ã‚ã‚‹ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³æˆ¦ç•¥ã§ã™[31, 50]ã€‚

Extending Thompson sampling to large action space and contextual bandits, an agent can compute parametric estimates of rewards of context-action pairs by sampling linear model parameters from the posterior belief [1, 4, 46â€“48] instead of sampling a point estimate from the reward posterior distribution.  
ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å¤§è¦æ¨¡ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“ãŠã‚ˆã³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«æ‹¡å¼µã™ã‚‹ã¨ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯å ±é…¬ã®äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ãƒã‚¤ãƒ³ãƒˆæ¨å®šã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã®ã§ã¯ãªãã€äº‹å¾Œä¿¡å¿µã‹ã‚‰ç·šå½¢ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ-ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒšã‚¢ã®å ±é…¬ã®ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¨å®šã‚’è¨ˆç®—ã§ãã¾ã™[1, 4, 46â€“48]ã€‚

Here, a linear Thompson sampling agent keeps track of a posterior belief $P[\hat{\theta}]$ over the parameters $\theta$.  
ã“ã“ã§ã€ç·šå½¢ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\theta$ã«å¯¾ã™ã‚‹äº‹å¾Œä¿¡å¿µ$P[\hat{\theta}]$ã‚’è¿½è·¡ã—ã¾ã™ã€‚

The agent chooses action  
ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã—ã¾ã™ã€‚

$$
A_t = \arg \max_{a \in A} \hat{\theta}_t^\top x_{t,a}, \quad \hat{\theta}_t \sim P[\hat{\theta}], \quad x_{t,a} = \text{concat}(\psi_{S_t}, \phi_a).
$$

The agent then updates its posterior belief of $P[\hat{\theta}]$ with the latest reward $R_{t+1}$, context $S_t$ and action $A_t$.  
ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯æ¬¡ã«ã€æœ€æ–°ã®å ±é…¬$R_{t+1}$ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ$S_t$ã€ãŠã‚ˆã³ã‚¢ã‚¯ã‚·ãƒ§ãƒ³$A_t$ã§$P[\hat{\theta}]$ã®äº‹å¾Œä¿¡å¿µã‚’æ›´æ–°ã—ã¾ã™ã€‚

The posterior update could be performed via Laplace approximation [36], and is also presented in industry adoptions of linear Thompson sampling [12].  
äº‹å¾Œæ›´æ–°ã¯ãƒ©ãƒ—ãƒ©ã‚¹è¿‘ä¼¼[36]ã‚’ä»‹ã—ã¦è¡Œã†ã“ã¨ãŒã§ãã€ç·šå½¢ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æ¥­ç•Œã§ã®æ¡ç”¨ã§ã‚‚ç¤ºã•ã‚Œã¦ã„ã¾ã™[12]ã€‚

The method above could be extended to neural networks as the agent can also keep a posterior belief over neural network parameters.  
ä¸Šè¨˜ã®æ–¹æ³•ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹äº‹å¾Œä¿¡å¿µã‚’ä¿æŒã§ãã‚‹ãŸã‚ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚‚æ‹¡å¼µã§ãã¾ã™ã€‚

To avoid heavy computation of Laplace approximation, various Bayesian methods have been proposed to achieve posterior updates for distributions over neural network parameters, including deep ensemble [35], ensemble sampling with prior networks [38], Bayes by Backprop [10], Hypermodels [19], Monte-Carlo Dropouts [20] and EpiNet [41].  
ãƒ©ãƒ—ãƒ©ã‚¹è¿‘ä¼¼ã®é‡ã„è¨ˆç®—ã‚’é¿ã‘ã‚‹ãŸã‚ã«ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹åˆ†å¸ƒã®äº‹å¾Œæ›´æ–°ã‚’é”æˆã™ã‚‹ãŸã‚ã«ã€ã•ã¾ã–ã¾ãªãƒ™ã‚¤ã‚ºæ³•ãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã¯ã€ãƒ‡ã‚£ãƒ¼ãƒ—ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«[35]ã€äº‹å‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ãŸã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°[38]ã€ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒƒãƒ—ã«ã‚ˆã‚‹ãƒ™ã‚¤ã‚º[10]ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ¢ãƒ‡ãƒ«[19]ã€ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ[20]ã€ãŠã‚ˆã³EpiNet[41]ãŒå«ã¾ã‚Œã¾ã™ã€‚

Among the methods above, the two ensemble methods [11, 39, 52] and Monte-Carlo Dropout [25] are the most commonly adopted methods for contextual bandits with parameter posterior sampling in a practical setting.  
ä¸Šè¨˜ã®æ–¹æ³•ã®ä¸­ã§ã€2ã¤ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•[11, 39, 52]ã¨ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ[25]ã¯ã€å®Ÿç”¨çš„ãªè¨­å®šã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿äº‹å¾Œã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ä¼´ã†ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«æœ€ã‚‚ä¸€èˆ¬çš„ã«æ¡ç”¨ã•ã‚Œã¦ã„ã‚‹æ–¹æ³•ã§ã™ã€‚

One additional line of work that leverages Thompson sampling for neural networks is neural Thompson sampling [61].  
ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãŸã‚ã®ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’æ´»ç”¨ã™ã‚‹ã‚‚ã†1ã¤ã®ç ”ç©¶ã®æµã‚Œã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°[61]ã§ã™ã€‚

In this approach, instead of sampling from parametersâ€™ posterior distribution, neural Thompson sampling directly samples from the posterior estimate of the reward of a context-action pair.  
ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã®ã§ã¯ãªãã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ-ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒšã‚¢ã®å ±é…¬ã®äº‹å¾Œæ¨å®šã‹ã‚‰ç›´æ¥ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚

A Neural Thompson sampling agent keeps track of a matrix $\Gamma$ initialized with $\Gamma = \lambda I$, $I âˆˆ R^{N Ã— N}$ is an identity matrix, where $\lambda > 0$ is a regularization parameter and $N$ is the parameter size of the neural network.  
ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€$\Gamma = \lambda I$ã§åˆæœŸåŒ–ã•ã‚ŒãŸè¡Œåˆ—$\Gamma$ã‚’è¿½è·¡ã—ã¾ã™ã€‚$I âˆˆ R^{N Ã— N}$ã¯å˜ä½è¡Œåˆ—ã§ã‚ã‚Šã€$\lambda > 0$ã¯æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã€$N$ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã§ã™ã€‚

Assuming a Gaussian distributed reward posterior, the variance of the distribution is estimated by  
ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®å ±é…¬äº‹å¾Œã‚’ä»®å®šã™ã‚‹ã¨ã€åˆ†å¸ƒã®åˆ†æ•£ã¯æ¬¡ã®ã‚ˆã†ã«æ¨å®šã•ã‚Œã¾ã™ã€‚

$$
\sigma_{t,A}^2 = \lambda g_\theta^\top \left( x_{t,A} \right) \Gamma^{-1} g_\theta \left( x_{t,A} \right),
$$

where $g_\theta$ is the gradient of the output with respect to $\theta$ for the entire neural network.  
ã“ã“ã§ã€$g_\theta$ã¯ã€å…¨ä½“ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«å¯¾ã™ã‚‹$\theta$ã«é–¢ã™ã‚‹å‡ºåŠ›ã®å‹¾é…ã§ã™ã€‚

The agent then chooses action  
ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯æ¬¡ã«ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã—ã¾ã™ã€‚

$$
A_t = \arg \max_{a \in A} \hat{R}_{t,a}, \quad \hat{R}_{t,a} \sim N(f_\theta(x_{t,a}), \alpha \sigma_{t,a}^2).
$$

where $\alpha$ is an exploration hyperparameter and $f_\theta$ is the neural network.  
ã“ã“ã§ã€$\alpha$ã¯æ¢ç´¢ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã€$f_\theta$ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚

The agent updates $\Gamma \leftarrow \Gamma + g_\theta (x_{t,A}) g_\theta (x_{t,A})^\top / m$ with $m$ being the width of the neural network, assuming all layers with the same width.  
ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€ã™ã¹ã¦ã®å±¤ãŒåŒã˜å¹…ã§ã‚ã‚‹ã¨ä»®å®šã—ã¦ã€$\Gamma \leftarrow \Gamma + g_\theta (x_{t,A}) g_\theta (x_{t,A})^\top / m$ã§æ›´æ–°ã—ã¾ã™ã€‚

To the best of the authorsâ€™ knowledge, the neural Thompson sampling strategy is not well adopted by industry due to its heavy computational complexity.  
è‘—è€…ã®çŸ¥ã‚‹é™ã‚Šã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã¯ã€ãã®é‡ã„è¨ˆç®—è¤‡é›‘æ€§ã®ãŸã‚ã«æ¥­ç•Œã§ã‚ã¾ã‚Šæ¡ç”¨ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚

#### 3.2 Upper Confidence Bound (UCB), Its Extensions and Related Adoptions
Upper confidence bound (UCB) is a general optimism facing uncertainty exploration strategy where the agent tends to choose actions that it has more uncertainties about.  
#### 3.2 ä¸Šé™ä¿¡é ¼åŒºé–“ï¼ˆUCBï¼‰ã€ãã®æ‹¡å¼µã¨é–¢é€£ã™ã‚‹æ¡ç”¨
ä¸Šé™ä¿¡é ¼åŒºé–“ï¼ˆUCBï¼‰ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚ˆã‚Šä¸ç¢ºå®Ÿæ€§ã®ã‚ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã™ã‚‹å‚¾å‘ãŒã‚ã‚‹ä¸€èˆ¬çš„ãªä¸ç¢ºå®Ÿæ€§ã«ç›´é¢ã—ãŸæ¥½è¦³ä¸»ç¾©ã®æ¢ç´¢æˆ¦ç•¥ã§ã™ã€‚

The purpose of doing so is to gather information that could help either identify the best action or eliminate bad actions.  
ãã®ç›®çš„ã¯ã€æœ€è‰¯ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç‰¹å®šã™ã‚‹ã‹ã€æ‚ªã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ’é™¤ã™ã‚‹ã®ã«å½¹ç«‹ã¤æƒ…å ±ã‚’åé›†ã™ã‚‹ã“ã¨ã§ã™ã€‚

Similar to the subsection above, we first consider a vanilla UCB algorithm [2, 6] for multi-armed bandits.  
ä¸Šè¨˜ã®ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨åŒæ§˜ã«ã€ã¾ãšã¯ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ãŸã‚ã®ãƒãƒ‹ãƒ©UCBã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ [2, 6]ã‚’è€ƒãˆã¾ã™ã€‚

A vanilla UCB agent selects an action $A_t$ by  
ãƒãƒ‹ãƒ©UCBã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€æ¬¡ã®ã‚ˆã†ã«ã‚¢ã‚¯ã‚·ãƒ§ãƒ³$A_t$ã‚’é¸æŠã—ã¾ã™ã€‚

$$
A_t = \arg \max_{a \in A} \left( \frac{1}{t} \sum_{\tau=1}^{t} R_\tau \mathbb{1}(A_\tau = a) + \alpha \sqrt{\frac{\ln(t)}{n_{t,a}}} \right),
$$

where $n_{t,A} = \sum_{\tau=1}^{t} \mathbb{1}(A_\tau = A)$ and $\alpha \in \mathbb{R}^+$ is a hyperparameter.  
ã“ã“ã§ã€$n_{t,A} = \sum_{\tau=1}^{t} \mathbb{1}(A_\tau = A)$ã§ã‚ã‚Šã€$\alpha \in \mathbb{R}^+$ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™ã€‚

There have been multiple lines of work analyzing vanilla UCBâ€™s impact on production recommender systems in terms of degenerating feedback loops [26, 32], modeling attritions [9] and its extension to Collaborative Filtering [40].  
ãƒãƒ‹ãƒ©UCBã®ç”Ÿç”£ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹å½±éŸ¿ã‚’ã€åŠ£åŒ–ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—[26, 32]ã€ãƒ¢ãƒ‡ãƒ«åŒ–ã®å–ªå¤±[9]ã€ãŠã‚ˆã³å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¸ã®æ‹¡å¼µ[40]ã®è¦³ç‚¹ã‹ã‚‰åˆ†æã™ã‚‹ç ”ç©¶ãŒã„ãã¤ã‹ã‚ã‚Šã¾ã™ã€‚

The immediate extension to UCB to linear bandit problems is LinUCB [16, 37].  
UCBã®ç·šå½¢ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã¸ã®å³æ™‚ã®æ‹¡å¼µã¯LinUCB[16, 37]ã§ã™ã€‚

A LinUCB agent initializes two variables for any new action $A$ that it has never seen before, $\Gamma_A = I_d$ (dæ¬¡å…ƒã®å˜ä½è¡Œåˆ—)ã¨$b_A = 0_{d \times 1}$ (dæ¬¡å…ƒã®ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«)ã§ã‚ã‚Šã€ã“ã“ã§$d$ã¯ç·šå½¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã§ã™ã€‚  
LinUCBã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€ã“ã‚Œã¾ã§è¦‹ãŸã“ã¨ã®ãªã„æ–°ã—ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³$A$ã«å¯¾ã—ã¦2ã¤ã®å¤‰æ•°ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚$\Gamma_A = I_d$ï¼ˆdæ¬¡å…ƒã®å˜ä½è¡Œåˆ—ï¼‰ã¨$b_A = 0_{d \times 1}$ï¼ˆdæ¬¡å…ƒã®ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã§ã‚ã‚Šã€ã“ã“ã§$d$ã¯ç·šå½¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã§ã™ã€‚

At time step $t$, given a new context representation $\psi_{S_t}$ and each action $\phi_A$, the agent concatenates the two vectors to get a context-action pair representation $x_{t,A} = \text{concat}(\psi_{S_t}, \phi_A)$.  
æ™‚åˆ»$t$ã«ãŠã„ã¦ã€æ–°ã—ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¾$\psi_{S_t}$ã¨å„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³$\phi_A$ãŒä¸ãˆã‚‰ã‚Œã‚‹ã¨ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯2ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’é€£çµã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ-ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒšã‚¢ã®è¡¨ç¾$x_{t,A} = \text{concat}(\psi_{S_t}, \phi_A)$ã‚’å¾—ã¾ã™ã€‚

The agent selects action  
ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã—ã¾ã™ã€‚

$$
A_t = \arg \max_{a \in A} \left( \Gamma_a^{-1} b_a \right)^\top x_{t,a} + \alpha \sqrt{x_{t,a}^\top \Gamma_a^{-1} x_{t,a}}.
$$

After receiving $R_{t+1}$, the agent then updates $\Gamma_{A_t}$ and $b_{A_t}$ with $\Gamma_{A_t} \leftarrow \Gamma_{A_t} + x_{t,A_t} x_{t,A_t}^\top$ and $b_{A_t} \leftarrow b_{A_t} + R_{t+1} x_{t,A_t}$.  
$R_{t+1}$ã‚’å—ã‘å–ã£ãŸå¾Œã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯$\Gamma_{A_t} \leftarrow \Gamma_{A_t} + x_{t,A_t} x_{t,A_t}^\top$ãŠã‚ˆã³$b_{A_t} \leftarrow b_{A_t} + R_{t+1} x_{t,A_t}$ã§$\Gamma_{A_t}$ã¨$b_{A_t}$ã‚’æ›´æ–°ã—ã¾ã™ã€‚

LinUCB is a popular algorithm for adoption in many industry usecases and particularly in the space of recommender systems [37, 44, 54, 63].  
LinUCBã¯ã€å¤šãã®æ¥­ç•Œã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã€ç‰¹ã«ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®åˆ†é‡ã§æ¡ç”¨ã•ã‚Œã‚‹äººæ°—ã®ã‚ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™[37, 44, 54, 63]ã€‚

With the rise of neural networks and deep learning, so comes the need for optimism based exploration neural methods.  
ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨æ·±å±¤å­¦ç¿’ã®å°é ­ã«ä¼´ã„ã€æ¥½è¦³ä¸»ç¾©ã«åŸºã¥ãæ¢ç´¢ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ‰‹æ³•ã®å¿…è¦æ€§ãŒç”Ÿã˜ã¾ã™ã€‚

NeuralUCB [64] and NeuralLinUCB [59] are two representing examples of such algorithms.  
NeuralUCB[64]ã¨NeuralLinUCB[59]ã¯ã€ãã®ã‚ˆã†ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ä»£è¡¨çš„ãªä¾‹ã§ã™ã€‚

We first introduce NeuralLinUCB as itâ€™s a natural extension of LinUCB.  
ã¾ãšã€NeuralLinUCBã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ã“ã‚Œã¯LinUCBã®è‡ªç„¶ãªæ‹¡å¼µã§ã™ã€‚

Given a neural network representation, NeuralLinUCB no longer initiates a matrix $\Gamma_A$ for each action $A$ and instead keeps track of a single matrix $\Gamma$ initialized with $\lambda I$ where $\lambda > 0$ is a regularization factor and $I âˆˆ R^{N Ã— N}$ is an identity matrix.  
ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¡¨ç¾ãŒä¸ãˆã‚‰ã‚Œã‚‹ã¨ã€NeuralLinUCBã¯å„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³$A$ã«å¯¾ã—ã¦è¡Œåˆ—$\Gamma_A$ã‚’åˆæœŸåŒ–ã›ãšã€ä»£ã‚ã‚Šã«$\lambda > 0$ãŒæ­£å‰‡åŒ–å› å­ã§ã‚ã‚Šã€$I âˆˆ R^{N Ã— N}$ãŒå˜ä½è¡Œåˆ—ã§ã‚ã‚‹å˜ä¸€ã®è¡Œåˆ—$\Gamma$ã‚’è¿½è·¡ã—ã¾ã™ã€‚

Suppose the neural networkâ€™s last layer representation is $\sigma_\theta(x_{t,A}) âˆˆ R^N$ and full model output $f_\theta(x_{t,A}) âˆˆ R$, the agent takes action  
ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€çµ‚å±¤ã®è¡¨ç¾ã‚’$\sigma_\theta(x_{t,A}) âˆˆ R^N$ã€å®Œå…¨ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‚’$f_\theta(x_{t,A}) âˆˆ R$ã¨ä»®å®šã™ã‚‹ã¨ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–ã‚Šã¾ã™ã€‚

$$
A_t = \arg \max_{a \in A} \left( f_\theta(x_{t,a}) + \alpha \sigma_\theta(x_{t,a})^\top \Gamma^{-1} \sigma_\theta(x_{t,a}) \right).
$$

with the same update for $\Gamma$ and $b$ except that now the update happens with the representation $\sigma_\theta(x_{t,A})$ instead of $x_{t,A}$.  
$\Gamma$ã¨$b$ã®æ›´æ–°ã¯åŒã˜ã§ã™ãŒã€ä»Šã¯æ›´æ–°ãŒè¡¨ç¾$\sigma_\theta(x_{t,A})$ã§è¡Œã‚ã‚Œã€$x_{t,A}$ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

NeuralUCB follows the same principle as above, but replacing $\sigma_\theta(x_{t,A})$ with the gradients of all parameters within the neural network.  
NeuralUCBã¯ä¸Šè¨˜ã¨åŒã˜åŸå‰‡ã«å¾“ã„ã¾ã™ãŒã€$\sigma_\theta(x_{t,A})$ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å†…ã®ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ã«ç½®ãæ›ãˆã¾ã™ã€‚

Although empirically the above neural methods, Neural Thompson sampling, Neural UCB and Neural LinUCB, have achieved good performance on synthetic datasets, their downside is that they all require inverting a square matrix with its dimension equal to either the entire neural networkâ€™s parameter size or the size of the last layer representation.  
çµŒé¨“çš„ã«ã¯ã€ä¸Šè¨˜ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ‰‹æ³•ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«UCBã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«LinUCBã¯åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è‰¯å¥½ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã¦ã„ã¾ã™ãŒã€å½¼ã‚‰ã®æ¬ ç‚¹ã¯ã€ã™ã¹ã¦ãŒæ­£æ–¹è¡Œåˆ—ã‚’åè»¢ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€ãã®æ¬¡å…ƒã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å…¨ä½“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã¾ãŸã¯æœ€çµ‚å±¤ã®è¡¨ç¾ã®ã‚µã‚¤ã‚ºã«ç­‰ã—ã„ã“ã¨ã§ã™ã€‚

Both in many cases are intractable, especially for real-world environments and complex neural networks.  
å¤šãã®ã‚±ãƒ¼ã‚¹ã§ã€ã“ã‚Œã¯æ‰±ã„ãã‚Œãªã„ã‚‚ã®ã§ã‚ã‚Šã€ç‰¹ã«å®Ÿä¸–ç•Œã®ç’°å¢ƒã‚„è¤‡é›‘ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã„ã¦ã¯ãã†ã§ã™ã€‚

#### 4 Epistemic Neural Recommendation
In this section, we introduce a novel architecture for scalable neural contextual bandit problems, drawing inspiration from Thompson sampling and recent developments in epistemic neural networks (ENN) and EpiNet [41].  
#### 4 ã‚¨ãƒ”ã‚¹ãƒ†ãƒŸãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨æœ€è¿‘ã®ã‚¨ãƒ”ã‚¹ãƒ†ãƒŸãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆENNï¼‰ãŠã‚ˆã³EpiNet[41]ã®é€²å±•ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¾—ãŸã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã®ãŸã‚ã®æ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

As discussed in Section 3.1, the objective of a deep learning-based Thompson sampling strategy is to accurately estimate the uncertainty of a prediction, pertaining to a context-action pair, while keeping computational costs to a minimum.  
ã‚»ã‚¯ã‚·ãƒ§ãƒ³3.1ã§è¿°ã¹ãŸã‚ˆã†ã«ã€æ·±å±¤å­¦ç¿’ã«åŸºã¥ããƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã®ç›®çš„ã¯ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ-ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒšã‚¢ã«é–¢é€£ã™ã‚‹äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ã‚’æ­£ç¢ºã«æ¨å®šã—ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹ã“ã¨ã§ã™ã€‚

A significant drawback of many neural methods for Thompson sampling and UCB is their computationally expensive uncertainty estimation process, which impedes their integration into real-world environments.  
ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚„UCBã®ãŸã‚ã®å¤šãã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ‰‹æ³•ã®é‡è¦ãªæ¬ ç‚¹ã¯ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã®é«˜ã„ä¸ç¢ºå®Ÿæ€§æ¨å®šãƒ—ãƒ­ã‚»ã‚¹ã§ã‚ã‚Šã€ã“ã‚ŒãŒå®Ÿä¸–ç•Œã®ç’°å¢ƒã¸ã®çµ±åˆã‚’å¦¨ã’ã¾ã™ã€‚

The primary aim of our proposed architecture and design is to address this very issue, while enhancing model performance.  
ç§ãŸã¡ã®ææ¡ˆã™ã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨è¨­è¨ˆã®ä¸»ãªç›®çš„ã¯ã€ã“ã®å•é¡Œã«å¯¾å‡¦ã—ãªãŒã‚‰ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã§ã™ã€‚

#### 4.1 Informative Neural Representation
A key aspect shared across all neural exploration methods is the generation of effective representations for context-action pairs.  
#### 4.1 æœ‰ç›Šãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«è¡¨ç¾
ã™ã¹ã¦ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ¢ç´¢æ‰‹æ³•ã«å…±é€šã™ã‚‹é‡è¦ãªå´é¢ã¯ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ-ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒšã‚¢ã®ãŸã‚ã®åŠ¹æœçš„ãªè¡¨ç¾ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚

Within the contextual bandit framework, there are three main representation factors to consider: action representation, context representation, and the interaction between context and action.  
ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®æ çµ„ã¿å†…ã§ã¯ã€è€ƒæ…®ã™ã¹ã3ã¤ã®ä¸»è¦ãªè¡¨ç¾è¦å› ãŒã‚ã‚Šã¾ã™ï¼šã‚¢ã‚¯ã‚·ãƒ§ãƒ³è¡¨ç¾ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¾ã€ãŠã‚ˆã³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ç›¸äº’ä½œç”¨ã§ã™ã€‚

We assume a general and unnormalized feature representation from the environment for both contexts and actions.  
ç§ãŸã¡ã¯ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ä¸¡æ–¹ã«å¯¾ã—ã¦ã€ç’°å¢ƒã‹ã‚‰ã®ä¸€èˆ¬çš„ã§éæ­£è¦åŒ–ã•ã‚ŒãŸç‰¹å¾´è¡¨ç¾ã‚’ä»®å®šã—ã¾ã™ã€‚

To distill both context and action feature representation vectors, we initially feed each vector into a linear layer followed by a ReLU activation function.  
ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ç‰¹å¾´è¡¨ç¾ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã«ã€æœ€åˆã«å„ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç·šå½¢å±¤ã«å…¥åŠ›ã—ã€ãã®å¾Œã«ReLUæ´»æ€§åŒ–é–¢æ•°ã‚’é©ç”¨ã—ã¾ã™ã€‚

This is subsequently followed by a Layer Normalization (LayerNorm) layer [7].  
ãã®å¾Œã€ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ï¼ˆLayerNormï¼‰å±¤[7]ãŒç¶šãã¾ã™ã€‚

Given that raw features can contain disproportionately large values that are challenging to normalize - particularly in an online bandit setting where data continually stream in - it is crucial to employ layer normalization.  
ç”Ÿã®ç‰¹å¾´ãŒæ­£è¦åŒ–ãŒé›£ã—ã„ä¸å‡è¡¡ã«å¤§ããªå€¤ã‚’å«ã‚€å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ç‰¹ã«ãƒ‡ãƒ¼ã‚¿ãŒç¶™ç¶šçš„ã«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã•ã‚Œã‚‹ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆè¨­å®šã§ã¯ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚

This technique ensures smoother gradients, promoting stability and generalizability [58].  
ã“ã®æŠ€è¡“ã¯ã€ã‚ˆã‚Šæ»‘ã‚‰ã‹ãªå‹¾é…ã‚’ä¿è¨¼ã—ã€å®‰å®šæ€§ã¨ä¸€èˆ¬åŒ–èƒ½åŠ›ã‚’ä¿ƒé€²ã—ã¾ã™[58]ã€‚

More formally,  
ã‚ˆã‚Šæ­£å¼ã«ã¯ã€

$$
h_\beta^{context}(\psi_{S_t}) = \text{LayerNorm}(\text{ReLU}(\beta_{context}^\top [\psi_{S_t}])),
$$

$$
h_\beta^{action}(\phi_A) = \text{LayerNorm}(\text{ReLU}(\beta_{action}^\top [\phi_A])).
$$

where $\theta_{context} âˆˆ R^{d_S}$ and $\theta_{action} âˆˆ R^{d_A}$ are the parameters for context and action summarization.  
ã“ã“ã§ã€$\theta_{context} âˆˆ R^{d_S}$ãŠã‚ˆã³$\theta_{action} âˆˆ R^{d_A}$ã¯ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®è¦ç´„ã®ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™ã€‚

Note that after summarization through these two layers, the outputs are of the same shape.  
ã“ã‚Œã‚‰ã®2ã¤ã®å±¤ã‚’é€šã˜ã¦è¦ç´„ã—ãŸå¾Œã€å‡ºåŠ›ã¯åŒã˜å½¢çŠ¶ã«ãªã‚Šã¾ã™ã€‚

Having summarized and normalized representations for both action and context, we now model the interaction between these two entities.  
ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ä¸¡æ–¹ã®è¦ç´„ãŠã‚ˆã³æ­£è¦åŒ–ã•ã‚ŒãŸè¡¨ç¾ã‚’æŒã£ã¦ã€ã“ã‚Œã‚‰2ã¤ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“ã®ç›¸äº’ä½œç”¨ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ã€‚

Collaborative Filtering [23, 29, 45] and Matrix Factorization [34] provide intuitive models for this interaction.  
å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°[23, 29, 45]ã¨è¡Œåˆ—åˆ†è§£[34]ã¯ã€ã“ã®ç›¸äº’ä½œç”¨ã®ãŸã‚ã®ç›´æ„Ÿçš„ãªãƒ¢ãƒ‡ãƒ«ã‚’æä¾›ã—ã¾ã™ã€‚

Inspired by these methods, we use element-wise multiplication to represent interactions,  
ã“ã‚Œã‚‰ã®æ–¹æ³•ã«è§¦ç™ºã•ã‚Œã¦ã€ç›¸äº’ä½œç”¨ã‚’è¡¨ç¾ã™ã‚‹ãŸã‚ã«è¦ç´ ã”ã¨ã®ä¹—ç®—ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

$$
I(\psi_{S_t}, \phi_A) = h_\beta^{context}(\psi_{S_t}) \odot h_\beta^{action}(\phi_A).
$$

Utilizing the above three representations, we concatenate the three vectors to derive  
ä¸Šè¨˜ã®3ã¤ã®è¡¨ç¾ã‚’åˆ©ç”¨ã—ã¦ã€3ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’é€£çµã—ã¦å°å‡ºã—ã¾ã™ã€‚

$$
x_{t,A} = \text{concat}(h_\beta^{context}(\psi_{S_t}), h_\beta^{action}(\phi_A), I(\psi_{S_t}, \phi_A)).
$$

Notably, this representation diverges from Neural Collaborative Filtering (NCF) [28] in two primary ways.  
ç‰¹ã«ã€ã“ã®è¡¨ç¾ã¯ã€2ã¤ã®ä¸»è¦ãªæ–¹æ³•ã§ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆNCFï¼‰[28]ã‹ã‚‰é€¸è„±ã—ã¾ã™ã€‚

Firstly, NCF can only manage id-listed features and it aggregates the concatenation of action and context through multi-layer perceptrons.  
ç¬¬ä¸€ã«ã€NCFã¯IDãƒªã‚¹ãƒˆã•ã‚ŒãŸç‰¹å¾´ã®ã¿ã‚’ç®¡ç†ã§ãã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®é€£çµã‚’å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã‚’é€šã˜ã¦é›†ç´„ã—ã¾ã™ã€‚

In contrast, our approach can handle feature vectors without making any preliminary assumptions about the input feature shape or values.  
å¯¾ç…§çš„ã«ã€ç§ãŸã¡ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€å…¥åŠ›ç‰¹å¾´ã®å½¢çŠ¶ã‚„å€¤ã«é–¢ã™ã‚‹äº‹å‰ã®ä»®å®šã‚’è¡Œã†ã“ã¨ãªãã€ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‡¦ç†ã§ãã¾ã™ã€‚

Secondly, we use the representations obtained through summarization for uncertainty estimation, as discussed in the following subsection.  
ç¬¬äºŒã«ã€æ¬¡ã®ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§èª¬æ˜ã™ã‚‹ã‚ˆã†ã«ã€ä¸ç¢ºå®Ÿæ€§æ¨å®šã®ãŸã‚ã«è¦ç´„ã‚’é€šã˜ã¦å¾—ã‚‰ã‚ŒãŸè¡¨ç¾ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

Moreover, we demonstrate in later sections through empirical studies that the direct application of NCF with uncertainty estimation yields inferior performance compared to our method.  
ã•ã‚‰ã«ã€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§å®Ÿè¨¼ç ”ç©¶ã‚’é€šã˜ã¦ã€ä¸ç¢ºå®Ÿæ€§æ¨å®šã‚’ä¼´ã†NCFã®ç›´æ¥çš„ãªé©ç”¨ãŒç§ãŸã¡ã®æ–¹æ³•ã¨æ¯”è¼ƒã—ã¦åŠ£ã£ãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ã‚‚ãŸã‚‰ã™ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚

#### 4.2 Epistemic Neural Recommendation (ENR)
Using the representation derived above, we can now design a neural network for both point estimate and uncertainty estimation.  
#### 4.2 ã‚¨ãƒ”ã‚¹ãƒ†ãƒŸãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆENRï¼‰
ä¸Šè¨˜ã§å°å‡ºã—ãŸè¡¨ç¾ã‚’ä½¿ç”¨ã—ã¦ã€ãƒã‚¤ãƒ³ãƒˆæ¨å®šã¨ä¸ç¢ºå®Ÿæ€§æ¨å®šã®ä¸¡æ–¹ã®ãŸã‚ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’è¨­è¨ˆã§ãã¾ã™ã€‚

For efficient epistemic uncertainty estimation using neural networks, we employ EpiNet [41], an auxiliary architecture for a general neural network.  
ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ãŸåŠ¹ç‡çš„ãªã‚¨ãƒ”ã‚¹ãƒ†ãƒŸãƒƒã‚¯ä¸ç¢ºå®Ÿæ€§æ¨å®šã®ãŸã‚ã«ã€ä¸€èˆ¬çš„ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è£œåŠ©ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚ã‚‹EpiNet[41]ã‚’æ¡ç”¨ã—ã¾ã™ã€‚

This architecture leverages the final layer representation of a neural network, along with an epistemic index $z$, to generate a sample from the posterior.  
ã“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€çµ‚å±¤ã®è¡¨ç¾ã¨ã‚¨ãƒ”ã‚¹ãƒ†ãƒŸãƒƒã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹$z$ã‚’æ´»ç”¨ã—ã¦ã€äº‹å¾Œã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

EpiNet presents a cost-effective method for constructing ENNs, which has been demonstrated to excel in making joint predictions for classification and supervised learning tasks, while achieving strong performance in synthesized neural logistic bandits with minimal additional computational costs.  
EpiNetã¯ã€ENNã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ã‚³ã‚¹ãƒˆåŠ¹ç‡ã®è‰¯ã„æ–¹æ³•ã‚’æä¾›ã—ã€åˆ†é¡ãŠã‚ˆã³æ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚¿ã‚¹ã‚¯ã®ãŸã‚ã®å…±åŒäºˆæ¸¬ã‚’è¡Œã†ã®ã«å„ªã‚Œã¦ã„ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ãŠã‚Šã€æœ€å°é™ã®è¿½åŠ è¨ˆç®—ã‚³ã‚¹ãƒˆã§åˆæˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã§å¼·åŠ›ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚

However, a drawback of only utilizing the last layer representation, particularly in a contextual bandit setting, is the diminished representation power.  
ã—ã‹ã—ã€ç‰¹ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆè¨­å®šã«ãŠã„ã¦ã€æœ€çµ‚å±¤ã®è¡¨ç¾ã®ã¿ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã®æ¬ ç‚¹ã¯ã€è¡¨ç¾åŠ›ãŒä½ä¸‹ã™ã‚‹ã“ã¨ã§ã™ã€‚

The interactions between context and action are summarized through multi-layers perceptrons for marginal predictions, and considerable information is lost in the process.  
ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ç›¸äº’ä½œç”¨ã¯ã€é™ç•Œäºˆæ¸¬ã®ãŸã‚ã«å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã‚’é€šã˜ã¦è¦ç´„ã•ã‚Œã€ãã®éç¨‹ã§ã‹ãªã‚Šã®æƒ…å ±ãŒå¤±ã‚ã‚Œã¾ã™ã€‚

Motivated by this, we introduce Epistemic Neural Recommendation (ENR) that generates improved posterior samples via more informative representations.  
ã“ã‚Œã«è§¦ç™ºã•ã‚Œã¦ã€ã‚ˆã‚Šæœ‰ç›Šãªè¡¨ç¾ã‚’é€šã˜ã¦æ”¹å–„ã•ã‚ŒãŸäº‹å¾Œã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã‚¨ãƒ”ã‚¹ãƒ†ãƒŸãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆENRï¼‰ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

The remaining architecture of ENR is comprised of three parts: a function $f_\theta^x : R^{3d_E} \to R$ for marginal prediction, and two functions $g_\sigma$ and $g_{\sigma_p}$, both mapping from $R^{3d_E}$ to $R^{d_z}$, for uncertainty estimation.  
ENRã®æ®‹ã‚Šã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€é™ç•Œäºˆæ¸¬ã®ãŸã‚ã®é–¢æ•°$f_\theta^x : R^{3d_E} \to R$ã¨ã€ä¸ç¢ºå®Ÿæ€§æ¨å®šã®ãŸã‚ã®2ã¤ã®é–¢æ•°$g_\sigma$ã¨$g_{\sigma_p}$ã‹ã‚‰æ§‹æˆã•ã‚Œã¦ãŠã‚Šã€ä¸¡æ–¹ã¨ã‚‚$R^{3d_E}$ã‹ã‚‰$R^{d_z}$ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¾ã™ã€‚

Upon extraction of $x_{t,A}$, we employ $f_\theta^x$ to make the marginal prediction via $f_\theta^x(x_{t,A})$.  
$x_{t,A}$ã‚’æŠ½å‡ºã—ãŸå¾Œã€$f_\theta^x$ã‚’ä½¿ç”¨ã—ã¦$f_\theta^x(x_{t,A})$ã‚’ä»‹ã—ã¦é™ç•Œäºˆæ¸¬ã‚’è¡Œã„ã¾ã™ã€‚

In addition to $f_\theta^x$, $g_\sigma$ and $g_{\sigma_p}$ are independently initialized through Glorot Initialization [8, 22], and $g_{\sigma_p}$ remains constant throughout its lifetime, thereby providing robust regularization.  
$f_\theta^x$ã«åŠ ãˆã¦ã€$g_\sigma$ã¨$g_{\sigma_p}$ã¯GlorotåˆæœŸåŒ–[8, 22]ã‚’é€šã˜ã¦ç‹¬ç«‹ã«åˆæœŸåŒ–ã•ã‚Œã€$g_{\sigma_p}$ã¯ãã®ç”Ÿæ¶¯ã‚’é€šã˜ã¦ä¸€å®šã®ã¾ã¾ã§ã‚ã‚Šã€å …ç‰¢ãªæ­£å‰‡åŒ–ã‚’æä¾›ã—ã¾ã™ã€‚

To initialize this segment of the network, ENR samples an epistemic index $z$ from its prior $P_z$, which could either be a discrete one-hot vector or a standard multivariate Gaussian sample.  
ã“ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã™ã‚‹ãŸã‚ã«ã€ENRã¯ãã®äº‹å‰$P_z$ã‹ã‚‰ã‚¨ãƒ”ã‚¹ãƒ†ãƒŸãƒƒã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹$z$ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€é›¢æ•£çš„ãªãƒ¯ãƒ³ãƒ›ãƒƒãƒˆãƒ™ã‚¯ãƒˆãƒ«ã¾ãŸã¯æ¨™æº–å¤šå¤‰é‡ã‚¬ã‚¦ã‚¹ã‚µãƒ³ãƒ—ãƒ«ã®ã„ãšã‚Œã‹ã§ã™ã€‚

The sampled $z$ is then concatenated with $x_{t,A}$ and processed through both $g_\sigma$ and $g_{\sigma_p}$.  
ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸ$z$ã¯æ¬¡ã«$x_{t,A}$ã¨é€£çµã•ã‚Œã€$g_\sigma$ã¨$g_{\sigma_p}$ã®ä¸¡æ–¹ã‚’é€šã˜ã¦å‡¦ç†ã•ã‚Œã¾ã™ã€‚

Consequently, the uncertainty estimation is $(g_\sigma(x_{t,A}, z) + g_{\sigma_p}(x_{t,A}, z))^\top z$.  
ãã®çµæœã€ä¸ç¢ºå®Ÿæ€§æ¨å®šã¯$(g_\sigma(x_{t,A}, z) + g_{\sigma_p}(x_{t,A}, z))^\top z$ã¨ãªã‚Šã¾ã™ã€‚

Hence the final output of ENR is  
ã—ãŸãŒã£ã¦ã€ENRã®æœ€çµ‚å‡ºåŠ›ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

$$
f_\theta(\psi_S, \phi_A, z) = f_\theta^x(x_{t,A}) + (g_\sigma(sg[x_{t,A}], z) + g_{\sigma_p}(sg[x_{t,A}], z))^\top z,
$$

where $x_{t,A}$ is defined in Equation 11, $\theta = (\theta_x, \sigma, \sigma_p, \beta_{context}, \beta_{action})$, and $sg[Â·]$ stops gradient flow of the parameter within the bracket.  
ã“ã“ã§ã€$x_{t,A}$ã¯å¼11ã§å®šç¾©ã•ã‚Œã€$\theta = (\theta_x, \sigma, \sigma_p, \beta_{context}, \beta_{action})$ã§ã‚ã‚Šã€$sg[Â·]$ã¯æ‹¬å¼§å†…ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ãƒ•ãƒ­ãƒ¼ã‚’åœæ­¢ã—ã¾ã™ã€‚

Note that, if the reward function is a binary variable, then a sigmoid would be added to the final output of $f_\theta(\psi_S, \phi_A, z)$ as neural logistic regression is a common setup in RS.  
å ±é…¬é–¢æ•°ãŒãƒã‚¤ãƒŠãƒªå¤‰æ•°ã§ã‚ã‚‹å ´åˆã€$f_\theta(\psi_S, \phi_A, z)$ã®æœ€çµ‚å‡ºåŠ›ã«ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰ãŒè¿½åŠ ã•ã‚Œã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã¯RSã§ä¸€èˆ¬çš„ãªè¨­å®šã§ã™ã€‚

With the neural network above, an ENR Thompson sampling agent executes action  
ä¸Šè¨˜ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦ã€ENRãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

$$
A_t = \arg \max_{a \in A} f_\theta(\psi_{S_t}, \phi_a, z), \quad z \sim P_z.
$$

After receiving reward $R_{t+1}$, then the agent can update the neural network with  
å ±é…¬$R_{t+1}$ã‚’å—ã‘å–ã£ãŸå¾Œã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯æ¬¡ã®ã‚ˆã†ã«ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ›´æ–°ã§ãã¾ã™ã€‚

$$
\theta \leftarrow \theta - \alpha \nabla_\theta L(R_{t+1}, f_\theta(\psi_{S_t}, \phi_{A_t}, z)),
$$

where $z \in Z$ through stochastic gradient descent, where $Z$ is a set of epistemic indices sampled from $P_z$ and $\alpha$ is the learning rate.  
ã“ã“ã§ã€$z \in Z$ã¯ç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•ã‚’é€šã˜ã¦ã€$Z$ã¯$P_z$ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸã‚¨ãƒ”ã‚¹ãƒ†ãƒŸãƒƒã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚»ãƒƒãƒˆã§ã‚ã‚Šã€$\alpha$ã¯å­¦ç¿’ç‡ã§ã™ã€‚

In practice, we perform batch updates through sampling from a replay buffer and uses other more advanced optimization strategies such as ADAM [33] for parameter updates.  
å®Ÿéš›ã«ã¯ã€ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ãƒãƒƒãƒæ›´æ–°ã‚’è¡Œã„ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã®ãŸã‚ã«ADAM[33]ãªã©ã®ä»–ã®ã‚ˆã‚Šé«˜åº¦ãªæœ€é©åŒ–æˆ¦ç•¥ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

The loss function here could be cross-entropy loss for neural logistic contextual bandits and mean squared error loss for contextual bandits with non-binary reward.  
ã“ã“ã§ã®æå¤±é–¢æ•°ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ãŸã‚ã®ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã¨ã€éãƒã‚¤ãƒŠãƒªå ±é…¬ã‚’æŒã¤ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ãŸã‚ã®å¹³å‡äºŒä¹—èª¤å·®æå¤±ã§ã™ã€‚

For the full illustration of the architecture, please refer to Figure 2. For a detailed algorithm illustration, please refer to Algorithm 1.  
ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å®Œå…¨ãªèª¬æ˜ã«ã¤ã„ã¦ã¯ã€å›³2ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚è©³ç´°ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®èª¬æ˜ã«ã¤ã„ã¦ã¯ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 1ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

We would like to also note that this algorithm is a full online bandit algorithm without any prior knowledge or pre-training.  
ã“ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€äº‹å‰çŸ¥è­˜ã‚„äº‹å‰å­¦ç¿’ãªã—ã®å®Œå…¨ãªã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚ã‚‹ã“ã¨ã‚‚æ³¨è¨˜ã—ã¦ãŠãã¾ã™ã€‚

This algorithm can also be extended to perform offline pretraining before online interactions to reduce the amount of exploration required.  
ã“ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã®å‰ã«ã‚ªãƒ•ãƒ©ã‚¤ãƒ³äº‹å‰å­¦ç¿’ã‚’è¡Œã£ã¦ã€å¿…è¦ãªæ¢ç´¢é‡ã‚’æ¸›ã‚‰ã™ã‚ˆã†ã«æ‹¡å¼µã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

We leave this for future work.  
ã“ã‚Œã«ã¤ã„ã¦ã¯ä»Šå¾Œã®ä½œæ¥­ã«æ®‹ã—ã¾ã™ã€‚

#### 5 Experiments
In this section, we will go through a set of experiments ranging from a toy experiment to large-scale real-world data experiments.  
#### 5 å®Ÿé¨“
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ç©å…·å®Ÿé¨“ã‹ã‚‰å¤§è¦æ¨¡ãªå®Ÿä¸–ç•Œãƒ‡ãƒ¼ã‚¿å®Ÿé¨“ã¾ã§ã®ä¸€é€£ã®å®Ÿé¨“ã‚’è¡Œã„ã¾ã™ã€‚

The dataset statistics are shown in Table 1. Each of the dataset has millions of interactions.  
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆã¯è¡¨1ã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯æ•°ç™¾ä¸‡ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã™ã€‚

We will compare ENR and EpiNet  
ç§ãŸã¡ã¯ENRã¨EpiNetã‚’æ¯”è¼ƒã—ã¾ã™ã€‚

**Table 1: Experiment Dataset Statistics**  
**è¡¨1: å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆ**

| Dataset | # Users | # Items | # Interactions |
|---------|---------|---------|----------------|
| MIND [57] | 1,000,000 | 160,000 | 15,000,000 |
| KuaiRec [21] | 1,411 | 3,327 | 4,676,570 |

against ğœ–Greedy, Ensemble Sampling with Prior Networks (Ensemble) [38], Neural Thompson Sampling (Neural TS) [61], Neural UCB [64], and Neural LinUCB [59], each with a similar sized neural network.  
ğœ–Greedyã€äº‹å‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ãŸã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰[38]ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«TSï¼‰[61]ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«UCB[64]ã€ãŠã‚ˆã³ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«LinUCB[59]ã¨æ¯”è¼ƒã—ã¾ã™ã€‚å„ã€…ã¯åŒæ§˜ã®ã‚µã‚¤ã‚ºã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æŒã£ã¦ã„ã¾ã™ã€‚

In addition, to assess advantages attributable to the architecture of ENR, we will also evaluate it against a few well recognized RS neural network architecture including, Neural Collaborative Filtering (NCF) [28], Neural Recommendation with Attentive MultiView Learning (NAML) [55], Neural Recommendation with MultiHead Self-Attention (NRMS) [56], Wide and Deep [14], and DeepFM [27], combined with exploration strategies above.  
ã•ã‚‰ã«ã€ENRã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«èµ·å› ã™ã‚‹åˆ©ç‚¹ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆNCFï¼‰[28]ã€æ³¨æ„ã‚’æŒã¤ãƒãƒ«ãƒãƒ“ãƒ¥ãƒ¼å­¦ç¿’ã«ã‚ˆã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ¨è–¦ï¼ˆNAMLï¼‰[55]ã€ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰è‡ªå·±æ³¨æ„ã«ã‚ˆã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ¨è–¦ï¼ˆNRMSï¼‰[56]ã€Wide and Deep [14]ã€ãŠã‚ˆã³DeepFM [27]ãªã©ã®ã„ãã¤ã‹ã®èªçŸ¥ã•ã‚ŒãŸRSãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã€ä¸Šè¨˜ã®æ¢ç´¢æˆ¦ç•¥ã‚’çµ„ã¿åˆã‚ã›ã¦è©•ä¾¡ã—ã¾ã™ã€‚

To ensure fairness of evaluation, we ensure that all neural network architectures other than vanilla MLP has a similar parameter size.  
è©•ä¾¡ã®å…¬å¹³æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã«ã€ãƒãƒ‹ãƒ©MLPä»¥å¤–ã®ã™ã¹ã¦ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒåŒæ§˜ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’æŒã¤ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚

See Table 3. The algorithms all adopt ADAM [33] for model optimization.  
è¡¨3ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ã™ã¹ã¦ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ã®ãŸã‚ã«ADAM[33]ã‚’æ¡ç”¨ã—ã¾ã™ã€‚

Note that for both Neural UCB and Neural Thompson Sampling, due to their scalability, we are only able to experiment them with their last layer versions, where we only compute gradients of the last layer of the neural networks.  
ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«UCBã¨ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ä¸¡æ–¹ã«ã¤ã„ã¦ã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®ãŸã‚ã«ã€ç§ãŸã¡ã¯ãã‚Œã‚‰ã®æœ€çµ‚å±¤ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã®ã¿å®Ÿé¨“ã§ãã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€çµ‚å±¤ã®å‹¾é…ã®ã¿ã‚’è¨ˆç®—ã—ã¾ã™ã€‚

The full Neural UCB and Neural Thompson Sampling agents require inverting matrices of sizes $10^6 Ã— 10^6$ (7.45 TB of memory and $O(10^{18})$ complexity) and $10^7 Ã— 10^7$ (745 TB of memory and $O(10^{21})$ complexity) for MIND and KuaiRec experiments respectively, which are intractable in most commercial machines.  
å®Œå…¨ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«UCBãŠã‚ˆã³ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€MINDãŠã‚ˆã³KuaiRecå®Ÿé¨“ã«å¯¾ã—ã¦ãã‚Œãã‚Œã‚µã‚¤ã‚º$10^6 Ã— 10^6$ï¼ˆ7.45 TBã®ãƒ¡ãƒ¢ãƒªã¨$O(10^{18})$ã®è¤‡é›‘ã•ï¼‰ãŠã‚ˆã³$10^7 Ã— 10^7$ï¼ˆ745 TBã®ãƒ¡ãƒ¢ãƒªã¨$O(10^{21})$ã®è¤‡é›‘ã•ï¼‰ã®è¡Œåˆ—ã‚’åè»¢ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€ã“ã‚Œã¯ã»ã¨ã‚“ã©ã®å•†æ¥­æ©Ÿæ¢°ã§ã¯æ‰±ã„ãã‚Œã¾ã›ã‚“ã€‚

See more details of computation costs in Table 4 and 5 respectively for two experiments and all experiments are performed on an AWS p4d24xlarge machine with 8 A100 GPUs, each GPU with 40 GB of GPU memory.  
è¨ˆç®—ã‚³ã‚¹ãƒˆã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€è¡¨4ãŠã‚ˆã³5ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ã™ã¹ã¦ã®å®Ÿé¨“ã¯ã€8ã¤ã®A100 GPUã‚’æŒã¤AWS p4d24xlargeãƒã‚·ãƒ³ã§å®Ÿè¡Œã•ã‚Œã€å„GPUã¯40 GBã®GPUãƒ¡ãƒ¢ãƒªã‚’æŒã£ã¦ã„ã¾ã™ã€‚

NeuralLinUCB requires 6x and 3.5x inference time compared to our method in two real-world experiments with a similar training time.  
NeuralLinUCBã¯ã€åŒæ§˜ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ã§2ã¤ã®å®Ÿä¸–ç•Œã®å®Ÿé¨“ã«ãŠã„ã¦ã€ç§ãŸã¡ã®æ–¹æ³•ã¨æ¯”è¼ƒã—ã¦6å€ãŠã‚ˆã³3.5å€ã®æ¨è«–æ™‚é–“ã‚’å¿…è¦ã¨ã—ã¾ã™ã€‚

Simplified versions of NeuralTS and NeuralUCB using only gradients from the last layer of NNs require 10x and 100x inference time compared to our method in these experiments.  
ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€çµ‚å±¤ã®å‹¾é…ã®ã¿ã‚’ä½¿ç”¨ã—ãŸNeuralTSãŠã‚ˆã³NeuralUCBã®ç°¡ç•¥åŒ–ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ã€ã“ã‚Œã‚‰ã®å®Ÿé¨“ã«ãŠã„ã¦ç§ãŸã¡ã®æ–¹æ³•ã¨æ¯”è¼ƒã—ã¦10å€ãŠã‚ˆã³100å€ã®æ¨è«–æ™‚é–“ã‚’å¿…è¦ã¨ã—ã¾ã™ã€‚

Lastly, Ensemble requires a similar inference time, but requires 5x training time compared to our method with parallel computing optimization.  
æœ€å¾Œã«ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã¯åŒæ§˜ã®æ¨è«–æ™‚é–“ã‚’å¿…è¦ã¨ã—ã¾ã™ãŒã€ç§ãŸã¡ã®æ–¹æ³•ã¨æ¯”è¼ƒã—ã¦5å€ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ã‚’å¿…è¦ã¨ã—ã¾ã™ã€‚ä¸¦åˆ—è¨ˆç®—ã®æœ€é©åŒ–ãŒã‚ã‚Šã¾ã™ã€‚

NeuralLinUCB, NeuralUCB, NeuralTS all require orders of magnitude higher inference cost and Ensemble requires orders of magnitude higher training cost.  
NeuralLinUCBã€NeuralUCBã€NeuralTSã¯ã™ã¹ã¦æ¡é•ã„ã«é«˜ã„æ¨è«–ã‚³ã‚¹ãƒˆã‚’å¿…è¦ã¨ã—ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã¯æ¡é•ã„ã«é«˜ã„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ã‚¹ãƒˆã‚’å¿…è¦ã¨ã—ã¾ã™ã€‚

We will use average reward as performance metric in the following experiments (average click-through rate and average user rating in MIND and KuaiRec respectively).  
æ¬¡ã®å®Ÿé¨“ã§ã¯ã€å¹³å‡å ±é…¬ã‚’ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªãƒƒã‚¯ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ï¼ˆãã‚Œãã‚ŒMINDãŠã‚ˆã³KuaiRecã§ã®å¹³å‡ã‚¯ãƒªãƒƒã‚¯ç‡ã¨å¹³å‡ãƒ¦ãƒ¼ã‚¶è©•ä¾¡ï¼‰ã€‚

We choose to not use classic metrics like NDCG or precision because they are supervised-learning metrics.  
NDCGã‚„ç²¾åº¦ã®ã‚ˆã†ãªå¤å…¸çš„ãªãƒ¡ãƒˆãƒªãƒƒã‚¯ã¯ã€æ•™å¸«ã‚ã‚Šå­¦ç¿’ãƒ¡ãƒˆãƒªãƒƒã‚¯ã§ã‚ã‚‹ãŸã‚ä½¿ç”¨ã—ãªã„ã“ã¨ã«ã—ã¾ã™ã€‚

In online bandits, the agentâ€™s optimal strategy could try to explore and retrieve information instead of optimizing these metrics.  
ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã§ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æœ€é©ãªæˆ¦ç•¥ã¯ã€ã“ã‚Œã‚‰ã®ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’æœ€é©åŒ–ã™ã‚‹ã®ã§ã¯ãªãã€æ¢ç´¢ã—ã¦æƒ…å ±ã‚’å–å¾—ã—ã‚ˆã†ã¨ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

#### 5.1 Toy Experiment
For this experiment, we define a synthetic environment to evaluate the performance of ENR compared to various baselines mentioned above.  
#### 5.1 ç©å…·å®Ÿé¨“
ã“ã®å®Ÿé¨“ã§ã¯ã€ä¸Šè¨˜ã®ã•ã¾ã–ã¾ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒã—ã¦ENRã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€åˆæˆç’°å¢ƒã‚’å®šç¾©ã—ã¾ã™ã€‚

According to Section 2, the synthetic environment is characterized as: Both action and context vectors are of dimension 100.  
ã‚»ã‚¯ã‚·ãƒ§ãƒ³2ã«å¾“ã£ã¦ã€åˆæˆç’°å¢ƒã¯æ¬¡ã®ã‚ˆã†ã«ç‰¹å¾´ä»˜ã‘ã‚‰ã‚Œã¾ã™ï¼šã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ™ã‚¯ãƒˆãƒ«ã¯ã©ã¡ã‚‰ã‚‚æ¬¡å…ƒ100ã§ã™ã€‚

$$
R_{t+1} \sim \text{Bernoulli}[\sigma(\theta^\top \text{concat}(\psi_{S_t}, \phi_{A_t}))].
$$

The size of A is 100. $\sigma$ is the Sigmoid function and both $\theta$ is unknown to all the agents.  
$A$ã®ã‚µã‚¤ã‚ºã¯100ã§ã™ã€‚$\sigma$ã¯ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã§ã‚ã‚Šã€$\theta$ã¯ã™ã¹ã¦ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã¨ã£ã¦æœªçŸ¥ã§ã™ã€‚

**Table 2: MIND Dataset Illustration**  
**è¡¨2: MINDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª¬æ˜**

| Impression ID | User ID | Time | User Interest History | News with Labels |
|---------------|---------|------|-----------------------|------------------|
| 91            | U397059 | 11/15/2019 10:22:32 AM | N106403 N71977 N97080 N102132 N129416-0 N26703-1 N120089-1 N53018-0 N89764-0 |

**Table 3: Parameter Sizes across Different Architectures**  
**è¡¨3: ç•°ãªã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£é–“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚º**

| Algorithm | MIND | KuaiRec |
|-----------|------|---------|
| MLP | $0.76 \times 10^6$ | $1.21 \times 10^7$ |
| Wide & Deep | / | $2.33 \times 10^7