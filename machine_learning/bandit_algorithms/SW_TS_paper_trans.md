refs: https://arxiv.org/html/2409.05181v3

# Sliding-Window Thompson Sampling for Non-Stationary Settings
# ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒ»ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹éå®šå¸¸è¨­å®š

## Abstract è¦ç´„

Non-stationary multi-armed bandits(NS-MABs) model sequential decision-making problems in which the expected rewards of a set of actions, a.k.a.arms, evolve over time.
éå®šå¸¸ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆï¼ˆNS-MABï¼‰ã¯ã€**ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚»ãƒƒãƒˆï¼ˆã„ã‚ã‚†ã‚‹ã‚¢ãƒ¼ãƒ ï¼‰ã®æœŸå¾…å ±é…¬ãŒæ™‚é–“ã¨ã¨ã‚‚ã«å¤‰åŒ–ã™ã‚‹é€æ¬¡çš„ãªæ„æ€æ±ºå®šå•é¡Œ**ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ã€‚
In this paper, we fill a gap in the literature by providing a novel analysis of Thompson sampling-inspired (TS) algorithms for NS-MABs that both corrects and generalizes existing work.
æœ¬è«–æ–‡ã§ã¯ã€æ—¢å­˜ã®ç ”ç©¶ã‚’ä¿®æ­£ã—ä¸€èˆ¬åŒ–ã™ã‚‹éå®šå¸¸ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆï¼ˆNS-MABï¼‰å‘ã‘ã®ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆTSï¼‰ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«é–¢ã™ã‚‹æ–°ã—ã„åˆ†æã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã€æ–‡çŒ®ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’åŸ‹ã‚ã¾ã™ã€‚
Specifically, we study the cumulative frequentist regret of two algorithms based on sliding-window TS approaches with different priors, namely Beta-SWTS and $\gamma$-SWGTS.
å…·ä½“çš„ã«ã¯ã€**ç•°ãªã‚‹äº‹å‰åˆ†å¸ƒã‚’æŒã¤ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦TSã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«åŸºã¥ã2ã¤ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **ã€ã™ãªã‚ã¡Beta-SWTSã¨$\gamma$-SWGTSã®ç´¯ç©é »åº¦ä¸»ç¾©çš„å¾Œæ‚”ã‚’ç ”ç©¶ã—ã¾ã™ã€‚
We derive a unifying regret upper bound for these algorithms that applies to any arbitrary NS-MAB (with either Bernoulli or subgaussian rewards).
ã“ã‚Œã‚‰ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«å¯¾ã—ã¦ã€ä»»æ„ã®éå®šå¸¸ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆï¼ˆãƒ™ãƒ«ãƒŒãƒ¼ã‚¤å ±é…¬ã¾ãŸã¯ã‚µãƒ–ã‚¬ã‚¦ã‚¹å ±é…¬ã®ã„ãšã‚Œã‹ã‚’æŒã¤ï¼‰ã«é©ç”¨å¯èƒ½ãªçµ±ä¸€çš„ãªå¾Œæ‚”ã®ä¸Šé™ã‚’å°å‡ºã—ã¾ã™ã€‚
Our result introduces new indices that capture the inherent sources of complexity in the learning problem.
ç§ãŸã¡ã®çµæœã¯ã€å­¦ç¿’å•é¡Œã«ãŠã‘ã‚‹å›ºæœ‰ã®è¤‡é›‘ã•ã®æºã‚’æ‰ãˆã‚‹æ–°ã—ã„æŒ‡æ¨™ã‚’å°å…¥ã—ã¾ã™ã€‚
Then, we specialize our general result to two of the most common NS-MAB settings: the abruptly changing and the smoothly changing environments, showing that it matches state-of-the-art results.
æ¬¡ã«ã€ä¸€èˆ¬çš„ãªçµæœã‚’æœ€ã‚‚ä¸€èˆ¬çš„ãª2ã¤ã®éå®šå¸¸ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆè¨­å®šã€ã™ãªã‚ã¡æ€¥æ¿€ã«å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã¨æ»‘ã‚‰ã‹ã«å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã«ç‰¹åŒ–ã—ã€æœ€å…ˆç«¯ã®çµæœã¨ä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
Finally, we evaluate the performance of the analyzed algorithms in simulated environments and compare them with state-of-the-art approaches for NS-MABs.
æœ€å¾Œã«ã€åˆ†æã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ€§èƒ½ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒã§è©•ä¾¡ã—ã€éå®šå¸¸ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«å¯¾ã™ã‚‹æœ€å…ˆç«¯ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨æ¯”è¼ƒã—ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## IIntroduction ã¯ã˜ã‚ã«

A multi-armed bandit [MAB, 32] problem is a sequential game between a learner and an environment. 
ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆï¼ˆMABï¼‰å•é¡Œã¯ã€å­¦ç¿’è€…ã¨ç’°å¢ƒã®é–“ã®é€æ¬¡ã‚²ãƒ¼ãƒ ã§ã™ã€‚
In each round $t$, the learner first chooses an action, often called arm, and the environment then reveals a reward. 
å„ãƒ©ã‚¦ãƒ³ãƒ‰$t$ã«ãŠã„ã¦ã€å­¦ç¿’è€…ã¯ã¾ãšã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆé€šå¸¸ã¯ã‚¢ãƒ¼ãƒ ã¨å‘¼ã°ã‚Œã‚‹ï¼‰ã‚’é¸æŠã—ã€ç’°å¢ƒã¯ãã®å¾Œå ±é…¬ã‚’æ˜ã‚‰ã‹ã«ã—ã¾ã™ã€‚
The goal of the learner is to balance exploration and exploitation, minimizing the expected cumulative regret, 
å­¦ç¿’è€…ã®ç›®æ¨™ã¯ã€æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚Šã€æœŸå¾…ã•ã‚Œã‚‹ç´¯ç©å¾Œæ‚”ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã§ã™ã€‚
defined as the performance difference, expressed in expected rewards, between playing the optimal arm and the learner. 
ã“ã‚Œã¯ã€æœ€é©ãªã‚¢ãƒ¼ãƒ ã‚’ãƒ—ãƒ¬ã‚¤ã™ã‚‹ã“ã¨ã¨å­¦ç¿’è€…ã®é–“ã®æœŸå¾…å ±é…¬ã§è¡¨ã•ã‚Œã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å·®ã¨ã—ã¦å®šç¾©ã•ã‚Œã¾ã™ã€‚
These algorithms have traditionally been studied in stationary settings where the environment does not change over time. 
**ã“ã‚Œã‚‰ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€ç’°å¢ƒãŒæ™‚é–“ã¨ã¨ã‚‚ã«å¤‰åŒ–ã—ãªã„å®šå¸¸çš„ãªè¨­å®šã§ä¼çµ±çš„ã«ç ”ç©¶ã•ã‚Œã¦ãã¾ã—ãŸã€‚**
As a consequence, the optimal arm $i^*$ is constant and does not depend on the round $t$. 
ãã®çµæœã€**æœ€é©ãªã‚¢ãƒ¼ãƒ  $i^*$ ã¯ä¸€å®š**ã§ã‚ã‚Šã€ãƒ©ã‚¦ãƒ³ãƒ‰$t$ã«ã¯ä¾å­˜ã—ã¾ã›ã‚“ã€‚(=ä¸€ç•ªè‰¯ã„ã‚¢ãƒ¼ãƒ ãŒæ™‚é–“ã«ã‚ˆã£ã¦å¤‰ã‚ã‚‰ãªã„!:thinking:)
However, many real-world applications, such as online advertising [37, 30], healthcare [35, 16, 18, 27] and dynamic pricing [21, 12], operate in environments that are changing over time. 
ã—ã‹ã—ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³åºƒå‘Š[37, 30]ã€ãƒ˜ãƒ«ã‚¹ã‚±ã‚¢[35, 16, 18, 27]ã€å‹•çš„ä¾¡æ ¼è¨­å®š[21, 12]ãªã©ã®**å¤šãã®å®Ÿä¸–ç•Œã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€æ™‚é–“ã¨ã¨ã‚‚ã«å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã§å‹•ä½œã—ã¾ã™**ã€‚
These are often referred to as non-stationary MABs (NS-MABs), where the world evolves independently of the actions taken by the learner. 
ã“ã‚Œã‚‰ã¯ã—ã°ã—ã°**éå®šå¸¸ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆï¼ˆNS-MABï¼‰**ã¨å‘¼ã°ã‚Œã€**ä¸–ç•Œã¯å­¦ç¿’è€…ãŒå–ã£ãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¨ã¯ç‹¬ç«‹ã—ã¦é€²åŒ–**ã—ã¾ã™ã€‚("ç‹¬ç«‹"ã£ã¦ã„ã†ã®ã¯ã€å­¦ç¿’è€…ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹å½±éŸ¿ã¨ã¯åˆ¥ã§ã€ç’°å¢ƒãŒå¤‰åŒ–ã™ã‚‹ã£ã¦ã“ã¨ã‹...!!:thinking:)
As a consequence, the optimal arm $i^*(t)$ is potentially different in every round $t$, making the decision problem more challenging. 
ãã®çµæœã€æœ€é©ãªã‚¢ãƒ¼ãƒ  $i^*(t)$ ã¯å„ãƒ©ã‚¦ãƒ³ãƒ‰ $t$ ã§ç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€æ„æ€æ±ºå®šå•é¡ŒãŒã‚ˆã‚Šå›°é›£ã«ãªã‚Šã¾ã™ã€‚
This requires the design of learning algorithms able to adapt to environment modifications. 
ã“ã‚Œã«ã¯ã€**ç’°å¢ƒã®å¤‰æ›´ã«é©å¿œã§ãã‚‹å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è¨­è¨ˆãŒå¿…è¦**ã§ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

In the past years, the bandit literature focused on the design of algorithms that handle specific classes of NS-MABs characterized by certain regularity conditions. 
éå»æ•°å¹´é–“ã€ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆæ–‡çŒ®ã¯ã€**ç‰¹å®šã®è¦å‰‡æ€§æ¡ä»¶ã«ã‚ˆã£ã¦ç‰¹å¾´ä»˜ã‘ã‚‰ã‚Œã‚‹NS-MAB**ã®ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã‚’å‡¦ç†ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è¨­è¨ˆã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã¾ã—ãŸã€‚(Sliding windowç³»æ‰‹æ³•ã£ã¦ã‚·ãƒ³ãƒ—ãƒ«ã ã‘ã©ã€ã‚„ã‚„å¾Œç™ºãªã®ã‹...!!:thinking:)
The piecewise-constant abruptly changing MABs [23, 40, 33, 6, 10, 11] are characterized by expected rewards that remain constant during some rounds and change at unknown rounds, called breakpoints. 
åŒºåˆ†å®šæ•°çš„ã«æ€¥æ¿€ã«å¤‰åŒ–ã™ã‚‹MAB[23, 40, 33, 6, 10, 11]ã¯ã€ã„ãã¤ã‹ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã§æœŸå¾…å ±é…¬ãŒä¸€å®šã§ã‚ã‚Šã€æœªçŸ¥ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã§å¤‰åŒ–ã™ã‚‹ã“ã¨ãŒç‰¹å¾´ã§ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã¯ãƒ–ãƒ¬ãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆã¨å‘¼ã°ã‚Œã¾ã™ã€‚
Another form of regularity are the smoothly changing MABs [15, 48] where the expected rewards vary by a limited amount across rounds. 
ã‚‚ã†ä¸€ã¤ã®è¦å‰‡æ€§ã®å½¢æ…‹ã¯ã€æœŸå¾…å ±é…¬ãŒãƒ©ã‚¦ãƒ³ãƒ‰é–“ã§é™ã‚‰ã‚ŒãŸé‡ã ã‘å¤‰åŒ–ã™ã‚‹æ»‘ã‚‰ã‹ã«å¤‰åŒ–ã™ã‚‹MAB[15, 48]ã§ã™ã€‚
Other forms of regularity include the rising [26, 36] and rotting [45] MABs, where the expected rewards can only increase or decrease in time, respectively, and the MABs with bounded variation [10], where the expected reward is constrained to have a finite cumulative variation over the learning horizon. 
ä»–ã®è¦å‰‡æ€§ã®å½¢æ…‹ã«ã¯ã€æœŸå¾…å ±é…¬ãŒæ™‚é–“ã¨ã¨ã‚‚ã«å¢—åŠ ã¾ãŸã¯æ¸›å°‘ã™ã‚‹ã“ã¨ã—ã‹ã§ããªã„ä¸Šæ˜‡[26, 36]ãŠã‚ˆã³è…æ•—[45]MABã€ãã—ã¦æœŸå¾…å ±é…¬ãŒå­¦ç¿’ã®ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã«ã‚ãŸã£ã¦æœ‰é™ã®ç´¯ç©å¤‰å‹•ã‚’æŒã¤ã‚ˆã†ã«åˆ¶ç´„ã•ã‚Œã‚‹æœ‰ç•Œå¤‰å‹•ã®MAB[10]ãŒå«ã¾ã‚Œã¾ã™ã€‚
Several algorithmic approaches have been adopted for addressing regret minimization in NS-MABs [e.g., 23, 15, 10, 48]. 
NS-MABã«ãŠã‘ã‚‹å¾Œæ‚”æœ€å°åŒ–ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€ã„ãã¤ã‹ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒæ¡ç”¨ã•ã‚Œã¦ã„ã¾ã™[e.g., 23, 15, 10, 48]ã€‚
Among them, Thompson sampling (TS) [47] is one of the most widely used bandit algorithms for its simplicity in implementation and its good empirical performance. 
ãã®ä¸­ã§ã‚‚ã€**ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆTSï¼‰[47]ã¯ã€å®Ÿè£…ã®ç°¡ä¾¿ã•ã¨è‰¯å¥½ãªçµŒé¨“çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãŸã‚ã«æœ€ã‚‚åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ä¸€ã¤**ã§ã™ã€‚
However, the classical TS algorithm is devised for stationary MABs where they enjoy strong theoretical guarantees [29, 4, 5]. 
**ã—ã‹ã—ã€å¤å…¸çš„ãªTSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€å¼·åŠ›ãªç†è«–çš„ä¿è¨¼ã‚’äº«å—ã™ã‚‹å®šå¸¸MABç”¨ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™**[29, 4, 5]ã€‚
Variations to the classical TS have been proposed to tackle NS-MABs including sliding-window [48] and discounted [39, 38, 17] approaches. 
**å¤å…¸çš„ãªTSã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒã€ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦[48]ã‚„å‰²å¼•[39, 38, 17]ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å«ã‚€NS-MABã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ææ¡ˆ**ã•ã‚Œã¦ã„ã¾ã™ã€‚
These algorithms come often with theoretical guarantees for specific classes of NS-MABs, namely piecewise-constant abruptly changing and smoothly changing. 
ã“ã‚Œã‚‰ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€ã—ã°ã—ã°ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã®NS-MABã€ã™ãªã‚ã¡åŒºåˆ†å®šæ•°çš„ã«æ€¥æ¿€ã«å¤‰åŒ–ã™ã‚‹ã‚‚ã®ã¨æ»‘ã‚‰ã‹ã«å¤‰åŒ–ã™ã‚‹ã‚‚ã®ã«å¯¾ã™ã‚‹ç†è«–çš„ä¿è¨¼ã‚’ä¼´ã„ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### Original Contributions æœ¬è«–æ–‡ã®ã‚ªãƒªã‚¸ãƒŠãƒ«ãªè²¢çŒ®

In this paper, differently from what is often done in literature, we provide a unifying analysis of sliding-window TS algorithms that does not rely on the specific form of non-stationarity (namely piecewise-constant abruptly changing and smoothly changing). 
æœ¬è«–æ–‡ã§ã¯ã€æ–‡çŒ®ã§ã—ã°ã—ã°è¡Œã‚ã‚Œã‚‹ã“ã¨ã¨ã¯ç•°ãªã‚Šã€**ç‰¹å®šã®éå®šå¸¸æ€§ã®å½¢æ…‹ï¼ˆã™ãªã‚ã¡åŒºåˆ†å®šæ•°çš„ã«æ€¥æ¿€ã«å¤‰åŒ–ã™ã‚‹ã‚‚ã®ã¨æ»‘ã‚‰ã‹ã«å¤‰åŒ–ã™ã‚‹ã‚‚ã®ï¼‰ã«ä¾å­˜ã—ãªã„ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦TSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **ã®çµ±ä¸€çš„ãªåˆ†æã‚’æä¾›ã—ã¾ã™ã€‚
(ãªã‚‹ã»ã©ã€SW-TSç³»ã®æ‰‹æ³•ã®å¼·ã¿ã£ã¦ã€éå®šå¸¸æ€§ã®å½¢æ…‹ã«ä¾å­˜ã—ãªã„ã£ã¦ã“ã¨ãªã®ã‹...!!:thinking:)
Our novel analysis shed lights on the inherent complexity of the regret minimization problem in general NS-MABs and introduces new quantities to characterize quantitatively such a complexity. 
ç§ãŸã¡ã®æ–°ã—ã„åˆ†æã¯ã€ä¸€èˆ¬çš„ãªNS-MABã«ãŠã‘ã‚‹å¾Œæ‚”æœ€å°åŒ–å•é¡Œã®å›ºæœ‰ã®è¤‡é›‘ã•ã«å…‰ã‚’å½“ã¦ã€ãã®è¤‡é›‘ã•ã‚’å®šé‡çš„ã«ç‰¹å¾´ä»˜ã‘ã‚‹æ–°ã—ã„é‡ã‚’å°å…¥ã—ã¾ã™ã€‚
Furthermore, we extend and correct the original analysis of TrovÃ² et al. [48]. 
ã•ã‚‰ã«ã€ç§ãŸã¡ã¯TrovÃ²ã‚‰[48]ã®å…ƒã®åˆ†æã‚’æ‹¡å¼µã—ã€ä¿®æ­£ã—ã¾ã™ã€‚
In Appendix D, we show that some passages of the analysis by TrovÃ² et al. [48] are incorrect. 
ä»˜éŒ²Dã§ã¯ã€TrovÃ²ã‚‰[48]ã®åˆ†æã®ã„ãã¤ã‹ã®éƒ¨åˆ†ãŒä¸æ­£ç¢ºã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
Finally, we show how the state-of-the-art results for the specific forms of non-stationarity (namely piecewise-constant abruptly changing and smoothly changing) can be retrieved as a particular case of our analysis. 
æœ€å¾Œã«ã€ç‰¹å®šã®éå®šå¸¸æ€§ã®å½¢æ…‹ï¼ˆã™ãªã‚ã¡åŒºåˆ†å®šæ•°çš„ã«æ€¥æ¿€ã«å¤‰åŒ–ã™ã‚‹ã‚‚ã®ã¨æ»‘ã‚‰ã‹ã«å¤‰åŒ–ã™ã‚‹ã‚‚ã®ï¼‰ã«å¯¾ã™ã‚‹æœ€å…ˆç«¯ã®çµæœãŒã€ç§ãŸã¡ã®åˆ†æã®ç‰¹åˆ¥ãªã‚±ãƒ¼ã‚¹ã¨ã—ã¦å†å–å¾—ã§ãã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

The content of the paper is summarized as follows: 
æœ¬è«–æ–‡ã®å†…å®¹ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¦ç´„ã•ã‚Œã¾ã™ï¼š

- In Section II, we survey the related works on TS algorithms and approaches for regret minimization in NS-MABs. 
  - ã‚»ã‚¯ã‚·ãƒ§ãƒ³IIã§ã¯ã€TSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨NS-MABã«ãŠã‘ã‚‹å¾Œæ‚”æœ€å°åŒ–ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«é–¢ã™ã‚‹é–¢é€£ç ”ç©¶ã‚’èª¿æŸ»ã—ã¾ã™ã€‚
- In Section III, we provide the setting, the assumptions on the reward distributions, and the definition of cumulative regret. 
  - ã‚»ã‚¯ã‚·ãƒ§ãƒ³IIIã§ã¯ã€è¨­å®šã€å ±é…¬åˆ†å¸ƒã«é–¢ã™ã‚‹ä»®å®šã€ãŠã‚ˆã³ç´¯ç©å¾Œæ‚”ã®å®šç¾©ã‚’æä¾›ã—ã¾ã™ã€‚
- In Section IV, we describe two TS-inspired algorithms, namely Beta-SWTS and $\gamma$-SWGTS based on a sliding-window approach, exploiting the $\tau$ (being $\tau$ the window size) most recent samples to estimate the expected rewards. 
  - ã‚»ã‚¯ã‚·ãƒ§ãƒ³IVã§ã¯ã€ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«åŸºã¥ãBeta-SWTSã¨$\gamma$-SWGTSã¨ã„ã†2ã¤ã®TSã«è§¦ç™ºã•ã‚ŒãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’èª¬æ˜ã—ã€æœŸå¾…å ±é…¬ã‚’æ¨å®šã™ã‚‹ãŸã‚ã«$\tau$ï¼ˆ$\tau$ã¯ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºï¼‰ã®æœ€ã‚‚æœ€è¿‘ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚

- In the first part of Section V, we introduce new quantities to characterize how complex is to learn with sliding-window algorithms in an NS-MAB with expected rewards evolving with no particular form of non-stationarity. 
  - ã‚»ã‚¯ã‚·ãƒ§ãƒ³Vã®å‰åŠã§ã¯ã€ç‰¹å®šã®éå®šå¸¸æ€§ã®å½¢æ…‹ã‚’æŒãŸãªã„æœŸå¾…å ±é…¬ãŒé€²åŒ–ã™ã‚‹NS-MABã«ãŠã„ã¦ã€ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã©ã‚Œã»ã©è¤‡é›‘ã§ã‚ã‚‹ã‹ã‚’ç‰¹å¾´ä»˜ã‘ã‚‹æ–°ã—ã„é‡ã‚’å°å…¥ã—ã¾ã™ã€‚
  In particular, we define two sets, namely the learnable set and the unlearnable set (Definition V.1), to describe in which rounds an algorithm exploiting the most recent samples only is expected to identify the optimal arm. 
  ç‰¹ã«ã€æœ€ã‚‚æœ€è¿‘ã®ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ã‚’åˆ©ç”¨ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒæœ€é©ãªã‚¢ãƒ¼ãƒ ã‚’ç‰¹å®šã™ã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã‚‹ãƒ©ã‚¦ãƒ³ãƒ‰ã‚’èª¬æ˜ã™ã‚‹ãŸã‚ã«ã€å­¦ç¿’å¯èƒ½ãªã‚»ãƒƒãƒˆã¨å­¦ç¿’ä¸å¯èƒ½ãªã‚»ãƒƒãƒˆï¼ˆå®šç¾©V.1ï¼‰ã¨ã„ã†2ã¤ã®ã‚»ãƒƒãƒˆã‚’å®šç¾©ã—ã¾ã™ã€‚
  Furthermore, we define a new suboptimality gap notion, $\Delta_{\tau}$ (Definition V.2) that will be employed in the analysis. 
  ã•ã‚‰ã«ã€åˆ†æã§ä½¿ç”¨ã•ã‚Œã‚‹æ–°ã—ã„éæœ€é©æ€§ã‚®ãƒ£ãƒƒãƒ—ã®æ¦‚å¿µ$\Delta_{\tau}$ï¼ˆå®šç¾©V.2ï¼‰ã‚’å®šç¾©ã—ã¾ã™ã€‚

- In the second part of Section V, we derive novel unifying regret upper bounds of the Beta-SWTS and $\gamma$-SWGTS algorithms described in Section IV, for Bernoulli and subgaussian rewards, respectively. 
  - ã‚»ã‚¯ã‚·ãƒ§ãƒ³Vã®å¾ŒåŠã§ã¯ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³IVã§èª¬æ˜ã—ãŸBeta-SWTSãŠã‚ˆã³$\gamma$-SWGTSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ–°ã—ã„çµ±ä¸€çš„ãªå¾Œæ‚”ä¸Šé™ã‚’ã€ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤ãŠã‚ˆã³ã‚µãƒ–ã‚¬ã‚¦ã‚¹å ±é…¬ã«å¯¾ã—ã¦å°å‡ºã—ã¾ã™ã€‚
  Our analysis exploits the quantities previously defined to characterize the complexity of the learning problem and makes no assumption on the underlying form of non-stationarity. 
  ç§ãŸã¡ã®åˆ†æã¯ã€å­¦ç¿’å•é¡Œã®è¤‡é›‘ã•ã‚’ç‰¹å¾´ä»˜ã‘ã‚‹ãŸã‚ã«ä»¥å‰ã«å®šç¾©ã•ã‚ŒãŸé‡ã‚’åˆ©ç”¨ã—ã€åŸºç¤ã¨ãªã‚‹éå®šå¸¸æ€§ã®å½¢æ…‹ã«é–¢ã™ã‚‹ä»®å®šã‚’è¡Œã„ã¾ã›ã‚“ã€‚

- We leverage the results of Section V to derive regret upper bounds for the abruptly changing NS-MABs (Section VI) and the smoothly changing NS-MABs (Section VII). 
  - ã‚»ã‚¯ã‚·ãƒ§ãƒ³Vã®çµæœã‚’åˆ©ç”¨ã—ã¦ã€æ€¥æ¿€ã«å¤‰åŒ–ã™ã‚‹NS-MABï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³VIï¼‰ãŠã‚ˆã³æ»‘ã‚‰ã‹ã«å¤‰åŒ–ã™ã‚‹NS-MABï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³VIIï¼‰ã®å¾Œæ‚”ä¸Šé™ã‚’å°å‡ºã—ã¾ã™ã€‚
  Moreover, we show how our bounds are comparable with the state-of-the-art ones derived with analyses tailored for the specific form of non-stationarity. 
  ã•ã‚‰ã«ã€ç§ãŸã¡ã®å¢ƒç•ŒãŒç‰¹å®šã®éå®šå¸¸æ€§ã®å½¢æ…‹ã«åˆã‚ã›ãŸåˆ†æã‹ã‚‰å°å‡ºã•ã‚ŒãŸæœ€å…ˆç«¯ã®ã‚‚ã®ã¨ã©ã®ã‚ˆã†ã«æ¯”è¼ƒå¯èƒ½ã§ã‚ã‚‹ã‹ã‚’ç¤ºã—ã¾ã™ã€‚

- In Section VIII, we experimentally compare the performance of the analyzed algorithms with those in the bandit literature that are devised to learn in non-stationary scenarios. 
  - ã‚»ã‚¯ã‚·ãƒ§ãƒ³VIIIã§ã¯ã€åˆ†æã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ã€éå®šå¸¸ã‚·ãƒŠãƒªã‚ªã§å­¦ç¿’ã™ã‚‹ãŸã‚ã«è¨­è¨ˆã•ã‚ŒãŸãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆæ–‡çŒ®ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨å®Ÿé¨“çš„ã«æ¯”è¼ƒã—ã¾ã™ã€‚

The proofs of the results presented in the main paper are reported in Appendix A and B. 
æœ¬è«–æ–‡ã§æç¤ºã•ã‚ŒãŸçµæœã®è¨¼æ˜ã¯ã€ä»˜éŒ²AãŠã‚ˆã³Bã«è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## IIRelated works II é–¢é€£ç ”ç©¶

In this section, we survey the main related works about TS and approaches for regret minimization in NS-MABs. 
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€TSã«é–¢ã™ã‚‹ä¸»ãªé–¢é€£ç ”ç©¶ã¨ã€NS-MABã«ãŠã‘ã‚‹å¾Œæ‚”æœ€å°åŒ–ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’èª¿æŸ»ã—ã¾ã™ã€‚

### II-A Thompson Sampling

TS was introduced in 1933 [47] for allocating experimental effort in online sequential decision-making problems, and its effectiveness has been investigated both empirically [14,44] and theoretically [5,29] only in the last decades. 
TSã¯1933å¹´ã«[47]ã‚ªãƒ³ãƒ©ã‚¤ãƒ³é€æ¬¡æ„æ€æ±ºå®šå•é¡Œã«ãŠã‘ã‚‹å®Ÿé¨“çš„åŠªåŠ›ã®é…åˆ†ã®ãŸã‚ã«å°å…¥ã•ã‚Œã€ãã®åŠ¹æœã¯éå»æ•°åå¹´ã«ã‚ãŸã‚Šã€å®Ÿè¨¼çš„[14,44]ãŠã‚ˆã³ç†è«–çš„[5,29]ã«èª¿æŸ»ã•ã‚Œã¦ãã¾ã—ãŸã€‚
The algorithm has found widespread applications in various fields, including online advertising [25,2,3], clinical trials [8], recommendation systems [30] and hyperparameter tuning for machine learning methods [28]. 
**ã“ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³åºƒå‘Š[25,2,3]ã€è‡¨åºŠè©¦é¨“[8]ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ [30]ã€ãŠã‚ˆã³æ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´[28]ãªã©ã€ã•ã¾ã–ã¾ãªåˆ†é‡ã§åºƒãå¿œç”¨**ã•ã‚Œã¦ã„ã¾ã™ã€‚
TS is optimal in the stationary case, i.e., achieving instance-dependent regret matching the lower bound [31]. 
**TSã¯å®šå¸¸çŠ¶æ…‹ã®å ´åˆã«æœ€é©**ã§ã‚ã‚Šã€ã™ãªã‚ã¡ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä¾å­˜ã®å¾Œæ‚”ãŒä¸‹é™[31]ã«ä¸€è‡´ã™ã‚‹ã“ã¨ã‚’é”æˆã—ã¾ã™ã€‚
However, it has been shown in multiple cases that in NS-MABs [24,48,34] or in adversarial settings [13] it provides poor performances in terms of regret. 
ã—ã‹ã—ã€**NS-MABs[24,48,34]ã‚„æ•µå¯¾çš„è¨­å®š[13]ã«ãŠã„ã¦ã¯ã€å¾Œæ‚”ã®è¦³ç‚¹ã‹ã‚‰æ€§èƒ½ãŒæ‚ªã„ã“ã¨ãŒè¤‡æ•°ã®ã‚±ãƒ¼ã‚¹ã§ç¤ºã•ã‚Œã¦ã„ã¾ã™**ã€‚(non-stationaryç’°å¢ƒã‚„adversarialç’°å¢ƒã§ã¯ã€TSã¯ã‚ã¾ã‚Šå¼·ããªã„ã®ã‹...!!:thinking:)

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### II-B éå®šå¸¸ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆ(Non-Stationary Bandits)

Lately,UCB1andTSalgorithms inspired the development of techniques to face the inherent complexities of NS-MABs[50]. 
æœ€è¿‘ã€UCB1ãŠã‚ˆã³TSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€NS-MABsï¼ˆéå®šå¸¸ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆï¼‰ã®å›ºæœ‰ã®è¤‡é›‘ã•ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã®æŠ€è¡“ã®é–‹ç™ºã‚’ä¿ƒã—ã¾ã—ãŸ[50]ã€‚
The main idea behind these newly crafted algorithms is to forget past observations, removing samples from the statistics of the armsâ€™ expected reward. 
**ã“ã‚Œã‚‰ã®æ–°ãŸã«ä½œæˆã•ã‚ŒãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ä¸»ãªã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã€éå»ã®è¦³æ¸¬ã‚’å¿˜ã‚Œã€ã‚¢ãƒ¼ãƒ ã®æœŸå¾…å ±é…¬ã®çµ±è¨ˆã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’é™¤å»ã™ã‚‹ã“ã¨**ã§ã™ã€‚

Two main approaches are present in the bandit literature to forget past observations: passive and active. 
**éå»ã®è¦³æ¸¬ã‚’å¿˜ã‚Œã‚‹ãŸã‚ã®ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆæ–‡çŒ®ã«ã¯ã€ä¸»ã«2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã‚ã‚Šã¾ã™ï¼šå—å‹•çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨èƒ½å‹•çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**ã§ã™ã€‚

The former iteratively discards the information coming from the far past, making decisions using only the most recent samples coming from the arms selected by the algorithms. 
å‰è€…ã¯ã€é ã„éå»ã‹ã‚‰ã®æƒ…å ±ã‚’åå¾©çš„ã«å»ƒæ£„ã—ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã£ã¦é¸æŠã•ã‚ŒãŸ**ã‚¢ãƒ¼ãƒ ã‹ã‚‰ã®æœ€æ–°ã®ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ã‚’ä½¿ç”¨ã—ã¦æ„æ€æ±ºå®šã‚’è¡Œã„ã¾ã™ã€‚**
Examples of such a family of algorithms are Discounted-TS[39], DUCB[24], which employ a multiplicative discount factor to reduce the impact of samples seen in the past. 
ã“ã®ã‚ˆã†ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ä¾‹ã¨ã—ã¦ã¯ã€Discounted-TS[39]ã‚„DUCB[24]ãŒã‚ã‚Šã€ã“ã‚Œã‚‰ã¯éå»ã«è¦‹ãŸã‚µãƒ³ãƒ—ãƒ«ã®å½±éŸ¿ã‚’æ¸›å°‘ã•ã›ã‚‹ãŸã‚ã«ä¹—æ³•çš„å‰²å¼•å› å­ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
It has been shown that these algorithms achieve regret of order $O(\sqrt{\Upsilon_{T}T}\log(T))$ in piecewise-constant abruptly changing environments, where $\Upsilon_{T}$ is the number breakpoint present during the learning horizon $T$. 
ã“ã‚Œã‚‰ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€åŒºåˆ†å®šæ•°ã®æ€¥æ¿€ã«å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã«ãŠã„ã¦ã€$O(\sqrt{\Upsilon_{T}T}\log(T))$ã®å¾Œæ‚”ã‚’é”æˆã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã“ã§ã€$\Upsilon_{T}$ã¯å­¦ç¿’ãƒ›ãƒ©ã‚¤ã‚ºãƒ³$T$ã®é–“ã«å­˜åœ¨ã™ã‚‹ãƒ–ãƒ¬ãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆã®æ•°ã§ã™ã€‚
Finally, SW-UCB[24] used a sliding-window approach in combination with an upper confidence bound to get a regret of order $O(\sqrt{\Upsilon_{T}T\log(T)})$ in the same setting. 
æœ€å¾Œã«ã€SW-UCB[24]ã¯ã€ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä¸Šé™ä¿¡é ¼åŒºé–“ã¨çµ„ã¿åˆã‚ã›ã¦ä½¿ç”¨ã—ã€åŒã˜è¨­å®šã§$O(\sqrt{\Upsilon_{T}T\log(T)})$ã®å¾Œæ‚”ã‚’å¾—ã¾ã—ãŸã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

Instead, the active approach encompasses the use of change-detection techniques[9] to decide when it is the case to discard old samples. 
ä¸€æ–¹ã€èƒ½å‹•çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€å¤ã„ã‚µãƒ³ãƒ—ãƒ«ã‚’å»ƒæ£„ã™ã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’æ±ºå®šã™ã‚‹ãŸã‚ã«å¤‰åŒ–æ¤œå‡ºæŠ€è¡“[9]ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
This occurs when a sufficiently large change affects the armsâ€™ expected rewards. 
ã“ã‚Œã¯ã€ååˆ†ã«å¤§ããªå¤‰åŒ–ãŒã‚¢ãƒ¼ãƒ ã®æœŸå¾…å ±é…¬ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã¨ãã«ç™ºç”Ÿã—ã¾ã™ã€‚
Among the active approaches to deal with the abruptly changing bandits, we mention CUSUM-UCB[33] and BR-MAB[40]. 
æ€¥æ¿€ã«å¤‰åŒ–ã™ã‚‹ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã®èƒ½å‹•çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ä¸­ã§ã€CUSUM-UCB[33]ã¨BR-MAB[40]ã‚’æŒ™ã’ã¾ã™ã€‚
They achieve a regret of order $O\left(\sqrt{\Upsilon_{T}T\log(\frac{T}{\Upsilon_{T}})}\right)$. 
ã“ã‚Œã‚‰ã¯ã€$O\left(\sqrt{\Upsilon_{T}T\log(\frac{T}{\Upsilon_{T}})}\right)$ã®å¾Œæ‚”ã‚’é”æˆã—ã¾ã™ã€‚
Instead, in the same setting, GLR-klUCB[11], based on the use of KL-UCB as a bandit selection algorithm and a nonparametric change point method, achieve an $O(\sqrt{\Upsilon_{T}T\log(T)})$ regret. 
åŒã˜è¨­å®šã®ä¸­ã§ã€KL-UCBã‚’ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆé¸æŠã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã—ã¦ä½¿ç”¨ã—ã€éãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯å¤‰åŒ–ç‚¹æ³•ã«åŸºã¥ãGLR-klUCB[11]ã¯ã€$O(\sqrt{\Upsilon_{T}T\log(T)})$ã®å¾Œæ‚”ã‚’é”æˆã—ã¾ã™ã€‚
Another approach that is worth mentioning is RExp3[10], which builds on Exp3[7], adding scheduled restarts to the original algorithm, and it handles arbitrary evolutions of the expected rewards as long as they are constrained within $[0,1]$ and the learner knows the total variation $V_{T}$ of the expected reward, providing an $O(V_{T}^{\frac{1}{3}}T^{\frac{2}{3}})$ regret. 
è¨€åŠã™ã‚‹ä¾¡å€¤ã®ã‚ã‚‹åˆ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯RExp3[10]ã§ã€ã“ã‚Œã¯Exp3[7]ã«åŸºã¥ãã€å…ƒã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã•ã‚ŒãŸå†èµ·å‹•ã‚’è¿½åŠ ã—ã€æœŸå¾…å ±é…¬ã®ä»»æ„ã®é€²åŒ–ã‚’æ‰±ã„ã¾ã™ã€‚æœŸå¾…å ±é…¬ãŒ$[0,1]$ã«åˆ¶ç´„ã•ã‚Œã€å­¦ç¿’è€…ãŒæœŸå¾…å ±é…¬ã®ç·å¤‰å‹•$V_{T}$ã‚’çŸ¥ã£ã¦ã„ã‚‹é™ã‚Šã€$O(V_{T}^{\frac{1}{3}}T^{\frac{2}{3}})$ã®å¾Œæ‚”ã‚’æä¾›ã—ã¾ã™ã€‚
Finally, different approaches to developing TS-like algorithms in NS-MABs resort to de-prioritizing information that more quickly loses usefulness and deriving a bound on the Bayesian regret of the algorithm. 
æœ€å¾Œã«ã€NS-MABsã«ãŠã‘ã‚‹TSã®ã‚ˆã†ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é–‹ç™ºã™ã‚‹ãŸã‚ã®ç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ã‚ˆã‚Šæ—©ãæœ‰ç”¨æ€§ã‚’å¤±ã†æƒ…å ±ã®å„ªå…ˆåº¦ã‚’ä¸‹ã’ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒ™ã‚¤ã‚ºå¾Œæ‚”ã«å¯¾ã™ã‚‹å¢ƒç•Œã‚’å°å‡ºã™ã‚‹ã“ã¨ã«ä¾å­˜ã—ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! èƒ½å‹•çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯å®Ÿé‹ç”¨ã®ãƒãƒ¼ãƒ‰ãƒ«é«˜ã„ã‹ã‚‚ã¨æ€ã„ã‚ã¾ã‚Šã¡ã‚ƒã‚“ã¨èª­ã‚“ã§ãªã„ãŒ -->

As a final remark, we point out that differently from CUSUM-UCB, GLR-klUCB and BR-MAB, we are able to characterize the regret for any NS-MAB, as long as the distribution of the rewards is either Bernoulli or subgaussian, and in a more general setting than the piecewise-constant abruptly-changing ones. 
æœ€å¾Œã«ã€CUSUM-UCBã€GLR-klUCBã€BR-MABã¨ã¯ç•°ãªã‚Šã€å ±é…¬ã®åˆ†å¸ƒãŒãƒ™ãƒ«ãƒŒãƒ¼ã‚¤ã¾ãŸã¯ã‚µãƒ–ã‚¬ã‚¦ã‚¹ã§ã‚ã‚‹é™ã‚Šã€ä»»æ„ã®NS-MABã«å¯¾ã™ã‚‹å¾Œæ‚”ã‚’ç‰¹å®šã§ãã‚‹ã“ã¨ã‚’æŒ‡æ‘˜ã—ã¾ã™ã€‚ã¾ãŸã€åŒºåˆ†å®šæ•°ã®æ€¥æ¿€ã«å¤‰åŒ–ã™ã‚‹ã‚‚ã®ã‚ˆã‚Šã‚‚ä¸€èˆ¬çš„ãªè¨­å®šã§ã‚ã‚‹ã“ã¨ã‚‚å¼·èª¿ã—ã¾ã™ã€‚
Furthermore, differently from the analysis of RExp3, we retrieve guarantees on the performance also for expected rewards that are not bounded in $[0,1]$. 
ã•ã‚‰ã«ã€RExp3ã®åˆ†æã¨ã¯ç•°ãªã‚Šã€$[0,1]$ã«åˆ¶ç´„ã•ã‚Œãªã„æœŸå¾…å ±é…¬ã«å¯¾ã—ã¦ã‚‚æ€§èƒ½ã®ä¿è¨¼ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
Moreover, we highlight that in the work by Liu et al. [34], the authors evaluate the Bayesian regret while we retrieve frequentist bounds on the performance that are notoriously more informative. 
ã•ã‚‰ã«ã€Liuã‚‰ã®ç ”ç©¶[34]ã§ã¯ã€è‘—è€…ãŸã¡ãŒãƒ™ã‚¤ã‚ºå¾Œæ‚”ã‚’è©•ä¾¡ã—ã¦ã„ã‚‹ã®ã«å¯¾ã—ã€ç§ãŸã¡ã¯æ€§èƒ½ã«é–¢ã™ã‚‹é »åº¦ä¸»ç¾©çš„ãªå¢ƒç•Œã‚’å–å¾—ã—ã¦ãŠã‚Šã€ã“ã‚Œã¯è‘—åã«ã‚ˆã‚Šæƒ…å ±é‡ãŒå¤šã„ã§ã™ã€‚
In [15], the authors dealt with non-stationary, smoothly-changing bandits, a setting in which the expected rewards evolve for a limited amount between two rounds. 
æ–‡çŒ®[15]ã§ã¯ã€è‘—è€…ãŸã¡ã¯éå®šå¸¸ã§æ»‘ã‚‰ã‹ã«å¤‰åŒ–ã™ã‚‹ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«å¯¾å‡¦ã—ã¦ãŠã‚Šã€ã“ã‚Œã¯æœŸå¾…å ±é…¬ãŒ2ãƒ©ã‚¦ãƒ³ãƒ‰ã®é–“ã«é™ã‚‰ã‚ŒãŸé‡ã§é€²åŒ–ã™ã‚‹è¨­å®šã§ã™ã€‚
They designed SW-KL-UCB they achieve a $O(H(\Delta,T)+\frac{T\log(\tau)}{\Delta^{2}\tau})$ regret, where the order of $H(\Delta,T)$ depends on the bandit instance and $\Delta$ is the minimum non-zero distance of the expected rewards within the learning horizon between the best arm and the suboptimal arms. 
å½¼ã‚‰ã¯SW-KL-UCBã‚’è¨­è¨ˆã—ã€$O(H(\Delta,T)+\frac{T\log(\tau)}{\Delta^{2}\tau})$ã®å¾Œæ‚”ã‚’é”æˆã—ã¾ã—ãŸã€‚ã“ã“ã§ã€$H(\Delta,T)$ã®ã‚ªãƒ¼ãƒ€ãƒ¼ã¯ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«ä¾å­˜ã—ã€$\Delta$ã¯æœ€è‰¯ã‚¢ãƒ¼ãƒ ã¨æœ€é©ã§ãªã„ã‚¢ãƒ¼ãƒ ã®é–“ã®å­¦ç¿’ãƒ›ãƒ©ã‚¤ã‚ºãƒ³å†…ã§ã®æœŸå¾…å ±é…¬ã®æœ€å°éã‚¼ãƒ­è·é›¢ã§ã™ã€‚
Recently paper [38] analyzed the regret of the $\gamma$-SWGTS algorithm. 
æœ€è¿‘ã®è«–æ–‡[38]ã§ã¯ã€$\gamma$-SWGTSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å¾Œæ‚”ãŒåˆ†æã•ã‚Œã¾ã—ãŸã€‚
However, the authors do not face the far more challenging Beta-Binomial case and consider only the piece-wise constant abruptly changing settings. 
ã—ã‹ã—ã€è‘—è€…ãŸã¡ã¯ã¯ã‚‹ã‹ã«å›°é›£ãªãƒ™ãƒ¼ã‚¿-äºŒé …åˆ†å¸ƒã®ã‚±ãƒ¼ã‚¹ã«ã¯å¯¾å‡¦ã›ãšã€åŒºåˆ†å®šæ•°ã®æ€¥æ¿€ã«å¤‰åŒ–ã™ã‚‹è¨­å®šã®ã¿ã‚’è€ƒæ…®ã—ã¦ã„ã¾ã™ã€‚
We also remark that [38] cite a preprint version of the present paper [19, https://arxiv.org/abs/2409.05181]. 
ã¾ãŸã€[38]ãŒæœ¬è«–æ–‡ã®ãƒ—ãƒ¬ãƒ—ãƒªãƒ³ãƒˆç‰ˆ[19, https://arxiv.org/abs/2409.05181]ã‚’å¼•ç”¨ã—ã¦ã„ã‚‹ã“ã¨ã‚‚æŒ‡æ‘˜ã—ã¾ã™ã€‚

<!-- é›‘ã«ã“ã“ã¾ã§èª­ã‚“ã ! -->

## III å•é¡Œå®šç¾©


At each round t âˆˆ âŸ¦ T âŸ§ ,4 where T âˆˆ â„• is the learning horizon, the learner selects an arm I t âˆˆ âŸ¦ K âŸ§ among a finite set of K arms and observes a realization of the reward X I t , t . 
å„ãƒ©ã‚¦ãƒ³ãƒ‰ $t \in \llbracket T \rrbracket$ ã«ãŠã„ã¦ã€$T \in \mathbb{N}$ ã¯å­¦ç¿’ã®ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã§ã‚ã‚Šã€å­¦ç¿’è€…ã¯ã€æœ‰é™ã® $K$ æœ¬ã®ã‚¢ãƒ¼ãƒ ã®ä¸­ã‹ã‚‰ $I_t \in \llbracket K \rrbracket$ ã‚’é¸æŠã—ã€å ±é…¬ $X_{I_t,t}$ ã®å®Ÿç¾ã‚’è¦³å¯Ÿã—ã¾ã™ã€‚
The reward for each arm i âˆˆ âŸ¦ K âŸ§ â‰” { 1 , â€¦ , K } at round t âˆˆ âŸ¦ T âŸ§ is modeled by a random variable X i , t described by a distribution unknown to the learner. 
å„ã‚¢ãƒ¼ãƒ  $i \in \llbracket K \rrbracket \coloneqq \{1,\ldots,K\}$ ã«å¯¾ã™ã‚‹å ±é…¬ã¯ã€ãƒ©ã‚¦ãƒ³ãƒ‰ $t \in \llbracket T \rrbracket$ ã«ãŠã„ã¦ã€ç¢ºç‡å¤‰æ•° $X_{i,t}$ ã«ã‚ˆã£ã¦ãƒ¢ãƒ‡ãƒ«åŒ–ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã¯å­¦ç¿’è€…ã«ã¯æœªçŸ¥ã®åˆ†å¸ƒã«ã‚ˆã£ã¦è¨˜è¿°ã•ã‚Œã¾ã™ã€‚
We denote by Î¼ i , t â‰” ğ”¼ â¢ [ X i , t ] the corresponding expected reward. 
ç§ãŸã¡ã¯ã€æœŸå¾…å ±é…¬ã‚’ $\mu_{i,t} \coloneqq \mathbb{E}[X_{i,t}]$ ã¨è¡¨è¨˜ã—ã¾ã™ã€‚ã“ã‚ŒãŒå¯¾å¿œã™ã‚‹æœŸå¾…å ±é…¬ã§ã™ã€‚
We study two types of distributions of the rewards encoded by the following assumptions.
ç§ãŸã¡ã¯ã€ä»¥ä¸‹ã®ä»®å®šã«ã‚ˆã£ã¦ç¬¦å·åŒ–ã•ã‚ŒãŸ**å ±é…¬ã®2ç¨®é¡ã®åˆ†å¸ƒ**ã‚’ç ”ç©¶ã—ã¾ã™ã€‚ 

- å®šå¼åŒ–ãƒ¡ãƒ¢:
  - (åŸºæœ¬çš„ã«ã¯ã€ã‚«ã‚¸ãƒã®ã‚¹ãƒ­ãƒƒãƒˆãƒã‚·ãƒ³ã®ä¾‹ã‚’æƒ³å®šã™ã‚Œã°OK!:thinking:)
  - ãƒ©ã‚¦ãƒ³ãƒ‰: $t \in \llbracket T \rrbracket$ (1ãƒ©ã‚¦ãƒ³ãƒ‰ç›®ã‹ã‚‰Tãƒ©ã‚¦ãƒ³ãƒ‰ç›®ã¾ã§)
  - ã‚¢ãƒ¼ãƒ : $i \in \llbracket K \rrbracket$ (1æœ¬ç›®ã®ã‚¢ãƒ¼ãƒ ã‹ã‚‰Kæœ¬ç›®ã®ã‚¢ãƒ¼ãƒ ã¾ã§)
  - å­¦ç¿’è€…: ãƒ©ã‚¦ãƒ³ãƒ‰ã”ã¨ã«ã‚¢ãƒ¼ãƒ ã‚’1æœ¬é¸æŠã™ã‚‹
  - å ±é…¬: å„ã‚¢ãƒ¼ãƒ $i$ã®ãƒ©ã‚¦ãƒ³ãƒ‰$t$ã§ã®å ±é…¬ã¯ç¢ºç‡å¤‰æ•° $X_{i,t}$ ã§ãƒ¢ãƒ‡ãƒ«åŒ–ã•ã‚Œã‚‹
    - æœŸå¾…å ±é…¬ (=è¦³æ¸¬ã•ã‚Œã‚‹å ±é…¬ã®æœŸå¾…å€¤): $\mu_{i,t} = \mathbb{E}[X_{i,t}]$
    - å ±é…¬åˆ†å¸ƒã¯ã€å­¦ç¿’è€…ã«ã¯æœªçŸ¥ã€‚å„ã‚¢ãƒ¼ãƒ  $i$ ã”ã¨ã« ãƒ©ã‚¦ãƒ³ãƒ‰ $t$ ã”ã¨ã«ç•°ãªã‚‹å¯èƒ½æ€§ã‚ã‚‹(non-stationary setting)

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

Assumption III.1 (Bernoulli rewards). 
ä»®å®š III.1 (ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤å ±é…¬).
For every arm i âˆˆ âŸ¦ K âŸ§ and round t âˆˆ âŸ¦ T âŸ§ , the reward X i , t is s.t. X i , t âˆ¼ Be â¢ ( Î¼ i , t ) , where Be â¢ ( Î¼ ) denotes a Bernoulli distribution with parameter Î¼ âˆˆ [ 0 , 1 ] .
å…¨ã¦ã®ã‚¢ãƒ¼ãƒ  $i \in K$ ã¨ãƒ©ã‚¦ãƒ³ãƒ‰ $t \in T$ ã«å¯¾ã—ã¦ã€å ±é…¬ $X_{i,t}$ ã¯ $X_{i,t} \sim \textit{Be}(\mu_{i,t})$ ã¨ãªã‚Šã¾ã™ã€‚ã“ã“ã§ã€$\textit{Be}(\mu)$ ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\mu \in [0,1]$ ã‚’æŒã¤ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤åˆ†å¸ƒã‚’ç¤ºã—ã¾ã™ã€‚
(ã¤ã¾ã‚Šã€å ±é…¬ãŒbinary(0ã‹1)ã§ã‚ã‚‹å ´åˆã®è¨­å®š...!!:thinking:)

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

Assumption III.2 (Subgaussian rewards). 
ä»®å®š III.2 (ã‚µãƒ–ã‚¬ã‚¦ã‚¹å ±é…¬).
For every arm i âˆˆ âŸ¦ K âŸ§ and round t âˆˆ âŸ¦ T âŸ§ , the reward X i , t is s.t. X i , t âˆ¼ SubG â¢ ( Î¼ i , t , Î» 2 ) , where SubG â¢ ( Î¼ , Î» 2 ) denotes a generic subgaussian distribution with finite mean Î¼ âˆˆ â„ and proxy variance Î» 2 .5
å…¨ã¦ã®ã‚¢ãƒ¼ãƒ  $i \in K $ ã¨ãƒ©ã‚¦ãƒ³ãƒ‰ $t \in T$ ã«å¯¾ã—ã¦ã€å ±é…¬ $X_{i,t}$ ã¯ $X_{i,t} \sim \textit{SubG}(\mu_{i,t},\lambda^{2})$ ã¨ãªã‚Šã¾ã™ã€‚ã“ã“ã§ã€$\textit{SubG}(\mu,\lambda^{2})$ ã¯æœ‰é™ã®å¹³å‡ $\mu \in \mathbb{R}$ ã¨ä»£ç†åˆ†æ•£ $\lambda^{2}$ ã‚’æŒã¤ä¸€èˆ¬çš„ãªã‚µãƒ–ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‚’ç¤ºã—ã¾ã™ã€‚5
(ã¤ã¾ã‚Šã€å ±é…¬ãŒé€£ç¶šå€¤ã‚’å–ã‚‹å ´åˆã®è¨­å®šã£ã¦ã“ã¨...!!æ™®é€šã®æ­£è¦åˆ†å¸ƒã˜ã‚ƒãªãã¦"ã‚µãƒ–"ã‚¬ã‚¦ã‚¹...??:thinking:)

---æ³¨é‡ˆ
A random variable $X$ with expectation $\mu$ is $\lambda^{2}$-subgaussian if for every $s \in \mathbb{R}$ it holds that 
æœŸå¾…å€¤ $\mu$ ã‚’æŒã¤ç¢ºç‡å¤‰æ•° $X$ ã¯ã€ã™ã¹ã¦ã® $s \in \mathbb{R}$ ã«å¯¾ã—ã¦ 
$\mathbb{E}[\exp(s(X-\mu))] \leq \exp(s^{2}\lambda^{2}/2)$. 
$\mathbb{E}[\exp(s(X-\mu))] \leq \exp(s^{2}\lambda^{2}/2)$ ã§ã‚ã‚‹ã¨ãã€$\lambda^{2}$-ã‚µãƒ–ã‚¬ã‚¦ã‚¹ã¨å‘¼ã°ã‚Œã¾ã™ã€‚ 
---

- ã‚µãƒ–ã‚¬ã‚¦ã‚¹å ±é…¬ã«é–¢ã™ã‚‹ãƒ¡ãƒ¢:
  - ã‚µãƒ–ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ = æ­£è¦åˆ†å¸ƒ(ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ)ã¨åŒã˜ãã‚‰ã„ã€ã‚‚ã—ãã¯ãã‚Œã‚ˆã‚Šã‚‚æ€¥é€Ÿã«å°¾éƒ¨ãŒæ¸›å°‘ã™ã‚‹åˆ†å¸ƒã€‚
  - ã€Œã‚ã‚‹ç¢ºç‡å¤‰æ•°XãŒ $\lambda^{2}$-ã‚µãƒ–ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã«å¾“ã†ã€ã“ã¨ã®å®šç¾©ãŒã€ä¸Šè¨˜ã®æ³¨é‡ˆã§æ›¸ã‹ã‚Œã¦ã‚‹å†…å®¹ã€‚ç¢ºç‡åˆ†å¸ƒã®åˆ†æ•£ãŒä¸€å®šä»¥ä¸‹ã«ãªã‚‹ã‚ˆã­ã€ã¨ã„ã†å®šç¾©...!!:thinking:

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

The goal of the learner $\mathfrak{A}$ is to minimize the expected cumulative dynamic frequentist regret $R_{T}(\mathfrak{A})$ over the learning horizon $T$, defined as the cumulative difference between the reward of an oracle that chooses at each time the arm with the largest expected reward at round $t$, defined as $i^{*}(t) \in \mathop{\text{argmax}}_{i \in \llbracket K \rrbracket} \mu_{i,t}$, and expected reward $\mu_{I_{t},t}$ of the arm $I_{t}$ selected by the learner for the round, formally: 
å­¦ç¿’è€… $\mathfrak{A}$ ã®ç›®æ¨™ã¯ã€å­¦ç¿’ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ $T$ ã«ã‚ãŸã‚‹æœŸå¾…ç´¯ç©å‹•çš„é »åº¦ä¸»ç¾©çš„å¾Œæ‚” $R_{T}(\mathfrak{A})$ ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã§ã™ã€‚ã“ã‚Œã¯ã€å„æ™‚ç‚¹ã§ãƒ©ã‚¦ãƒ³ãƒ‰ $t$ ã«ãŠã‘ã‚‹æœ€å¤§ã®æœŸå¾…å ±é…¬ã‚’æŒã¤ã‚¢ãƒ¼ãƒ ã‚’é¸æŠã™ã‚‹ã‚ªãƒ©ã‚¯ãƒ«ã®å ±é…¬ã¨ã€å­¦ç¿’è€…ãŒãƒ©ã‚¦ãƒ³ãƒ‰ã®ãŸã‚ã«é¸æŠã—ãŸã‚¢ãƒ¼ãƒ  $I_{t}$ ã®æœŸå¾…å ±é…¬ $\mu_{I_{t},t}$ ã¨ã®ç´¯ç©å·®ã¨ã—ã¦æ­£å¼ã«å®šç¾©ã•ã‚Œã¾ã™ã€‚$i^{*}(t) \in \mathop{\text{argmax}}_{i \in \llbracket K \rrbracket} \mu_{i,t}$ ã¨å®šç¾©ã•ã‚Œã¾ã™ã€‚

$$
R_{T}(A) := \mathbb{E} \left[ \sum_{t=1}^{T} \left( \mu_{i^{*}(t),t} - \mu_{I_{t},t} \right) \right].
\tag{1}
$$

where the expected value is taken w.r.t. the randomness of the rewards and the possible randomness of the algorithm. 
ã“ã“ã§ã€æœŸå¾…å€¤ã¯ã€å ±é…¬ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§(=å ±é…¬ã®ç¢ºç‡åˆ†å¸ƒ!:thinking:)ã¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å¯èƒ½ãªãƒ©ãƒ³ãƒ€ãƒ æ€§(=æ„æ€æ±ºå®šæ–¹ç­–ã®ç¢ºç‡åˆ†å¸ƒ!:thinking:)ã«é–¢ã—ã¦å–ã‚‰ã‚Œã¾ã™ã€‚
In the following, as is often done in the NS-MABs literature (e.g., [11,33,40,48,24]) we provide results on the expected value of the pull of the arms $\mathbb{E}[N_{i,T}]$, where $N_{i,T}$ is the random variable representing the number of total pulls of the arm $i$ at round $T$ excluding the rounds in which $i$ is optimal, formally defined as $N_{i,T} = \sum_{t=1}^{T} \mathds{1}\{I_{t}=i,\,i\neq i^{*}(t)\}$. 
ä»¥ä¸‹ã§ã¯ã€NS-MABsã®æ–‡çŒ®ï¼ˆä¾‹ï¼š[11,33,40,48,24]ï¼‰ã§ã‚ˆãè¡Œã‚ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«ã€**ã‚¢ãƒ¼ãƒ ã®ãƒ—ãƒ«ã®æœŸå¾…å€¤ $\mathbb{E}[N_{i,T}]$ ã«é–¢ã™ã‚‹çµæœã‚’æä¾›ã—ã¾ã™ã€‚ã“ã“ã§ã€$N_{i,T}$ ã¯ã€ã‚¢ãƒ¼ãƒ  $i$ ã®ç·ãƒ—ãƒ«æ•°ã‚’è¡¨ã™ç¢ºç‡å¤‰æ•°**ã§ã‚ã‚Šã€$i$ ãŒæœ€é©ã§ã‚ã‚‹ãƒ©ã‚¦ãƒ³ãƒ‰ã‚’é™¤å¤–ã—ã¦ã„ã¾ã™ã€‚(iãŒæœ€é©ã ã£ãŸãƒ©ã‚¦ãƒ³ãƒ‰ã‚’é™¤å¤–ã™ã‚‹ã®ã¯ãªã‚“ã§ã ã‚...??ç´¯ç©regretã®è¨ˆç®—ã§ã¯ç„¡æ„å‘³ã ã‹ã‚‰...??:thinking:)
æ­£å¼ã«ã¯ã€$N_{i,T} = \sum_{t=1}^{T} \mathds{1}\{I_{t}=i,\,i\neq i^{*}(t)\}$ ã¨å®šç¾©ã•ã‚Œã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## IVAlgorithms IV ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

We analyze twosliding-windowalgorithms, namely theBeta-SWTS, proposed in[48], and theÎ³ğ›¾\gammaitalic_Î³-SWGTS, introduced byFiandri etal. [20], both inspired by the classical TS algorithm. 
ç§ãŸã¡ã¯ã€**2ã¤ã®ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **ã€ã™ãªã‚ã¡ã€[48]ã§ææ¡ˆã•ã‚ŒãŸ**Beta-SWTS**ã¨ã€Fiandriã‚‰ã«ã‚ˆã£ã¦å°å…¥ã•ã‚ŒãŸ**Î³-SWGTS**ã‚’åˆ†æã—ã¾ã™ã€‚ã©ã¡ã‚‰ã‚‚å¤å…¸çš„ãªTSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«è§¦ç™ºã•ã‚Œã¦ã„ã¾ã™ã€‚(GTSã®Gã¯ã‚¬ã‚¦ã‚¹ã®æ„å‘³??:thinking:)
Similarly to what happens withSW-UCB, they handle the problem posed by the dynamical nature of the expected rewards by exploiting only the subset of the most recent collected rewards, i.e., within a sliding window of sizeÏ„âˆˆâ„•ğœâ„•\tau\in\mathbb{N}italic_Ï„ âˆˆ blackboard_N. 
SW-UCBã¨åŒæ§˜ã«ã€å½¼ã‚‰ã¯**æœŸå¾…å ±é…¬ã®å‹•çš„ãªæ€§è³ªã«ã‚ˆã£ã¦å¼•ãèµ·ã“ã•ã‚Œã‚‹å•é¡Œã‚’ã€æœ€è¿‘åé›†ã•ã‚ŒãŸå ±é…¬ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã®ã¿ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§å¯¾å‡¦**ã—ã¾ã™ã€‚ã™ãªã‚ã¡ã€ã‚µã‚¤ã‚º $\tau \in \mathbb{N}$ ã®ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã§ã§ã™ã€‚
(ã†ã‚“ã†ã‚“ã€SWç³»ã®æ‰‹æ³•ã¯ã‚ã‹ã‚Šã‚„ã™ã„...!:thinking:)
This allows us to handle the bias given by the least recent collected rewards, which, in an NS-MAB, may be non-representative of the current expected rewards. 
ã“ã‚Œã«ã‚ˆã‚Šã€NS-MABã«ãŠã„ã¦ã€æœ€ã‚‚æœ€è¿‘åé›†ã•ã‚ŒãŸå ±é…¬ãŒç¾åœ¨ã®æœŸå¾…å ±é…¬ã‚’ä»£è¡¨ã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€æœ€ã‚‚å¤ã„åé›†ã•ã‚ŒãŸå ±é…¬ã«ã‚ˆã‚‹ãƒã‚¤ã‚¢ã‚¹ã‚’æ‰±ã†ã“ã¨ãŒã§ãã¾ã™ã€‚(??)

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

The pseudocode ofBeta-SWTSfor Bernoulli-distributed rewards is presented in Algorithm1, while the pseudocode ofÎ³ğ›¾\gammaitalic_Î³-SWGTSfor subgaussian rewards is presented in Algorithm2. 
Bernoulliåˆ†å¸ƒã®å ±é…¬ã«å¯¾ã™ã‚‹Beta-SWTSã®æ“¬ä¼¼ã‚³ãƒ¼ãƒ‰ã¯ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 1ã«ç¤ºã•ã‚Œã¦ãŠã‚Šã€ã‚µãƒ–ã‚¬ã‚¦ã‚¹å ±é…¬ã«å¯¾ã™ã‚‹Î³-SWGTSã®æ“¬ä¼¼ã‚³ãƒ¼ãƒ‰ã¯ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 2ã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚
They are based on the principle ofconjugate-priorupdates. 
å½¼ã‚‰ã¯å…±å½¹äº‹å‰æ›´æ–°ã®åŸå‰‡ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚
The key difference from the classical TS stands in discarding older examples, thanks to the window widthÏ„ğœ\tauitalic_Ï„, through a sliding-window mechanism. 
**å¤å…¸çš„ãªTSã¨ã®ä¸»ãªé•ã„ã¯ã€ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’é€šã˜ã¦ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å¹…Ï„ã«ã‚ˆã£ã¦å¤ã„ä¾‹ã‚’æ¨ã¦ã‚‹ã“ã¨**ã«ã‚ã‚Šã¾ã™ã€‚(ã†ã‚“ã†ã‚“ã€‚SWç³»ã‚¢ãƒ—ãƒ­ãƒ¼ã¨ã¯ãã†ã ã‚ˆã­:thinking:)
This way, the prior remains sufficiently spread over time, ensuring ongoing exploration, essential to deal with non-stationarity. 
ã“ã®ã‚ˆã†ã«ã—ã¦ã€äº‹å‰åˆ†å¸ƒã¯æ™‚é–“ã¨ã¨ã‚‚ã«ååˆ†ã«åºƒãŒã‚Šç¶šã‘ã€éå®šå¸¸æ€§ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ä¸å¯æ¬ ãªç¶™ç¶šçš„ãªæ¢ç´¢ã‚’ä¿è¨¼ã—ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

- å„ãƒ©ã‚¦ãƒ³ãƒ‰$t \in T $ãŠã‚ˆã³ã‚¢ãƒ¼ãƒ  $i \in  K$ ã«å¯¾ã—ã¦ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\mu_{i,t}$ ã®äº‹å‰åˆ†å¸ƒã‚’ $\nu_{i,t}$ ã§ç¤ºã—ã¾ã™ã€‚
- Beta-SWTSã§ã¯ã€ç„¡æƒ…å ±äº‹å‰åˆ†å¸ƒ(uninformative prior)ãŒè¨­å®šã•ã‚Œã¾ã™ã€‚
- ã™ãªã‚ã¡ã€$\nu_{i,1} := \text{Beta}(1,1)$ï¼ˆè¡Œ3ï¼‰ã§ã‚ã‚Šã€ã“ã“ã§$\text{Beta}(\alpha,\beta)$ ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\alpha,\beta \geq 0$ ã‚’æŒã¤ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒã§ã™ã€‚
- ãƒ©ã‚¦ãƒ³ãƒ‰ $t$ ã«ãŠã‘ã‚‹ã‚¢ãƒ¼ãƒ  $i$ ã®æœŸå¾…å ±é…¬ã®äº‹å¾Œåˆ†å¸ƒã¯ã€$\nu_{i,t} := \text{Beta}(S_{i,t,\tau} + 1, N_{i,t,\tau} - S_{i,t,\tau} + 1)$ ã§ä¸ãˆã‚‰ã‚Œã¾ã™ã€‚(ã†ã‚“ã†ã‚“ã€‚binaryå ±é…¬ã®æ™®é€šã®banditã ...!:thinking:)
- ã“ã“ã§ã€$N_{i,t,\tau} := \sum_{s=\max{\{t-\tau,1\}}}^{t-1}\mathds{1}{\{I_{s}=i\}}$ã¯ã€ã‚¢ãƒ¼ãƒ  $i$ ãŒç›´è¿‘ã® $\min{\{t,\tau\}}$ ãƒ©ã‚¦ãƒ³ãƒ‰ã§é¸æŠã•ã‚ŒãŸå›æ•°ã§ã™ã€‚
- ã¾ãŸã€$S_{i,t,\tau} := \sum_{s=\max\{{t-\tau,1}\}}^{t-1}X_{i,s}\mathds{1}{\{I_{s}=i\}}$ã¯ã€ã‚¢ãƒ¼ãƒ $i$ãŒç›´è¿‘ã®$\min{\{t,\tau\}}$ãƒ©ã‚¦ãƒ³ãƒ‰ã§åé›†ã—ãŸç´¯ç©å ±é…¬ã§ã™ã€‚
- å„ãƒ©ã‚¦ãƒ³ãƒ‰ $t$ ãŠã‚ˆã³å„ã‚¢ãƒ¼ãƒ  $i$ ã«å¯¾ã—ã¦ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ $\theta_{i,t,\tau}$ ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ«ã‚’å¼•ãã¾ã™ã€‚ã“ã‚Œã¯ã€ã„ã‚ã‚†ã‚‹ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒ«ã§ã™ï¼ˆè¡Œ5ï¼‰ã€‚
- æ¬¡ã«ã€ã‚µãƒ³ãƒ—ãƒ«ã®å€¤ãŒæœ€ã‚‚å¤§ãã„ã‚¢ãƒ¼ãƒ ãŒé¸æŠã•ã‚Œã¾ã™ï¼ˆè¡Œ6ï¼‰ã€‚
- åé›†ã—ãŸå ±é…¬ $X_{I_{t},t}$ ã«åŸºã¥ã„ã¦ã€äº‹å‰åˆ†å¸ƒ $\nu_{i,t+1}$ ãŒæ›´æ–°ã•ã‚Œã¾ã™ï¼ˆè¡Œ10ï¼‰ã€‚(=è¦³æ¸¬å ±é…¬ã«åŸºã¥ã„ã¦ä¿¡å¿µã‚’æ›´æ–°...!:thinking:)

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

- Î³-SWGTSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€ã„ãã¤ã‹ã®é•ã„ã‚’é™¤ã„ã¦ã€Beta-SWTSã¨åŒã˜åŸå‰‡ã‚’å…±æœ‰ã—ã¦ã„ã¾ã™ã€‚ 
- ç‰¹ã«ã€ã™ã¹ã¦ã®ã‚¢ãƒ¼ãƒ ãŒ1å›ãƒ—ãƒ¬ã‚¤ã•ã‚Œã‚‹Kãƒ©ã‚¦ãƒ³ãƒ‰ã®åˆæœŸåŒ–ã®å¾Œï¼ˆè¡Œ3ï¼‰ã€å„ãƒ©ã‚¦ãƒ³ãƒ‰$t$ã«ãŠã„ã¦ã€äº‹å‰åˆ†å¸ƒã¯ $\nu_{i,t} := \mathcal{N}(\frac{S_{i,t,\tau}}{N_{i,t,\tau}}, \frac{1}{\gamma N_{i,t,\tau}})$ ã¨ã—ã¦å®šç¾©ã•ã‚Œã¾ã™ã€‚
  - ä¸€æ¬¡å…ƒæ­£è¦åˆ†å¸ƒ...!:thinking:
  - ã“ã“ã§ã€$\mathcal{N}(\alpha,\beta)$ ã¯å¹³å‡ $\alpha \in \mathbb{R}$ã€åˆ†æ•£$\beta \geq 0$ ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã§ã™ã€‚
  - $S_{i,t,\tau}$ ãŠã‚ˆã³ $N_{i,t,\tau}$ ã¯ä¸Šè¨˜ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œ(=beta-SWTS)ã¨åŒã˜!)ã€$\gamma > 0$ã¯å¾Œã§è¨­å®šã•ã‚Œã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™ã€‚
- å„ãƒ©ã‚¦ãƒ³ãƒ‰ $t$ ãŠã‚ˆã³å„ã‚¢ãƒ¼ãƒ  $i$ ã«å¯¾ã—ã¦ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ $\nu_{i,t}$ ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ« $\theta_{i,t,\tau}$ ã‚’å¼•ãï¼ˆè¡Œ13ï¼‰ã€æœ€ã‚‚å¤§ããªãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒ«ã‚’æŒã¤ã‚¢ãƒ¼ãƒ ãŒé¸æŠã•ã‚Œã¾ã™ï¼ˆè¡Œ14ï¼‰ã€‚
- ã‚¢ãƒ¼ãƒ ã«é–¢ã™ã‚‹æƒ…å ±ãŒãªã„å ´åˆã€ã™ãªã‚ã¡$N_{i,t,\tau} = 0$ã®ã¨ãã€ã‚¢ãƒ¼ãƒ ã¯å¼·åˆ¶çš„ã«ãƒ—ãƒ¬ã‚¤ã•ã‚Œã€äº‹å‰åˆ†å¸ƒãŒå¸¸ã«æ˜ç¢ºã«å®šç¾©ã•ã‚Œã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼ˆè¡Œ10ï¼‰ã€‚
- æ¬¡ã«ã€åé›†ã—ãŸå ±é…¬$X_{I_{t},t}$ã«åŸºã¥ã„ã¦ã€äº‹å‰åˆ†å¸ƒ$\nu_{i,t+1}$ãŒæ›´æ–°ã•ã‚Œã¾ã™ï¼ˆè¡Œ19ï¼‰ã€‚(=ãƒ—ãƒ¬ã‚¤ã«ã¤ã„ã¦è¦³æ¸¬ã•ã‚ŒãŸå ±é…¬ã«åŸºã¥ã„ã¦ã€ä¿¡å¿µã‚’æ›´æ–°...!:thinking:)

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## VRegret Analysis for the General Non-Stationary Environment ä¸€èˆ¬éå®šå¸¸ç’°å¢ƒã«ãŠã‘ã‚‹ãƒ¬ã‚°ãƒ¬ãƒƒãƒˆåˆ†æ

In this paper, we investigate NS-MABs in a unifying framework allowing the mean rewards $\mu_{i,t}$ to change arbitrarily over time with no particular regularity, as long as the Assumption III.1 or Assumption III.2 is met. 
æœ¬è«–æ–‡ã§ã¯ã€**æœŸå¾…å ±é…¬ $\mu_{i,t}$ ãŒç‰¹å®šã®è¦å‰‡æ€§ãªã—ã«æ™‚é–“ã¨ã¨ã‚‚ã«ä»»æ„ã«å¤‰åŒ–ã™ã‚‹ã“ã¨ã‚’è¨±å¯ã™ã‚‹çµ±ä¸€çš„ãªæ çµ„ã¿**ã®ä¸­ã§ã€NS-MABã‚’èª¿æŸ»ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ä»®å®š III.1 ã¾ãŸã¯ä»®å®š III.2 ãŒæº€ãŸã•ã‚Œã‚‹é™ã‚Šã§ã™ã€‚
Beginning from this general regret analysis, in Sections VI and VII, we particularize it for the cases in which $\mu_{i,t}$ satisfies additional regularity conditions, i.e., abrupt and smoothly changing, respectively. 
ã“ã®ä¸€èˆ¬çš„ãªãƒ¬ã‚°ãƒ¬ãƒƒãƒˆåˆ†æã‹ã‚‰å§‹ã‚ã¦ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ VI ã¨ VII ã§ã¯ã€$\mu_{i,t}$ ãŒè¿½åŠ ã®è¦å‰‡æ€§æ¡ä»¶ã€ã™ãªã‚ã¡æ€¥æ¿€ã«å¤‰åŒ–ã™ã‚‹å ´åˆã¨æ»‘ã‚‰ã‹ã«å¤‰åŒ–ã™ã‚‹å ´åˆã«ç‰¹åŒ–ã—ã¾ã™ã€‚
We start the analysis by introducing a definition to characterize the rounds during which the algorithms can effectively assess the best arm even in the presence of non-stationarity. 
ç§ãŸã¡ã¯ã€éå®šå¸¸æ€§ãŒå­˜åœ¨ã™ã‚‹å ´åˆã§ã‚‚ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒåŠ¹æœçš„ã«æœ€è‰¯ã®ã‚¢ãƒ¼ãƒ ã‚’è©•ä¾¡ã§ãã‚‹ãƒ©ã‚¦ãƒ³ãƒ‰ã‚’ç‰¹å¾´ä»˜ã‘ã‚‹å®šç¾©ã‚’å°å…¥ã™ã‚‹ã“ã¨ã‹ã‚‰åˆ†æã‚’å§‹ã‚ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

**Definition V.1** (Unlearnable set $F_{\tau}$ and learnable set $F_{\tau}^{C}$)
**å®šç¾© V.1**

For every window size $\tau \in \mathbb{N}$, the unlearnable set $\mathcal{F}_{\tau}$ is defined as any superset of $\mathcal{F}_{\tau}^{\prime}$ defined as: 
ã™ã¹ã¦ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º $\tau \in \mathbb{N}$ ã«å¯¾ã—ã¦ã€å­¦ç¿’ä¸å¯èƒ½ãªé›†åˆ $\mathcal{F}_{\tau}$ ã¯ã€æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã‚‹ $\mathcal{F}_{\tau}^{\prime}$ ã®ä»»æ„ã®ä¸Šä½é›†åˆã¨ã—ã¦å®šç¾©ã•ã‚Œã¾ã™ã€‚

$$
F
\tag{2}
$$

and the learnable set $\mathcal{F}_{\tau}^{\complement}$ is defined as $\mathcal{F}_{\tau}^{\complement} \coloneqq \llbracket T \rrbracket \setminus \mathcal{F}_{\tau}$. 
ãã—ã¦ã€å­¦ç¿’å¯èƒ½ãªé›†åˆ $\mathcal{F}_{\tau}^{\complement}$ ã¯ã€$\mathcal{F}_{\tau}^{\complement} \coloneqq \llbracket T \rrbracket \setminus \mathcal{F}_{\tau}$ ã¨å®šç¾©ã•ã‚Œã¾ã™ã€‚

Notice that by definition, for every round $t \in \mathcal{F}_{\tau}^{\complement}$, the following inequality holds true for all $i \neq i^{*}(t)$: 
å®šç¾©ã«ã‚ˆã‚Šã€ã™ã¹ã¦ã®ãƒ©ã‚¦ãƒ³ãƒ‰ $t \in \mathcal{F}_{\tau}^{\complement}$ ã«å¯¾ã—ã¦ã€æ¬¡ã®ä¸ç­‰å¼ãŒã™ã¹ã¦ã® $i \neq i^{*}(t)$ ã«å¯¾ã—ã¦æˆã‚Šç«‹ã¡ã¾ã™ã€‚

Intuitively, $\mathcal{F}_{\tau}^{\complement}$ collects all the rounds $t \in \llbracket T \rrbracket$ such that the smallest expected reward of the optimal arm $i^{*}(t)$ within the last $\tau$ rounds is larger than the largest expected reward of all other arms in the same interval spanning the length of the sliding window $\tau$. 
ç›´æ„Ÿçš„ã«ã¯ã€$\mathcal{F}_{\tau}^{\complement}$ ã¯ã€æœ€é©ã‚¢ãƒ¼ãƒ  $i^{*}(t)$ ã®éå» $\tau$ ãƒ©ã‚¦ãƒ³ãƒ‰å†…ã®æœ€å°æœŸå¾…å ±é…¬ãŒã€åŒã˜ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®é•·ã• $\tau$ ã«ã‚ãŸã‚‹ä»–ã®ã™ã¹ã¦ã®ã‚¢ãƒ¼ãƒ ã®æœ€å¤§æœŸå¾…å ±é…¬ã‚ˆã‚Šã‚‚å¤§ãã„ãƒ©ã‚¦ãƒ³ãƒ‰ $t \in \llbracket T \rrbracket$ ã‚’é›†ã‚ã¾ã™ã€‚

This enables the introduction of a general definition for the suboptimality gaps $\Delta_{\tau}$ that encodes how challenging it is to identify the optimal arm relying on the rewards collected in the past $\tau$ rounds only. 
ã“ã‚Œã«ã‚ˆã‚Šã€éå» $\tau$ ãƒ©ã‚¦ãƒ³ãƒ‰ã§åé›†ã•ã‚ŒãŸå ±é…¬ã®ã¿ã«ä¾å­˜ã—ã¦æœ€é©ã‚¢ãƒ¼ãƒ ã‚’ç‰¹å®šã™ã‚‹ã“ã¨ãŒã©ã‚Œã»ã©å›°é›£ã§ã‚ã‚‹ã‹ã‚’ç¬¦å·åŒ–ã™ã‚‹ä¸€èˆ¬çš„ãªå®šç¾©ã§ã‚ã‚‹ã‚µãƒ–ã‚ªãƒ—ãƒ†ã‚£ãƒãƒªãƒ†ã‚£ã‚®ãƒ£ãƒƒãƒ— $\Delta_{\tau}$ ã‚’å°å…¥ã§ãã¾ã™ã€‚

**Definition V.2**
**å®šç¾© V.2**

For every window size $\tau \in \mathbb{N}$, the general suboptimality gap is defined as follows: 
ã™ã¹ã¦ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º $\tau \in \mathbb{N}$ ã«å¯¾ã—ã¦ã€ä¸€èˆ¬çš„ãªã‚µãƒ–ã‚ªãƒ—ãƒ†ã‚£ãƒãƒªãƒ†ã‚£ã‚®ãƒ£ãƒƒãƒ—ã¯æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã¾ã™ã€‚

The suboptimality gap $\Delta_{\tau} > 0$ quantifies a minimum non-zero distance in terms of expected reward between the optimal arm $i^{*}(t)$ and all other arms across all rounds $t \in \mathcal{F}_{\tau}^{\complement}$. 
ã‚µãƒ–ã‚ªãƒ—ãƒ†ã‚£ãƒãƒªãƒ†ã‚£ã‚®ãƒ£ãƒƒãƒ— $\Delta_{\tau} > 0$ ã¯ã€æœ€é©ã‚¢ãƒ¼ãƒ  $i^{*}(t)$ ã¨ã™ã¹ã¦ã®ä»–ã®ã‚¢ãƒ¼ãƒ ã¨ã®é–“ã®æœŸå¾…å ±é…¬ã«é–¢ã™ã‚‹æœ€å°ã®éã‚¼ãƒ­è·é›¢ã‚’å®šé‡åŒ–ã—ã¾ã™ã€‚

We are now ready to present the result on the upper bound of the expected number of pulls for the analyzed algorithms. 
ã“ã‚Œã§ã€åˆ†æã•ã‚ŒãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æœŸå¾…ã•ã‚Œã‚‹å¼•ãæ•°ã®ä¸Šé™ã«é–¢ã™ã‚‹çµæœã‚’æç¤ºã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚

**Theorem V.1**  
**å®šç† V.1**

Under Assumption III.1 and $\tau \in \mathbb{N}$, for Beta-SWTS the following holds true for every arm $i \in \llbracket K \rrbracket$: 
ä»®å®š III.1 ãŠã‚ˆã³ $\tau \in \mathbb{N}$ ã®ä¸‹ã§ã€Beta-SWTS ã«å¯¾ã—ã¦ã€ã™ã¹ã¦ã®ã‚¢ãƒ¼ãƒ  $i \in \llbracket K \rrbracket$ ã«å¯¾ã—ã¦æ¬¡ã®ã“ã¨ãŒæˆã‚Šç«‹ã¡ã¾ã™ã€‚

**Theorem V.2**  
**å®šç† V.2**

Under Assumption III.2, $\tau \in \mathbb{N}$, for $\gamma$-SWGTS with $\gamma \leq \min\{\frac{1}{4\lambda^{2}},1\}$ the following holds true for every arm $i \in \llbracket K \rrbracket$: 
ä»®å®š III.2 ã®ä¸‹ã§ã€$\tau \in \mathbb{N}$ ã«å¯¾ã—ã¦ã€$\gamma$-SWGTS ã§ $\gamma \leq \min\{\frac{1}{4\lambda^{2}},1\}$ ã®å ´åˆã€ã™ã¹ã¦ã®ã‚¢ãƒ¼ãƒ  $i \in \llbracket K \rrbracket$ ã«å¯¾ã—ã¦æ¬¡ã®ã“ã¨ãŒæˆã‚Šç«‹ã¡ã¾ã™ã€‚

These results capture a trade-off in the choice of the window size $\tau$. 
ã“ã‚Œã‚‰ã®çµæœã¯ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º $\tau$ ã®é¸æŠã«ãŠã‘ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æ‰ãˆã¦ã„ã¾ã™ã€‚

Specifically, we observe that, given a window size $\tau$, the regret is decomposed in two contributions, namely: (A) $A$, being the cardinality of the unlearnable set $|\mathcal{F}_{\tau}|$, i.e., a superset of the set of rounds in which no algorithm exploiting only the $\tau$ most recent samples can distinguish consistently the best arm from the suboptimal ones; 
å…·ä½“çš„ã«ã¯ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º $\tau$ ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€ãƒ¬ã‚°ãƒ¬ãƒƒãƒˆã¯2ã¤ã®å¯„ä¸ã«åˆ†è§£ã•ã‚Œã‚‹ã“ã¨ãŒè¦³å¯Ÿã•ã‚Œã¾ã™ã€‚ã™ãªã‚ã¡ã€(A) $A$ ã¯å­¦ç¿’ä¸å¯èƒ½ãªé›†åˆã®åŸºæ•° $|\mathcal{F}_{\tau}|$ ã§ã‚ã‚Šã€ã“ã‚Œã¯ $\tau$ ã®æœ€ã‚‚æœ€è¿‘ã®ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ã‚’åˆ©ç”¨ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒæœ€è‰¯ã®ã‚¢ãƒ¼ãƒ ã¨ã‚µãƒ–ã‚ªãƒ—ãƒ†ã‚£ãƒãƒ«ãªã‚¢ãƒ¼ãƒ ã‚’ä¸€è²«ã—ã¦åŒºåˆ¥ã§ããªã„ãƒ©ã‚¦ãƒ³ãƒ‰ã®é›†åˆã®ä¸Šä½é›†åˆã§ã™ã€‚

(B) $B$, corresponding to the expected number of pulls of the suboptimal arm within the learnable set. 
(B) $B$ ã¯ã€å­¦ç¿’å¯èƒ½ãªé›†åˆå†…ã®ã‚µãƒ–ã‚ªãƒ—ãƒ†ã‚£ãƒãƒ«ã‚¢ãƒ¼ãƒ ã®æœŸå¾…ã•ã‚Œã‚‹å¼•ãæ•°ã«å¯¾å¿œã—ã¾ã™ã€‚

We can see that (A) tends to increase with $\tau$ and (B) decreases with $\tau$. 
(A) ã¯ $\tau$ ã¨ã¨ã‚‚ã«å¢—åŠ ã™ã‚‹å‚¾å‘ãŒã‚ã‚Šã€(B) ã¯ $\tau$ ã¨ã¨ã‚‚ã«æ¸›å°‘ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

Notice that dealing with subgaussian reward, a term that accounts for the (possibly) greater uncertainty for the realization of the rewards appears, namely $\gamma$. 
ã‚µãƒ–ã‚¬ã‚¦ã‚¹å ±é…¬ã‚’æ‰±ã†éš›ã«ã¯ã€å ±é…¬ã®å®Ÿç¾ã«å¯¾ã™ã‚‹ï¼ˆãŠãã‚‰ãï¼‰ã‚ˆã‚Šå¤§ããªä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ã™ã‚‹é …ã€ã™ãªã‚ã¡ $\gamma$ ãŒç¾ã‚Œã¾ã™ã€‚

Similarly, an additional (C) term arises for $\gamma$-SWGTS, taking into account the forced exploration to ensure the posterior distribution is always well defined. 
åŒæ§˜ã«ã€$\gamma$-SWGTS ã«å¯¾ã—ã¦ã¯ã€äº‹å¾Œåˆ†å¸ƒãŒå¸¸ã«æ˜ç¢ºã«å®šç¾©ã•ã‚Œã‚‹ã“ã¨ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã®å¼·åˆ¶çš„ãªæ¢ç´¢ã‚’è€ƒæ…®ã—ãŸè¿½åŠ ã® (C) é …ãŒç¾ã‚Œã¾ã™ã€‚

In the next sections, we discuss how these results compare to the ones retrieved in the literature for the most common stationary bandits. 
æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ã“ã‚Œã‚‰ã®çµæœãŒæœ€ã‚‚ä¸€èˆ¬çš„ãªå®šå¸¸ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«é–¢ã™ã‚‹æ–‡çŒ®ã§å¾—ã‚‰ã‚ŒãŸçµæœã¨ã©ã®ã‚ˆã†ã«æ¯”è¼ƒã•ã‚Œã‚‹ã‹ã‚’è­°è«–ã—ã¾ã™ã€‚

Figure 1 provides an example showing how the choice of the window size $\tau$ affects the cardinalities of $\mathcal{F}_{\tau}$ and $\mathcal{F}_{\tau}^{\complement}$. 
å›³1ã¯ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º $\tau$ ã®é¸æŠãŒ $\mathcal{F}_{\tau}$ ã¨ $\mathcal{F}_{\tau}^{\complement}$ ã®åŸºæ•°ã«ã©ã®ã‚ˆã†ã«å½±éŸ¿ã™ã‚‹ã‹ã‚’ç¤ºã™ä¾‹ã‚’æä¾›ã—ã¾ã™ã€‚

The figure depicts a setting in which the optimal arm is the same until an abrupt change occurs. 
ã“ã®å›³ã¯ã€æœ€é©ã‚¢ãƒ¼ãƒ ãŒæ€¥æ¿€ãªå¤‰åŒ–ãŒèµ·ã“ã‚‹ã¾ã§åŒã˜ã§ã‚ã‚‹è¨­å®šã‚’æã„ã¦ã„ã¾ã™ã€‚

This partitions the learning horizon into the intervals $\mathcal{I}_{1}$, $\mathcal{I}_{2}$, and $\mathcal{I}_{3}$. 
ã“ã‚Œã«ã‚ˆã‚Šã€å­¦ç¿’ã®åœ°å¹³ç·šãŒåŒºé–“ $\mathcal{I}_{1}$ã€$\mathcal{I}_{2}$ã€ãŠã‚ˆã³ $\mathcal{I}_{3}$ ã«åˆ†å‰²ã•ã‚Œã¾ã™ã€‚

We consider three different values for the window size $\tau_{1} > \tau_{2} > \tau_{3}$. 
ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã«å¯¾ã—ã¦ã€$\tau_{1} > \tau_{2} > \tau_{3}$ ã®3ã¤ã®ç•°ãªã‚‹å€¤ã‚’è€ƒãˆã¾ã™ã€‚

As the window size increases, the cardinality of $\mathcal{F}_{\tau}^{\complement}$ decreases, as depicted below the figure. 
ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºãŒå¢—åŠ ã™ã‚‹ã«ã¤ã‚Œã¦ã€$\mathcal{F}_{\tau}^{\complement}$ ã®åŸºæ•°ã¯æ¸›å°‘ã—ã¾ã™ã€‚ã“ã‚Œã¯å›³ã®ä¸‹ã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚

Indeed, the learnable sets exclude those rounds for which the window overlaps with two different intervals. 
å®Ÿéš›ã€å­¦ç¿’å¯èƒ½ãªé›†åˆã¯ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãŒ2ã¤ã®ç•°ãªã‚‹åŒºé–“ã¨é‡ãªã‚‹ãƒ©ã‚¦ãƒ³ãƒ‰ã‚’é™¤å¤–ã—ã¾ã™ã€‚

Conversely, when we set a small window, e.g., $\tau_{3}$, the set $\mathcal{F}_{\tau_{3}}^{\complement}$ includes more rounds while guaranteeing that a generic algorithm exploiting samples from the window is capable of selecting the best arm consistently. 
é€†ã«ã€å°ã•ãªã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã€ä¾‹ãˆã° $\tau_{3}$ ã‚’è¨­å®šã™ã‚‹ã¨ã€é›†åˆ $\mathcal{F}_{\tau_{3}}^{\complement}$ ã¯ã‚ˆã‚Šå¤šãã®ãƒ©ã‚¦ãƒ³ãƒ‰ã‚’å«ã¿ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’åˆ©ç”¨ã™ã‚‹ä¸€èˆ¬çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒä¸€è²«ã—ã¦æœ€è‰¯ã®ã‚¢ãƒ¼ãƒ ã‚’é¸æŠã§ãã‚‹ã“ã¨ã‚’ä¿è¨¼ã—ã¾ã™ã€‚

This is due to the fact that, for smaller window size, the algorithms are able to adapt faster to the new form of the expected rewards. 
ã“ã‚Œã¯ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºãŒå°ã•ã„ã»ã©ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒæœŸå¾…å ±é…¬ã®æ–°ã—ã„å½¢ã«ã‚ˆã‚Šæ—©ãé©å¿œã§ãã‚‹ãŸã‚ã§ã™ã€‚

However, choosing $\tau$ too small, as suggested by term (B) of Theorems V.1 and V.2, can lead to a large number of pulls of the suboptimal arms, proportional to $\widetilde{O}\left(\frac{T}{\tau}\right)$, as the algorithms become too explorative. 
ã—ã‹ã—ã€å®šç† V.1 ãŠã‚ˆã³ V.2 ã®é … (B) ã§ç¤ºå”†ã•ã‚Œã‚‹ã‚ˆã†ã«ã€$\tau$ ã‚’å°ã•ãé¸ã³ã™ãã‚‹ã¨ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒéåº¦ã«æ¢ç´¢çš„ã«ãªã‚‹ãŸã‚ã€ã‚µãƒ–ã‚ªãƒ—ãƒ†ã‚£ãƒãƒ«ã‚¢ãƒ¼ãƒ ã®å¼•ãæ•°ãŒå¤§ãããªã‚Šã€$O\left(\frac{T}{\tau}\right)$ ã«æ¯”ä¾‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

As a final remark, we highlight that we do not ask for any specific regularity for the expected rewards, so the results hold for any arbitrary NS-MAB, e.g., also for the rising restless or the rotting restless bandits. 
æœ€å¾Œã«ã€æœŸå¾…å ±é…¬ã«å¯¾ã—ã¦ç‰¹å®šã®è¦å‰‡æ€§ã‚’æ±‚ã‚ãªã„ã“ã¨ã‚’å¼·èª¿ã—ã¾ã™ã®ã§ã€çµæœã¯ä»»æ„ã® NS-MAB ã«å¯¾ã—ã¦æˆã‚Šç«‹ã¡ã¾ã™ã€‚ä¾‹ãˆã°ã€ä¸Šæ˜‡ã™ã‚‹ãƒ¬ã‚¹ãƒˆãƒ¬ã‚¹ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚„è…æ•—ã™ã‚‹ãƒ¬ã‚¹ãƒˆãƒ¬ã‚¹ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«ã‚‚é©ç”¨ã•ã‚Œã¾ã™ã€‚

Now, we are ready to show the results these theorems imply for the most common NS-MAB, i.e., abruptly changing and smoothly changing ones. 
ã“ã‚Œã§ã€ã“ã‚Œã‚‰ã®å®šç†ãŒæœ€ã‚‚ä¸€èˆ¬çš„ãª NS-MABã€ã™ãªã‚ã¡æ€¥æ¿€ã«å¤‰åŒ–ã™ã‚‹ã‚‚ã®ã¨æ»‘ã‚‰ã‹ã«å¤‰åŒ–ã™ã‚‹ã‚‚ã®ã«å¯¾ã—ã¦ç¤ºå”†ã™ã‚‹çµæœã‚’ç¤ºã™æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚

Figure 1: 
å›³1ï¼š



## VIRegret Analysis for Abruptly Changing Environments

VI Regret Analysis for Abruptly Changing Environments  
çªç„¶å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã«ãŠã‘ã‚‹VIã®å¾Œæ‚”åˆ†æ

We now consider the piece-wise constant abruptly-changing environment, i.e., those scenarios in which the expected rewards of the arms remain the same during subsets of the learning horizon called phases, and the phase changes at unknown rounds called breakpoints (Figure 2a).  
ã“ã“ã§ã¯ã€éƒ¨åˆ†çš„ã«å®šæ•°ã®çªç„¶å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã€ã™ãªã‚ã¡ã€ã‚¢ãƒ¼ãƒ ã®æœŸå¾…å ±é…¬ãŒå­¦ç¿’ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã§ã‚ã‚‹ãƒ•ã‚§ãƒ¼ã‚ºã®é–“ã¯åŒã˜ã¾ã¾ã§ã€æœªçŸ¥ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚ã‚‹ãƒ–ãƒ¬ãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆã§ãƒ•ã‚§ãƒ¼ã‚ºãŒå¤‰åŒ–ã™ã‚‹ã‚·ãƒŠãƒªã‚ªã‚’è€ƒãˆã¾ã™ï¼ˆå›³2aï¼‰ã€‚

First, we introduce some quantities used to characterize the regret.  
ã¾ãšã€å¾Œæ‚”ã‚’ç‰¹å¾´ã¥ã‘ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ã„ãã¤ã‹ã®é‡ã‚’å°å…¥ã—ã¾ã™ã€‚

Second, we express Theorem V.1 and Theorem V.2 in terms of these newly defined quantities, comparing them with those of the state-of-the-art algorithms devised for this setting.  
æ¬¡ã«ã€ã“ã‚Œã‚‰ã®æ–°ãŸã«å®šç¾©ã•ã‚ŒãŸé‡ã‚’ç”¨ã„ã¦å®šç†V.1ãŠã‚ˆã³å®šç†V.2ã‚’è¡¨ç¾ã—ã€ã“ã®è¨­å®šã®ãŸã‚ã«è€ƒæ¡ˆã•ã‚ŒãŸæœ€å…ˆç«¯ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ã‚‚ã®ã¨æ¯”è¼ƒã—ã¾ã™ã€‚

Finally, we show that our results apply to a far more general class of abruptly-changing NS-MABs where the expected reward is not constrained to remain constant within each phase.  
æœ€å¾Œã«ã€ç§ãŸã¡ã®çµæœãŒã€å„ãƒ•ã‚§ãƒ¼ã‚ºå†…ã§æœŸå¾…å ±é…¬ãŒä¸€å®šã§ã‚ã‚‹ã“ã¨ã«åˆ¶ç´„ã•ã‚Œãªã„ã€ã‚ˆã‚Šä¸€èˆ¬çš„ãªçªç„¶å¤‰åŒ–ã™ã‚‹NS-MABã®ã‚¯ãƒ©ã‚¹ã«é©ç”¨ã•ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚

### Definition VI.1

A breakpoint is a round $t \in \llbracket 2,T \rrbracket$ such that there exists $i \in \llbracket K \rrbracket$ for which holds $\mu_{i,t} \neq \mu_{i,t-1}$.  
**å®šç¾© VI.1**  
ãƒ–ãƒ¬ãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ã€$t \in \llbracket 2,T \rrbracket$ ã§ã‚ã‚Šã€$i \in \llbracket K \rrbracket$ ãŒå­˜åœ¨ã—ã¦ $\mu_{i,t} \neq \mu_{i,t-1}$ ãŒæˆã‚Šç«‹ã¤ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã™ã€‚

Let us denote with $b_{\psi}$ as the $\psi$-th breakpoint $1 < b_{1} < \ldots < b_{\Upsilon_{T}} < T$, where $\Upsilon_{T} \in \llbracket T \rrbracket$ is the total number of breakpoints over a learning horizon $T$.  
$\psi$-thãƒ–ãƒ¬ãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ $b_{\psi}$ ã¨è¡¨è¨˜ã—ã€$1 < b_{1} < \ldots < b_{\Upsilon_{T}} < T$ ã¨ã—ã€ã“ã“ã§ $\Upsilon_{T} \in \llbracket T \rrbracket$ ã¯å­¦ç¿’ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ $T$ ã«ãŠã‘ã‚‹ãƒ–ãƒ¬ãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆã®ç·æ•°ã§ã™ã€‚

The breakpoints partition the learning horizon $\llbracket T \rrbracket$ into phases $\mathcal{F}_{\psi}$ and pseudophases $\mathcal{F}_{\psi,\tau}^{*}$.  
ãƒ–ãƒ¬ãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆã¯å­¦ç¿’ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ $\llbracket T \rrbracket$ ã‚’ãƒ•ã‚§ãƒ¼ã‚º $\mathcal{F}_{\psi}$ ã¨æ“¬ä¼¼ãƒ•ã‚§ãƒ¼ã‚º $\mathcal{F}_{\psi,\tau}^{*}$ ã«åˆ†å‰²ã—ã¾ã™ã€‚

Formally, using the convention that $b_{0} = 1$ and $b_{\Upsilon_{T}+1} = T$:  
å½¢å¼çš„ã«ã¯ã€$b_{0} = 1$ ãŠã‚ˆã³ $b_{\Upsilon_{T}+1} = T$ ã¨ã„ã†è¦ç´„ã‚’ç”¨ã„ã¾ã™ï¼š

### Definition VI.2

Let $T \in \mathbb{N}$ be the learning horizon and $\psi \in \llbracket \Upsilon_{T}+1 \rrbracket$, we define the $\psi$-th phase as:  
**å®šç¾© VI.2**  
$T \in \mathbb{N}$ ã‚’å­¦ç¿’ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã¨ã—ã€$\psi \in \llbracket \Upsilon_{T}+1 \rrbracket$ ã¨ã™ã‚‹ã¨ã€$\psi$-thãƒ•ã‚§ãƒ¼ã‚ºã‚’æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã—ã¾ã™ï¼š

It is worth noting that the optimal arm $i^{*}(t)$ is for sure constant within each phase $\psi \in \llbracket \Psi_{T}+1 \rrbracket$, i.e., we can appropriately denote it as $i^{*}_{\psi}$.  
æœ€é©ã‚¢ãƒ¼ãƒ  $i^{*}(t)$ ã¯ã€å„ãƒ•ã‚§ãƒ¼ã‚º $\psi \in \llbracket \Psi_{T}+1 \rrbracket$ å†…ã§ç¢ºå®Ÿã«ä¸€å®šã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã™ã‚‹ä¾¡å€¤ãŒã‚ã‚Šã¾ã™ã€‚ã™ãªã‚ã¡ã€é©åˆ‡ã« $i^{*}_{\psi}$ ã¨è¡¨è¨˜ã§ãã¾ã™ã€‚

### Definition VI.3

Let $T \in \mathbb{N}$ be the learning horizon, a window size $\tau$, and $\psi \in \llbracket 2,\Upsilon_{T}+1 \rrbracket$, the $\psi$-th pseudophase is defined as:  
**å®šç¾© VI.3**  
$T \in \mathbb{N}$ ã‚’å­¦ç¿’ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º $\tau$ã€ãŠã‚ˆã³ $\psi \in \llbracket 2,\Upsilon_{T}+1 \rrbracket$ ã¨ã™ã‚‹ã¨ã€$\psi$-thæ“¬ä¼¼ãƒ•ã‚§ãƒ¼ã‚ºã‚’æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã—ã¾ã™ï¼š

and $\mathcal{F}_{1,\tau}^{*} = \mathcal{F}_{1}$.  
ãã—ã¦ã€$\mathcal{F}_{1,\tau}^{*} = \mathcal{F}_{1}$ã€‚

When $\tau$ is longer than the phase, the pseudophase is empty, i.e., where $\mathcal{F}_{\psi,\tau}^{*} = \{\}$ for $\tau \geq b_{\psi} - b_{\psi-1}$.  
$\tau$ ãŒãƒ•ã‚§ãƒ¼ã‚ºã‚ˆã‚Šã‚‚é•·ã„å ´åˆã€æ“¬ä¼¼ãƒ•ã‚§ãƒ¼ã‚ºã¯ç©ºã«ãªã‚Šã¾ã™ã€‚ã™ãªã‚ã¡ã€$\mathcal{F}_{\psi,\tau}^{*} = \{\}$ ã¨ãªã‚Šã¾ã™ã€‚

Finally, we define $\mathcal{F}_{\tau}^{*} = \bigcup_{\psi=1}^{\Upsilon_{T}+1} \mathcal{F}_{\psi,\tau}^{*}$.  
æœ€å¾Œã«ã€$\mathcal{F}_{\tau}^{*} = \bigcup_{\psi=1}^{\Upsilon_{T}+1} \mathcal{F}_{\psi,\tau}^{*}$ ã¨å®šç¾©ã—ã¾ã™ã€‚

The intuition behind the definition of the pseudophase is that if we use an algorithm $\mathfrak{A}$ relying on a sliding window of size $\tau$ during the rounds of the pseudophase $\mathcal{F}_{\psi,\tau}^{*}$, the algorithm $\mathfrak{A}$ uses only on rewards belonging to the single phase $\mathcal{F}_{\psi}$.  
æ“¬ä¼¼ãƒ•ã‚§ãƒ¼ã‚ºã®å®šç¾©ã®èƒŒå¾Œã«ã‚ã‚‹ç›´æ„Ÿã¯ã€æ“¬ä¼¼ãƒ•ã‚§ãƒ¼ã‚º $\mathcal{F}_{\psi,\tau}^{*}$ ã®ãƒ©ã‚¦ãƒ³ãƒ‰ä¸­ã«ã‚µã‚¤ã‚º $\tau$ ã®ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«ä¾å­˜ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  $\mathfrak{A}$ ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  $\mathfrak{A}$ ã¯å˜ä¸€ã®ãƒ•ã‚§ãƒ¼ã‚º $\mathcal{F}_{\psi}$ ã«å±ã™ã‚‹å ±é…¬ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã„ã†ã“ã¨ã§ã™ã€‚

We provide a graphical representation of the definitions introduced above in Figure 2a.  
ä¸Šè¨˜ã®å®šç¾©ã®ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ãªè¡¨ç¾ã‚’å›³2aã«ç¤ºã—ã¾ã™ã€‚

In particular, we have two breakpoints ($\Upsilon_{T}=2$), and three phases $\mathcal{F}_{1}, \mathcal{F}_{2}, \mathcal{F}_{3}$.  
ç‰¹ã«ã€2ã¤ã®ãƒ–ãƒ¬ãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆï¼ˆ$\Upsilon_{T}=2$ï¼‰ã¨3ã¤ã®ãƒ•ã‚§ãƒ¼ã‚º $\mathcal{F}_{1}, \mathcal{F}_{2}, \mathcal{F}_{3}$ ãŒã‚ã‚Šã¾ã™ã€‚

Given a window size of $\tau$, we have three pseudophases $\mathcal{F}_{1,\tau}^{*}, \mathcal{F}_{2,\tau}^{*}, \mathcal{F}_{3,\tau}^{*}$, where the last two pseudophases start $\tau$ rounds after the start of the corresponding phase.  
ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º $\tau$ ã‚’è€ƒãˆã‚‹ã¨ã€3ã¤ã®æ“¬ä¼¼ãƒ•ã‚§ãƒ¼ã‚º $\mathcal{F}_{1,\tau}^{*}, \mathcal{F}_{2,\tau}^{*}, \mathcal{F}_{3,\tau}^{*}$ ãŒã‚ã‚Šã€æœ€å¾Œã®2ã¤ã®æ“¬ä¼¼ãƒ•ã‚§ãƒ¼ã‚ºã¯å¯¾å¿œã™ã‚‹ãƒ•ã‚§ãƒ¼ã‚ºã®é–‹å§‹ã‹ã‚‰ $\tau$ ãƒ©ã‚¦ãƒ³ãƒ‰å¾Œã«å§‹ã¾ã‚Šã¾ã™ã€‚

Let us characterize the sets introduced in Definition VI.1, namely $\mathcal{F}_{\tau}$ and $\mathcal{F}_{\tau}^{\complement}$, using the concepts of phase and pseudophase.  
å®šç¾© VI.1 ã§å°å…¥ã•ã‚ŒãŸé›†åˆã€ã™ãªã‚ã¡ $\mathcal{F}_{\tau}$ ã¨ $\mathcal{F}_{\tau}^{\complement}$ ã‚’ã€ãƒ•ã‚§ãƒ¼ã‚ºã¨æ“¬ä¼¼ãƒ•ã‚§ãƒ¼ã‚ºã®æ¦‚å¿µã‚’ç”¨ã„ã¦ç‰¹å¾´ã¥ã‘ã¾ã—ã‚‡ã†ã€‚

We can express $\mathcal{F}_{\tau}$ as the union of the set of rounds of length $\tau$ after every breakpoint, formally:  
$\mathcal{F}_{\tau}$ ã‚’ã€ã™ã¹ã¦ã®ãƒ–ãƒ¬ãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆã®å¾Œã®é•·ã• $\tau$ ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã®é›†åˆã®å’Œã¨ã—ã¦è¡¨ç¾ã§ãã¾ã™ã€‚å½¢å¼çš„ã«ã¯ï¼š

Consequently, we have $\mathcal{F}_{\tau}^{\complement} = \mathcal{F}_{\tau}^{*}$.  
ã—ãŸãŒã£ã¦ã€$\mathcal{F}_{\tau}^{\complement} = \mathcal{F}_{\tau}^{*}$ ã¨ãªã‚Šã¾ã™ã€‚

Therefore, since for any round $t \in \llbracket T \rrbracket$ belonging to a pseudophase, the algorithms using a sliding window of size $\tau$ uses samples coming from a single phase, we have that for any $t \in \mathcal{F}_{\tau}^{*}$:  
ã—ãŸãŒã£ã¦ã€æ“¬ä¼¼ãƒ•ã‚§ãƒ¼ã‚ºã«å±ã™ã‚‹ä»»æ„ã®ãƒ©ã‚¦ãƒ³ãƒ‰ $t \in \llbracket T \rrbracket$ ã«å¯¾ã—ã¦ã€ã‚µã‚¤ã‚º $\tau$ ã®ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯å˜ä¸€ã®ãƒ•ã‚§ãƒ¼ã‚ºã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€ä»»æ„ã® $t \in \mathcal{F}_{\tau}^{*}$ ã«å¯¾ã—ã¦æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

which corresponds to the learnable set in Definition VI.1.  
ã“ã‚Œã¯å®šç¾© VI.1 ã«ãŠã‘ã‚‹å­¦ç¿’å¯èƒ½ãªé›†åˆã«å¯¾å¿œã—ã¾ã™ã€‚

The latter inequality follows from the fact that any round $t \in \mathcal{F}_{\tau}^{*}$ belongs to a pseudophase $\mathcal{F}_{\psi,\tau}^{*}$ and, therefore, all the times $t' \in \llbracket t-\tau,t-1 \rrbracket$ belong to a single phase $\mathcal{F}_{\psi}$.  
å¾Œè€…ã®ä¸ç­‰å¼ã¯ã€ä»»æ„ã®ãƒ©ã‚¦ãƒ³ãƒ‰ $t \in \mathcal{F}_{\tau}^{*}$ ãŒæ“¬ä¼¼ãƒ•ã‚§ãƒ¼ã‚º $\mathcal{F}_{\psi,\tau}^{*}$ ã«å±ã—ã€ã—ãŸãŒã£ã¦ã€ã™ã¹ã¦ã®æ™‚åˆ» $t' \in \llbracket t-\tau,t-1 \rrbracket$ ãŒå˜ä¸€ã®ãƒ•ã‚§ãƒ¼ã‚º $\mathcal{F}_{\psi}$ ã«å±ã™ã‚‹ã¨ã„ã†äº‹å®Ÿã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚

By definition of the general suboptimality gap (Definition V.2), we have:  
ä¸€èˆ¬çš„ãªã‚µãƒ–ã‚ªãƒ—ãƒ†ã‚£ãƒãƒªãƒ†ã‚£ã‚®ãƒ£ãƒƒãƒ—ã®å®šç¾©ï¼ˆå®šç¾© V.2ï¼‰ã«ã‚ˆã‚Šã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

Notice that the definition of $\Delta_{\tau}$, if $\tau$ is such that no pseudophase is empty, corresponds to the definition of $\Delta$ in the work by [23] in the case of piecewise-constant setting.  
$\Delta_{\tau}$ ã®å®šç¾©ã¯ã€$\tau$ ãŒæ“¬ä¼¼ãƒ•ã‚§ãƒ¼ã‚ºãŒç©ºã§ãªã„å ´åˆã«ã€éƒ¨åˆ†çš„ã«å®šæ•°ã®è¨­å®šã«ãŠã‘ã‚‹ [23] ã®ä½œæ¥­ã® $\Delta$ ã®å®šç¾©ã«å¯¾å¿œã™ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

We are now ready to present the results on the upper bounds of the number of plays in the abruptly changing environment.  
çªç„¶å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã«ãŠã‘ã‚‹ãƒ—ãƒ¬ã‚¤æ•°ã®ä¸Šé™ã«é–¢ã™ã‚‹çµæœã‚’æç¤ºã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚

### Theorem VI.1

Under Assumptions III.1, $\tau \in \mathbb{N}$, for Beta-SWTS the following holds:  
**å®šç† VI.1**  
ä»®å®š III.1 ã®ä¸‹ã§ã€$\tau \in \mathbb{N}$ ã®å ´åˆã€Beta-SWTS ã«å¯¾ã—ã¦æ¬¡ã®ã“ã¨ãŒæˆã‚Šç«‹ã¡ã¾ã™ï¼š

### Theorem VI.2

Under Assumptions III.2, $\tau \in \mathbb{N}$, for $\gamma$-SWGTS with $\gamma \leq \min\{\frac{1}{4\lambda^{2}},1\}$ it holds that:  
**å®šç† VI.2**  
ä»®å®š III.2 ã®ä¸‹ã§ã€$\tau \in \mathbb{N}$ ã®å ´åˆã€$\gamma$-SWGTS ã«å¯¾ã—ã¦ $\gamma \leq \min\{\frac{1}{4\lambda^{2}},1\}$ ãŒæˆã‚Šç«‹ã¡ã¾ã™ï¼š

Let us further analyze the bounds obtained.  
å¾—ã‚‰ã‚ŒãŸå¢ƒç•Œã‚’ã•ã‚‰ã«åˆ†æã—ã¾ã—ã‚‡ã†ã€‚

Making a direct comparison with Theorem V.1 and V.2 for the general NS-MAB setting, we now appreciate a clearer formulation for the cardinality of the unlearnable set.  
ä¸€èˆ¬çš„ãª NS-MAB è¨­å®šã«å¯¾ã™ã‚‹å®šç† V.1 ãŠã‚ˆã³ V.2 ã¨ç›´æ¥æ¯”è¼ƒã™ã‚‹ã“ã¨ã§ã€å­¦ç¿’ä¸å¯èƒ½ãªé›†åˆã®åŸºæ•°ã«å¯¾ã™ã‚‹ã‚ˆã‚Šæ˜ç¢ºãªå®šå¼åŒ–ã‚’ç†è§£ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

In fact, in abruptly changing environments, it is convenient to characterize the unlearnable set as the set of rounds length $\tau$ after every breakpoint.  
å®Ÿéš›ã€çªç„¶å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã§ã¯ã€å­¦ç¿’ä¸å¯èƒ½ãªé›†åˆã‚’ã™ã¹ã¦ã®ãƒ–ãƒ¬ãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆã®å¾Œã®é•·ã• $\tau$ ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã®é›†åˆã¨ã—ã¦ç‰¹å¾´ã¥ã‘ã‚‹ã“ã¨ãŒä¾¿åˆ©ã§ã™ã€‚

In these $\Upsilon_{T}$ rounds, we cannot guarantee that the algorithms will be able to distinguish the best arm from the suboptimal ones.  
ã“ã‚Œã‚‰ã® $\Upsilon_{T}$ ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã¯ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒæœ€é©ãªã‚¢ãƒ¼ãƒ ã¨ã‚µãƒ–ã‚ªãƒ—ãƒ†ã‚£ãƒãƒ«ãªã‚¢ãƒ¼ãƒ ã‚’åŒºåˆ¥ã§ãã‚‹ã“ã¨ã‚’ä¿è¨¼ã§ãã¾ã›ã‚“ã€‚

Figure 2a provides an explicit graphical representation of the quantities introduced.  
å›³2aã¯ã€å°å…¥ã•ã‚ŒãŸé‡ã®æ˜ç¤ºçš„ãªã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«è¡¨ç¾ã‚’æä¾›ã—ã¾ã™ã€‚

In particular, we see that in the first $\tau$ rounds of each phase, the rewards gathered within the window size are not representative of the current expected rewards, as they may include examples from rounds in which the ranking of the arms is different.  
ç‰¹ã«ã€å„ãƒ•ã‚§ãƒ¼ã‚ºã®æœ€åˆã® $\tau$ ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã¯ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºå†…ã§åé›†ã•ã‚ŒãŸå ±é…¬ãŒç¾åœ¨ã®æœŸå¾…å ±é…¬ã‚’ä»£è¡¨ã™ã‚‹ã‚‚ã®ã§ã¯ãªãã€ã‚¢ãƒ¼ãƒ ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãŒç•°ãªã‚‹ãƒ©ã‚¦ãƒ³ãƒ‰ã‹ã‚‰ã®ä¾‹ã‚’å«ã‚€å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

The order for the expected number of pulls of the suboptimal arm within the learnable set matches the state-of-the-art order in $T, \tau$, and $\Delta_{\tau}$ for the expected number of pulls for a sliding window algorithm, even when applied to a stationary bandit [23].  
å­¦ç¿’å¯èƒ½ãªé›†åˆå†…ã®ã‚µãƒ–ã‚ªãƒ—ãƒ†ã‚£ãƒãƒ«ã‚¢ãƒ¼ãƒ ã®æœŸå¾…ã•ã‚Œã‚‹å¼•ãå›ã—ã®é †åºã¯ã€ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æœŸå¾…ã•ã‚Œã‚‹å¼•ãå›ã—ã®ãŸã‚ã®æœ€å…ˆç«¯ã®é †åºã¨ä¸€è‡´ã—ã€å®šå¸¸ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆ [23] ã«é©ç”¨ã•ã‚Œã‚‹å ´åˆã§ã‚‚åŒæ§˜ã§ã™ã€‚

Since existing algorithms for this setting are devised to handle environments with expected rewards bounded in $[0,1]$, in order to compare the results obtained we only consider the piecewise-constant abruptly-changing environment with Bernoulli rewards.  
ã“ã®è¨­å®šã®æ—¢å­˜ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€æœŸå¾…å ±é…¬ãŒ $[0,1]$ ã«åˆ¶ç´„ã•ã‚Œã‚‹ç’°å¢ƒã‚’æ‰±ã†ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€å¾—ã‚‰ã‚ŒãŸçµæœã‚’æ¯”è¼ƒã™ã‚‹ãŸã‚ã«ã€ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤å ±é…¬ã‚’æŒã¤éƒ¨åˆ†çš„ã«å®šæ•°ã®çªç„¶å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã®ã¿ã‚’è€ƒæ…®ã—ã¾ã™ã€‚

Let us assume $\Delta_{\tau}$ constant w.r.t. $T$, as done in the NS-MAB literature and let us choose $\tau \propto T \ln(T)$.  
$\Delta_{\tau}$ ãŒ $T$ ã«å¯¾ã—ã¦ä¸€å®šã§ã‚ã‚‹ã¨ä»®å®šã—ã€NS-MAB æ–‡çŒ®ã§è¡Œã‚ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«ã€$\tau \propto T \ln(T)$ ã‚’é¸ã³ã¾ã—ã‚‡ã†ã€‚

From Theorem VI.1 and VI.2, we derive the following guarantees on the regret:  
å®šç† VI.1 ãŠã‚ˆã³ VI.2 ã‹ã‚‰ã€å¾Œæ‚”ã«é–¢ã™ã‚‹æ¬¡ã®ä¿è¨¼ã‚’å°ãã¾ã™ï¼š

that is the same order of the guarantees on the regret of SW-UCB.  
ã“ã‚Œã¯ SW-UCB ã®å¾Œæ‚”ã«é–¢ã™ã‚‹ä¿è¨¼ã¨åŒã˜é †åºã§ã™ã€‚

Even if GLR-klUCB relies on an active approach to deal with non-stationary bandits, it also retrieves the same order for the bounds on the regret.  
GLR-klUCB ãŒéå®šå¸¸ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ä¾å­˜ã—ã¦ã„ã‚‹å ´åˆã§ã‚‚ã€å¾Œæ‚”ã®å¢ƒç•Œã«å¯¾ã—ã¦åŒã˜é †åºã‚’å–å¾—ã—ã¾ã™ã€‚

Finally, CUSUM-UCB and BR-MAB can achieve the following upper bound on the regret:  
æœ€å¾Œã«ã€CUSUM-UCB ã¨ BR-MAB ã¯å¾Œæ‚”ã«é–¢ã™ã‚‹æ¬¡ã®ä¸Šé™ã‚’é”æˆã§ãã¾ã™ï¼š

which is better than the previous one only for a $\Upsilon_{T}$ factor in the logarithmic term.  
ã“ã‚Œã¯ã€å¯¾æ•°é …ã«ãŠã‘ã‚‹ $\Upsilon_{T}$ ã®å› å­ã«å¯¾ã—ã¦ã®ã¿å‰ã®ã‚‚ã®ã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã¾ã™ã€‚

The results of Theorem V.1 and Theorem V.2 hold for a way more general setting than the piece-wise constant abruptly-changing NS-MABs.  
å®šç† V.1 ãŠã‚ˆã³å®šç† V.2 ã®çµæœã¯ã€éƒ¨åˆ†çš„ã«å®šæ•°ã®çªç„¶å¤‰åŒ–ã™ã‚‹ NS-MAB ã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«ä¸€èˆ¬çš„ãªè¨­å®šã«å¯¾ã—ã¦æˆã‚Šç«‹ã¡ã¾ã™ã€‚

In Figure 2b, we highlight the rounds belonging to the unlearnable set in yellow and the rounds belonging to the learnable set in green for a setting in which the expected rewards are not constant but the expected reward of the optimal arm never intersects that of the suboptimal ones in every phase.  
å›³2bã§ã¯ã€æœŸå¾…å ±é…¬ãŒä¸€å®šã§ãªã„ãŒã€æœ€é©ã‚¢ãƒ¼ãƒ ã®æœŸå¾…å ±é…¬ãŒå„ãƒ•ã‚§ãƒ¼ã‚ºã§ã‚µãƒ–ã‚ªãƒ—ãƒ†ã‚£ãƒãƒ«ãªã‚¢ãƒ¼ãƒ ã®ãã‚Œã¨äº¤å·®ã—ãªã„è¨­å®šã«ãŠã„ã¦ã€å­¦ç¿’ä¸å¯èƒ½ãªé›†åˆã«å±ã™ã‚‹ãƒ©ã‚¦ãƒ³ãƒ‰ã‚’é»„è‰²ã§ã€å­¦ç¿’å¯èƒ½ãªé›†åˆã«å±ã™ã‚‹ãƒ©ã‚¦ãƒ³ãƒ‰ã‚’ç·‘ã§å¼·èª¿è¡¨ç¤ºã—ã¾ã™ã€‚

Note that the cardinality of the learnable and unlearnable sets are the same as those of the NS-MAB described by Figure 2a.  
å­¦ç¿’å¯èƒ½ãªé›†åˆã¨å­¦ç¿’ä¸å¯èƒ½ãªé›†åˆã®åŸºæ•°ã¯ã€å›³2aã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ NS-MAB ã®ãã‚Œã¨åŒã˜ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

Thus, it is not surprising that Theorem VI.1 and Theorem VI.2 hold even for the second setting.  
ã—ãŸãŒã£ã¦ã€å®šç† VI.1 ãŠã‚ˆã³å®šç† VI.2 ãŒç¬¬äºŒã®è¨­å®šã§ã‚‚æˆã‚Šç«‹ã¤ã“ã¨ã¯é©šãã¹ãã“ã¨ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

This represents a generality of our analysis that, to the best of the authorsâ€™ knowledge, is not captured by the existing NS-MAB literature.  
ã“ã‚Œã¯ã€è‘—è€…ã®çŸ¥è­˜ã®é™ã‚Šã§ã¯ã€æ—¢å­˜ã® NS-MAB æ–‡çŒ®ã§ã¯æ‰ãˆã‚‰ã‚Œã¦ã„ãªã„ç§ãŸã¡ã®åˆ†æã®ä¸€èˆ¬æ€§ã‚’è¡¨ã—ã¦ã„ã¾ã™ã€‚

We refer to the class of NS-MABs as (general) abruptly-changing, which can be formally defined through a notion of general breakpoint.  
ç§ãŸã¡ã¯ã€ä¸€èˆ¬çš„ãªãƒ–ãƒ¬ãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆã®æ¦‚å¿µã‚’é€šã˜ã¦æ­£å¼ã«å®šç¾©ã§ãã‚‹ NS-MAB ã®ã‚¯ãƒ©ã‚¹ã‚’ï¼ˆä¸€èˆ¬çš„ãªï¼‰çªç„¶å¤‰åŒ–ã™ã‚‹ã‚‚ã®ã¨å‘¼ã³ã¾ã™ã€‚

### Definition VI.4

A set of $\Upsilon_{T}+1$ rounds $1 \equiv b_{0} < b_{1} < \ldots < b_{\Upsilon_{T}} < T \equiv b_{\Upsilon_{T}+1}$ are generalized breakpoints if for every $\psi \in \llbracket \Upsilon_{T}+1 \rrbracket$ it holds that:  
**å®šç¾© VI.4**  
$\Upsilon_{T}+1$ ãƒ©ã‚¦ãƒ³ãƒ‰ã®é›†åˆ $1 \equiv b_{0} < b_{1} < \ldots < b_{\Upsilon_{T}} < T \equiv b_{\Upsilon_{T}+1}$ ã¯ã€ã™ã¹ã¦ã® $\psi \in \llbracket \Upsilon_{T}+1 \rrbracket$ ã«å¯¾ã—ã¦æ¬¡ã®ã“ã¨ãŒæˆã‚Šç«‹ã¤å ´åˆã€ä¸€èˆ¬åŒ–ã•ã‚ŒãŸãƒ–ãƒ¬ãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆã§ã™ï¼š

for every arm $i \in \llbracket K \rrbracket \setminus \{i^{*}(t)\}$.  
ã™ã¹ã¦ã®ã‚¢ãƒ¼ãƒ  $i \in \llbracket K \rrbracket \setminus \{i^{*}(t)\}$ ã«å¯¾ã—ã¦ã€‚

Notice that, similarly to the previous case, by definition, the optimal arm does not change within two breakpoints, i.e., $i^{*}(t) = i^{*}_{\psi}$ for every $t \in \llbracket b_{\psi-1},b_{\psi}-1 \rrbracket$ and interval $\psi \in \llbracket \Upsilon_{T}+1 \rrbracket$.  
å‰ã®ã‚±ãƒ¼ã‚¹ã¨åŒæ§˜ã«ã€å®šç¾©ã«ã‚ˆã‚Šã€æœ€é©ã‚¢ãƒ¼ãƒ ã¯2ã¤ã®ãƒ–ãƒ¬ãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆå†…ã§å¤‰åŒ–ã—ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã™ãªã‚ã¡ã€$i^{*}(t) = i^{*}_{\psi}$ ã¯ã€ã™ã¹ã¦ã® $t \in \llbracket b_{\psi-1},b_{\psi}-1 \rrbracket$ ãŠã‚ˆã³åŒºé–“ $\psi \in \llbracket \Upsilon_{T}+1 \rrbracket$ ã«å¯¾ã—ã¦æˆã‚Šç«‹ã¡ã¾ã™ã€‚

The definitions of phases and pseudophases (Definition VI.2 and Definition VI.3) still hold with the new definition of the breakpoint.  
ãƒ•ã‚§ãƒ¼ã‚ºã¨æ“¬ä¼¼ãƒ•ã‚§ãƒ¼ã‚ºã®å®šç¾©ï¼ˆå®šç¾© VI.2 ãŠã‚ˆã³å®šç¾© VI.3ï¼‰ã¯ã€æ–°ã—ã„ãƒ–ãƒ¬ãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆã®å®šç¾©ã§ã‚‚ä¾ç„¶ã¨ã—ã¦æˆã‚Šç«‹ã¡ã¾ã™ã€‚

Again, when sampling within an arbitrary pseudophase $\mathcal{F}_{\psi,\tau}^{*}$, since we use only samples belonging to phase $\mathcal{F}_{\psi}$ for which it holds by definition that $\min_{t \in \llbracket b_{\psi-1},b_{\psi}-1 \rrbracket}\{\mu_{i^{*}(t),t}\} > \max_{t \in \llbracket b_{\psi-1},b_{\psi}-1 \rrbracket}\{\mu_{i,t}\}$, also the following holds true for any $t \in \mathcal{F}_{\tau}^{*}$:  
å†ã³ã€ä»»æ„ã®æ“¬ä¼¼ãƒ•ã‚§ãƒ¼ã‚º $\mathcal{F}_{\psi,\tau}^{*}$ å†…ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹å ´åˆã€å®šç¾©ã«ã‚ˆã‚Šã€ãƒ•ã‚§ãƒ¼ã‚º $\mathcal{F}_{\psi}$ ã«å±ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€æ¬¡ã®ã“ã¨ãŒä»»æ„ã® $t \in \mathcal{F}_{\tau}^{*}$ ã«å¯¾ã—ã¦æˆã‚Šç«‹ã¡ã¾ã™ï¼š

which corresponds to the learnable set in Definition V.1.  
ã“ã‚Œã¯å®šç¾© V.1 ã«ãŠã‘ã‚‹å­¦ç¿’å¯èƒ½ãªé›†åˆã«å¯¾å¿œã—ã¾ã™ã€‚



## VIIRegret Analysis for Smoothly Changing Environments
## VII æ»‘ã‚‰ã‹ã«å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã«ãŠã‘ã‚‹å¾Œæ‚”åˆ†æ

We now study what can be inferred from Theorems V.1 and V.2 in the smoothly changing environments, i.e., those scenarios in which the expected reward of each arm is allowed to vary only for a limited amount between consecutive rounds. 
ã“ã“ã§ã¯ã€æ»‘ã‚‰ã‹ã«å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã«ãŠã‘ã‚‹å®šç† V.1 ãŠã‚ˆã³ V.2 ã‹ã‚‰ä½•ãŒæ¨æ¸¬ã§ãã‚‹ã‹ã‚’ç ”ç©¶ã—ã¾ã™ã€‚ã™ãªã‚ã¡ã€å„ã‚¢ãƒ¼ãƒ ã®æœŸå¾…å ±é…¬ãŒé€£ç¶šã™ã‚‹ãƒ©ã‚¦ãƒ³ãƒ‰é–“ã§é™ã‚‰ã‚ŒãŸç¯„å›²å†…ã§ã®ã¿å¤‰å‹•ã™ã‚‹ã‚·ãƒŠãƒªã‚ªã§ã™ã€‚

The regret analysis through breakpoints is unsuitable for an environment in which the expected rewards evolve smoothly. 
ãƒ–ãƒ¬ãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’é€šã˜ãŸå¾Œæ‚”åˆ†æã¯ã€æœŸå¾…å ±é…¬ãŒæ»‘ã‚‰ã‹ã«é€²åŒ–ã™ã‚‹ç’°å¢ƒã«ã¯é©ã—ã¦ã„ã¾ã›ã‚“ã€‚

In what follows, we characterize the regret the algorithms suffer in these settings introducing the most common definitions and assumptions used in the smoothly changing environment literature, deriving the implications for the sets introduced in Definition V.1. 
ä»¥ä¸‹ã§ã¯ã€ã“ã‚Œã‚‰ã®è¨­å®šã«ãŠã„ã¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒè¢«ã‚‹å¾Œæ‚”ã‚’ç‰¹å¾´ã¥ã‘ã€æ»‘ã‚‰ã‹ã«å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã®æ–‡çŒ®ã§ä½¿ç”¨ã•ã‚Œã‚‹æœ€ã‚‚ä¸€èˆ¬çš„ãªå®šç¾©ã¨ä»®å®šã‚’å°å…¥ã—ã€å®šç¾© V.1 ã§å°å…¥ã•ã‚ŒãŸé›†åˆã«å¯¾ã™ã‚‹å«æ„ã‚’å°ãå‡ºã—ã¾ã™ã€‚

Finally, we compare our results with the state-of-the-art results for the setting. 
æœ€å¾Œã«ã€ç§ãŸã¡ã®çµæœã‚’ã“ã®è¨­å®šã«ãŠã‘ã‚‹æœ€å…ˆç«¯ã®çµæœã¨æ¯”è¼ƒã—ã¾ã™ã€‚

### V.1 Assumption VII.1
### V.1 ä»®å®š VII.1

The expected reward of the arms is Lipschitz continuous if there exists $\sigma < +\infty$ such that for every round $t, t' \in \llbracket T \rrbracket$ and arm $i \in \llbracket K \rrbracket$ we have:
ã‚¢ãƒ¼ãƒ ã®æœŸå¾…å ±é…¬ã¯ãƒªãƒ—ã‚·ãƒƒãƒ„é€£ç¶šã§ã‚ã‚‹ã€‚ã‚‚ã— $\sigma < +\infty$ ãŒå­˜åœ¨ã—ã€ã™ã¹ã¦ã®ãƒ©ã‚¦ãƒ³ãƒ‰ $t, t' \in \llbracket T \rrbracket$ ãŠã‚ˆã³ã‚¢ãƒ¼ãƒ  $i \in \llbracket K \rrbracket$ ã«å¯¾ã—ã¦æ¬¡ãŒæˆã‚Šç«‹ã¤ãªã‚‰ã°ï¼š

$$
\text{(14)}
$$

### Assumption VII.2
### ä»®å®š VII.2

Let $\Delta' > 2\sigma\tau > 0$ be finite, we define $\mathcal{F}_{\Delta',T}$ as:
$\Delta' > 2\sigma\tau > 0$ ã‚’æœ‰é™ã¨ã—ã€$\mathcal{F}_{\Delta',T}$ ã‚’æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã—ã¾ã™ï¼š

$$
\text{(15)}
$$

There exist $\beta \in [0,1]$ and finite $F < +\infty$, such that $|\mathcal{F}_{\Delta',T}| \leq FT^{\beta}$.
$\beta \in [0,1]$ ã‹ã¤æœ‰é™ã® $F < +\infty$ ãŒå­˜åœ¨ã—ã€æ¬¡ãŒæˆã‚Šç«‹ã¤ï¼š

$$
|\mathcal{F}_{\Delta',T}| \leq FT^{\beta}
$$

Notice that Assumption 11 in [15] is a particular case of the above assumption when $\beta = 1$. 
[15] ã®ä»®å®š 11 ã¯ã€$\beta = 1$ ã®ã¨ãã«ä¸Šè¨˜ã®ä»®å®šã®ç‰¹åˆ¥ãªå ´åˆã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

We, instead, follow the line of [48], considering an arbitrary order of $T^{\beta}$.
ç§ãŸã¡ã¯ã€ä»£ã‚ã‚Šã« [48] ã®è€ƒãˆæ–¹ã«å¾“ã„ã€$T^{\beta}$ ã®ä»»æ„ã®é †åºã‚’è€ƒæ…®ã—ã¾ã™ã€‚

In the proof of Theorem VII.1, we show that, under Assumptions VII.1 and VII.2, considering the complement set $\mathcal{F}_{\Delta',T}^{\complement} \coloneqq \llbracket T \rrbracket \setminus \mathcal{F}_{\Delta',T}$, for every round $t \in \mathcal{F}_{\Delta',T}^{\complement}$, it holds that:
å®šç† VII.1 ã®è¨¼æ˜ã«ãŠã„ã¦ã€ä»®å®š VII.1 ãŠã‚ˆã³ VII.2 ã®ä¸‹ã§ã€è£œé›†åˆ $\mathcal{F}_{\Delta',T}^{\complement} \coloneqq \llbracket T \rrbracket \setminus \mathcal{F}_{\Delta',T}$ ã‚’è€ƒæ…®ã™ã‚‹ã¨ã€ã™ã¹ã¦ã®ãƒ©ã‚¦ãƒ³ãƒ‰ $t \in \mathcal{F}_{\Delta',T}^{\complement}$ ã«å¯¾ã—ã¦æ¬¡ãŒæˆã‚Šç«‹ã¤ï¼š

$$
\text{(16)}
$$

This implies that $\mathcal{F}_{\tau} = \mathcal{F}_{\Delta',T}$.
ã“ã‚Œã¯ã€$\mathcal{F}_{\tau} = \mathcal{F}_{\Delta',T}$ ã§ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚

From this fact, it is easy to prove that also $\Delta_{\tau} = \Delta' - 2\sigma\tau$.
ã“ã®äº‹å®Ÿã‹ã‚‰ã€$\Delta_{\tau} = \Delta' - 2\sigma\tau$ ã§ã‚ã‚‹ã“ã¨ã‚‚ç°¡å˜ã«è¨¼æ˜ã§ãã¾ã™ã€‚

We are now ready to present the results on the upper bounds of the number of pulls of suboptimal arms for the smoothly changing environment.
ã“ã‚Œã§ã€æ»‘ã‚‰ã‹ã«å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã«ãŠã‘ã‚‹éæœ€é©ã‚¢ãƒ¼ãƒ ã®å¼•ãå›æ•°ã®ä¸Šé™ã«é–¢ã™ã‚‹çµæœã‚’æç¤ºã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚

### Theorem VII.1
### å®šç† VII.1

Under Assumptions III.1, VII.1, and VII.2, $\tau \in \mathbb{N}$, for Beta-SWTS, it holds that:
ä»®å®š III.1, VII.1, ãŠã‚ˆã³ VII.2 ã®ä¸‹ã§ã€$\tau \in \mathbb{N}$ ã®å ´åˆã€Beta-SWTS ã«å¯¾ã—ã¦æ¬¡ãŒæˆã‚Šç«‹ã¡ã¾ã™ï¼š

$$
\text{(17)}
$$

### Theorem VII.2
### å®šç† VII.2

Under Assumptions III.2, VII.1, and VII.2, $\tau \in \mathbb{N}$, for $\gamma$-SWGTS with $\gamma \leq \min\left\{\frac{1}{4\lambda^{2}},1\right\}$, it holds that:
ä»®å®š III.2, VII.1, ãŠã‚ˆã³ VII.2 ã®ä¸‹ã§ã€$\tau \in \mathbb{N}$ ã®å ´åˆã€$\gamma$-SWGTS ã§ $\gamma \leq \min\left\{\frac{1}{4\lambda^{2}},1\right\}$ ã®ã¨ãã€æ¬¡ãŒæˆã‚Šç«‹ã¡ã¾ã™ï¼š

$$
\text{(18)}
$$

Again, we identify the two main contributions, the cardinality of the unlearnable set and the expected number of pulls within the learnable set. 
å†ã³ã€ç§ãŸã¡ã¯äºŒã¤ã®ä¸»è¦ãªå¯„ä¸ã€ã™ãªã‚ã¡å­¦ç¿’ä¸å¯èƒ½ãªé›†åˆã®åŸºæ•°ã¨å­¦ç¿’å¯èƒ½ãªé›†åˆå†…ã®å¼•ãå›ã—ã®æœŸå¾…æ•°ã‚’ç‰¹å®šã—ã¾ã™ã€‚

The former can be bounded, under Assumption VII.2, by $FT^{\beta}$.
å‰è€…ã¯ã€ä»®å®š VII.2 ã®ä¸‹ã§ $FT^{\beta}$ ã«ã‚ˆã£ã¦åˆ¶ç´„ã•ã‚Œã¾ã™ã€‚

The latter is characterized by a sub-optimality gap $\Delta_{\tau}$ that depends on the smoothness parameter $\sigma$ and on the window size $\tau$, capturing the fact that in the rounds in which the distance between the best arm and the suboptimal ones is lower-bounded by $\Delta'$ (as defined in Assumption VII.2), the smooth evolution allows to identify the optimal arm. 
å¾Œè€…ã¯ã€æ»‘ã‚‰ã‹ã•ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\sigma$ ã¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º $\tau$ ã«ä¾å­˜ã™ã‚‹éæœ€é©æ€§ã‚®ãƒ£ãƒƒãƒ— $\Delta_{\tau}$ ã«ã‚ˆã£ã¦ç‰¹å¾´ä»˜ã‘ã‚‰ã‚Œã¾ã™ã€‚ã“ã‚Œã¯ã€æœ€è‰¯ã®ã‚¢ãƒ¼ãƒ ã¨éæœ€é©ãªã‚¢ãƒ¼ãƒ ã¨ã®è·é›¢ãŒ $\Delta'$ï¼ˆä»®å®š VII.2 ã§å®šç¾©ã•ã‚ŒãŸï¼‰ã«ã‚ˆã£ã¦ä¸‹é™ã•ã‚Œã‚‹ãƒ©ã‚¦ãƒ³ãƒ‰ã«ãŠã„ã¦ã€æ»‘ã‚‰ã‹ãªé€²åŒ–ãŒæœ€é©ãªã‚¢ãƒ¼ãƒ ã‚’ç‰¹å®šã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

We remark that the order of $T, \tau$ and $\Delta_{\tau}$ matches the state-of-the-art results when applied to stationary bandits.
$T, \tau$ ãŠã‚ˆã³ $\Delta_{\tau}$ ã®é †åºã¯ã€å®šå¸¸ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«é©ç”¨ã—ãŸå ´åˆã®æœ€å…ˆç«¯ã®çµæœã¨ä¸€è‡´ã™ã‚‹ã“ã¨ã‚’æŒ‡æ‘˜ã—ã¾ã™ã€‚

Let us compare the previous results with the state-of-the-art ones in an environment characterized by Bernoulli rewards. 
å‰è¿°ã®çµæœã‚’ã€ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤å ±é…¬ã«ã‚ˆã£ã¦ç‰¹å¾´ä»˜ã‘ã‚‰ã‚Œã‚‹ç’°å¢ƒã«ãŠã‘ã‚‹æœ€å…ˆç«¯ã®çµæœã¨æ¯”è¼ƒã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

The order for the regret is given by:
å¾Œæ‚”ã®é †åºã¯æ¬¡ã®ã‚ˆã†ã«ä¸ãˆã‚‰ã‚Œã¾ã™ï¼š

$$
\text{(19)}
$$

matching the order of the regret obtained in Theorem D.2 by Combes and Proutiere [15] for SW-KL-UCB.
ã“ã‚Œã¯ã€Combes ã¨ Proutiere [15] ã«ã‚ˆã£ã¦ SW-KL-UCB ã®ãŸã‚ã«å¾—ã‚‰ã‚ŒãŸå®šç† D.2 ã®å¾Œæ‚”ã®é †åºã¨ä¸€è‡´ã—ã¾ã™ã€‚



## VIIIExperiments VIII å®Ÿé¨“

We experimentally evaluate our algorithms w.r.t.the state-of-the-art algorithms for NS-MABs. 
ç§ãŸã¡ã¯ã€NS-MABã«é–¢ã™ã‚‹æœ€å…ˆç«¯ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«å¯¾ã—ã¦ã€ç§ãŸã¡ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿé¨“çš„ã«è©•ä¾¡ã—ã¾ã™ã€‚

In particular, we considered the following baseline algorithms:Rexp3[10], an NS-MAB algorithm based on variation budget,SW-KL-UCB[22], one of the most effective stationary MAB algorithms,Ser4[6], which considers best arm switches during the process, and sliding-window algorithms that are generally able to deal with non-stationary bandit settings such asSW-UCB[24],SW-KL-UCB[15]. 
ç‰¹ã«ã€ä»¥ä¸‹ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è€ƒæ…®ã—ã¾ã—ãŸï¼šRexp3[10]ï¼ˆå¤‰å‹•äºˆç®—ã«åŸºã¥ãNS-MABã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰ã€SW-KL-UCB[22]ï¼ˆæœ€ã‚‚åŠ¹æœçš„ãªå®šå¸¸MABã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®1ã¤ï¼‰ã€ãƒ—ãƒ­ã‚»ã‚¹ä¸­ã«æœ€è‰¯ã®ã‚¢ãƒ¼ãƒ ã®åˆ‡ã‚Šæ›¿ãˆã‚’è€ƒæ…®ã™ã‚‹Ser4[6]ã€ãŠã‚ˆã³SW-UCB[24]ã‚„SW-KL-UCB[15]ã®ã‚ˆã†ãªéå®šå¸¸ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆè¨­å®šã«å¯¾å‡¦ã§ãã‚‹ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚

We include an algorithm meant for stationary bandits, i.e.,TS[47], to show the impact of the sliding window approach on the regret in dynamic scenarios. 
å®šå¸¸ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆç”¨ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ã™ãªã‚ã¡TS[47]ã‚’å«ã‚ã¦ã€å‹•çš„ã‚·ãƒŠãƒªã‚ªã«ãŠã‘ã‚‹ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å½±éŸ¿ã‚’ç¤ºã—ã¾ã™ã€‚

The parameters for all the baseline algorithms have been set as recommended in the corresponding papers (see also AppendixCfor details). 
ã™ã¹ã¦ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€å¯¾å¿œã™ã‚‹è«–æ–‡ã§æ¨å¥¨ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã™ï¼ˆè©³ç´°ã¯ä»˜éŒ²Cã‚‚å‚ç…§ã—ã¦ãã ã•ã„ï¼‰ã€‚

For all experiments, we considerK=10ğ¾10K=10italic_K = 10arms and set the learning horizon toT=5â‹…104ğ‘‡â‹…5superscript104T=5\cdot 10^{4}italic_T = 5 â‹… 10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT. 
ã™ã¹ã¦ã®å®Ÿé¨“ã«ãŠã„ã¦ã€$K=10$ï¼ˆã‚¢ãƒ¼ãƒ ã®æ•°ï¼‰ã‚’è€ƒæ…®ã—ã€å­¦ç¿’ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã‚’$T=5\cdot 10^{4}$ã«è¨­å®šã—ã¾ã™ã€‚

The rewards for a chosen armiğ‘–iitalic_iwill be sampled from a Bernoulli distribution whose probability of success at timetğ‘¡titalic_tis given byÎ¼i,tsubscriptğœ‡ğ‘–ğ‘¡\mu_{i,t}italic_Î¼ start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPTthat will evolve over rounds as specified in the following. 
é¸æŠã•ã‚ŒãŸã‚¢ãƒ¼ãƒ $i$ã®å ±é…¬ã¯ã€æ™‚é–“$t$ã«ãŠã‘ã‚‹æˆåŠŸç¢ºç‡ãŒ$\mu_{i,t}$ã§ã‚ã‚‹ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã€ä»¥ä¸‹ã«ç¤ºã™ã‚ˆã†ã«ãƒ©ã‚¦ãƒ³ãƒ‰ã”ã¨ã«é€²åŒ–ã—ã¾ã™ã€‚

Since we derived above that the order of cumulative regret for our algorithms is the same as that ofSW-UCB, we set the window sizeÏ„ğœ\tauitalic_Ï„for TS-like approaches toÏ„=4TlnTğœ4ğ‘‡ğ‘‡\tau=4\sqrt{T\ln{T}}italic_Ï„ = 4 square-root start_ARG italic_T roman_ln italic_T end_ARG, as also prescribed byGarivier and Moulines [23]. 
ç§ãŸã¡ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç´¯ç©çš„ãªå¾Œæ‚”ã®ã‚ªãƒ¼ãƒ€ãƒ¼ãŒSW-UCBã¨åŒã˜ã§ã‚ã‚‹ã“ã¨ã‚’ä¸Šã§å°å‡ºã—ãŸãŸã‚ã€TSã®ã‚ˆã†ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º$\tau$ã‚’$\tau=4\sqrt{T\ln{T}}$ã«è¨­å®šã—ã¾ã™ã€‚ã“ã‚Œã¯Garivierã¨Moulines[23]ã«ã‚ˆã£ã¦ã‚‚è¦å®šã•ã‚Œã¦ã„ã¾ã™ã€‚

Regarding our algorithms, we also provide a sensitivity analysis evaluating the cumulative regret for different choices of the window sizeÏ„ğœ\tauitalic_Ï„. 
ç§ãŸã¡ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«é–¢ã—ã¦ã€ç•°ãªã‚‹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º$\tau$ã®é¸æŠã«å¯¾ã™ã‚‹ç´¯ç©çš„ãªå¾Œæ‚”ã‚’è©•ä¾¡ã™ã‚‹æ„Ÿåº¦åˆ†æã‚‚æä¾›ã—ã¾ã™ã€‚

We tested our algorithms assuming to misspecify the order of the sliding window w.r.t.the learning horizonTğ‘‡Titalic_T, formally, we setÎ±âˆˆ{0.2,0.4,0.5,0.6,0.8}ğ›¼0.20.40.50.60.8\alpha\in\{0.2,0.4,0.5,0.6,0.8\}italic_Î± âˆˆ { 0.2 , 0.4 , 0.5 , 0.6 , 0.8 }andÏ„=TÎ±ğœsuperscriptğ‘‡ğ›¼\tau=T^{\alpha}italic_Ï„ = italic_T start_POSTSUPERSCRIPT italic_Î± end_POSTSUPERSCRIPT. 
ç§ãŸã¡ã¯ã€å­¦ç¿’ãƒ›ãƒ©ã‚¤ã‚ºãƒ³$T$ã«å¯¾ã™ã‚‹ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ã‚ªãƒ¼ãƒ€ãƒ¼ã‚’èª¤ã£ã¦æŒ‡å®šã™ã‚‹ã“ã¨ã‚’ä»®å®šã—ã¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã—ãŸã€‚å½¢å¼çš„ã«ã¯ã€$\alpha \in \{0.2, 0.4, 0.5, 0.6, 0.8\}$ã¨ã—ã€$\tau=T^{\alpha}$ã¨è¨­å®šã—ã¾ã™ã€‚

For the sake of notation, we denote the theoretically-based choice for the parameter, i.e.,Ï„=4TlnT,ğœ4ğ‘‡ğ‘‡\tau=4\sqrt{T\ln{T}},italic_Ï„ = 4 square-root start_ARG italic_T roman_ln italic_T end_ARG ,asÏ„=T0.5ğœsuperscriptğ‘‡0.5\tau=T^{0.5}italic_Ï„ = italic_T start_POSTSUPERSCRIPT 0.5 end_POSTSUPERSCRIPTin the sensitivity analysis. 
è¨˜æ³•ã®ãŸã‚ã«ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç†è«–ã«åŸºã¥ãé¸æŠã€ã™ãªã‚ã¡$\tau=4\sqrt{T\ln{T}}$ã‚’ã€æ„Ÿåº¦åˆ†æã«ãŠã„ã¦$\tau=T^{0.5}$ã¨è¡¨è¨˜ã—ã¾ã™ã€‚

We denote withÎ±TSsubscriptğ›¼ğ‘‡ğ‘†\alpha_{TS}italic_Î± start_POSTSUBSCRIPT italic_T italic_S end_POSTSUBSCRIPTthe misspecification of the sliding window forBetas-SWTSandÎ±GTSsubscriptğ›¼ğºğ‘‡ğ‘†\alpha_{GTS}italic_Î± start_POSTSUBSCRIPT italic_G italic_T italic_S end_POSTSUBSCRIPTthe one forÎ³ğ›¾\gammaitalic_Î³-SWGTS. 
$\alpha_{TS}$ã¯Betas-SWTSã®ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®èª¤æŒ‡å®šã‚’ã€$\alpha_{GTS}$ã¯$\gamma$-SWGTSã®èª¤æŒ‡å®šã‚’ç¤ºã—ã¾ã™ã€‚

In the following, the results for the different algorithmsğ”„ğ”„\mathfrak{A}fraktur_Aare provided in terms of the empirical cumulated regretR^t(ğ”„)subscript^ğ‘…ğ‘¡ğ”„\hat{R}_{t}(\mathfrak{A})over^ start_ARG italic_R end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( fraktur_A )averaged over50505050independent runs. 
ä»¥ä¸‹ã«ã€ç•°ãªã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ $\mathfrak{A}$ã«å¯¾ã™ã‚‹çµæœã‚’ã€$R_{t}(\mathfrak{A})$ï¼ˆ50å›ã®ç‹¬ç«‹ã—ãŸå®Ÿè¡Œã®å¹³å‡ï¼‰ã¨ã„ã†å½¢ã§æä¾›ã—ã¾ã™ã€‚æ¨™æº–åå·®ã¯åŠé€æ˜ã®é ˜åŸŸã¨ã—ã¦ç¤ºã•ã‚Œã¾ã™ã€‚



### VIII-A çªç„¶å¤‰åŒ–ã™ã‚‹ã‚·ãƒŠãƒªã‚ª

In this scenario, we perform two experiments. 
ã“ã®ã‚·ãƒŠãƒªã‚ªã§ã¯ã€2ã¤ã®å®Ÿé¨“ã‚’è¡Œã„ã¾ã™ã€‚

First, we test the algorithms in a piecewise-constant, abruptly-changing setting. 
ã¾ãšã€æ®µéšçš„ã«ä¸€å®šã§çªç„¶å¤‰åŒ–ã™ã‚‹è¨­å®šã§ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚

The evolution of the expected reward over time of the arms is provided in Figure 3a, and the formal definition of the expected reward evolution over phases is provided in Appendix C. 
ã‚¢ãƒ¼ãƒ ã®æœŸå¾…å ±é…¬ã®æ™‚é–“ã«å¯¾ã™ã‚‹é€²åŒ–ã¯å›³3aã«ç¤ºã•ã‚Œã¦ãŠã‚Šã€ãƒ•ã‚§ãƒ¼ã‚ºã”ã¨ã®æœŸå¾…å ±é…¬ã®é€²åŒ–ã®æ­£å¼ãªå®šç¾©ã¯ä»˜éŒ²Cã«è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚

In the second experiment, we test the algorithms in a general abruptly-changing scenario, i.e., the expected rewards within each phase evolve arbitrarily between two breakpoints. 
2ã¤ç›®ã®å®Ÿé¨“ã§ã¯ã€ä¸€èˆ¬çš„ãªçªç„¶å¤‰åŒ–ã™ã‚‹ã‚·ãƒŠãƒªã‚ªã§ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚ã¤ã¾ã‚Šã€å„ãƒ•ã‚§ãƒ¼ã‚ºå†…ã®æœŸå¾…å ±é…¬ã¯2ã¤ã®ãƒ–ãƒ¬ãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆã®é–“ã§ä»»æ„ã«é€²åŒ–ã—ã¾ã™ã€‚

The evolution of the expected rewards is represented in Figure 4a, and the formal definition of the expected reward evolution over time is provided in Appendix C. 
æœŸå¾…å ±é…¬ã®é€²åŒ–ã¯å›³4aã«ç¤ºã•ã‚Œã¦ãŠã‚Šã€æ™‚é–“ã«å¯¾ã™ã‚‹æœŸå¾…å ±é…¬ã®é€²åŒ–ã®æ­£å¼ãªå®šç¾©ã¯ä»˜éŒ²Cã«è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚

In both settings the optimal arm is $10$ during the $\mathcal{F}_{1}$ and $\mathcal{F}_{3}$ phases and arm $11$ during the $\mathcal{F}_{2}$ and $\mathcal{F}_{4}$ phases. 
ä¸¡æ–¹ã®è¨­å®šã«ãŠã„ã¦ã€æœ€é©ãªã‚¢ãƒ¼ãƒ ã¯ãƒ•ã‚§ãƒ¼ã‚º$\mathcal{F}_{1}$ãŠã‚ˆã³$\mathcal{F}_{3}$ã®é–“ã¯$10$ã§ã‚ã‚Šã€ãƒ•ã‚§ãƒ¼ã‚º$\mathcal{F}_{2}$ãŠã‚ˆã³$\mathcal{F}_{4}$ã®é–“ã¯ã‚¢ãƒ¼ãƒ $11$ã§ã™ã€‚



#### Results çµæœ

The results of the regret of the analyzed algorithms are provided in Figures 3b and 4b. 
åˆ†æã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å¾Œæ‚”ã®çµæœã¯ã€å›³3bãŠã‚ˆã³4bã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚

Since similar conclusions can be drawn from both experiments, for the sake of presentation, we focus on the description of the former. 
ä¸¡æ–¹ã®å®Ÿé¨“ã‹ã‚‰é¡ä¼¼ã®çµè«–ãŒå°ãå‡ºã›ã‚‹ãŸã‚ã€ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®éƒ½åˆä¸Šã€å‰è€…ã®èª¬æ˜ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ã€‚

The algorithms providing the worst performance overall are Rexp3 and Ser4. 
å…¨ä½“çš„ã«æœ€ã‚‚æ‚ªã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æä¾›ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯Rexp3ã¨Ser4ã§ã™ã€‚

We believe this can be explained by the way some hyperparameters are set based on theoretical considerations, which should be tuned depending on the specific scenario to provide better performance. 
ã“ã‚Œã¯ã€ã„ãã¤ã‹ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒç†è«–çš„è€ƒæ…®ã«åŸºã¥ã„ã¦è¨­å®šã•ã‚Œã¦ã„ã‚‹ãŸã‚ã§ã‚ã‚Šã€ç‰¹å®šã®ã‚·ãƒŠãƒªã‚ªã«å¿œã˜ã¦èª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚

During the first phase $\mathcal{F}_{1}$, the best-performing algorithm is TS, since the setting is comparable to a stationary environment during the phase and it is the only algorithm considering the entire history to take decisions. 
æœ€åˆã®ãƒ•ã‚§ãƒ¼ã‚º$\mathcal{F}_{1}$ã®é–“ã€æœ€ã‚‚è‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯TSã§ã™ã€‚ãªãœãªã‚‰ã€ã“ã®è¨­å®šã¯ãƒ•ã‚§ãƒ¼ã‚ºä¸­ã®å®šå¸¸ç’°å¢ƒã«é¡ä¼¼ã—ã¦ãŠã‚Šã€å…¨å±¥æ­´ã‚’è€ƒæ…®ã—ã¦æ„æ€æ±ºå®šã‚’è¡Œã†å”¯ä¸€ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã ã‹ã‚‰ã§ã™ã€‚

As soon as we change phase, and consequently, the optimal arm changes, all the algorithms start accumulating regret at an increased rate. 
ãƒ•ã‚§ãƒ¼ã‚ºã‚’å¤‰æ›´ã™ã‚‹ã¨ã€æœ€é©ãªã‚¢ãƒ¼ãƒ ã‚‚å¤‰ã‚ã‚Šã€ã™ã¹ã¦ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯å¢—åŠ ã—ãŸé€Ÿåº¦ã§å¾Œæ‚”ã‚’è“„ç©ã—å§‹ã‚ã¾ã™ã€‚

In particular, the TS algorithm cannot address this change, and its performance degrades as multiple changes occur. 
ç‰¹ã«ã€TSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã“ã®å¤‰åŒ–ã«å¯¾å‡¦ã§ããšã€è¤‡æ•°ã®å¤‰åŒ–ãŒç™ºç”Ÿã™ã‚‹ã«ã¤ã‚Œã¦ãã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒä½ä¸‹ã—ã¾ã™ã€‚

Conversely, its sliding window counterpart Beta-SWTS provides the best performances starting from the initial part of phase $\mathcal{F}_{2}$ (t â‰ˆ 12.000), showing that forgetting the past is an effective strategy in such a scenario. 
é€†ã«ã€ãã®ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®å¯¾å¿œç‰©ã§ã‚ã‚‹Beta-SWTSã¯ã€ãƒ•ã‚§ãƒ¼ã‚º$\mathcal{F}_{2}$ã®åˆæœŸéƒ¨åˆ†ï¼ˆ$t \approx 12.000$ï¼‰ã‹ã‚‰æœ€è‰¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æä¾›ã—ã€éå»ã‚’å¿˜ã‚Œã‚‹ã“ã¨ãŒã“ã®ã‚ˆã†ãªã‚·ãƒŠãƒªã‚ªã§åŠ¹æœçš„ãªæˆ¦ç•¥ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

By the end of the learning horizon, most of the sliding-window-based approaches are able to outperform the TS algorithm. 
å­¦ç¿’ã®ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã®çµ‚ã‚ã‚Šã¾ã§ã«ã€ã»ã¨ã‚“ã©ã®ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯TSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä¸Šå›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

The fact that $\gamma$-SWGTS is not the best-performing algorithm in this setting is due to the fact that it is designed for generic subgaussian rewards, while the other ones are specifically crafted for Bernoulli rewards. 
$\gamma$-SWGTSãŒã“ã®è¨­å®šã§æœ€ã‚‚è‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ãªã„ç†ç”±ã¯ã€ä¸€èˆ¬çš„ãªã‚µãƒ–ã‚¬ã‚¦ã‚¹å ±é…¬ã®ãŸã‚ã«è¨­è¨ˆã•ã‚Œã¦ã„ã‚‹ã®ã«å¯¾ã—ã€ä»–ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤å ±é…¬ã®ãŸã‚ã«ç‰¹åˆ¥ã«ä½œã‚‰ã‚Œã¦ã„ã‚‹ã‹ã‚‰ã§ã™ã€‚

Therefore, in its design, it needs to introduce more exploration to deal with possibly more complex distribution than the Bernoulli. 
ã—ãŸãŒã£ã¦ã€ãã®è¨­è¨ˆã§ã¯ã€ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤ã‚ˆã‚Šã‚‚ãŠãã‚‰ãã‚ˆã‚Šè¤‡é›‘ãªåˆ†å¸ƒã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€ã‚ˆã‚Šå¤šãã®æ¢ç´¢ã‚’å°å…¥ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚



#### Sensitivity Analysis æ„Ÿåº¦åˆ†æ

Let us focus on the sensitivity analysis provided in Figure3cand4c. 
å›³3cãŠã‚ˆã³4cã«ç¤ºã•ã‚Œã¦ã„ã‚‹æ„Ÿåº¦åˆ†æã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã—ã‚‡ã†ã€‚

In both environments, we see that for smaller window sizes, i.e., $\alpha=0.2$, the algorithms become too explorative, leading to a larger regret at the end of the learning horizon. 
ä¸¡æ–¹ã®ç’°å¢ƒã«ãŠã„ã¦ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºãŒå°ã•ã„å ´åˆã€ã™ãªã‚ã¡ã€$\alpha=0.2$ã®ã¨ãã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯éåº¦ã«æ¢ç´¢çš„ã«ãªã‚Šã€å­¦ç¿’ã®ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã®çµ‚ã‚ã‚Šã«ãŠã„ã¦å¤§ããªå¾Œæ‚”ã‚’å¼•ãèµ·ã“ã—ã¾ã™ã€‚

This means that we are too aggressive in discarding samples used for the armsâ€™ reward estimates, preventing the algorithms from converging to an optimum when the environment is not changing, i.e., we are not switching to the following phase. 
ã“ã‚Œã¯ã€ã‚¢ãƒ¼ãƒ ã®å ±é…¬æ¨å®šã«ä½¿ç”¨ã•ã‚Œã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚’æ¨ã¦ã‚‹ã“ã¨ã«å¯¾ã—ã¦éåº¦ã«æ”»æ’ƒçš„ã§ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã€ç’°å¢ƒãŒå¤‰åŒ–ã—ã¦ã„ãªã„ã¨ãã€ã™ãªã‚ã¡æ¬¡ã®ãƒ•ã‚§ãƒ¼ã‚ºã«åˆ‡ã‚Šæ›¿ãˆã¦ã„ãªã„ã¨ãã«ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒæœ€é©è§£ã«åæŸã™ã‚‹ã®ã‚’å¦¨ã’ã¾ã™ã€‚

As the window size increases, the performance for both algorithms improves, achieving the minimum at the suggested window size (i.e., $\tau=4\sqrt{T\log(T)}$) for Beta-SWTS, while $\gamma$-SWGTS reaches its best performance at $\alpha=0.8$, further highlighting the explorative nature of sampling from a Gaussian distribution in a Bernoulli setting. 
ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹ã«ã¤ã‚Œã¦ã€ä¸¡æ–¹ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ€§èƒ½ãŒå‘ä¸Šã—ã€Beta-SWTSã®å ´åˆã¯ææ¡ˆã•ã‚ŒãŸã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºï¼ˆã™ãªã‚ã¡ã€$\tau=4\sqrt{T\log(T)}$ï¼‰ã§æœ€å°å€¤ã‚’é”æˆã—ã¾ã™ã€‚ä¸€æ–¹ã€$\gamma$-SWGTSã¯$\alpha=0.8$ã§æœ€é«˜ã®æ€§èƒ½ã«é”ã—ã€ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤è¨­å®šã«ãŠã‘ã‚‹ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æ¢ç´¢çš„æ€§è³ªã‚’ã•ã‚‰ã«å¼·èª¿ã—ã¦ã„ã¾ã™ã€‚



### VIII-B ã‚¹ãƒ ãƒ¼ã‚ºã«å¤‰åŒ–ã™ã‚‹ã‚·ãƒŠãƒªã‚ª

Smoothly Changing Scenario
ã‚¹ãƒ ãƒ¼ã‚ºã«å¤‰åŒ–ã™ã‚‹ã‚·ãƒŠãƒªã‚ª

Similarly to what has been done by Combes and Proutiere [15], we test our algorithms on an instance of a smoothly changing environment, as depicted in Figure 5a. 
Combesã¨Proutiere [15]ãŒè¡Œã£ãŸã‚ˆã†ã«ã€ç§ãŸã¡ã¯Figure 5aã«ç¤ºã•ã‚Œã¦ã„ã‚‹ã‚¹ãƒ ãƒ¼ã‚ºã«å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚

In this setting, the smoothness parameter is set to $\sigma=0.0001$. 
ã“ã®è¨­å®šã§ã¯ã€ã‚¹ãƒ ãƒ¼ã‚ºã•ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯$\sigma=0.0001$ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚

We report the formal evolution of the expected reward and additional results on other smoothly changing environments with different values for the smoothness parameter $\sigma$ in Appendix C. 
æœŸå¾…å ±é…¬ã®æ­£å¼ãªé€²åŒ–ã¨ã€ã‚¹ãƒ ãƒ¼ã‚ºã•ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\sigma$ã®ç•°ãªã‚‹å€¤ã‚’æŒã¤ä»–ã®ã‚¹ãƒ ãƒ¼ã‚ºã«å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã«é–¢ã™ã‚‹è¿½åŠ çµæœã‚’Appendix Cã§å ±å‘Šã—ã¾ã™ã€‚

Even in this environment, the optimal arm changes over time so that each arm is optimal for at least one round over the selected learning horizon. 
ã“ã®ç’°å¢ƒã§ã‚‚ã€æœ€é©ãªã‚¢ãƒ¼ãƒ ã¯æ™‚é–“ã¨ã¨ã‚‚ã«å¤‰åŒ–ã—ã€å„ã‚¢ãƒ¼ãƒ ã¯é¸æŠã•ã‚ŒãŸå­¦ç¿’ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã®é–“ã«å°‘ãªãã¨ã‚‚1ãƒ©ã‚¦ãƒ³ãƒ‰ã¯æœ€é©ã«ãªã‚Šã¾ã™ã€‚



#### Results çµæœ

The cumulative regret is provided in Figure5b. 
ç´¯ç©å¾Œæ‚”ã¯Figure5bã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚

Among the worst performing algorithms we have Ser4, Rexp3, and SW-KL-UCB. 
æœ€ã‚‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒæ‚ªã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¯ã€Ser4ã€Rexp3ã€ãŠã‚ˆã³SW-KL-UCBãŒã‚ã‚Šã¾ã™ã€‚

Even in this case, the issue is related to the initialization of the parameters that may play a crucial role in having low regret. 
ã“ã®å ´åˆã§ã‚‚ã€å•é¡Œã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸåŒ–ã«é–¢é€£ã—ã¦ãŠã‚Šã€ä½ã„å¾Œæ‚”ã‚’å¾—ã‚‹ãŸã‚ã«é‡è¦ãªå½¹å‰²ã‚’æœãŸã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

In this setting Beta-SWTS outperforms all the other algorithms in $t \in [30.000, 50.000]$. 
ã“ã®è¨­å®šã§ã¯ã€Beta-SWTSãŒ$t \in [30.000, 50.000]$ã®ç¯„å›²ã§ä»–ã®ã™ã¹ã¦ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä¸Šå›ã‚Šã¾ã™ã€‚

Indeed, it is particularly effective in dealing with cases in which arms whose expected reward was among the lowest becomes optimal. 
å®Ÿéš›ã€æœŸå¾…å ±é…¬ãŒæœ€ã‚‚ä½ã„ã‚¢ãƒ¼ãƒ ãŒæœ€é©ã«ãªã‚‹ã‚±ãƒ¼ã‚¹ã«å¯¾å‡¦ã™ã‚‹ã®ã«ç‰¹ã«åŠ¹æœçš„ã§ã™ã€‚

For instance, in $t \in [10.000, 20.000]$, phase in which $a_{10}$ become optimal, the Beta-SWTS is providing the lowest increase rate among the analyzed algorithms. 
ä¾‹ãˆã°ã€$t \in [10.000, 20.000]$ã®ãƒ•ã‚§ãƒ¼ã‚ºã§ã¯ã€$a_{10}$ãŒæœ€é©ã«ãªã‚‹ã¨ãã€Beta-SWTSã¯åˆ†æã•ã‚ŒãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ä¸­ã§æœ€ã‚‚ä½ã„å¢—åŠ ç‡ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚

Once more, the classical TS algorithm is outperformed by its sliding-window counterpart in $t \in [30.000, 50.000]$. 
å†ã³ã€å¤å…¸çš„ãªTSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€$t \in [30.000, 50.000]$ã®ç¯„å›²ã§ãã®ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®å¯¾å¿œç‰©ã«åŠ£ã£ã¦ã„ã¾ã™ã€‚

Similarly to what happened in the generalized abruptly changing environments, the performance of $\gamma$-SWGTS displays moderate performance in this setting due to the more general formulation of the algorithm. 
ä¸€èˆ¬åŒ–ã•ã‚ŒãŸæ€¥æ¿€ã«å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã§èµ·ã“ã£ãŸã“ã¨ã¨åŒæ§˜ã«ã€$\gamma$-SWGTSã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ã€ã“ã®è¨­å®šã§ã¯ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ã‚ˆã‚Šä¸€èˆ¬çš„ãªå®šå¼åŒ–ã®ãŸã‚ã«ä¸­ç¨‹åº¦ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã¾ã™ã€‚



#### Sensitivity Analysis æ„Ÿåº¦åˆ†æ

The sensitivity analysis is presented in Figure5c. 
æ„Ÿåº¦åˆ†æã¯Figure5cã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚

The behavior is similar to what we presented in the abruptly-changing scenario. 
ãã®æŒ™å‹•ã¯ã€æ€¥æ¿€ã«å¤‰åŒ–ã™ã‚‹ã‚·ãƒŠãƒªã‚ªã§ç¤ºã—ãŸã‚‚ã®ã¨ä¼¼ã¦ã„ã¾ã™ã€‚

More specifically, for small sliding window sizes, the algorithms tend to explore more than is needed. 
ã‚ˆã‚Šå…·ä½“çš„ã«ã¯ã€å°ã•ãªã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã®å ´åˆã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯å¿…è¦ä»¥ä¸Šã«æ¢ç´¢ã™ã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚

Conversely, for larger values of the window size, the performance tends to collapse to almost the same regret curve. 
é€†ã«ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã®å€¤ãŒå¤§ãããªã‚‹ã¨ã€æ€§èƒ½ã¯ã»ã¼åŒã˜å¾Œæ‚”æ›²ç·šã«åæŸã™ã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚

However, for $\alpha=1$, i.e., using the classical TS, would provide a significantly large regret, which shows the necessity to introduce at least a limited amount of forgetting in such settings. 
ã—ã‹ã—ã€$\alpha=1$ã€ã™ãªã‚ã¡å¤å…¸çš„ãªTSã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€éå¸¸ã«å¤§ããªå¾Œæ‚”ãŒç”Ÿã˜ã‚‹ãŸã‚ã€ãã®ã‚ˆã†ãªè¨­å®šã§ã¯å°‘ãªãã¨ã‚‚é™ã‚‰ã‚ŒãŸé‡ã®å¿˜å´ã‚’å°å…¥ã™ã‚‹å¿…è¦æ€§ãŒç¤ºã•ã‚Œã¾ã™ã€‚

5c  
TS  
(a)  
(b)  
(c)  
Figure 5:  



## IXConclusions IX çµè«–

We have characterized the performance of TS-like algorithms designed for NS-MABs, namely Beta-SWTS and $\gamma$-SWGTS, in a general formulation for non-stationary setting, deriving general regret bounds to characterize the learning process in any arbitrary environment, for Bernoulli and subgaussian rewards, respectively. 
ç§ãŸã¡ã¯ã€éå®šå¸¸è¨­å®šã®ä¸€èˆ¬çš„ãªå®šå¼åŒ–ã«ãŠã„ã¦ã€NS-MABsï¼ˆéå®šå¸¸ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆï¼‰å‘ã‘ã«è¨­è¨ˆã•ã‚ŒãŸTSãƒ©ã‚¤ã‚¯ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€å…·ä½“çš„ã«ã¯Beta-SWTSã¨$\gamma$-SWGTSã®æ€§èƒ½ã‚’ç‰¹å¾´ã¥ã‘ã€ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤å ±é…¬ãŠã‚ˆã³ã‚µãƒ–ã‚¬ã‚¦ã‚¹å ±é…¬ã«å¯¾ã—ã¦ä»»æ„ã®ç’°å¢ƒã«ãŠã‘ã‚‹å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç‰¹å¾´ã¥ã‘ã‚‹ä¸€èˆ¬çš„ãªå¾Œæ‚”å¢ƒç•Œã‚’å°å‡ºã—ã¾ã—ãŸã€‚

We have shown how such a general result applies to two of the most common non-stationary settings in the literature, namely the abruptly changing environment and the smoothly changing one, deriving upper bounds on the regret that are in line with the state of the art. 
ç§ãŸã¡ã¯ã€ã“ã®ã‚ˆã†ãªä¸€èˆ¬çš„ãªçµæœãŒæ–‡çŒ®ã«ãŠã‘ã‚‹æœ€ã‚‚ä¸€èˆ¬çš„ãª2ã¤ã®éå®šå¸¸è¨­å®šã€ã™ãªã‚ã¡æ€¥æ¿€ã«å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã¨æ»‘ã‚‰ã‹ã«å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã«ã©ã®ã‚ˆã†ã«é©ç”¨ã•ã‚Œã‚‹ã‹ã‚’ç¤ºã—ã€æœ€å…ˆç«¯ã®ç ”ç©¶ã«æ²¿ã£ãŸå¾Œæ‚”ã®ä¸Šé™ã‚’å°å‡ºã—ã¾ã—ãŸã€‚

Finally, we have performed numerical validations of the proposed algorithms against the baselines that represent the state-of-the-art solutions for learning in dynamic scenarios, showing how the sliding window approach applied to the TS algorithm is a viable solution to deal with Non-Stationary settings. 
æœ€å¾Œã«ã€å‹•çš„ã‚·ãƒŠãƒªã‚ªã«ãŠã‘ã‚‹å­¦ç¿’ã®æœ€å…ˆç«¯ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡¨ã™ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«å¯¾ã—ã¦ææ¡ˆã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ•°å€¤çš„æ¤œè¨¼ã‚’è¡Œã„ã€TSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«é©ç”¨ã•ã‚ŒãŸã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒéå®šå¸¸è¨­å®šã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã®å®Ÿè¡Œå¯èƒ½ãªè§£æ±ºç­–ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚

Future lines of research include developing specialized TS-like algorithms that take into account the specific nature of the non-stationarity or extending the analysis to non-stationary cases in which the arms reward presents a structure among them, such as linear bandits. 
ä»Šå¾Œã®ç ”ç©¶ã®æ–¹å‘æ€§ã«ã¯ã€éå®šå¸¸æ€§ã®ç‰¹å®šã®æ€§è³ªã‚’è€ƒæ…®ã—ãŸå°‚é–€çš„ãªTSãƒ©ã‚¤ã‚¯ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é–‹ç™ºã‚„ã€ã‚¢ãƒ¼ãƒ ã®å ±é…¬ãŒç·šå½¢ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ã‚ˆã†ã«ãã‚Œã‚‰ã®é–“ã«æ§‹é€ ã‚’æŒã¤éå®šå¸¸ã‚±ãƒ¼ã‚¹ã¸ã®åˆ†æã®æ‹¡å¼µãŒå«ã¾ã‚Œã¾ã™ã€‚



## References å‚è€ƒæ–‡çŒ®

- Abramowitz and Stegun [1968]â†‘Milton Abramowitz and IreneA Stegun.Handbook of mathematical functions with formulas, graphs, and mathematical tables.US Government printing office, 55, 1968.
- Abramowitz and Stegun [1968]â†‘ãƒŸãƒ«ãƒˆãƒ³ãƒ»ã‚¢ãƒ–ãƒ©ãƒ¢ã‚¦ã‚£ãƒƒãƒ„ã¨ã‚¢ã‚¤ãƒªãƒ¼ãƒ³ãƒ»Aãƒ»ã‚¹ãƒ†ã‚°ãƒ³ã€‚æ•°å¼ã€ã‚°ãƒ©ãƒ•ã€æ•°å­¦çš„è¡¨ã‚’å«ã‚€æ•°å­¦é–¢æ•°ã®ãƒãƒ³ãƒ‰ãƒ–ãƒƒã‚¯ã€‚ã‚¢ãƒ¡ãƒªã‚«åˆè¡†å›½æ”¿åºœå°åˆ·å±€ã€55ã€1968å¹´ã€‚
  
- Agarwal [2013]â†‘Deepak Agarwal.Computational advertising: the linkedin way.InProceedings of the Conference on Information & Knowledge Management (CIKM), pages 1585â€“1586, 2013.
- Agarwal [2013]â†‘ãƒ‡ã‚£ãƒ¼ãƒ‘ã‚¯ãƒ»ã‚¢ã‚¬ãƒ«ãƒ¯ãƒ«ã€‚è¨ˆç®—åºƒå‘Šï¼šLinkedInã®æ–¹æ³•ã€‚æƒ…å ±ã¨çŸ¥è­˜ç®¡ç†ã«é–¢ã™ã‚‹ä¼šè­°ã®è­°äº‹éŒ²ï¼ˆCIKMï¼‰ã€ãƒšãƒ¼ã‚¸1585â€“1586ã€2013å¹´ã€‚

- Agarwal etal. [2014]â†‘Deepak Agarwal, BoLong, Jonathan Traupman, Doris Xin, and Liang Zhang.Laser: A scalable response prediction platform for online advertising.InProceedings of the ACM international conference on Web Search and Data Mining (WSDM), pages 173â€“182, 2014.
- Agarwal etal. [2014]â†‘ãƒ‡ã‚£ãƒ¼ãƒ‘ã‚¯ãƒ»ã‚¢ã‚¬ãƒ«ãƒ¯ãƒ«ã€ãƒœãƒ»ãƒ­ãƒ³ã€ã‚¸ãƒ§ãƒŠã‚µãƒ³ãƒ»ãƒˆãƒ©ã‚¦ãƒ—ãƒãƒ³ã€ãƒ‰ãƒªã‚¹ãƒ»ã‚·ãƒ³ã€ãƒªãƒ£ãƒ³ãƒ»ãƒãƒ£ãƒ³ã€‚ãƒ¬ãƒ¼ã‚¶ãƒ¼ï¼šã‚ªãƒ³ãƒ©ã‚¤ãƒ³åºƒå‘Šã®ãŸã‚ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªå¿œç­”äºˆæ¸¬ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã€‚ACMå›½éš›ä¼šè­°ã®è­°äº‹éŒ²ï¼ˆWSDMï¼‰ã€ãƒšãƒ¼ã‚¸173â€“182ã€2014å¹´ã€‚

- Agrawal and Goyal [2012]â†‘Shipra Agrawal and Navin Goyal.Analysis of thompson sampling for the multi-armed bandit problem.InProceedings of the Conference on learning theory (COLT), 2012.
- Agrawal and Goyal [2012]â†‘ã‚·ãƒ—ãƒ©ãƒ»ã‚¢ã‚°ãƒ©ãƒ¯ãƒ«ã¨ãƒŠãƒ“ãƒ³ãƒ»ã‚´ãƒ¤ãƒ«ã€‚ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã«å¯¾ã™ã‚‹ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®åˆ†æã€‚å­¦ç¿’ç†è«–ã«é–¢ã™ã‚‹ä¼šè­°ã®è­°äº‹éŒ²ï¼ˆCOLTï¼‰ã€2012å¹´ã€‚

- Agrawal and Goyal [2017]â†‘Shipra Agrawal and Navin Goyal.Near-optimal regret bounds for thompson sampling.Journal of the ACM (JACM), 64(5):1â€“24, 2017.
- Agrawal and Goyal [2017]â†‘ã‚·ãƒ—ãƒ©ãƒ»ã‚¢ã‚°ãƒ©ãƒ¯ãƒ«ã¨ãƒŠãƒ“ãƒ³ãƒ»ã‚´ãƒ¤ãƒ«ã€‚ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãŸã‚ã®ã»ã¼æœ€é©ãªå¾Œæ‚”å¢ƒç•Œã€‚ACMã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ï¼ˆJACMï¼‰ã€64(5):1â€“24ã€2017å¹´ã€‚

- Allesiardo etal. [2017]â†‘Robin Allesiardo, RaphaÃ«l FÃ©raud, and Odalric-Ambrym Maillard.The non-stationary stochastic multi-armed bandit problem.International Journal of Data Science and Analytics, 3(4):267â€“283, 2017.
- Allesiardo etal. [2017]â†‘ãƒ­ãƒ“ãƒ³ãƒ»ã‚¢ãƒ¬ã‚·ã‚¢ãƒ«ãƒ‰ã€ãƒ©ãƒ•ã‚¡ã‚¨ãƒ«ãƒ»ãƒ•ã‚§ãƒ­ãƒ¼ã€ã‚ªãƒ€ãƒªãƒƒã‚¯ãƒ»ã‚¢ãƒ³ãƒ–ãƒªãƒ ãƒ»ãƒã‚¤ãƒ©ãƒ«ãƒ‰ã€‚éå®šå¸¸ç¢ºç‡çš„ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã€‚ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã¨åˆ†æã®å›½éš›ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ã€3(4):267â€“283ã€2017å¹´ã€‚

- Auer etal. [2002]â†‘Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.Finite-time analysis of the multiarmed bandit problem.Machine learning, 47:235â€“256, 2002.
- Auer etal. [2002]â†‘ãƒ”ãƒ¼ã‚¿ãƒ¼ãƒ»ã‚¢ã‚¦ã‚¢ãƒ¼ã€ãƒ‹ã‚³ãƒ­ãƒ»ãƒã‚§ãƒ¼ã‚¶ãƒ»ãƒ“ã‚¢ãƒ³ã‚­ã€ãƒãƒ¼ãƒ«ãƒ»ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼ã€‚ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã®æœ‰é™æ™‚é–“åˆ†æã€‚æ©Ÿæ¢°å­¦ç¿’ã€47:235â€“256ã€2002å¹´ã€‚

- Aziz etal. [2021]â†‘Maryam Aziz, Emilie Kaufmann, and Marie-Karelle Riviere.On multi-armed bandit designs for dose-finding trials.Journal of Machine Learning Research (JMLR), 22(14):1â€“38, 2021.
- Aziz etal. [2021]â†‘ãƒãƒªã‚¢ãƒ ãƒ»ã‚¢ã‚¸ã‚ºã€ã‚¨ãƒŸãƒªãƒ¼ãƒ»ã‚«ã‚¦ãƒ•ãƒãƒ³ã€ãƒãƒªãƒ¼ãƒ»ã‚«ãƒ¬ãƒ¼ãƒ«ãƒ»ãƒªãƒ“ã‚¨ãƒ¼ãƒ«ã€‚ç”¨é‡æ¢ç´¢è©¦é¨“ã®ãŸã‚ã®ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒ‡ã‚¶ã‚¤ãƒ³ã«ã¤ã„ã¦ã€‚æ©Ÿæ¢°å­¦ç¿’ç ”ç©¶ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ï¼ˆJMLRï¼‰ã€22(14):1â€“38ã€2021å¹´ã€‚

- Basseville etal. [1993]â†‘Michele Basseville, IgorV Nikiforov, etal.Detection of abrupt changes: theory and application.Prentice Hall Englewood Cliffs, 104, 1993.
- Basseville etal. [1993]â†‘ãƒŸã‚±ãƒ¼ãƒ¬ãƒ»ãƒãƒƒã‚»ãƒ´ã‚£ãƒ«ã€ã‚¤ã‚´ãƒ¼ãƒ«ãƒ»Vãƒ»ãƒ‹ã‚­ãƒ•ã‚©ãƒ­ãƒ•ã€ä»–ã€‚æ€¥æ¿€ãªå¤‰åŒ–ã®æ¤œå‡ºï¼šç†è«–ã¨å¿œç”¨ã€‚ãƒ—ãƒ¬ãƒ³ãƒ†ã‚£ã‚¹ãƒ»ãƒ›ãƒ¼ãƒ«ã€ã‚¨ãƒ³ã‚²ãƒ«ã‚¦ãƒƒãƒ‰ãƒ»ã‚¯ãƒªãƒ•ã‚¹ã€104ã€1993å¹´ã€‚

- Besbes etal. [2014]â†‘Omar Besbes, Yonatan Gur, and Assaf Zeevi.Stochastic multi-armed-bandit problem with non-stationary rewards.Advances in neural information processing systems (NeurIPS), 2014.
- Besbes etal. [2014]â†‘ã‚ªãƒãƒ¼ãƒ«ãƒ»ãƒ™ã‚¹ãƒ™ã‚¹ã€ãƒ¨ãƒŠã‚¿ãƒ³ãƒ»ã‚°ãƒ«ã€ã‚¢ãƒƒã‚µãƒ•ãƒ»ã‚¼ã‚¨ãƒ“ã€‚éå®šå¸¸å ±é…¬ã‚’æŒã¤ç¢ºç‡çš„ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã€‚ç¥çµŒæƒ…å ±å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®é€²å±•ï¼ˆNeurIPSï¼‰ã€2014å¹´ã€‚

- Besson etal. [2022]â†‘Lilian Besson, Emilie Kaufmann, Odalric-Ambrym Maillard, and Julien Seznec.Efficient change-point detection for tackling piecewise-stationary bandits.Journal of Mchine Learning Research (JMLR), 23(77):1â€“40, 2022.
- Besson etal. [2022]â†‘ãƒªãƒªã‚¢ãƒ³ãƒ»ãƒ™ãƒƒã‚½ãƒ³ã€ã‚¨ãƒŸãƒªãƒ¼ãƒ»ã‚«ã‚¦ãƒ•ãƒãƒ³ã€ã‚ªãƒ€ãƒªãƒƒã‚¯ãƒ»ã‚¢ãƒ³ãƒ–ãƒªãƒ ãƒ»ãƒã‚¤ãƒ©ãƒ«ãƒ‰ã€ã‚¸ãƒ¥ãƒªã‚¢ãƒ³ãƒ»ã‚»ã‚ºãƒãƒƒã‚¯ã€‚åŒºåˆ†çš„å®šå¸¸ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã®åŠ¹ç‡çš„ãªå¤‰åŒ–ç‚¹æ¤œå‡ºã€‚æ©Ÿæ¢°å­¦ç¿’ç ”ç©¶ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ï¼ˆJMLRï¼‰ã€23(77):1â€“40ã€2022å¹´ã€‚

- Bi etal. [2024]â†‘Wenjie Bi, Bing Wang, and Haiying Liu.Personalized dynamic pricing based on improved thompson sampling.Mathematics, 12(8):1123, 2024.
- Bi etal. [2024]â†‘ã‚¦ã‚§ãƒ³ã‚¸ãƒ¼ãƒ»ãƒ“ã€ãƒ“ãƒ³ãƒ»ãƒ¯ãƒ³ã€ãƒã‚¤ã‚¤ãƒ³ãƒ»ãƒªã‚¦ã€‚æ”¹è‰¯ã•ã‚ŒãŸãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«åŸºã¥ãå€‹åˆ¥åŒ–ã•ã‚ŒãŸå‹•çš„ä¾¡æ ¼è¨­å®šã€‚æ•°å­¦ã€12(8):1123ã€2024å¹´ã€‚

- Cesa-Bianchi and Lugosi [2006]â†‘Nicolo Cesa-Bianchi and GÃ¡bor Lugosi.Prediction, learning, and games.Cambridge university press, 2006.
- Cesa-Bianchi and Lugosi [2006]â†‘ãƒ‹ã‚³ãƒ­ãƒ»ãƒã‚§ãƒ¼ã‚¶ãƒ»ãƒ“ã‚¢ãƒ³ã‚­ã¨ã‚¬ãƒ¼ãƒœãƒ«ãƒ»ãƒ«ã‚´ã‚·ã€‚äºˆæ¸¬ã€å­¦ç¿’ã€ã‚²ãƒ¼ãƒ ã€‚ã‚±ãƒ³ãƒ–ãƒªãƒƒã‚¸å¤§å­¦å‡ºç‰ˆã€2006å¹´ã€‚

- Chapelle and Li [2011]â†‘Olivier Chapelle and Lihong Li.An empirical evaluation of thompson sampling.InAdvances in Neural Information Processing Systems (NeurIPS), 2011.
- Chapelle and Li [2011]â†‘ã‚ªãƒªãƒ“ã‚¨ãƒ»ã‚·ãƒ£ãƒšãƒ«ã¨ãƒªãƒ›ãƒ³ãƒ»ãƒªãƒ¼ã€‚ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®å®Ÿè¨¼è©•ä¾¡ã€‚ç¥çµŒæƒ…å ±å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®é€²å±•ï¼ˆNeurIPSï¼‰ã€2011å¹´ã€‚

- Combes and Proutiere [2014]â†‘Richard Combes and Alexandre Proutiere.Unimodal bandits: Regret lower bounds and optimal algorithms.InProceedings of the International Conference on Machine Learning (ICML), volume32, pages 521â€“529, 2014.
- Combes and Proutiere [2014]â†‘ãƒªãƒãƒ£ãƒ¼ãƒ‰ãƒ»ã‚³ãƒ³ãƒ–ã¨ã‚¢ãƒ¬ã‚¯ã‚µãƒ³ãƒ‰ãƒ«ãƒ»ãƒ—ãƒ«ãƒ†ã‚£ã‚¨ãƒ¼ãƒ«ã€‚å˜å³°ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆï¼šå¾Œæ‚”ã®ä¸‹é™ã¨æœ€é©ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚å›½éš›æ©Ÿæ¢°å­¦ç¿’ä¼šè­°ã®è­°äº‹éŒ²ï¼ˆICMLï¼‰ã€ç¬¬32å·»ã€ãƒšãƒ¼ã‚¸521â€“529ã€2014å¹´ã€‚

- Dasgupta etal. [2024]â†‘Arpan Dasgupta, Gagan Jain, Arun Suggala, Karthikeyan Shanmugam, Milind Tambe, and Aparna Taneja.Bayesian collaborative bandits with thompson sampling for improved outreach in maternal health program.2024.URLhttps://arxiv.org/abs/2410.21405.
- Dasgupta etal. [2024]â†‘ã‚¢ãƒ«ãƒ‘ãƒ³ãƒ»ãƒ€ã‚¹ã‚°ãƒ—ã‚¿ã€ã‚¬ã‚¬ãƒ³ãƒ»ã‚¸ãƒ£ã‚¤ãƒ³ã€ã‚¢ãƒ©ãƒ³ãƒ»ã‚µãƒƒã‚¬ãƒ©ã€ã‚«ãƒ«ãƒ†ã‚£ã‚±ãƒ¤ãƒ³ãƒ»ã‚·ãƒ£ãƒŒãƒ ã‚¬ãƒ ã€ãƒŸãƒªãƒ³ãƒ‰ãƒ»ã‚¿ãƒ³ãƒ™ã€ã‚¢ãƒ‘ãƒ«ãƒŠãƒ»ã‚¿ãƒã‚¸ãƒ£ã€‚æ¯å­ä¿å¥ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«ãŠã‘ã‚‹ã‚¢ã‚¦ãƒˆãƒªãƒ¼ãƒã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã®ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ç”¨ã„ãŸãƒ™ã‚¤ã‚ºçš„å”èª¿ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã€‚2024å¹´ã€‚URLhttps://arxiv.org/abs/2410.21405ã€‚

- deFreitasFonseca etal. [2024]â†‘Gustavo deFreitasFonseca, LucasCoelho eSilva, and Paulo AndrÃ©Lima deCastro.Addressing non-stationarity with relaxed f-discounted-sliding-window thompson sampling.In2024 IEEE International Conference on Omni-layer Intelligent Systems (COINS), pages 1â€“6. IEEE, 2024.
- deFreitasFonseca etal. [2024]â†‘ã‚°ã‚¹ã‚¿ãƒœãƒ»ãƒ‡ãƒ»ãƒ•ãƒ¬ã‚¤ã‚¿ã‚¹ãƒ»ãƒ•ã‚©ãƒ³ã‚»ã‚«ã€ãƒ«ãƒ¼ã‚«ã‚¹ãƒ»ã‚³ã‚¨ãƒ¼ãƒªãƒ§ãƒ»ã‚¨ãƒ»ã‚·ãƒ«ãƒã€ãƒ‘ã‚¦ãƒ­ãƒ»ã‚¢ãƒ³ãƒ‰ãƒ¬ãƒ»ãƒªãƒãƒ»ãƒ‡ãƒ»ã‚«ã‚¹ãƒˆãƒ­ã€‚ãƒªãƒ©ãƒƒã‚¯ã‚¹ã—ãŸfå‰²å¼•ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹éå®šå¸¸æ€§ã¸ã®å¯¾å‡¦ã€‚2024å¹´IEEEå›½éš›ä¼šè­°ã‚ªãƒ ãƒ‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆCOINSï¼‰ã€ãƒšãƒ¼ã‚¸1â€“6ã€‚IEEEã€2024å¹´ã€‚

- Dixit etal. [2023]â†‘KrishnaKant Dixit, Devvret Verma, SureshKumar Muthuvel, KLaxminarayanamma, Mukesh Kumar, and Amit Srivastava.Thompson sampling algorithm for personalized treatment recommendations in healthcare.In2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI), volume1, pages 1â€“6. IEEE, 2023.
- Dixit etal. [2023]â†‘ã‚¯ãƒªã‚·ãƒ¥ãƒŠãƒ»ã‚«ãƒ³ãƒˆãƒ»ãƒ‡ã‚£ã‚¯ã‚·ãƒƒãƒˆã€ãƒ‡ãƒ´ãƒ´ãƒ¬ãƒˆãƒ»ãƒ´ã‚§ãƒ«ãƒã€ã‚¹ãƒ¬ã‚·ãƒ¥ãƒ»ã‚¯ãƒãƒ¼ãƒ«ãƒ»ãƒ ãƒˆã‚¥ãƒ´ã‚§ãƒ«ã€Kãƒ»ãƒ©ã‚¯ã‚·ãƒ¥ãƒŸãƒŠãƒ©ãƒ¤ãƒ³ã‚¢ãƒã€ãƒ ã‚±ã‚·ãƒ¥ãƒ»ã‚¯ãƒãƒ¼ãƒ«ã€ã‚¢ãƒŸãƒƒãƒˆãƒ»ã‚¹ãƒªãƒã‚¹ã‚¿ãƒã€‚ãƒ˜ãƒ«ã‚¹ã‚±ã‚¢ã«ãŠã‘ã‚‹å€‹åˆ¥åŒ–ã•ã‚ŒãŸæ²»ç™‚æ¨å¥¨ã®ãŸã‚ã®ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚2023å¹´äººå·¥çŸ¥èƒ½ã¨ãƒ˜ãƒ«ã‚¹ã‚±ã‚¢ç”£æ¥­ã®é©æ–°ã«é–¢ã™ã‚‹å›½éš›ä¼šè­°ï¼ˆICAIIHIï¼‰ã€ç¬¬1å·»ã€ãƒšãƒ¼ã‚¸1â€“6ã€‚IEEEã€2023å¹´ã€‚

- Fiandri etal. [2024]â†‘Marco Fiandri, AlbertoMaria Metelli, and Francesco TrovÃ².Sliding-window thompson sampling for non-stationary settings.2024.URLhttps://arxiv.org/abs/2409.051810.
- Fiandri etal. [2024]â†‘ãƒãƒ«ã‚³ãƒ»ãƒ•ã‚£ã‚¢ãƒ³ãƒ‰ãƒªã€ã‚¢ãƒ«ãƒ™ãƒ«ãƒˆãƒ»ãƒãƒªã‚¢ãƒ»ãƒ¡ãƒ†ãƒƒãƒªã€ãƒ•ãƒ©ãƒ³ãƒã‚§ã‚¹ã‚³ãƒ»ãƒˆãƒ­ãƒ´ã‚©ã€‚éå®šå¸¸è¨­å®šã®ãŸã‚ã®ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚2024å¹´ã€‚URLhttps://arxiv.org/abs/2409.051810ã€‚

- Fiandri etal. [2025]â†‘Marco Fiandri, AlbertoMaria Metelli, and Francesco TrovÃ².Thompson sampling-like algorithms for stochastic rising bandits, 2025.URLhttps://arxiv.org/abs/2505.12092.
- Fiandri etal. [2025]â†‘ãƒãƒ«ã‚³ãƒ»ãƒ•ã‚£ã‚¢ãƒ³ãƒ‰ãƒªã€ã‚¢ãƒ«ãƒ™ãƒ«ãƒˆãƒ»ãƒãƒªã‚¢ãƒ»ãƒ¡ãƒ†ãƒƒãƒªã€ãƒ•ãƒ©ãƒ³ãƒã‚§ã‚¹ã‚³ãƒ»ãƒˆãƒ­ãƒ´ã‚©ã€‚ç¢ºç‡çš„ä¸Šæ˜‡ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ãŸã‚ã®ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ä¼¼ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€2025å¹´ã€‚URLhttps://arxiv.org/abs/2505.12092ã€‚

- Ganti etal. [2018]â†‘Ravi Ganti, Matyas Sustik, Quoc Tran, and Brian Seaman.Thompson sampling for dynamic pricing.2018.URLhttps://arxiv.org/abs/1802.03050.
- Ganti etal. [2018]â†‘ãƒ©ãƒ“ãƒ»ã‚¬ãƒ³ãƒ†ã‚£ã€ãƒãƒ†ã‚£ã‚¢ã‚¹ãƒ»ã‚µã‚¹ãƒ†ã‚£ã‚¯ã€ã‚¯ã‚©ãƒƒã‚¯ãƒ»ãƒˆãƒ©ãƒ³ã€ãƒ–ãƒ©ã‚¤ã‚¢ãƒ³ãƒ»ã‚·ãƒ¼ãƒãƒ³ã€‚å‹•çš„ä¾¡æ ¼è¨­å®šã®ãŸã‚ã®ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚2018å¹´ã€‚URLhttps://arxiv.org/abs/1802.03050ã€‚

- Garivier and CappÃ© [2011]â†‘AurÃ©lien Garivier and Olivier CappÃ©.The kl-ucb algorithm for bounded stochastic bandits and beyond.InProceedings of the Conference On Learning Theory (COLT), pages 359â€“376, 2011.
- Garivier and CappÃ© [2011]â†‘ã‚ªãƒ¼ãƒ¬ãƒªã‚¢ãƒ³ãƒ»ã‚¬ãƒªãƒ“ã‚¨ã¨ã‚ªãƒªãƒ“ã‚¨ãƒ»ã‚«ãƒƒãƒšã€‚åˆ¶ç´„ä»˜ãç¢ºç‡çš„ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ãŸã‚ã®kl-ucbã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚å­¦ç¿’ç†è«–ã«é–¢ã™ã‚‹ä¼šè­°ã®è­°äº‹éŒ²ï¼ˆCOLTï¼‰ã€ãƒšãƒ¼ã‚¸359â€“376ã€2011å¹´ã€‚

- Garivier and Moulines [2008]â†‘AurÃ©lien Garivier and Eric Moulines.On upper-confidence bound policies for non-stationary bandit problems.2008.URLhttps://arxiv.org/abs/0805.3415.
- Garivier and Moulines [2008]â†‘ã‚ªãƒ¼ãƒ¬ãƒªã‚¢ãƒ³ãƒ»ã‚¬ãƒªãƒ“ã‚¨ã¨ã‚¨ãƒªãƒƒã‚¯ãƒ»ãƒ ãƒ¼ãƒªãƒã‚¹ã€‚éå®šå¸¸ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã«å¯¾ã™ã‚‹ä¸Šé™ä¿¡é ¼åŒºé–“ãƒãƒªã‚·ãƒ¼ã«ã¤ã„ã¦ã€‚2008å¹´ã€‚URLhttps://arxiv.org/abs/0805.3415ã€‚

- Garivier and Moulines [2011]â†‘AurÃ©lien Garivier and Eric Moulines.On upper-confidence bound policies for switching bandit problems.InProceedings of the international conference on Algorithmic Learning Theory (ALT), 2011.
- Garivier and Moulines [2011]â†‘ã‚ªãƒ¼ãƒ¬ãƒªã‚¢ãƒ³ãƒ»ã‚¬ãƒªãƒ“ã‚¨ã¨ã‚¨ãƒªãƒƒã‚¯ãƒ»ãƒ ãƒ¼ãƒªãƒã‚¹ã€‚ã‚¹ã‚¤ãƒƒãƒãƒ³ã‚°ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã«å¯¾ã™ã‚‹ä¸Šé™ä¿¡é ¼åŒºé–“ãƒãƒªã‚·ãƒ¼ã«ã¤ã„ã¦ã€‚ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒŸãƒƒã‚¯å­¦ç¿’ç†è«–ã«é–¢ã™ã‚‹å›½éš›ä¼šè­°ã®è­°äº‹éŒ²ï¼ˆALTï¼‰ã€2011å¹´ã€‚

- Graepel etal. [2010]â†‘Thore Graepel, JoaquinQuinonero Candela, Thomas Borchert, and Ralf Herbrich.Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoftâ€™s bing search engine.Omnipress, 2010.
- Graepel etal. [2010]â†‘ãƒˆãƒ¼ãƒ¬ãƒ»ã‚°ãƒ¬ãƒšãƒ«ã€ãƒ›ã‚¢ã‚­ãƒ³ãƒ»ã‚­ãƒãƒãƒ­ãƒ»ã‚«ãƒ³ãƒ‡ãƒ©ã€ãƒˆãƒ¼ãƒã‚¹ãƒ»ãƒœãƒ«ãƒ’ã‚§ãƒ«ãƒˆã€ãƒ©ãƒ«ãƒ•ãƒ»ãƒ˜ãƒ«ãƒ–ãƒªãƒƒãƒ’ã€‚ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆã®Bingæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã«ãŠã‘ã‚‹ã‚¹ãƒãƒ³ã‚µãƒ¼æ¤œç´¢åºƒå‘Šã®ãŸã‚ã®ã‚¦ã‚§ãƒ–ã‚¹ã‚±ãƒ¼ãƒ«ãƒ™ã‚¤ã‚ºçš„ã‚¯ãƒªãƒƒã‚¯ç‡äºˆæ¸¬ã€‚ã‚ªãƒ ãƒ‹ãƒ—ãƒ¬ã‚¹ã€2010å¹´ã€‚

- Heidari etal. [2016]â†‘Hoda Heidari, MichaelJ Kearns, and Aaron Roth.Tight policy regret bounds for improving and decaying bandits.InProceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pages 1562â€“1570, 2016.
- Heidari etal. [2016]â†‘ãƒ›ãƒ€ãƒ»ãƒ˜ã‚¤ãƒ€ãƒªã€ãƒã‚¤ã‚±ãƒ«ãƒ»Jãƒ»ã‚«ãƒ¼ãƒ³ã€ã‚¢ãƒ¼ãƒ­ãƒ³ãƒ»ãƒ­ã‚¹ã€‚æ”¹å–„ãŠã‚ˆã³æ¸›è¡°ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ãŸã‚ã®å³å¯†ãªãƒãƒªã‚·ãƒ¼å¾Œæ‚”å¢ƒç•Œã€‚å›½éš›äººå·¥çŸ¥èƒ½åˆåŒä¼šè­°ï¼ˆIJCAIï¼‰ã®è­°äº‹éŒ²ã€ãƒšãƒ¼ã‚¸1562â€“1570ã€2016å¹´ã€‚

- Jaiswal etal. [2025]â†‘Prateek Jaiswal, Esmaeil Keyvanshokooh, and Junyu Cao.Deconfounded warm-start thompson sampling with applications to precision medicine, 2025.URLhttps://arxiv.org/abs/2505.17283.
- Jaiswal etal. [2025]â†‘ãƒ—ãƒ©ãƒ†ã‚£ãƒ¼ã‚¯ãƒ»ã‚¸ãƒ£ã‚¤ã‚¹ãƒ¯ãƒ«ã€ã‚¨ã‚¹ãƒã‚¤ãƒ«ãƒ»ã‚±ã‚¤ãƒãƒ³ã‚·ãƒ§ã‚³ãƒ•ã€ã‚¸ãƒ¥ãƒ³ãƒ¦ãƒ»ã‚«ã‚ªã€‚ç²¾å¯†åŒ»ç™‚ã¸ã®å¿œç”¨ã‚’ä¼´ã†ãƒ‡ã‚³ãƒ³ãƒ•ã‚¡ã‚¦ãƒ³ãƒ‰ãƒ»ã‚¦ã‚©ãƒ¼ãƒ ã‚¹ã‚¿ãƒ¼ãƒˆãƒ»ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€2025å¹´ã€‚URLhttps://arxiv.org/abs/2505.17283ã€‚

- Kandasamy etal. [2018]â†‘Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and EricP Xing.Neural architecture search with bayesian optimisation and optimal transport.Advances in neural information processing systems (NeurIPS), 2018.
- Kandasamy etal. [2018]â†‘ã‚­ãƒ«ãƒ†ãƒ´ã‚¡ã‚µãƒ³ãƒ»ã‚«ãƒ³ãƒ€ã‚µãƒŸã€ã‚¦ã‚£ãƒªãƒ¼ãƒ»ãƒã‚¤ã‚¹ãƒ¯ãƒ³ã‚¬ãƒ¼ã€ã‚¸ã‚§ãƒ•ãƒ»ã‚·ãƒ¥ãƒŠã‚¤ãƒ€ãƒ¼ã€ãƒãƒ«ãƒŠãƒã‚¹ãƒ»ãƒã‚³ã‚ºã€ã‚¨ãƒªãƒƒã‚¯ãƒ»Pãƒ»ã‚·ãƒ³ã€‚ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã¨æœ€é©è¼¸é€ã‚’ç”¨ã„ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¢ç´¢ã€‚ç¥çµŒæƒ…å ±å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®é€²å±•ï¼ˆNeurIPSï¼‰ã€2018å¹´ã€‚

- Kaufmann etal. [2012]â†‘Emilie Kaufmann, Nathaniel Korda, and RÃ©mi Munos.Thompson sampling: An asymptotically optimal finite-time analysis.InProceedings of the international conference on Algorithmic Learning Theory (ALT), 2012.
- Kaufmann etal. [2012]â†‘ã‚¨ãƒŸãƒªãƒ¼ãƒ»ã‚«ã‚¦ãƒ•ãƒãƒ³ã€ãƒŠã‚µãƒ‹ã‚¨ãƒ«ãƒ»ã‚³ãƒ«ãƒ€ã€ãƒ¬ãƒŸãƒ»ãƒ ãƒã‚¹ã€‚ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼šæ¼¸è¿‘çš„ã«æœ€é©ãªæœ‰é™æ™‚é–“åˆ†æã€‚ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒŸãƒƒã‚¯å­¦ç¿’ç†è«–ã«é–¢ã™ã‚‹å›½éš›ä¼šè­°ã®è­°äº‹éŒ²ï¼ˆALTï¼‰ã€2012å¹´ã€‚

- Kawale etal. [2015]â†‘Jaya Kawale, HungH Bui, Branislav Kveton, Long Tran-Thanh, and Sanjay Chawla.Efficient thompson sampling for online matrix-factorization recommendation.Advances in neural information processing systems (NeurIPS), 2015.
- Kawale etal. [2015]â†‘ã‚¸ãƒ£ãƒ¤ãƒ»ã‚«ãƒ¯ãƒ¬ã€ãƒ•ãƒ³Hãƒ»ãƒ–ã‚¤ã€ãƒ–ãƒ©ãƒ‹ã‚¹ãƒ©ãƒ•ãƒ»ã‚¯ãƒ´ã‚§ãƒˆãƒ³ã€ãƒ­ãƒ³ã‚°ãƒ»ãƒˆãƒ©ãƒ³ãƒ»ã‚¿ãƒ³ã€ã‚µãƒ³ã‚¸ã‚§ã‚¤ãƒ»ãƒãƒ£ã‚¦ãƒ©ã€‚ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è¡Œåˆ—å› å­åˆ†è§£æ¨è–¦ã®ãŸã‚ã®åŠ¹ç‡çš„ãªãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚ç¥çµŒæƒ…å ±å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®é€²å±•ï¼ˆNeurIPSï¼‰ã€2015å¹´ã€‚

- Lai and Robbins [1985]â†‘TzeLeung Lai and Herbert Robbins.Asymptotically efficient adaptive allocation rules.Advances in applied mathematics, 6(1):4â€“22, 1985.
- Lai and Robbins [1985]â†‘ãƒ©ã‚¤ãƒ»ãƒ„ã‚§ãƒ«ãƒ³ã¨ãƒãƒ¼ãƒãƒ¼ãƒˆãƒ»ãƒ­ãƒ“ãƒ³ã‚ºã€‚æ¼¸è¿‘çš„ã«åŠ¹ç‡çš„ãªé©å¿œçš„é…åˆ†ãƒ«ãƒ¼ãƒ«ã€‚å¿œç”¨æ•°å­¦ã®é€²å±•ã€6(1):4â€“22ã€1985å¹´ã€‚

- Lattimore and SzepesvÃ¡ri [2020]â†‘Tor Lattimore and Csaba SzepesvÃ¡ri.Bandit algorithms.Cambridge University Press, 2020.
- Lattimore and SzepesvÃ¡ri [2020]â†‘ãƒˆãƒ¼ãƒ«ãƒ»ãƒ©ãƒƒãƒ†ã‚£ãƒ¢ã‚¢ã¨ãƒãƒ£ãƒãƒ»ã‚»ãƒšã‚·ãƒ¥ãƒ´ã‚¡ãƒªã€‚ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚ã‚±ãƒ³ãƒ–ãƒªãƒƒã‚¸å¤§å­¦å‡ºç‰ˆã€2020å¹´ã€‚

- Liu etal. [2018]â†‘Fang Liu, Joohyun Lee, and Ness Shroff.A change-detection based framework for piecewise-stationary multi-armed bandit problem.InProceedings of the Conference on Artificial Intelligence (AAAI), volume32, 2018.
- Liu etal. [2018]â†‘ãƒ•ã‚¡ãƒ³ãƒ»ãƒªã‚¦ã€ã‚¸ãƒ§ãƒ¼ãƒ’ãƒ§ãƒ³ãƒ»ãƒªãƒ¼ã€ãƒã‚¹ãƒ»ã‚·ãƒ¥ãƒ­ãƒ•ã€‚åŒºåˆ†çš„å®šå¸¸ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã®ãŸã‚ã®å¤‰åŒ–æ¤œå‡ºã«åŸºã¥ããƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‚äººå·¥çŸ¥èƒ½ã«é–¢ã™ã‚‹ä¼šè­°ã®è­°äº‹éŒ²ï¼ˆAAAIï¼‰ã€ç¬¬32å·»ã€2018å¹´ã€‚

- Liu etal. [2023]â†‘Yueyang Liu, Benjamin VanRoy, and Kuang Xu.Nonstationary bandit learning via predictive sampling.International Conference on Artificial Intelligence and Statistics (AISTATS), 2023.
- Liu etal. [2023]â†‘ãƒ¦ã‚¨ãƒ¤ãƒ³ãƒ»ãƒªã‚¦ã€ãƒ™ãƒ³ã‚¸ãƒ£ãƒŸãƒ³ãƒ»ãƒãƒ³ãƒ­ã‚¤ã€ã‚¯ã‚¡ãƒ³ãƒ»ã‚·ãƒ¥ãƒ¼ã€‚äºˆæ¸¬ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹éå®šå¸¸ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå­¦ç¿’ã€‚äººå·¥çŸ¥èƒ½ã¨çµ±è¨ˆã«é–¢ã™ã‚‹å›½éš›ä¼šè­°ï¼ˆAISTATSï¼‰ã€2023å¹´ã€‚

- Lu etal. [2021]â†‘Yangyi Lu, Ziping Xu, and Ambuj Tewari.Bandit algorithms for precision medicine.2021.URLhttps://arxiv.org/abs/2108.04782.
- Lu etal. [2021]â†‘ãƒ¤ãƒ³ã‚¤ãƒ¼ãƒ»ãƒ«ã€ã‚¸ãƒ¼ãƒ”ãƒ³ãƒ»ã‚·ãƒ¥ãƒ¼ã€ã‚¢ãƒ³ãƒ–ã‚¸ãƒ¥ãƒ»ãƒ†ãƒ¯ãƒªã€‚ç²¾å¯†åŒ»ç™‚ã®ãŸã‚ã®ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚2021å¹´ã€‚URLhttps://arxiv.org/abs/2108.04782ã€‚

- Metelli etal. [2022]â†‘AlbertoMaria Metelli, Francesco TrovÃ², Matteo Pirola, and Marcello Restelli.Stochastic rising bandits.InProceedings of the International Conference on Machine Learning (ICML), volume 162, pages 15421â€“15457, 2022.
- Metelli etal. [2022]â†‘ã‚¢ãƒ«ãƒ™ãƒ«ãƒˆãƒ»ãƒãƒªã‚¢ãƒ»ãƒ¡ãƒ†ãƒƒãƒªã€ãƒ•ãƒ©ãƒ³ãƒã‚§ã‚¹ã‚³ãƒ»ãƒˆãƒ­ãƒ´ã‚©ã€ãƒãƒƒãƒ†ã‚ªãƒ»ãƒ”ãƒ­ãƒ©ã€ãƒãƒ«ãƒã‚§ãƒ­ãƒ»ãƒ¬ã‚¹ãƒ†ãƒƒãƒªã€‚ç¢ºç‡çš„ä¸Šæ˜‡ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã€‚å›½éš›æ©Ÿæ¢°å­¦ç¿’ä¼šè­°ã®è­°äº‹éŒ²ï¼ˆICMLï¼‰ã€ç¬¬162å·»ã€ãƒšãƒ¼ã‚¸15421â€“15457ã€2022å¹´ã€‚

- Pandey and Olston [2006]â†‘Sandeep Pandey and Christopher Olston.Handling advertisements of unknown quality in search advertising.InAdvances in Neural Information Processing Systems (NeurIPS), 2006.
- Pandey and Olston [2006]â†‘ã‚µãƒ³ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ»ãƒ‘ãƒ³ãƒ‡ã‚¤ã¨ã‚¯ãƒªã‚¹ãƒˆãƒ•ã‚¡ãƒ¼ãƒ»ã‚ªãƒ«ã‚¹ãƒˆãƒ³ã€‚æ¤œç´¢åºƒå‘Šã«ãŠã‘ã‚‹æœªçŸ¥ã®å“è³ªã®åºƒå‘Šã®å–ã‚Šæ‰±ã„ã€‚ç¥çµŒæƒ…å ±å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®é€²å±•ï¼ˆNeurIPSï¼‰ã€2006å¹´ã€‚

- Qi etal. [2025]â†‘Han Qi, Fei Guo, and LiZhu.Thompson sampling for non-stationary bandit problems.Entropy, 27(1):51, 2025.
- Qi etal. [2025]â†‘ãƒãƒ³ãƒ»ãƒãƒ¼ã€ãƒ•ã‚§ã‚¤ãƒ»ã‚°ã‚ªã€ãƒªãƒ»ã‚ºãƒ¼ã€‚éå®šå¸¸ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã®ãŸã‚ã®ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€27(1):51ã€2025å¹´ã€‚

- Raj and Kalyani [2017]â†‘Vishnu Raj and Sheetal Kalyani.Taming non-stationary bandits: A bayesian approach.2017.URLhttps://arxiv.org/abs/1707.09727.
- Raj and Kalyani [2017]â†‘ãƒ´ã‚£ã‚·ãƒ¥ãƒŒãƒ»ãƒ©ãƒ¼ã‚¸ã¨ã‚·ãƒ¼ã‚¿ãƒ«ãƒ»ã‚«ãƒªãƒ¤ãƒ‹ã€‚éå®šå¸¸ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®åˆ¶å¾¡ï¼šãƒ™ã‚¤ã‚ºçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€‚2017å¹´ã€‚URLhttps://arxiv.org/abs/1707.09727ã€‚

- Re etal. [2021]â†‘Gerlando Re, Fabio Chiusano, Francesco TrovÃ², Diego Carrera, Giacomo Boracchi, and Marcello Restelli.Exploiting history data for nonstationary multi-armed bandit.InProceedings of the European Conference on Machine Learning (ECML), pages 51â€“66, 2021.
- Re etal. [2021]â†‘ã‚¸ã‚§ãƒ«ãƒ©ãƒ³ãƒ‰ãƒ»ãƒ¬ã€ãƒ•ã‚¡ãƒ“ã‚ªãƒ»ã‚­ã‚¦ã‚¶ãƒ¼ãƒã€ãƒ•ãƒ©ãƒ³ãƒã‚§ã‚¹ã‚³ãƒ»ãƒˆãƒ­ãƒ´ã‚©ã€ãƒ‡ã‚£ã‚¨ã‚´ãƒ»ã‚«ãƒ¬ãƒ©ã€ã‚¸ãƒ£ã‚³ãƒ¢ãƒ»ãƒœãƒ©ãƒƒã‚­ã€ãƒãƒ«ãƒã‚§ãƒ­ãƒ»ãƒ¬ã‚¹ãƒ†ãƒƒãƒªã€‚éå®šå¸¸ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ãŸã‚ã®å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨ã€‚æ¬§å·æ©Ÿæ¢°å­¦ç¿’ä¼šè­°ã®è­°äº‹éŒ²ï¼ˆECMLï¼‰ã€ãƒšãƒ¼ã‚¸51â€“66ã€2021å¹´ã€‚

- Rigollet and HÃ¼tter [2023]â†‘Philippe Rigollet and Jan-Christian HÃ¼tter.High-dimensional statistics.2023.URLhttps://arxiv.org/abs/2310.19244.
- Rigollet and HÃ¼tter [2023]â†‘ãƒ•ã‚£ãƒªãƒƒãƒ—ãƒ»ãƒªã‚´ãƒ¬ã¨ãƒ¤ãƒ³ãƒ»ã‚¯ãƒªã‚¹ãƒãƒ£ãƒ³ãƒ»ãƒ’ãƒ¥ãƒƒã‚¿ãƒ¼ã€‚é«˜æ¬¡å…ƒçµ±è¨ˆã€‚2023å¹´ã€‚URLhttps://arxiv.org/abs/2310.19244ã€‚

- Roch [2024]â†‘Sebastien Roch.Modern discrete probability: An essential toolkit.Cambridge University Press, 2024.
- Roch [2024]â†‘ã‚»ãƒã‚¹ãƒãƒ£ãƒ³ãƒ»ãƒ­ãƒƒã‚·ãƒ¥ã€‚ç¾ä»£ã®é›¢æ•£ç¢ºç‡ï¼šå¿…é ˆãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆã€‚ã‚±ãƒ³ãƒ–ãƒªãƒƒã‚¸å¤§å­¦å‡ºç‰ˆã€2024å¹´ã€‚

- Russo and VanRoy [2016]â†‘Daniel Russo and Benjamin VanRoy.An information-theoretic analysis of thompson sampling.Journal of Machine Learning Research (JMLR), 17(68):1â€“30, 2016.
- Russo and VanRoy [2016]â†‘ãƒ€ãƒ‹ã‚¨ãƒ«ãƒ»ãƒ«ãƒƒã‚½ã¨ãƒ™ãƒ³ã‚¸ãƒ£ãƒŸãƒ³ãƒ»ãƒãƒ³ãƒ­ã‚¤ã€‚ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æƒ…å ±ç†è«–çš„åˆ†æã€‚æ©Ÿæ¢°å­¦ç¿’ç ”ç©¶ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ï¼ˆJMLRï¼‰ã€17(68):1â€“30ã€2016å¹´ã€‚

- Scott [2010]â†‘Steven Scott.A modern bayesian look at the multi-armed bandit.Applied Stochastic Models in Business and Industry, 26:639 â€“ 658, 11 2010.
- Scott [2010]â†‘ã‚¹ãƒ†ã‚£ãƒ¼ãƒ–ãƒ³ãƒ»ã‚¹ã‚³ãƒƒãƒˆã€‚ç¾ä»£ã®ãƒ™ã‚¤ã‚ºçš„è¦–ç‚¹ã‹ã‚‰è¦‹ãŸãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã€‚ãƒ“ã‚¸ãƒã‚¹ã¨ç”£æ¥­ã«ãŠã‘ã‚‹å¿œç”¨ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ã€26:639 â€“ 658ã€2010å¹´11æœˆã€‚

- Seznec etal. [2019]â†‘Julien Seznec, Andrea Locatelli, Alexandra Carpentier, Alessandro Lazaric, and Michal Valko.Rotting bandits are no harder than stochastic ones.InThe International Conference on Artificial Intelligence and Statistics (AISTATS), volume89, pages 2564â€“2572, 2019.
- Seznec etal. [2019]â†‘ã‚¸ãƒ¥ãƒªã‚¢ãƒ³ãƒ»ã‚»ã‚ºãƒãƒƒã‚¯ã€ã‚¢ãƒ³ãƒ‰ãƒ¬ã‚¢ãƒ»ãƒ­ã‚«ãƒ†ãƒƒãƒªã€ã‚¢ãƒ¬ã‚¯ã‚µãƒ³ãƒ‰ãƒ©ãƒ»ã‚«ãƒ«ãƒ‘ãƒ³ãƒ†ã‚£ã‚¨ã€ã‚¢ãƒ¬ãƒƒã‚µãƒ³ãƒ‰ãƒ­ãƒ»ãƒ©ã‚¶ãƒªãƒƒã‚¯ã€ãƒŸãƒãƒ«ãƒ»ãƒ´ã‚¡ãƒ«ã‚³ã€‚è…æ•—ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã¯ç¢ºç‡çš„ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚ˆã‚Šã‚‚é›£ã—ããªã„ã€‚äººå·¥çŸ¥èƒ½ã¨çµ±è¨ˆã«é–¢ã™ã‚‹å›½éš›ä¼šè­°ï¼ˆAISTATSï¼‰ã€ç¬¬89å·»ã€ãƒšãƒ¼ã‚¸2564â€“2572ã€2019å¹´ã€‚

- Seznec etal. [2020]â†‘Julien Seznec, Pierre Menard, Alessandro Lazaric, and Michal Valko.A single algorithm for both restless and rested rotting bandits.InProceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), volume 108, pages 3784â€“3794, 2020.
- Seznec etal. [2020]â†‘ã‚¸ãƒ¥ãƒªã‚¢ãƒ³ãƒ»ã‚»ã‚ºãƒãƒƒã‚¯ã€ãƒ”ã‚¨ãƒ¼ãƒ«ãƒ»ãƒ¡ãƒŠãƒ«ã€ã‚¢ãƒ¬ãƒƒã‚µãƒ³ãƒ‰ãƒ­ãƒ»ãƒ©ã‚¶ãƒªãƒƒã‚¯ã€ãƒŸãƒãƒ«ãƒ»ãƒ´ã‚¡ãƒ«ã‚³ã€‚è½ã¡ç€ã‹ãªã„ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã¨è½ã¡ç€ã„ãŸãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ä¸¡æ–¹ã«å¯¾ã™ã‚‹å˜ä¸€ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚äººå·¥çŸ¥èƒ½ã¨çµ±è¨ˆã«é–¢ã™ã‚‹å›½éš›ä¼šè­°ï¼ˆAISTATSï¼‰ã®è­°äº‹éŒ²ã€ç¬¬108å·»ã€ãƒšãƒ¼ã‚¸3784â€“3794ã€2020å¹´ã€‚

- Thompson [1933]â†‘WilliamR. Thompson.On the likelihood that one unknown probability exceeds another in view of the evidence of two samples.Biometrika, 25(3/4):285â€“294, 1933.
- Thompson [1933]â†‘ã‚¦ã‚£ãƒªã‚¢ãƒ Rãƒ»ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã€‚2ã¤ã®ã‚µãƒ³ãƒ—ãƒ«ã®è¨¼æ‹ ã«åŸºã¥ã„ã¦ã€1ã¤ã®æœªçŸ¥ã®ç¢ºç‡ãŒåˆ¥ã®ç¢ºç‡ã‚’è¶…ãˆã‚‹å¯èƒ½æ€§ã«ã¤ã„ã¦ã€‚ãƒã‚¤ã‚ªãƒ¡ãƒˆãƒªã‚«ã€25(3/4):285â€“294ã€1933å¹´ã€‚

- TrovÃ² etal. [2020]â†‘Francesco TrovÃ², Stefano Paladino, Marcello Restelli, and Nicola Gatti.Sliding-window thompson sampling for non-stationary settings.Journal of Artificial Intelligence Research (JAIR), 68:311â€“364, 2020.
- TrovÃ² etal. [2020]â†‘ãƒ•ãƒ©ãƒ³ãƒã‚§ã‚¹ã‚³ãƒ»ãƒˆãƒ­ãƒ´ã‚©ã€ã‚¹ãƒ†ãƒ•ã‚¡ãƒãƒ»ãƒ‘ãƒ©ãƒ‡ã‚£ãƒ¼ãƒã€ãƒãƒ«ãƒã‚§ãƒ­ãƒ»ãƒ¬ã‚¹ãƒ†ãƒƒãƒªã€ãƒ‹ã‚³ãƒ©ãƒ»ã‚¬ãƒƒãƒ†ã‚£ã€‚éå®šå¸¸è¨­å®šã®ãŸã‚ã®ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚äººå·¥çŸ¥èƒ½ç ”ç©¶ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ï¼ˆJAIRï¼‰ã€68:311â€“364ã€2020å¹´ã€‚

- Wang [1993]â†‘Y.H. Wang.On the number of successes in independent trials.Statistica Sinica, 3(2):295â€“312, 1993.
- Wang [1993]â†‘Y.H.ãƒ¯ãƒ³ã€‚ç‹¬ç«‹è©¦è¡Œã«ãŠã‘ã‚‹æˆåŠŸã®æ•°ã«ã¤ã„ã¦ã€‚çµ±è¨ˆå­¦ã‚·ãƒ‹ã‚«ã€3(2):295â€“312ã€1993å¹´ã€‚

- Whittle [1988]â†‘P.Whittle.Restless bandits: Activity allocation in a changing world.Journal of Applied Probability, 25:287â€“298, 1988.ISSN 00219002.
- Whittle [1988]â†‘Pãƒ»ã‚¦ã‚£ãƒƒãƒˆãƒ«ã€‚è½ã¡ç€ã‹ãªã„ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆï¼šå¤‰åŒ–ã™ã‚‹ä¸–ç•Œã«ãŠã‘ã‚‹æ´»å‹•é…åˆ†ã€‚å¿œç”¨ç¢ºç‡ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ã€25:287â€“298ã€1988å¹´ã€‚ISSN 00219002ã€‚



## Code Availability ã‚³ãƒ¼ãƒ‰ã®å…¥æ‰‹å¯èƒ½æ€§

All the codes are publicly available at the following link: https://github.com/albertometelli/stochastic-rising-bandits.
ã™ã¹ã¦ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ä»¥ä¸‹ã®ãƒªãƒ³ã‚¯ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™: https://github.com/albertometelli/stochastic-rising-bandits.



## Appendix A ä»˜éŒ² A

We now present two Lemmas that will be useful throughout the analysis.  
ã“ã“ã§ã¯ã€åˆ†æã«å½¹ç«‹ã¤2ã¤ã®è£œé¡Œã‚’ç¤ºã—ã¾ã™ã€‚

### Definition A.1 å®šç¾© A.1

Let $i,i^{\prime} \in \llbracket K \rrbracket$ be two arms, $t \in \llbracket T \rrbracket$ be a round, $\tau \in \llbracket T \rrbracket$ be the window, and $y_{i^{\prime},t} \in (0,1)$ be a threshold, we define:  
$i,i^{\prime} \in \llbracket K \rrbracket$ ã‚’2ã¤ã®ã‚¢ãƒ¼ãƒ ã¨ã—ã€$t \in \llbracket T \rrbracket$ ã‚’ãƒ©ã‚¦ãƒ³ãƒ‰ã€$\tau \in \llbracket T \rrbracket$ ã‚’ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã€$y_{i^{\prime},t} \in (0,1)$ ã‚’é–¾å€¤ã¨ã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã—ã¾ã™ï¼š

$$
\mathcal{F}_{t} 
$$  
ã“ã“ã§ã€$\mathcal{F}_{t}$ ã¯ã€ãƒ©ã‚¦ãƒ³ãƒ‰ $t$ ã¾ã§ã«ãƒ—ãƒ¬ã‚¤ã•ã‚ŒãŸã‚¢ãƒ¼ãƒ ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨è¦³æ¸¬ã•ã‚ŒãŸå ±é…¬ã«ã‚ˆã£ã¦èª˜å°ã•ã‚Œã‚‹ãƒ•ã‚£ãƒ«ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ã€‚

### Definition A.2 å®šç¾© A.2

For each $i \in \llbracket K \rrbracket$, we define the set of rounds $t \in \mathcal{F}_{\tau}^{\complement}$ and $i \neq i^{*}(t)$ as $\mathcal{F}_{i,\tau}^{\complement}$. Formally:  
å„ $i \in \llbracket K \rrbracket$ ã«å¯¾ã—ã¦ã€$t \in \mathcal{F}_{\tau}^{\complement}$ ã‹ã¤ $i \neq i^{*}(t)$ ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã®é›†åˆã‚’ $\mathcal{F}_{i,\tau}^{\complement}$ ã¨å®šç¾©ã—ã¾ã™ã€‚å½¢å¼çš„ã«ã¯ï¼š

$$
\mathcal{F}_{i,\tau}^{\complement} 
$$  

We propose a slight modification of Lemma 5.1 from [20] and Lemma C.1 from [20], to obtain results that are more suitable to describe the regret in restless setting.  
ç§ãŸã¡ã¯ã€[20] ã®è£œé¡Œ 5.1 ã¨ [20] ã®è£œé¡Œ C.1 ã®ã‚ãšã‹ãªä¿®æ­£ã‚’ææ¡ˆã—ã€è½ã¡ç€ãã®ãªã„è¨­å®šã«ãŠã‘ã‚‹å¾Œæ‚”ã‚’èª¬æ˜ã™ã‚‹ã®ã«ã‚ˆã‚Šé©ã—ãŸçµæœã‚’å¾—ã¾ã™ã€‚

### Lemma 1 è£œé¡Œ 1

**Beta-SWTS**  
Let $T \in \mathbb{N}$ be the learning horizon, $\tau \in \llbracket T \rrbracket$ the window size, for the Beta-SWTS algorithm it holds for every free parameter $\omega \in \llbracket 0,T \rrbracket$ that:  
$T \in \mathbb{N}$ ã‚’å­¦ç¿’ã®ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã¨ã—ã€$\tau \in \llbracket T \rrbracket$ ã‚’ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã¨ã™ã‚‹ã¨ã€Beta-SWTS ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«å¯¾ã—ã¦ã€ä»»æ„ã®è‡ªç”±ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\omega \in \llbracket 0,T \rrbracket$ ã«å¯¾ã—ã¦æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

The proof will follow the same steps of the proof in [20] with some changes to adapt to the restless setting.  
è¨¼æ˜ã¯ã€è½ã¡ç€ãã®ãªã„è¨­å®šã«é©å¿œã™ã‚‹ãŸã‚ã®ã„ãã¤ã‹ã®å¤‰æ›´ã‚’åŠ ãˆã¤ã¤ã€[20] ã®è¨¼æ˜ã®åŒã˜ã‚¹ãƒ†ãƒƒãƒ—ã«å¾“ã„ã¾ã™ã€‚

We define the event $E_{i}(t) \coloneqq \{\theta_{i,t,\tau} \leq y_{i,t}\}$. Thus, assigning immediate regret equal to one for every round in $\mathcal{F}_{\tau}$ the following holds:  
ã‚¤ãƒ™ãƒ³ãƒˆ $E_{i}(t) \coloneqq \{\theta_{i,t,\tau} \leq y_{i,t}\}$ ã‚’å®šç¾©ã—ã¾ã™ã€‚ã—ãŸãŒã£ã¦ã€$\mathcal{F}_{\tau}$ ã®ã™ã¹ã¦ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã«å¯¾ã—ã¦å³æ™‚ã®å¾Œæ‚”ã‚’1ã¨ã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

$$
\text{(22)}
$$  

Let us first face term (A):  
ã¾ãšã€é … (A) ã«å–ã‚Šçµ„ã¿ã¾ã—ã‚‡ã†ï¼š

$$
\text{(A)}
$$  

Observe that (C) can be bounded by Lemma 8. Thus, the above inequality can be rewritten as:  
(C) ã¯è£œé¡Œ 8 ã«ã‚ˆã£ã¦åˆ¶ç´„ã•ã‚Œã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã—ãŸãŒã£ã¦ã€ä¸Šè¨˜ã®ä¸ç­‰å¼ã¯æ¬¡ã®ã‚ˆã†ã«æ›¸ãæ›ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

$$
\text{(A)}
$$  

We now focus on the term (D). Defining $\mathcal{T} := \{t \in \mathcal{F}_{i,\tau}^{\complement} : 1 - \mathbb{P}(\theta_{i,t,\tau} \leq y_{i,t} \mid \mathcal{F}_{t-1}) > \frac{1}{\tau}, N_{i,t,\tau} \geq \omega\}$ assign $\mathcal{T}$ conditional-set.  
æ¬¡ã«ã€é … (D) ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ã€‚$\mathcal{T} := \{t \in \mathcal{F}_{i,\tau}^{\complement} : 1 - \mathbb{P}(\theta_{i,t,\tau} \leq y_{i,t} \mid \mathcal{F}_{t-1}) > \frac{1}{\tau}, N_{i,t,\tau} \geq \omega\}$ ã‚’å®šç¾©ã—ã€$\mathcal{T}$ æ¡ä»¶ä»˜ãé›†åˆã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚

$$
\mathcal{T} := \{t \in \mathcal{F}_{i,\tau}^{\complement} : 1 - \mathbb{P}(\theta_{i,t,\tau} \leq y_{i,t} \mid \mathcal{F}_{t-1}) > \frac{1}{\tau}, N_{i,t,\tau} \geq \omega\}
$$  

and $\mathcal{T}^{\prime} := \{t \in \mathcal{F}_{i,\tau}^{\complement} : 1 - \mathbb{P}(\theta_{i,t,\tau} \leq y_{i,t} \mid \mathcal{F}_{t-1}) \leq \frac{1}{\tau}, N_{i,t,\tau} \geq \omega\}$ assign $\mathcal{T}^{\prime}$ conditional-set.  
ã¾ãŸã€$\mathcal{T}^{\prime} := \{t \in \mathcal{F}_{i,\tau}^{\complement} : 1 - \mathbb{P}(\theta_{i,t,\tau} \leq y_{i,t} \mid \mathcal{F}_{t-1}) \leq \frac{1}{\tau}, N_{i,t,\tau} \geq \omega\}$ ã‚’å®šç¾©ã—ã€$\mathcal{T}^{\prime}$ æ¡ä»¶ä»˜ãé›†åˆã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚

$$
\mathcal{T}^{\prime} := \{t \in \mathcal{F}_{i,\tau}^{\complement} : 1 - \mathbb{P}(\theta_{i,t,\tau} \leq y_{i,t} \mid \mathcal{F}_{t-1}) \leq \frac{1}{\tau}, N_{i,t,\tau} \geq \omega\}
$$  

Now we focus on term (B). We have:  
ä»Šã€é … (B) ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ã€‚æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

$$
\text{(32)}
$$  

In order to bound (B) we need to bound (E).  
(B) ã‚’åˆ¶ç´„ã™ã‚‹ãŸã‚ã«ã¯ã€(E) ã‚’åˆ¶ç´„ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

Let $i^{\prime} = \operatorname{argmax}_{i \neq i^{*}(t)} \theta_{i,t,\tau}$. Then, we have:  
$i^{\prime} = \operatorname{argmax}_{i \neq i^{*}(t)} \theta_{i,t,\tau}$ ã¨ã—ã¾ã™ã€‚ã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

$$
\text{(33)}
$$  

The statement follows by summing all the terms.  
ã™ã¹ã¦ã®é …ã‚’åˆè¨ˆã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€å‘½é¡ŒãŒæˆã‚Šç«‹ã¡ã¾ã™ã€‚

âˆ

### Lemma 2 è£œé¡Œ 2

**-SWGTS**  
Let $T \in \mathbb{N}$ be the learning horizon, $\tau \in \llbracket T \rrbracket$ be the window size, for the $\gamma$-ET-SWGTS algorithm the following holds for every $i \neq i^{*}(t)$ and free parameters $\omega \in \llbracket T \rrbracket$ and $\epsilon > 0$:  
$T \in \mathbb{N}$ ã‚’å­¦ç¿’ã®ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã¨ã—ã€$\tau \in \llbracket T \rrbracket$ ã‚’ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã¨ã™ã‚‹ã¨ã€$\gamma$-ET-SWGTS ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«å¯¾ã—ã¦ã€ä»»æ„ã® $i \neq i^{*}(t)$ ã¨è‡ªç”±ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\omega \in \llbracket T \rrbracket$ ãŠã‚ˆã³ $\epsilon > 0$ ã«å¯¾ã—ã¦æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

We define the event $E_{i}(t) \coloneqq \{\theta_{i,t,\tau} \leq y_{i,t}\}$. Thus, the following holds, assigning "error" equal to one for every round in $\mathcal{F}_{\tau}$:  
ã‚¤ãƒ™ãƒ³ãƒˆ $E_{i}(t) \coloneqq \{\theta_{i,t,\tau} \leq y_{i,t}\}$ ã‚’å®šç¾©ã—ã¾ã™ã€‚ã—ãŸãŒã£ã¦ã€$\mathcal{F}_{\tau}$ ã®ã™ã¹ã¦ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã«å¯¾ã—ã¦ "ã‚¨ãƒ©ãƒ¼" ã‚’1ã¨ã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

$$
\text{(36)}
$$  

Let us first face term (A):  
ã¾ãšã€é … (A) ã«å–ã‚Šçµ„ã¿ã¾ã—ã‚‡ã†ï¼š

$$
\text{(A)}
$$  

Observe that (C) can be bounded by Lemma 8. Thus, the above inequality can be rewritten as:  
(C) ã¯è£œé¡Œ 8 ã«ã‚ˆã£ã¦åˆ¶ç´„ã•ã‚Œã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã—ãŸãŒã£ã¦ã€ä¸Šè¨˜ã®ä¸ç­‰å¼ã¯æ¬¡ã®ã‚ˆã†ã«æ›¸ãæ›ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

$$
\text{(A)}
$$  

We now focus on the term (D). Defining $\mathcal{T} := \{t \in \mathcal{F}_{i,\tau}^{\complement} : 1 - \mathbb{P}(\theta_{i,t,\tau} \leq y_{i,t} \mid \mathcal{F}_{t-1}) > \frac{1}{\tau \epsilon_{i}}, N_{i,t,\tau} \geq \omega\}$ assign $\mathcal{T}$ conditional-set.  
æ¬¡ã«ã€é … (D) ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ã€‚$\mathcal{T} := \{t \in \mathcal{F}_{i,\tau}^{\complement} : 1 - \mathbb{P}(\theta_{i,t,\tau} \leq y_{i,t} \mid \mathcal{F}_{t-1}) > \frac{1}{\tau \epsilon_{i}}, N_{i,t,\tau} \geq \omega\}$ ã‚’å®šç¾©ã—ã€$\mathcal{T}$ æ¡ä»¶ä»˜ãé›†åˆã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚

$$
\mathcal{T} := \{t \in \mathcal{F}_{i,\tau}^{\complement} : 1 - \mathbb{P}(\theta_{i,t,\tau} \leq y_{i,t} \mid \mathcal{F}_{t-1}) > \frac{1}{\tau \epsilon_{i}}, N_{i,t,\tau} \geq \omega\}
$$  

and $\mathcal{T}^{\prime} := \{t \in \mathcal{F}_{i,\tau}^{\complement} : 1 - \mathbb{P}(\theta_{i,t,\tau} \leq y_{i,t} \mid \mathcal{F}_{t-1}) \leq \frac{1}{\tau \epsilon_{i}}, N_{i,t,\tau} \geq \omega\}$ assign $\mathcal{T}^{\prime}$ conditional-set.  
ã¾ãŸã€$\mathcal{T}^{\prime} := \{t \in \mathcal{F}_{i,\tau}^{\complement} : 1 - \mathbb{P}(\theta_{i,t,\tau} \leq y_{i,t} \mid \mathcal{F}_{t-1}) \leq \frac{1}{\tau \epsilon_{i}}, N_{i,t,\tau} \geq \omega\}$ ã‚’å®šç¾©ã—ã€$\mathcal{T}^{\prime}$ æ¡ä»¶ä»˜ãé›†åˆã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚

$$
\mathcal{T}^{\prime} := \{t \in \mathcal{F}_{i,\tau}^{\complement} : 1 - \mathbb{P}(\theta_{i,t,\tau} \leq y_{i,t} \mid \mathcal{F}_{t-1}) \leq \frac{1}{\tau \epsilon_{i}}, N_{i,t,\tau} \geq \omega\}
$$  

Term (B) is bounded exactly as in the proof of Lemma 1. The statement follows by summing all the terms.  
é … (B) ã¯è£œé¡Œ 1 ã®è¨¼æ˜ã¨åŒæ§˜ã«åˆ¶ç´„ã•ã‚Œã¾ã™ã€‚ã™ã¹ã¦ã®é …ã‚’åˆè¨ˆã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€å‘½é¡ŒãŒæˆã‚Šç«‹ã¡ã¾ã™ã€‚

âˆ



## Appendix BProofs ä»˜éŒ²B è¨¼æ˜

Appendix B  
ä»˜éŒ²B  

See V.1  
V.1  

First of all, let us recall Lemma 1:  
ã¾ãšæœ€åˆã«ã€è£œé¡Œ1ã‚’æ€ã„å‡ºã—ã¾ã—ã‚‡ã†ï¼š  

Let us define the two threshold quantities $x_{i,t}$ and $y_{i,t}$ for $t \in \mathcal{F}_{i,\tau}^{\complement}$ (the time the policy-maker has to choose the arm) as:  
$ t \in \mathcal{F}_{i,\tau}^{\complement} $ ã®ãŸã‚ã®2ã¤ã®é–¾å€¤é‡ $x_{i,t}$ ã¨ $y_{i,t}$ ã‚’å®šç¾©ã—ã¾ã™ï¼ˆæ”¿ç­–æ±ºå®šè€…ãŒã‚¢ãƒ¼ãƒ ã‚’é¸æŠã™ã‚‹ãŸã‚ã®æ™‚é–“ï¼‰ï¼š

$$
\Delta_{i,t,\tau} = \min_{t^{\prime} \in \llbracket t - \tau, t - 1 \rrbracket} \{ \mu_{i^{*}(t), t^{\prime}} \} - \max_{t^{\prime} \in \llbracket t - 1, t - \tau \rrbracket} \{ \mu_{i(t), t^{\prime}} \}
$$  
$$
\Delta_{i,t,\tau} = \min_{t^{\prime} \in \llbracket t - \tau, t - 1 \rrbracket} \{ \mu_{i^{*}(t), t^{\prime}} \} - \max_{t^{\prime} \in \llbracket t - 1, t - \tau \rrbracket} \{ \mu_{i(t), t^{\prime}} \}
$$  

we will always consider in the following analysis the choices:  
ä»¥ä¸‹ã®åˆ†æã§ã¯ã€å¸¸ã«æ¬¡ã®é¸æŠè‚¢ã‚’è€ƒæ…®ã—ã¾ã™ï¼š  

Notice then that the following quantities will have their minima for those $t \in \mathcal{F}_{\tau}^{\complement}$ such that $\Delta_{i,t,\tau} = \Delta_{\tau}$:  
æ¬¡ã«ã€ä»¥ä¸‹ã®é‡ã¯ã€$\Delta_{i,t,\tau} = \Delta_{\tau}$ ã¨ãªã‚‹ã‚ˆã†ãª $t \in \mathcal{F}_{\tau}^{\complement}$ ã«å¯¾ã—ã¦æœ€å°å€¤ã‚’æŒã¤ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ï¼š  

$$
\Delta_{i,t,\tau} = \Delta_{\tau}
$$  
$$
\Delta_{i,t,\tau} = \Delta_{\tau}
$$  

and independently from the time $t \in \llbracket T \rrbracket$ in which happens, they will always have the same value.  
ãã—ã¦ã€ã“ã‚ŒãŒèµ·ã“ã‚‹æ™‚é–“ $t \in \llbracket T \rrbracket$ ã«é–¢ä¿‚ãªãã€å¸¸ã«åŒã˜å€¤ã‚’æŒã¡ã¾ã™ã€‚  

We refer to the minimum values the quantities above can get in $t \in \mathcal{F}_{i,\tau}^{\complement}$ as:  
ä¸Šè¨˜ã®é‡ãŒ $t \in \mathcal{F}_{i,\tau}^{\complement}$ ã§å¾—ã‚‰ã‚Œã‚‹æœ€å°å€¤ã‚’æ¬¡ã®ã‚ˆã†ã«å‘¼ã³ã¾ã™ï¼š  

$$
\text{(48)}
$$  
$$
\text{(48)}
$$  

We choose $\omega = \frac{\ln(\tau)}{2(x_{i} - y_{i})^{2}}$ and define $\hat{\mu}_{i,t,\tau} = \frac{S_{i,t,\tau}}{N_{i,t,\tau}}$.  
$\omega = \frac{\ln(\tau)}{2(x_{i} - y_{i})^{2}}$ ã‚’é¸ã³ã€$\hat{\mu}_{i,t,\tau} = \frac{S_{i,t,\tau}}{N_{i,t,\tau}}$ ã‚’å®šç¾©ã—ã¾ã™ã€‚  

We will consider $\tau \geq e$.  
$\tau \geq e$ ã‚’è€ƒæ…®ã—ã¾ã™ã€‚  

We first tackle Term $(\mathcal{S}.1)$.  
ã¾ãšã€é … $(\mathcal{S}.1)$ ã«å–ã‚Šçµ„ã¿ã¾ã™ã€‚  



#### Term (ğ’®.1ğ’®.1\mathcal{S}.1caligraphic_S .1) 

We have: 
ç§ãŸã¡ã¯æ¬¡ã®ã“ã¨ã‚’æŒã£ã¦ã„ã¾ã™ï¼š

(49) 
(49)

(50) 
(50)

(51) 
(51)

First, we face term(ğ’®.1.2)ğ’®.1.2(\mathcal{S}.1.2)( caligraphic_S .1.2 ), for each summand in the sum holds the following: 
ã¾ãšã€term(ğ’®.1.2)ğ’®.1.2(\mathcal{S}.1.2)( caligraphic_S .1.2 )ã«ç›´é¢ã—ã¾ã™ã€‚å’Œã®å„é …ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

(52) 
(52)

(53) 
(53)

(54) 
(54)

(55) 
(55)

(56) 
(56)

where the inequality from Equation (54) to Equation (55) follow from the Chernoff-Hoeffding inequality. 
ã“ã“ã§ã€å¼(54)ã‹ã‚‰å¼(55)ã¸ã®ä¸ç­‰å¼ã¯ã€ãƒã‚§ãƒ«ãƒãƒ•ãƒ»ãƒ›ãƒ•ãƒ‡ã‚£ãƒ³ã‚°ä¸ç­‰å¼ã«å¾“ã„ã¾ã™ã€‚

Summing over all the roundtğ‘¡titalic_t, we obtain(ğ’®.1.2)â‰¤TÏ„ğ’®.1.2ğ‘‡ğœ(\mathcal{S}.1.2)\leq\frac{T}{\tau}( caligraphic_S .1.2 ) â‰¤ divide start_ARG italic_T end_ARG start_ARG italic_Ï„ end_ARG 
ã™ã¹ã¦ã®roundtğ‘¡titalic_tã‚’åˆè¨ˆã™ã‚‹ã¨ã€(ğ’®.1.2)â‰¤TÏ„ğ’®.1.2ğ‘‡ğœ(\mathcal{S}.1.2)\leq\frac{T}{\tau}( caligraphic_S .1.2 ) â‰¤ divide start_ARG italic_T end_ARG start_ARG italic_Ï„ end_ARGãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚

We now focus on term(ğ’®.1.1)ğ’®.1.1(\mathcal{S}.1.1)( caligraphic_S .1.1 ). 
æ¬¡ã«ã€term(ğ’®.1.1)ğ’®.1.1(\mathcal{S}.1.1)( caligraphic_S .1.1 )ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ã€‚

We want to assess if it is possible for condition(âˆ—)(*)( âˆ— )to happen, in order to do so evaluate the following: 
æ¡ä»¶(âˆ—)(*)( âˆ— )ãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã‹ã©ã†ã‹ã‚’è©•ä¾¡ã—ãŸã„ã®ã§ã€æ¬¡ã®ã“ã¨ã‚’è©•ä¾¡ã—ã¾ã™ï¼š

54 
54

55 
55

(57) 
(57)

(58) 
(58)

(59) 
(59)

(60) 
(60)

(61) 
(61)

(62) 
(62)

(63) 
(63)

where for the last inequality, we exploited the Pinsker inequality. 
æœ€å¾Œã®ä¸ç­‰å¼ã§ã¯ã€ãƒ”ãƒ³ã‚¹ã‚«ãƒ¼ä¸ç­‰å¼ã‚’åˆ©ç”¨ã—ã¾ã—ãŸã€‚

Equation(59) was derived by exploiting the fact that on the eventxi,tâ‰¥Î¼^i,t,Ï„subscriptğ‘¥ğ‘–ğ‘¡subscript^ğœ‡ğ‘–ğ‘¡ğœx_{i,t}\geq\hat{\mu}_{i,t,\tau}italic_x start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT â‰¥ over^ start_ARG italic_Î¼ end_ARG start_POSTSUBSCRIPT italic_i , italic_t , italic_Ï„ end_POSTSUBSCRIPT 
å¼(59)ã¯ã€äº‹è±¡$x_{i,t}\geq\hat{\mu}_{i,t,\tau}$ã®äº‹å®Ÿã‚’åˆ©ç”¨ã—ã¦å°å‡ºã•ã‚Œã¾ã—ãŸã€‚

a sample fromBeta(xi,tNi,t,Ï„+1,(1âˆ’xi,t)Ni,t,Ï„+1)Betasubscriptğ‘¥ğ‘–ğ‘¡subscriptğ‘ğ‘–ğ‘¡ğœ11subscriptğ‘¥ğ‘–ğ‘¡subscriptğ‘ğ‘–ğ‘¡ğœ1\text{Beta}\left(x_{i,t}N_{i,t,\tau}+1,(1-x_{i,t})N_{i,t,\tau}+1\right) 
$\text{Beta}\left(x_{i,t}N_{i,t,\tau}+1,(1-x_{i,t})N_{i,t,\tau}+1\right)$ã®ã‚µãƒ³ãƒ—ãƒ«ã¯ã€

is likely to be as large as a sample fromBeta(Î¼^i,t,Ï„Ni,t,Ï„+1,(1âˆ’Î¼^i,t,Ï„)Ni,t,Ï„+1)Betasubscript^ğœ‡ğ‘–ğ‘¡ğœsubscriptğ‘ğ‘–ğ‘¡ğœ11subscript^ğœ‡ğ‘–ğ‘¡ğœsubscriptğ‘ğ‘–ğ‘¡ğœ1\text{Beta}(\hat{\mu}_{i,t,\tau}N_{i,t,\tau}+1,(1-\hat{\mu}_{i,t,\tau})N_{i,t,\tau}+1) 
$\text{Beta}(\hat{\mu}_{i,t,\tau}N_{i,t,\tau}+1,(1-\hat{\mu}_{i,t,\tau})N_{i,t,\tau}+1)$ã®ã‚µãƒ³ãƒ—ãƒ«ã¨åŒã˜ãã‚‰ã„å¤§ãããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

reported formally in Lemma11. 
ã“ã‚Œã¯ã€è£œé¡Œ11ã§æ­£å¼ã«å ±å‘Šã•ã‚Œã¦ã„ã¾ã™ã€‚

Equation (60) follows from Fact4, while Equation61from Lemma10 
å¼(60)ã¯Fact4ã‹ã‚‰å°ã‹ã‚Œã€å¼(61)ã¯è£œé¡Œ10ã‹ã‚‰å°ã‹ã‚Œã¾ã™ã€‚

Therefore, forÏ‰=logÏ„2(yiâˆ’xi)2ğœ”ğœ2superscriptsubscriptğ‘¦ğ‘–subscriptğ‘¥ğ‘–2\omega=\frac{\log{\tau}}{2(y_{i}-x_{i})^{2}}italic_Ï‰ = divide start_ARG roman_log italic_Ï„ end_ARG start_ARG 2 ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG 
ã—ãŸãŒã£ã¦ã€$\omega=\frac{\log{\tau}}{2(y_{i}-x_{i})^{2}}$ã¨ãªã‚Šã¾ã™ã€‚

we have: 
ç§ãŸã¡ã¯æ¬¡ã®ã“ã¨ã‚’å¾—ã¾ã™ï¼š

59 
59

11 
11

60 
60

4 
4

61 
61

10 
10

(64) 
(64)

Then, it follows that condition(âˆ—)(*)( âˆ— )is never met, and each summand in(ğ’®.1.1)ğ’®.1.1(\mathcal{S}.1.1)( caligraphic_S .1.1 )is equal to zero, so(ğ’®.1.1)=0ğ’®.1.10(\mathcal{S}.1.1)=0( caligraphic_S .1.1 ) = 0. 
ã—ãŸãŒã£ã¦ã€æ¡ä»¶(âˆ—)(*)( âˆ— )ã¯æ±ºã—ã¦æº€ãŸã•ã‚Œãšã€(ğ’®.1.1)ğ’®.1.1(\mathcal{S}.1.1)( caligraphic_S .1.1 )ã®å„é …ã¯ã‚¼ãƒ­ã«ç­‰ã—ããªã‚‹ãŸã‚ã€(ğ’®.1.1)=0ğ’®.1.10(\mathcal{S}.1.1)=0( caligraphic_S .1.1 ) = 0ã¨ãªã‚Šã¾ã™ã€‚



#### Term (ğ’®.2ğ’®.2\mathcal{S}.2caligraphic_S .2) ç”¨èª (ğ’®.2ğ’®.2\mathcal{S}.2caligraphic_S .2)

We can rewrite the term (ğ’®.2ğ’®.2\mathcal{S}.2caligraphic_S .2) as follows: 
ç”¨èª (ğ’®.2ğ’®.2\mathcal{S}.2caligraphic_S .2) ã‚’æ¬¡ã®ã‚ˆã†ã«æ›¸ãæ›ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

(65) 
(65)

(66) 
(66)

Exploiting the fact that $\mathbb{E}[XY]=\mathbb{E}[X\mathbb{E}[Y\mid X]]$, we can rewrite both $(\mathcal{S}.2.1)$ and $(\mathcal{S}.2.2)$ as: 
$\mathbb{E}[XY]=\mathbb{E}[X\mathbb{E}[Y\mid X]]$ ã¨ã„ã†äº‹å®Ÿã‚’åˆ©ç”¨ã—ã¦ã€$(\mathcal{S}.2.1)$ ã¨ $(\mathcal{S}.2.2)$ ã®ä¸¡æ–¹ã‚’æ¬¡ã®ã‚ˆã†ã«æ›¸ãæ›ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

(67) 
(67)

(68) 
(68)

Let us first tackle term $(\mathcal{S}.2.1)$: 
ã¾ãšã€ç”¨èª $(\mathcal{S}.2.1)$ ã«å–ã‚Šçµ„ã¿ã¾ã—ã‚‡ã†ï¼š

(69) 
(69)

Taking inspiration from peeling-like arguments, let us decompose the event $\mathcal{C}_1$ in $\lceil\log(\tau)\rceil$ sub-events $\mathcal{C}_{1,j}$ for $j\geq 1$ defined as follow: 
å‰¥ãŒã—ã®ã‚ˆã†ãªè­°è«–ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¾—ã¦ã€ã‚¤ãƒ™ãƒ³ãƒˆ $\mathcal{C}_1$ ã‚’ $\lceil\log(\tau)\rceil$ ã®éƒ¨åˆ†ã‚¤ãƒ™ãƒ³ãƒˆ $\mathcal{C}_{1,j}$ ã«åˆ†è§£ã—ã¾ã—ã‚‡ã†ã€‚ã“ã“ã§ $j\geq 1$ ã¨å®šç¾©ã—ã¾ã™ï¼š

(70) 
(70)

with the convention: 
æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã—ã¾ã™ï¼š

(71) 
(71)

notice that $\lceil\log(\tau)\rceil$ of such sub-events are enough as by definition $N_{i,t,\tau}\leq\tau$ holds. This yields to: 
ã“ã®ã‚ˆã†ãªéƒ¨åˆ†ã‚¤ãƒ™ãƒ³ãƒˆã® $\lceil\log(\tau)\rceil$ ã¯ååˆ†ã§ã‚ã‚Šã€å®šç¾©ã«ã‚ˆã‚Š $N_{i,t,\tau}\leq\tau$ ãŒæˆã‚Šç«‹ã¡ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šæ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

(72) 
(72)

Let $\Delta_{i}^{\prime}\coloneqq\mu_{i^{*},\mathcal{F}_{\tau}^{\complement}}-y_{i}$, we can rewrite term $(\mathcal{S}.2.1)$ as: 
$\Delta_{i}^{\prime}\coloneqq\mu_{i^{*},\mathcal{F}_{\tau}^{\complement}}-y_{i}$ ã¨ã—ã€ç”¨èª $(\mathcal{S}.2.1)$ ã‚’æ¬¡ã®ã‚ˆã†ã«æ›¸ãæ›ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

(73) 
(73)

(74) 
(74)

notice that, for each $j$, the only summands that will contribute to the sum will be those for which condition $\mathcal{C}_{1,j}$ holds true. Thus, for each $j$, the following will hold: 
å„ $j$ ã«å¯¾ã—ã¦ã€åˆè¨ˆã«å¯„ä¸ã™ã‚‹å”¯ä¸€ã®é …ã¯æ¡ä»¶ $\mathcal{C}_{1,j}$ ãŒæˆã‚Šç«‹ã¤ã‚‚ã®ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã—ãŸãŒã£ã¦ã€å„ $j$ ã«å¯¾ã—ã¦ã€æ¬¡ã®ã“ã¨ãŒæˆã‚Šç«‹ã¡ã¾ã™ï¼š

(75) 
(75)

We are now interested in evaluating $(*)$ for each $j$. For this purpose we rewrite it as: 
æ¬¡ã«ã€å„ $j$ ã«å¯¾ã—ã¦ $(*)$ ã‚’è©•ä¾¡ã™ã‚‹ã“ã¨ã«èˆˆå‘³ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®ç›®çš„ã®ãŸã‚ã«ã€æ¬¡ã®ã‚ˆã†ã«æ›¸ãæ›ãˆã¾ã™ï¼š

(76) 
(76)

where the expected value $\mathbb{E}_{N_{j}^{\prime},\ \underline{\mu}_{i^{*}(t)}}[\cdot]$ is taken over all the values of $N_{j-1}<N_{j}^{\prime}\leq N_{j}$ and over all different histories $\underline{\mu}_{i^{*}(t)}$ that yield to $N_{j}^{\prime}$ trials, with $\underline{\mu}_{i^{*}(t)}$ being the set of the $N_{j}^{\prime}$ probabilities of success of every trial of the best arm that make $\mathcal{C}_{1,j}$ true. 
æœŸå¾…å€¤ $\mathbb{E}_{N_{j}^{\prime},\ \underline{\mu}_{i^{*}(t)}}[\cdot]$ ã¯ã€$N_{j-1}<N_{j}^{\prime}\leq N_{j}$ ã®ã™ã¹ã¦ã®å€¤ã¨ã€$\mathcal{C}_{1,j}$ ã‚’çœŸã«ã™ã‚‹æœ€è‰¯ã®ã‚¢ãƒ¼ãƒ ã®ã™ã¹ã¦ã®è©¦è¡Œã®æˆåŠŸç¢ºç‡ã® $N_{j}^{\prime}$ ã®é›†åˆã§ã‚ã‚‹ $\underline{\mu}_{i^{*}(t)}$ ã®ã™ã¹ã¦ã®ç•°ãªã‚‹å±¥æ­´ã«ã‚ãŸã£ã¦å–ã‚‰ã‚Œã¾ã™ã€‚

(77) 
(77)

In order to bound these terms, we remember that $p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$. 
ã“ã‚Œã‚‰ã®é …ã‚’åˆ¶ç´„ã™ã‚‹ãŸã‚ã«ã€$p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$ ã‚’æ€ã„å‡ºã—ã¾ã™ã€‚

(78) 
(78)

In order to bound these terms, we remember that $p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$. 
ã“ã‚Œã‚‰ã®é …ã‚’åˆ¶ç´„ã™ã‚‹ãŸã‚ã«ã€$p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$ ã‚’æ€ã„å‡ºã—ã¾ã™ã€‚

(79) 
(79)

In order to bound these terms, we remember that $p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$. 
ã“ã‚Œã‚‰ã®é …ã‚’åˆ¶ç´„ã™ã‚‹ãŸã‚ã«ã€$p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$ ã‚’æ€ã„å‡ºã—ã¾ã™ã€‚

(80) 
(80)

In order to bound these terms, we remember that $p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$. 
ã“ã‚Œã‚‰ã®é …ã‚’åˆ¶ç´„ã™ã‚‹ãŸã‚ã«ã€$p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$ ã‚’æ€ã„å‡ºã—ã¾ã™ã€‚

(81) 
(81)

In order to bound these terms, we remember that $p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$. 
ã“ã‚Œã‚‰ã®é …ã‚’åˆ¶ç´„ã™ã‚‹ãŸã‚ã«ã€$p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$ ã‚’æ€ã„å‡ºã—ã¾ã™ã€‚

(82) 
(82)

In order to bound these terms, we remember that $p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$. 
ã“ã‚Œã‚‰ã®é …ã‚’åˆ¶ç´„ã™ã‚‹ãŸã‚ã«ã€$p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$ ã‚’æ€ã„å‡ºã—ã¾ã™ã€‚

(83) 
(83)

In order to bound these terms, we remember that $p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$. 
ã“ã‚Œã‚‰ã®é …ã‚’åˆ¶ç´„ã™ã‚‹ãŸã‚ã«ã€$p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$ ã‚’æ€ã„å‡ºã—ã¾ã™ã€‚

(84) 
(84)

In order to bound these terms, we remember that $p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$. 
ã“ã‚Œã‚‰ã®é …ã‚’åˆ¶ç´„ã™ã‚‹ãŸã‚ã«ã€$p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$ ã‚’æ€ã„å‡ºã—ã¾ã™ã€‚

(85) 
(85)

In order to bound these terms, we remember that $p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$. 
ã“ã‚Œã‚‰ã®é …ã‚’åˆ¶ç´„ã™ã‚‹ãŸã‚ã«ã€$p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$ ã‚’æ€ã„å‡ºã—ã¾ã™ã€‚

(86) 
(86)

In order to bound these terms, we remember that $p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$. 
ã“ã‚Œã‚‰ã®é …ã‚’åˆ¶ç´„ã™ã‚‹ãŸã‚ã«ã€$p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$ ã‚’æ€ã„å‡ºã—ã¾ã™ã€‚

(87) 
(87)

In order to bound these terms, we remember that $p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$. 
ã“ã‚Œã‚‰ã®é …ã‚’åˆ¶ç´„ã™ã‚‹ãŸã‚ã«ã€$p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$ ã‚’æ€ã„å‡ºã—ã¾ã™ã€‚

(88) 
(88)

In order to bound these terms, we remember that $p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$. 
ã“ã‚Œã‚‰ã®é …ã‚’åˆ¶ç´„ã™ã‚‹ãŸã‚ã«ã€$p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$ ã‚’æ€ã„å‡ºã—ã¾ã™ã€‚

(89) 
(89)

In order to bound these terms, we remember that $p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$. 
ã“ã‚Œã‚‰ã®é …ã‚’åˆ¶ç´„ã™ã‚‹ãŸã‚ã«ã€$p_{i^{*}(t),t,\tau}^{i}=\mathbb{P}(Beta(S_{i^{*}(t),t,\tau}+1,F_{i^{*}(t),t,\tau}+1)>y_{i,t}|\mathcal{F}_{t-1})=F^{B}_{N_{j}^{\prime}+1,y_{i,t}}(S_{i^{*}(t),t,\tau})$ ã‚’æ€ã„å‡ºã—ã¾ã™ã€‚

(90) 
(90)

The inequality from Equation (88) to Equation (89) follows again from Lemma 8, while the last inequality is derived by the fact that by definition $N_{j}/N_{j-1}=e$. 
å¼ (88) ã‹ã‚‰å¼ (89) ã¸ã®ä¸ç­‰å¼ã¯å†ã³è£œé¡Œ 8 ã‹ã‚‰å°ã‹ã‚Œã€æœ€å¾Œã®ä¸ç­‰å¼ã¯å®šç¾©ã«ã‚ˆã‚Š $N_{j}/N_{j-1}=e$ ã§ã‚ã‚‹ã“ã¨ã‹ã‚‰å°ã‹ã‚Œã¾ã™ã€‚

(91) 
(91)

We tackle now term $(\mathcal{S}.2.2)$, making the same consideration that we have done from Equation (75), we infer that the only terms that will contribute to the summands are those for which condition $\mathcal{C}_{2}$ holds true, formally: 
æ¬¡ã«ã€ç”¨èª $(\mathcal{S}.2.2)$ ã«å–ã‚Šçµ„ã¿ã€å¼ (75) ã§è¡Œã£ãŸã®ã¨åŒã˜è€ƒå¯Ÿã‚’è¡Œã†ã¨ã€åˆè¨ˆã«å¯„ä¸ã™ã‚‹å”¯ä¸€ã®é …ã¯æ¡ä»¶ $\mathcal{C}_{2}$ ãŒæˆã‚Šç«‹ã¤ã‚‚ã®ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚å½¢å¼çš„ã«ã¯ï¼š

(92) 
(92)

Again, by using Lemma 9 we can bound term $(*)$ with the bounds provided in Lemma 4 for the stationary bandit with expected reward for the best arm equal to $\mu_{i^{*}}^{\prime}$, defined as above. 
å†ã³ã€è£œé¡Œ 9 ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€æœ€è‰¯ã®ã‚¢ãƒ¼ãƒ ã®æœŸå¾…å ±é…¬ãŒä¸Šè¨˜ã®ã‚ˆã†ã«å®šç¾©ã•ã‚ŒãŸ $\mu_{i^{*}}^{\prime}$ ã«ç­‰ã—ã„å®šå¸¸ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ãŸã‚ã®è£œé¡Œ 4 ã§æä¾›ã•ã‚ŒãŸåˆ¶ç´„ã‚’ç”¨ã„ã¦ã€é … $(*)$ ã‚’åˆ¶ç´„ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

(93) 
(93)

where, from Equation (94) to Equation (95) we used the Pinskerâ€™s Inequality, namely: $D_{i,t}\geq 2\Delta_{i}^{\prime\prime 2}$. 
ã“ã“ã§ã€å¼ (94) ã‹ã‚‰å¼ (95) ã¸ã®ä¸ç­‰å¼ã¯ãƒ”ãƒ³ã‚¹ã‚«ãƒ¼ã®ä¸ç­‰å¼ã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚ã™ãªã‚ã¡ã€$D_{i,t}\geq 2\Delta_{i}^{\prime\prime 2}$ ã§ã™ã€‚

(94) 
(94)

Then, summing over all rounds we get $(\mathcal{S}.2.2)\leq\frac{T}{\tau}$. 
æ¬¡ã«ã€ã™ã¹ã¦ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã‚’åˆè¨ˆã™ã‚‹ã¨ã€$(\mathcal{S}.2.2)\leq\frac{T}{\tau}$ ã¨ãªã‚Šã¾ã™ã€‚

(95) 
(95)

The result of the statement follows by summing all the terms, remembering that by definition $\Delta_{i}^{\prime}=\frac{\Delta_{\tau}}{3}$. 
ã“ã®çµæœã¯ã€ã™ã¹ã¦ã®é …ã‚’åˆè¨ˆã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦å¾—ã‚‰ã‚Œã€å®šç¾©ã«ã‚ˆã‚Š $\Delta_{i}^{\prime}=\frac{\Delta_{\tau}}{3}$ ã§ã‚ã‚‹ã“ã¨ã‚’æ€ã„å‡ºã—ã¾ã™ã€‚

(96) 
(96)

âˆ 
âˆ

See V.2 
V.2 ã‚’å‚ç…§ã—ã¦ãã ã•ã„

We recall Lemma 2: 
è£œé¡Œ 2 ã‚’æ€ã„å‡ºã—ã¾ã™ï¼š

(97) 
(97)

Let us define $x_{i,t}$ and $y_{i,t}$ for $t\in\mathcal{F}_{i,\tau}^{\complement}$ as: 
$ t\in\mathcal{F}_{i,\tau}^{\complement}$ ã®ã¨ãã€$x_{i,t}$ ã¨ $y_{i,t}$ ã‚’æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã—ã¾ã™ï¼š

(98) 
(98)

with $\Delta_{i,t,\tau}=\min_{t^{\prime}\in\llbracket t-\tau,t-1\rrbracket}\{\mu_{i^{*}(t),t^{\prime}}\}-\max_{t^{\prime}\in\llbracket t-\tau,t-1\rrbracket}\{\mu_{i(t),t^{\prime}}\}$. 
$\Delta_{i,t,\tau}=\min_{t^{\prime}\in\llbracket t-\tau,t-1\rrbracket}\{\mu_{i^{*}(t),t^{\prime}}\}-\max_{t^{\prime}\in\llbracket t-\tau,t-1\rrbracket}\{\mu_{i(t),t^{\prime}}\}$ ã§ã™ã€‚

(99) 
(99)

Notice then that the following quantities will have their minima for those $t\in\mathcal{F}_{i,\tau}^{\complement}$ such that $\Delta_{i,t,\tau}=\Delta_{\tau}$. 
ã—ãŸãŒã£ã¦ã€æ¬¡ã®é‡ã¯ã€$\Delta_{i,t,\tau}=\Delta_{\tau}$ ã¨ãªã‚‹ $t\in\mathcal{F}_{i,\tau}^{\complement}$ ã«å¯¾ã—ã¦æœ€å°å€¤ã‚’æŒã¤ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

(100) 
(100)

and independently from the time $t\in\llbracket T\rrbracket$ in which happens, they will always have the same value. 
ãã—ã¦ã€ç™ºç”Ÿã™ã‚‹æ™‚é–“ $t\in\llbracket T\rrbracket$ ã«ä¾å­˜ã›ãšã€å¸¸ã«åŒã˜å€¤ã‚’æŒã¡ã¾ã™ã€‚

(101) 
(101)

We refer to the minimum values the quantities above can get in $t\in\mathcal{F}_{\tau}^{\complement}$ as: 
ä¸Šè¨˜ã®é‡ãŒ $t\in\mathcal{F}_{\tau}^{\complement}$ ã§å¾—ã‚‰ã‚Œã‚‹æœ€å°å€¤ã‚’æ¬¡ã®ã‚ˆã†ã«å‘¼ã³ã¾ã™ï¼š

(102) 
(102)

We choose $\omega=\frac{288\log(\tau\Delta_{\tau}^{2}+e^{6})}{\gamma\Delta_{\tau}^{2}}$, $\epsilon_{i}=\Delta_{\tau}^{2}$, $\tau\geq e$ and $\hat{\mu}_{i,t,\tau}=\frac{S_{i,t,\tau}}{N_{i,t,\tau}}$. 
$\omega=\frac{288\log(\tau\Delta_{\tau}^{2}+e^{6})}{\gamma\Delta_{\tau}^{2}}$ã€$\epsilon_{i}=\Delta_{\tau}^{2}$ã€$\tau\geq e$ ãŠã‚ˆã³ $\hat{\mu}_{i,t,\tau}=\frac{S_{i,t,\tau}}{N_{i,t,\tau}}$ ã‚’é¸ã³ã¾ã™ã€‚

(103) 
(103)



#### Term (ğ’®.1ğ’®.1\mathcal{S}.1caligraphic_S .1) ç”¨èª (ğ’®.1)

Decomposing the term in two contributions, we obtain:
é …ã‚’äºŒã¤ã®å¯„ä¸ã«åˆ†è§£ã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

(100)  
(100)

(101)  
(101)

(102)  
(102)

We first tackle term(ğ’®.1.2)ğ’®.1.2(\mathcal{S}.1.2)( caligraphic_S .1.2 ), considering each summand we get:
ã¾ãšã€é …(ğ’®.1.2)ğ’®.1.2(\mathcal{S}.1.2)( caligraphic_S .1.2 )ã«å–ã‚Šçµ„ã¿ã€å„å’Œé …ã‚’è€ƒæ…®ã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

(103)  
(103)

(104)  
(104)

(105)  
(105)

(106)  
(106)

(107)  
(107)

Where the inequality from Equation (104) to Equation (105) follows from the Chernoff bounds for subgaussian random variables, reported formally in Lemma7.
å¼(104)ã‹ã‚‰å¼(105)ã¸ã®ä¸ç­‰å¼ã¯ã€ãƒ¬ãƒ7ã§æ­£å¼ã«å ±å‘Šã•ã‚ŒãŸã‚µãƒ–ã‚¬ã‚¦ã‚¹éç¨‹ã®ç¢ºç‡å¤‰æ•°ã«å¯¾ã™ã‚‹ãƒã‚§ãƒ«ãƒãƒ•å¢ƒç•Œã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚

Facing term(ğ’®.2.1)ğ’®.2.1(\mathcal{S}.2.1)( caligraphic_S .2.1 ), we want to evaluate if ever condition(âˆ—)(*)( âˆ— )is met.
é …(ğ’®.2.1)ğ’®.2.1(\mathcal{S}.2.1)( caligraphic_S .2.1 )ã«ç›´é¢ã—ã€æ¡ä»¶(âˆ—)(*)( âˆ— )ãŒæº€ãŸã•ã‚Œã‚‹ã‹ã©ã†ã‹ã‚’è©•ä¾¡ã—ãŸã„ã¨æ€ã„ã¾ã™ã€‚

In order to do so let us consider:
ãã®ãŸã‚ã«ã€æ¬¡ã®ã“ã¨ã‚’è€ƒãˆã¾ã—ã‚‡ã†ï¼š

(108)  
(108)

where the inequality in Equation (108) follows from Lemma11.
å¼(108)ã®ä¸ç­‰å¼ã¯ã€ãƒ¬ãƒ11ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚

Using Lemma6:
ãƒ¬ãƒ6ã‚’ä½¿ç”¨ã™ã‚‹ã¨ï¼š

(109)  
(109)

(110)  
(110)

which is smaller than $ \frac{1}{\tau\Delta_{\tau}^{2}} $ because $ \omega \geq \frac{2\ln\left(\tau\Delta_{\tau}^{2}\right)}{\gamma\left(y_{i}-x_{i}\right)^{2}} $.
ã“ã‚Œã¯ $ \frac{1}{\tau\Delta_{\tau}^{2}} $ ã‚ˆã‚Šå°ã•ãã€$ \omega \geq \frac{2\ln\left(\tau\Delta_{\tau}^{2}\right)}{\gamma\left(y_{i}-x_{i}\right)^{2}} $ ã§ã™ã€‚

Substituting, we get:
ä»£å…¥ã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

(111)  
(111)

So that condition(âˆ—)(*)( âˆ— )is never met and $ \mathcal{S}.1.1=0 $.
ã—ãŸãŒã£ã¦ã€æ¡ä»¶(âˆ—)(*)( âˆ— )ã¯æ±ºã—ã¦æº€ãŸã•ã‚Œãšã€$ \mathcal{S}.1.1=0 $ ã¨ãªã‚Šã¾ã™ã€‚



#### Term (ğ’®.2ğ’®.2\mathcal{S}.2caligraphic_S .2)
We decompose it as:
(112)
é …(ğ’®.2.1)ğ’®.2.1(\mathcal{S}.2.1)( caligraphic_S .2.1 )ã‚’è€ƒãˆã¾ã™ã€‚ç§ãŸã¡ã¯ã€Beta-TSã®è¨¼æ˜ã§è¡Œã£ãŸã‚ˆã†ã«ã€å½¢å¼çš„ã«é …ã‚’æ›¸ãç›´ã—ã¾ã™ã€‚
(112)
Let us face term(ğ’®.2.1)ğ’®.2.1(\mathcal{S}.2.1)( caligraphic_S .2.1 ). We rewrite the term, similarly to what we have done for the Beta-TS proof, formally:
Beta-TS
(113)
Let us evaluate what happens when ğ’1ğ’1\mathcal{C}1caligraphic_C 1 holds true, i.e., those cases in which the summands within the summation in Equation (112) are different from zero.
(113)
ğ’1ğ’1\mathcal{C}1caligraphic_C 1 ãŒçœŸã§ã‚ã‚‹ã¨ãã€ã™ãªã‚ã¡å¼(112)ã®åˆè¨ˆå†…ã®é …ãŒã‚¼ãƒ­ã§ãªã„å ´åˆã«ä½•ãŒèµ·ã“ã‚‹ã‹ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
We will show that whenever condition ğ’1ğ’1\mathcal{C}1caligraphic_C 1 holds true (âˆ—)(*)( âˆ— ) is bounded by a constant.
ç§ãŸã¡ã¯ã€æ¡ä»¶ ğ’1ğ’1\mathcal{C}1caligraphic_C 1 ãŒçœŸã§ã‚ã‚‹ã¨ãã€(âˆ—)(*)( âˆ— ) ãŒå®šæ•°ã§åˆ¶ç´„ã•ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
We will show that for any realization of the number of pulls within a time window Ï„ğœ\tauitalic_Ï„ such that condition ğ’1ğ’1\mathcal{C}1caligraphic_C 1 holds true (i.e. number of pulls jğ‘—jitalic_j of the optimal arm within the time window less than Ï‰ğœ”\omegaitalic_Ï‰) the expected value of G_j G_{j}italic_G start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is bounded by a constant for all jğ‘—jitalic_j defined as earlier.
ç§ãŸã¡ã¯ã€æ¡ä»¶ ğ’1ğ’1\mathcal{C}1caligraphic_C 1 ãŒçœŸã§ã‚ã‚‹ã‚ˆã†ãªæ™‚é–“ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ Ï„ğœ\tauitalic_Ï„ å†…ã®å¼•ãã®æ•°ã®ä»»æ„ã®å®Ÿç¾ã«å¯¾ã—ã¦ã€æœ€é©ãªã‚¢ãƒ¼ãƒ ã®å¼•ãã®æ•° jğ‘—jitalic_j ãŒ Ï‰ğœ”\omegaitalic_Ï‰ ã‚ˆã‚Šå°‘ãªã„å ´åˆã€ã™ã¹ã¦ã® jğ‘—jitalic_j ã«å¯¾ã—ã¦ G_j G_{j}italic_G start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ã®æœŸå¾…å€¤ãŒå®šæ•°ã§åˆ¶ç´„ã•ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
Let Î˜_j \Theta_{j}roman_Î˜ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT denote a ğ’©(Î¼^iâˆ—(t),j,1/Î³_j) ğ’©subscript^ğœ‡superscriptğ‘–ğ‘¡ğ‘—1ğ›¾ğ‘—\mathcal{N}\left({\hat{\mu}}_{i^{*}(t),j},\frac{1}{\gamma j}\right)caligraphic_N ( over^ start_ARG italic_Î¼ end_ARG start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) , italic_j end_POSTSUBSCRIPT , divide start_ARG 1 end_ARG start_ARG italic_Î³ italic_j end_ARG ) distributed Gaussian random variable, where Î¼^iâˆ—(t),j subscript^ğœ‡superscriptğ‘–ğ‘¡ğ‘—{\hat{\mu}}_{i^{*}(t),j} over^ start_ARG italic_Î¼ end_ARG start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) , italic_j end_POSTSUBSCRIPT is the sample mean of the optimal armâ€™s rewards played jğ‘—jitalic_j times within a time window Ï„ğœ\tauitalic_Ï„ at time tâˆˆâ„±i,Ï„âˆğ‘¡ superscriptsubscriptâ„±ğ‘–ğœcomplement tâˆˆ\mathcal{F}_{i,\tau}^{\complement}italic_t âˆˆ caligraphic_F start_POSTSUBSCRIPT italic_i , italic_Ï„ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT.
Î˜_j \Theta_{j}roman_Î˜ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ã‚’ ğ’©(Î¼^iâˆ—(t),j,1/Î³_j) ğ’©subscript^ğœ‡superscriptğ‘–ğ‘¡ğ‘—1ğ›¾ğ‘—\mathcal{N}\left({\hat{\mu}}_{i^{*}(t),j},\frac{1}{\gamma j}\right)caligraphic_N ( over^ start_ARG italic_Î¼ end_ARG start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) , italic_j end_POSTSUBSCRIPT , divide start_ARG 1 end_ARG start_ARG italic_Î³ italic_j end_ARG ) ã§åˆ†å¸ƒã™ã‚‹ã‚¬ã‚¦ã‚¹ãƒ©ãƒ³ãƒ€ãƒ å¤‰æ•°ã¨ã—ã€Î¼^iâˆ—(t),j subscript^ğœ‡superscriptğ‘–ğ‘¡ğ‘—{\hat{\mu}}_{i^{*}(t),j} over^ start_ARG italic_Î¼ end_ARG start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) , italic_j end_POSTSUBSCRIPT ã‚’æ™‚é–“ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ Ï„ğœ\tauitalic_Ï„ å†…ã§ jğ‘—jitalic_j å›ãƒ—ãƒ¬ã‚¤ã•ã‚ŒãŸæœ€é©ã‚¢ãƒ¼ãƒ ã®å ±é…¬ã®ã‚µãƒ³ãƒ—ãƒ«å¹³å‡ã¨ã—ã¾ã™ã€‚
Let G_j G_{j}italic_G start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT be the geometric random variable denoting the number of consecutive independent trials until and including the trial where a sample of Î˜_j \Theta_{j}roman_Î˜ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT becomes greater than y_{i,t} y_{i,t}italic_y start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT.
G_j G_{j}italic_G start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ã‚’ã€Î˜_j \Theta_{j}roman_Î˜ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ã®ã‚µãƒ³ãƒ—ãƒ«ãŒ y_{i,t} y_{i,t}italic_y start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT ã‚ˆã‚Šå¤§ãããªã‚‹ã¾ã§ã®é€£ç¶šã—ãŸç‹¬ç«‹è©¦è¡Œã®å›æ•°ã‚’ç¤ºã™å¹¾ä½•å­¦çš„ãƒ©ãƒ³ãƒ€ãƒ å¤‰æ•°ã¨ã—ã¾ã™ã€‚
Consider now an arbitrary realization where the best arm has been played jğ‘—jitalic_j times and with sample expected rewards ğ”¼[Î¼^iâˆ—(t),j] ğ”¼delimited-[]subscript^ğœ‡superscriptğ‘–ğ‘¡ğ‘—\mathbb{E}[{\hat{\mu}}_{i^{*}(t),j}] blackboard_E [ over^ start_ARG italic_Î¼ end_ARG start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) , italic_j end_POSTSUBSCRIPT ], respecting condition ğ’1ğ’1\mathcal{C}1caligraphic_C 1 then observe that p_{i^{*}(t),t,\tau} = \operatorname{Pr}\left(\Theta_{j}>y_{i,t}\mid\mathbb{F}_{\tau_{j}}\right) italic_p start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) , italic_t , italic_Ï„ end_POSTSUBSCRIPT = roman_Pr ( roman_Î˜ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT > italic_y start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT âˆ£ blackboard_F start_POSTSUBSCRIPT italic_Ï„ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) and:
Consider now an arbitrary realization where the best arm has been played jğ‘—jitalic_j times and with sample expected rewards ğ”¼[Î¼^iâˆ—(t),j] ğ”¼delimited-[]subscript^ğœ‡superscriptğ‘–ğ‘¡ğ‘—\mathbb{E}[{\hat{\mu}}_{i^{*}(t),j}] blackboard_E [ over^ start_ARG italic_Î¼ end_ARG start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) , italic_j end_POSTSUBSCRIPT ], respecting condition ğ’1ğ’1\mathcal{C}1caligraphic_C 1 then observe that p_{i^{*}(t),t,\tau} = \operatorname{Pr}\left(\Theta_{j}>y_{i,t}\mid\mathbb{F}_{\tau_{j}}\right) italic_p start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUBSCRIPT ( italic_t ) , italic_t , italic_Ï„ end_POSTSUBSCRIPT = roman_Pr ( roman_Î˜ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT > italic_y start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT âˆ£ blackboard_F start_POSTSUBSCRIPT italic_Ï„ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) and:
112
(114)
where by ğ”¼_{j\mid\mathcal{C}1}[\cdot] blackboard_E start_POSTSUBSCRIPT italic_j âˆ£ caligraphic_C 1 end_POSTSUBSCRIPT [ â‹… ] we denote the expected value taken over every jğ‘—jitalic_j (and every possible ğ”¼[Î¼^iâˆ—(t),j] ğ”¼delimited-[]subscript^ğœ‡superscriptğ‘–ğ‘¡ğ‘—\mathbb{E}[{\hat{\mu}}_{i^{*}(t),j}] blackboard_E [ over^ start_ARG italic_Î¼ end_ARG start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) , italic_j end_POSTSUBSCRIPT ] compatible with jğ‘—jitalic_j pulls) respecting condition ğ’1ğ’1\mathcal{C}1caligraphic_C 1.
ã“ã“ã§ã€ğ”¼_{j\mid\mathcal{C}1}[\cdot] blackboard_E start_POSTSUBSCRIPT italic_j âˆ£ caligraphic_C 1 end_POSTSUBSCRIPT [ â‹… ] ã¯ã€ã™ã¹ã¦ã® jğ‘—jitalic_j (ãŠã‚ˆã³ jğ‘—jitalic_j å¼•ãã«å¯¾å¿œã™ã‚‹ã™ã¹ã¦ã®å¯èƒ½ãª ğ”¼[Î¼^iâˆ—(t),j] ğ”¼delimited-[]subscript^ğœ‡superscriptğ‘–ğ‘¡ğ‘—\mathbb{E}[{\hat{\mu}}_{i^{*}(t),j}] blackboard_E [ over^ start_ARG italic_Î¼ end_ARG start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) , italic_j end_POSTSUBSCRIPT ] ã‚’å«ã‚€) ã«å¯¾ã—ã¦ã€æ¡ä»¶ ğ’1ğ’1\mathcal{C}1caligraphic_C 1 ã‚’å°Šé‡ã—ã¦æœŸå¾…å€¤ã‚’ç¤ºã—ã¾ã™ã€‚
Consider any integer râ‰¥1 ğ‘Ÿ1r\geq 1 italic_r â‰¥ 1. Let z=lnr ğ‘§ğ‘Ÿ z=\sqrt{\ln r} italic_z = square-root start_ARG roman_ln italic_r end_ARG and let random variable MAX_r denote the maximum of r ğ‘Ÿritalic_r independent samples of Î˜_j \Theta_{j}roman_Î˜ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT.
ä»»æ„ã®æ•´æ•° râ‰¥1 ğ‘Ÿ1r\geq 1 italic_r â‰¥ 1 ã‚’è€ƒãˆã¾ã™ã€‚z=lnr ğ‘§ğ‘Ÿ z=\sqrt{\ln r} italic_z = square-root start_ARG roman_ln italic_r end_ARG ã¨ã—ã€ãƒ©ãƒ³ãƒ€ãƒ å¤‰æ•° MAX_r ã‚’ Î˜_j \Theta_{j}roman_Î˜ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ã® r ğ‘Ÿritalic_r ç‹¬ç«‹ã‚µãƒ³ãƒ—ãƒ«ã®æœ€å¤§å€¤ã¨ã—ã¾ã™ã€‚
We abbreviate Î¼^iâˆ—(t),j subscript^ğœ‡superscriptğ‘–ğ‘¡ğ‘—{\hat{\mu}}_{i^{*}(t),j} over^ start_ARG italic_Î¼ end_ARG start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) , italic_j end_POSTSUBSCRIPT to Î¼^iâˆ— subscript^ğœ‡superscriptğ‘–{\hat{\mu}}_{i^{*}} over^ start_ARG italic_Î¼ end_ARG start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT end_POSTSUBSCRIPT and we will abbreviate min_{t^{\prime}\in\llbracket t-\tau,t-1\rrbracket}\{\mu_{i^{*}(t),t^{\prime}}\} roman_min start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âˆˆ âŸ¦ italic_t - italic_Ï„ , italic_t - 1 âŸ§ end_POSTSUBSCRIPT { italic_Î¼ start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) , italic_t start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_POSTSUBSCRIPT } as Î¼_{i^{*}} italic_Î¼ start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUBSCRIPT in the following.
ç§ãŸã¡ã¯ Î¼^iâˆ—(t),j subscript^ğœ‡superscriptğ‘–ğ‘¡ğ‘—{\hat{\mu}}_{i^{*}(t),j} over^ start_ARG italic_Î¼ end_ARG start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) , italic_j end_POSTSUBSCRIPT ã‚’ Î¼^iâˆ— subscript^ğœ‡superscriptğ‘–{\hat{\mu}}_{i^{*}} over^ start_ARG italic_Î¼ end_ARG start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ã¨ç•¥ã—ã€æ¬¡ã« min_{t^{\prime}\in\llbracket t-\tau,t-1\rrbracket}\{\mu_{i^{*}(t),t^{\prime}}\} roman_min start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âˆˆ âŸ¦ italic_t - italic_Ï„ , italic_t - 1 âŸ§ end_POSTSUBSCRIPT { italic_Î¼ start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) , italic_t start_POSTSUPERSCRIPT â€² end_POSTSUBSCRIPT end_POSTSUBSCRIPT } ã‚’ Î¼_{i^{*}} italic_Î¼ start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUBSCRIPT ã¨ç•¥ã—ã¾ã™ã€‚
Then for any integer râ‰¥1 ğ‘Ÿ1r\geq 1 italic_r â‰¥ 1:
(138)
(139)
(140)
(141)
where we used that y_{i,t} = Î¼_{i^{*}} - \frac{\Delta_{i,t,\tau}}{3} italic_y start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT = italic_Î¼ start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUBSCRIPT - divide start_ARG roman_Î” start_POSTSUBSCRIPT italic_i , italic_t , italic_Ï„ end_POSTSUBSCRIPT end_ARG start_ARG 3 end_ARG.
ãã®å¾Œã€ä»»æ„ã®æ•´æ•° râ‰¥1 ğ‘Ÿ1r\geq 1 italic_r â‰¥ 1 ã«å¯¾ã—ã¦:
(138)
(139)
(140)
(141)
ã“ã“ã§ã€y_{i,t} = Î¼_{i^{*}} - \frac{\Delta_{i,t,\tau}}{3} italic_y start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT = italic_Î¼ start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUBSCRIPT - divide start_ARG roman_Î” start_POSTSUBSCRIPT italic_i , italic_t , italic_Ï„ end_POSTSUBSCRIPT end_ARG start_ARG 3 end_ARG ã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚
Now, since jâ‰¥Ï‰=288\ln(Ï„Î”Ï„^2+e^6) \gamma Î”Ï„^2 â‰¥ 288\ln(Ï„Î”_{i,t,\tau}^2+e^6) \gamma(Î”_{i,t,\tau})^2 ğ‘—ğœ”288ğœsuperscriptsubscriptÎ”ğœ^2superscriptğ‘’^6ğ›¾superscriptsubscriptÎ”ğœ^2 j\geq\omega=\frac{288\ln\left(\tau\Delta_{\tau}^{2}+e^{6}\right)}{\gamma\Delta_{\tau}^{2}}\geq\frac{288\ln\left(\tau\Delta_{i,t,\tau}^{2}+e^{6}\right)}{\gamma(\Delta_{i,t,\tau})^{2}} italic_j â‰¥ italic_Ï‰ = divide start_ARG 288 roman_ln ( italic_Ï„ roman_Î” start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_e start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT ) end_ARG start_ARG italic_Î³ roman_Î” start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG â‰¥ divide start_ARG 288 roman_ln ( italic_Ï„ roman_Î” start_POSTSUBSCRIPT italic_i , italic_t , italic_Ï„ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_e start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT ) end_ARG start_ARG italic_Î³ ( roman_Î” start_POSTSUBSCRIPT italic_i , italic_t , italic_Ï„ end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG for tâˆˆâ„±Ï„ ğ‘¡subscriptâ„±ğœ tâˆˆ\mathcal{F}_{\tau} italic_t âˆˆ caligraphic_F start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT, as Î”_{i,t,\tau} â‰¥ Î”_{\tau} roman_Î” start_POSTSUBSCRIPT italic_i , italic_t , italic_Ï„ end_POSTSUBSCRIPT â‰¥ roman_Î” start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT, we have that:
Now, since jâ‰¥Ï‰=288\ln(Ï„Î”Ï„^2+e^6) \gamma Î”Ï„^2 â‰¥ 288\ln(Ï„Î”_{i,t,\tau}^2+e^6) \gamma(Î”_{i,t,\tau})^2 ğ‘—ğœ”288ğœsuperscriptsubscriptÎ”ğœ^2superscriptğ‘’^6ğ›¾superscriptsubscriptÎ”ğœ^2 j\geq\omega=\frac{288\ln\left(\tau\Delta_{\tau}^{2}+e^{6}\right)}{\gamma\Delta_{\tau}^{2}}\geq\frac{288\ln\left(\tau\Delta_{i,t,\tau}^{2}+e^{6}\right)}{\gamma(\Delta_{i,t,\tau})^{2}} italic_j â‰¥ italic_Ï‰ = divide start_ARG 288 roman_ln ( italic_Ï„ roman_Î” start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_e start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT ) end_ARG start_ARG italic_Î³ roman_Î” start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG â‰¥ divide start_ARG 288 roman_ln ( italic_Ï„ roman_Î” start_POSTSUBSCRIPT italic_i , italic_t , italic_Ï„ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_e start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT ) end_ARG start_ARG italic_Î³ ( roman_Î” start_POSTSUBSCRIPT italic_i , italic_t , italic_Ï„ end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG for tâˆˆâ„±Ï„ ğ‘¡subscriptâ„±ğœ tâˆˆ\mathcal{F}_{\tau} italic_t âˆˆ caligraphic_F start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT, as Î”_{i,t,\tau} â‰¥ Î”_{\tau} roman_Î” start_POSTSUBSCRIPT italic_i , italic_t , italic_Ï„ end_POSTSUBSCRIPT â‰¥ roman_Î” start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT, we have that:
(142)
Therefore, for râ‰¤(Ï„Î”_{i,t,\tau}^2+e^6)^2 ğ‘ŸsuperscriptsubscriptÎ”ğ‘–ğ‘¡ğœ2superscriptğ‘’^62r\leq\left(\tau\Delta_{i,t,\tau}^{2}+e^{6}\right)^{2} italic_r â‰¤ ( italic_Ï„ roman_Î” start_POSTSUBSCRIPT italic_i , italic_t , italic_Ï„ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_e start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT:
(143)
Then, since Î˜_j \Theta_{j}roman_Î˜ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is ğ’©(Î¼^iâˆ—,j,1/Î³_j) ğ’©subscript^ğœ‡superscriptğ‘–ğ‘—1ğ›¾ğ‘—\mathcal{N}\left({\hat{\mu}}_{i^{*},j},\frac{1}{\gamma j}\right) caligraphic_N ( over^ start_ARG italic_Î¼ end_ARG start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT , italic_j end_POSTSUBSCRIPT , divide start_ARG 1 end_ARG start_ARG italic_Î³ italic_j end_ARG ) distributed random variable, using the upper bound in Lemma 6, we obtain for any instantiation F_{Ï„_j} of history ğ”½_{Ï„_j} \mathbb{F}_{\tau_{j}} blackboard_F start_POSTSUBSCRIPT italic_Ï„ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT,
(144)
Then, since Î˜_j \Theta_{j}roman_Î˜ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is ğ’©(Î¼^iâˆ—,j,1/Î³_j) ğ’©subscript^ğœ‡superscriptğ‘–ğ‘—1ğ›¾ğ‘—\mathcal{N}\left({\hat{\mu}}_{i^{*},j},\frac{1}{\gamma j}\right) caligraphic_N ( over^ start_ARG italic_Î¼ end_ARG start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT , italic_j end_POSTSUBSCRIPT , divide start_ARG 1 end_ARG start_ARG italic_Î³ italic_j end_ARG ) distributed random variable, using the upper bound in Lemma 6, we obtain for any instantiation F_{Ï„_j} of history ğ”½_{Ï„_j} \mathbb{F}_{\tau_{j}} blackboard_F start_POSTSUBSCRIPT italic_Ï„ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT,
(144)
being jâ‰¥Ï‰ ğ‘—ğœ” j\geq\omega italic_j â‰¥ italic_Ï‰. This implies:
(145)
ã¾ãŸã€ä»»æ„ã® t ğ‘¡ t italic_t ã«ã¤ã„ã¦æ¡ä»¶ ğ’2ğ’2\mathcal{C}2caligraphic_C 2 ãŒçœŸã§ã‚ã‚‹å ´åˆã€jâ‰¥Ï‰ ğ‘—ğœ” j\geq\omega italic_j â‰¥ italic_Ï‰ ã¨ãªã‚Šã€7 ã‚’ä½¿ç”¨ã™ã‚‹ã¨æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
(146)
(147)
where the last inequality of Equation (146) follows from the fact that:
(148)
(149)
(150)
where the last inequality follows as by definition, we will always have that Î¼_{i^{*}} - \mathbb{E}[{\hat{\mu}}_{i^{*}}] \leq 0 italic_Î¼ start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUBSCRIPT - blackboard_E [ over^ start_ARG italic_Î¼ end_ARG start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ] â‰¤ 0.
(151)
Let T'=(Ï„Î”_{i,t,\tau}^2+e^6)^2 ğ‘‡^{\prime}=\left(\tau\Delta_{i,t,\tau}^{2}+e^{6}\right)^{2} italic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = ( italic_Ï„ roman_Î” start_POSTSUBSCRIPT italic_i , italic_t , italic_Ï„ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_e start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT.
(151)
ã—ãŸãŒã£ã¦ã€1â‰¤râ‰¤T' 1 ğ‘Ÿsuperscriptsubscriptğ‘‡â€² 1\leq r\leq T^{\prime} 1 â‰¤ italic_r â‰¤ italic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT
(152)
When râ‰¥T'â‰¥e^{12} ğ‘Ÿsuperscriptğ‘‡â€²superscriptğ‘’^{12} r\geq T^{\prime}\geq e^{12} italic_r â‰¥ italic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT â‰¥ italic_e start_POSTSUPERSCRIPT 12 end_POSTSUPERSCRIPT, we obtain:
(153)
ã™ã¹ã¦ã®å¢ƒç•Œã‚’çµ„ã¿åˆã‚ã›ã‚‹ã¨ã€ç§ãŸã¡ã¯ j ğ‘— jitalic_j ã«ä¾å­˜ã—ãªã„å¢ƒç•Œã‚’å°å‡ºã—ã¾ã—ãŸ:
(154)
(155)
(156)
(157)
So that:
(158)
The statement follows by summing all the terms.
ã“ã®ä¸»å¼µã¯ã€ã™ã¹ã¦ã®é …ã‚’åˆè¨ˆã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦å°ã‹ã‚Œã¾ã™ã€‚
âˆ
See VI.1
VI.1
The proof follows by defining â„±Ï„ subscript â„±ğœ \mathcal{F}_{\tau} caligraphic_F start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT as the set of times of length Ï„ğœ\tau italic_Ï„ after every breakpoint, and noticing that by definition of the general abruptly changing setting, we have for any tâˆˆâ„±Ï„âˆ ğ‘¡ subscript â„±ğœ complement tâˆˆ\mathcal{F}_{\tau}^{\complement} italic_t âˆˆ caligraphic_F start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT, as we have demonstrated in the main paper, that:
âˆ
See VI.2
VI.2
The proof, yet again, follows by defining â„±Ï„ subscript â„±ğœ \mathcal{F}_{\tau} caligraphic_F start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT as the set of times of length Ï„ğœ\tau italic_Ï„ after every breakpoint, and noticing that by definition of the general abruptly changing setting we have for any tâˆˆâ„±Ï„âˆ ğ‘¡ subscript â„±ğœ complement tâˆˆ\mathcal{F}_{\tau}^{\complement} italic_t âˆˆ caligraphic_F start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT, as we have demonstrated in the main paper, that:
âˆ
See VII.1
VII.1
To derive the bound, we will assign "error" equal to one for every tâˆˆâ„±Î”â€²,T ğ‘¡ subscript â„± superscript Î”â€² ğ‘‡ tâˆˆ\mathcal{F}_{\Delta^{\prime},T} italic_t âˆˆ caligraphic_F start_POSTSUBSCRIPT roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_T end_POSTSUBSCRIPT and we will study what happens in â„±Î”â€²,Tâˆ superscriptsubscript â„± superscriptsubscript Î”â€² ğ‘‡ complement \mathcal{F}_{\Delta^{\prime},T}^{\complement} caligraphic_F start_POSTSUBSCRIPT roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUBSCRIPT , italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT.
å¢ƒç•Œã‚’å°å‡ºã™ã‚‹ãŸã‚ã«ã€ç§ãŸã¡ã¯ tâˆˆâ„±Î”â€²,T ğ‘¡ subscript â„± superscript Î”â€² ğ‘‡ tâˆˆ\mathcal{F}_{\Delta^{\prime},T} italic_t âˆˆ caligraphic_F start_POSTSUBSCRIPT roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUBSCRIPT , italic_T end_POSTSUBSCRIPT ã®ã™ã¹ã¦ã«å¯¾ã—ã¦ "error" ã‚’1ã«è¨­å®šã—ã¾ã™ã€‚ãã—ã¦ã€â„±Î”â€²,Tâˆ superscriptsubscript â„± superscriptsubscript Î”â€² ğ‘‡ complement \mathcal{F}_{\Delta^{\prime},T}^{\complement} caligraphic_F start_POSTSUBSCRIPT roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUBSCRIPT , italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT ã§ä½•ãŒèµ·ã“ã‚‹ã‹ã‚’èª¿ã¹ã¾ã™ã€‚
Notice that by definition of â„±Î”â€²,Tâˆ superscriptsubscript â„± superscriptsubscript Î”â€² ğ‘‡ complement \mathcal{F}_{\Delta^{\prime},T}^{\complement} caligraphic_F start_POSTSUBSCRIPT roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUBSCRIPT , italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT we will have that âˆ€iâ‰ iâˆ—(t) for-all ğ‘– superscript ğ‘– ğ‘¡ âˆ€ i\neq i^{*}(t) âˆ€ italic_i â‰  italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ):
â„±Î”â€²,Tâˆ superscriptsubscript â„± superscriptsubscript Î”â€² ğ‘‡ complement \mathcal{F}_{\Delta^{\prime},T}^{\complement} caligraphic_F start_POSTSUBSCRIPT roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUBSCRIPT , italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT ã®å®šç¾©ã«ã‚ˆã‚Šã€âˆ€iâ‰ iâˆ—(t) for-all ğ‘– superscript ğ‘– ğ‘¡ âˆ€ i\neq i^{*}(t) âˆ€ italic_i â‰  italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) ãŒæˆã‚Šç«‹ã¡ã¾ã™ã€‚
Using the Lipschitz assumption we can infer that for iâ‰ iâˆ—(t) ğ‘– superscript ğ‘– ğ‘¡ i\neq i^{*}(t) italic_i â‰  italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ):
ãƒªãƒ—ã‚·ãƒƒãƒ„ä»®å®šã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€iâ‰ iâˆ—(t) ğ‘– superscript ğ‘– ğ‘¡ i\neq i^{*}(t) italic_i â‰  italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) ã«å¯¾ã—ã¦æ¨æ¸¬ã§ãã¾ã™ã€‚
and, similarly, by making use of the Lipscithz assumption, we obtain, for iâ‰ iâˆ—(t) ğ‘– superscript ğ‘– ğ‘¡ i\neq i^{*}(t) italic_i â‰  italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ):
åŒæ§˜ã«ã€ãƒªãƒ—ã‚·ãƒƒãƒ„ä»®å®šã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€iâ‰ iâˆ—(t) ğ‘– superscript ğ‘– ğ‘¡ i\neq i^{*}(t) italic_i â‰  italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) ã«å¯¾ã—ã¦å¾—ã‚‰ã‚Œã¾ã™ã€‚
Substituting we obtain:
ç½®ãæ›ãˆã‚‹ã“ã¨ã§å¾—ã‚‰ã‚Œã¾ã™:
so that due to the introduced assumptions, we have:
å°å…¥ã•ã‚ŒãŸä»®å®šã«ã‚ˆã‚Šã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™:
Notice that is the assumption for the general theorem, so we will have that â„±Î”â€²,Tâˆ = â„±Ï„âˆ superscriptsubscript â„± superscriptsubscript Î”â€² ğ‘‡ complement superscriptsubscript â„±ğœ complement \mathcal{F}_{\Delta^{\prime},T}^{\complement}=\mathcal{F}_{\tau}^{\complement} caligraphic_F start_POSTSUBSCRIPT roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUBSCRIPT , italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT = caligraphic_F start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT, this yields to the desired result noticing that by definition Î”Ï„=Î”â€²âˆ’2ÏƒÏ„ subscript Î”ğœ superscript Î”â€² 2 ğœ ğœ \Delta_{\tau}=\Delta^{\prime}-2\sigma\tau roman_Î” start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT = roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT - 2 italic_Ïƒ italic_Ï„.
æ³¨æ„ã™ã¹ãã¯ã€ã“ã‚Œã¯ä¸€èˆ¬å®šç†ã®ä»®å®šã§ã‚ã‚‹ãŸã‚ã€â„±Î”â€²,Tâˆ=â„±Ï„âˆ superscriptsubscript â„± superscriptsubscript Î”â€² ğ‘‡ complement superscriptsubscript â„±ğœ complement \mathcal{F}_{\Delta^{\prime},T}^{\complement}=\mathcal{F}_{\tau}^{\complement} caligraphic_F start_POSTSUBSCRIPT roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUBSCRIPT , italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT = caligraphic_F start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT ã¨ãªã‚Šã€å®šç¾©ã«ã‚ˆã‚Š Î”Ï„=Î”â€²âˆ’2ÏƒÏ„ subscript Î”ğœ superscript Î”â€² 2 ğœ ğœ \Delta_{\tau}=\Delta^{\prime}-2\sigma\tau roman_Î” start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT = roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT - 2 italic_Ïƒ italic_Ï„ ã¨ãªã‚Šã¾ã™ã€‚
âˆ
See VII.2
VII.2
In order to derive the bound we will assign "error" equal to one for every tâˆˆâ„±Î”â€²,T ğ‘¡ subscript â„± superscript Î”â€² ğ‘‡ tâˆˆ\mathcal{F}_{\Delta^{\prime},T} italic_t âˆˆ caligraphic_F start_POSTSUBSCRIPT roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUBSCRIPT , italic_T end_POSTSUBSCRIPT and we will study what happens in â„±Î”â€²,Tâˆ superscriptsubscript â„± superscriptsubscript Î”â€² ğ‘‡ complement \mathcal{F}_{\Delta^{\prime},T}^{\complement} caligraphic_F start_POSTSUBSCRIPT roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUBSCRIPT , italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT, i.e. the set of times tâˆˆâŸ¦TâŸ§ tâˆˆ\llbracket T\rrbracket italic_t âˆˆ âŸ¦ italic_T âŸ§ such that tâˆ‰â„±Î”â€²,T ğ‘¡ subscript â„± superscript Î”â€² ğ‘‡ t\notin\mathcal{F}_{\Delta^{\prime},T} italic_t âˆ‰ caligraphic_F start_POSTSUBSCRIPT roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUBSCRIPT , italic_T end_POSTSUBSCRIPT.
å¢ƒç•Œã‚’å°å‡ºã™ã‚‹ãŸã‚ã«ã€ç§ãŸã¡ã¯ tâˆˆâ„±Î”â€²,T ğ‘¡ subscript â„± superscript Î”â€² ğ‘‡ tâˆˆ\mathcal{F}_{\Delta^{\prime},T} italic_t âˆˆ caligraphic_F start_POSTSUBSCRIPT roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUBSCRIPT , italic_T end_POSTSUBSCRIPT ã®ã™ã¹ã¦ã«å¯¾ã—ã¦ "error" ã‚’1ã«è¨­å®šã—ã¾ã™ã€‚ãã—ã¦ã€â„±Î”â€²,Tâˆ superscriptsubscript â„± superscriptsubscript Î”â€² ğ‘‡ complement \mathcal{F}_{\Delta^{\prime},T}^{\complement} caligraphic_F start_POSTSUBSCRIPT roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUBSCRIPT , italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT ã§ä½•ãŒèµ·ã“ã‚‹ã‹ã‚’èª¿ã¹ã¾ã™ã€‚ã™ãªã‚ã¡ã€tâˆˆâŸ¦TâŸ§ tâˆˆ\llbracket T\rrbracket italic_t âˆˆ âŸ¦ italic_T âŸ§ ã§ tâˆ‰â„±Î”â€²,T ğ‘¡ subscript â„± superscript Î”â€² ğ‘‡ t\notin\mathcal{F}_{\Delta^{\prime},T} italic_t âˆ‰ caligraphic_F start_POSTSUBSCRIPT roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUBSCRIPT , italic_T end_POSTSUBSCRIPT ã¨ãªã‚‹æ™‚é–“ã®é›†åˆã§ã™ã€‚
Notice that by definition of â„±Î”â€²,Tâˆ superscriptsubscript â„± superscriptsubscript Î”â€² ğ‘‡ complement \mathcal{F}_{\Delta^{\prime},T}^{\complement} caligraphic_F start_POSTSUBSCRIPT roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUBSCRIPT , italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT we will have that âˆ€iâ‰ iâˆ—(t) for-all ğ‘– superscript ğ‘– ğ‘¡ âˆ€ i\neq i^{*}(t) âˆ€ italic_i â‰  italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ):
â„±Î”â€²,Tâˆ superscriptsubscript â„± superscriptsubscript Î”â€² ğ‘‡ complement \mathcal{F}_{\Delta^{\prime},T}^{\complement} caligraphic_F start_POSTSUBSCRIPT roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUBSCRIPT , italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT ã®å®šç¾©ã«ã‚ˆã‚Šã€âˆ€iâ‰ iâˆ—(t) for-all ğ‘– superscript ğ‘– ğ‘¡ âˆ€ i\neq i^{*}(t) âˆ€ italic_i â‰  italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) ãŒæˆã‚Šç«‹ã¡ã¾ã™ã€‚
Using the Lipschitz assumption, we can infer that for iâ‰ iâˆ—(t) ğ‘– superscript ğ‘– ğ‘¡ i\neq i^{*}(t) italic_i â‰  italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ):
ãƒªãƒ—ã‚·ãƒƒãƒ„ä»®å®šã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€iâ‰ iâˆ—(t) ğ‘– superscript ğ‘– ğ‘¡ i\neq i^{*}(t) italic_i â‰  italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) ã«å¯¾ã—ã¦æ¨æ¸¬ã§ãã¾ã™ã€‚
and, similarly, by making use of the Lipschitz assumption, we obtain, for iâ‰ iâˆ—(t) ğ‘– superscript ğ‘– ğ‘¡ i\neq i^{*}(t) italic_i â‰  italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ):
åŒæ§˜ã«ã€ãƒªãƒ—ã‚·ãƒƒãƒ„ä»®å®šã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€iâ‰ iâˆ—(t) ğ‘– superscript ğ‘– ğ‘¡ i\neq i^{*}(t) italic_i â‰  italic_i start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) ã«å¯¾ã—ã¦å¾—ã‚‰ã‚Œã¾ã™ã€‚
Substituting we obtain:
ç½®ãæ›ãˆã‚‹ã“ã¨ã§å¾—ã‚‰ã‚Œã¾ã™:
so that due to the introduced assumptions, we have:
å°å…¥ã•ã‚ŒãŸä»®å®šã«ã‚ˆã‚Šã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™:
Notice that is the assumption for the general theorem, so we will have that â„±Î”â€²,Tâˆ = â„±Ï„âˆ superscriptsubscript â„± superscriptsubscript Î”â€² ğ‘‡ complement superscriptsubscript â„±ğœ complement \mathcal{F}_{\Delta^{\prime},T}^{\complement}=\mathcal{F}_{\tau}^{\complement} caligraphic_F start_POSTSUBSCRIPT roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUBSCRIPT , italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT = caligraphic_F start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT, this yields to the desired result noticing that by definition Î”Ï„=Î”â€²âˆ’2ÏƒÏ„ subscript Î”ğœ superscript Î”â€² 2 ğœ ğœ \Delta_{\tau}=\Delta^{\prime}-2\sigma\tau roman_Î” start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT = roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT - 2 italic_Ïƒ italic_Ï„.
æ³¨æ„ã™ã¹ãã¯ã€ã“ã‚Œã¯ä¸€èˆ¬å®šç†ã®ä»®å®šã§ã‚ã‚‹ãŸã‚ã€â„±Î”â€²,Tâˆ=â„±Ï„âˆ superscriptsubscript â„± superscriptsubscript Î”â€² ğ‘‡ complement superscriptsubscript â„±ğœ complement \mathcal{F}_{\Delta^{\prime},T}^{\complement}=\mathcal{F}_{\tau}^{\complement} caligraphic_F start_POSTSUBSCRIPT roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUBSCRIPT , italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT = caligraphic_F start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT ã¨ãªã‚Šã€å®šç¾©ã«ã‚ˆã‚Š Î”Ï„=Î”â€²âˆ’2ÏƒÏ„ subscript Î”ğœ superscript Î”â€² 2 ğœ ğœ \Delta_{\tau}=\Delta^{\prime}-2\sigma\tau roman_Î” start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT = roman_Î” start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT - 2 italic_Ïƒ italic_Ï„.
âˆ



## Appendix CExperimental details ä»˜éŒ²C å®Ÿé¨“ã®è©³ç´°

Appendix C ä»˜éŒ²C



### Parameters ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

The choices of the parameters of the algorithms we compared R-less/ed-UCB with are the following:
ç§ãŸã¡ãŒæ¯”è¼ƒã—ãŸR-less/ed-UCBã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é¸æŠã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚

R-less/ed-UCB
R-less/ed-UCB
- â€¢ Rexp3: $\gamma=\min\left\{1,\sqrt{\frac{K\log{K}}{(e-1)\Delta_{T}}}\right\}$, $\Delta_{T}=\lceil(K\log{K})^{1/3}(T/V_{T})^{2/3}\rceil$ as recommended by Besbes et al. [10];
- â€¢ Rexp3: $\gamma=\min\left\{1,\sqrt{\frac{K\log{K}}{(e-1)\Delta_{T}}}\right\}$ï¼ˆ$\gamma=\min\{1,\sqrt{\frac{K\log{K}}{(e-1)\Delta_{T}}}\}$ï¼‰ã€$\Delta_{T}=\lceil(K\log{K})^{1/3}(T/V_{T})^{2/3}\rceil$ï¼ˆ$\Delta_{T}=\lceil(K\log{K})^{1/3}(T/V_{T})^{2/3}\rceil$ï¼‰ã¯ã€Besbes et al. [10]ã«ã‚ˆã£ã¦æ¨å¥¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

- â€¢ KL-UCB: $c=3$ as required by the theoretical results on the regret provided by Garivier and CappÃ© [22];
- â€¢ KL-UCB: $c=3$ï¼ˆ$c=3$ï¼‰ã¯ã€Garivierã¨CappÃ© [22]ã«ã‚ˆã£ã¦æä¾›ã•ã‚ŒãŸå¾Œæ‚”ã«é–¢ã™ã‚‹ç†è«–çš„çµæœã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚

- â€¢ Ser4: according to what suggested by Allesiardo et al. [6] we selected $\delta=1/T$, $\epsilon=\frac{1}{KT}$, and $\phi=\sqrt{\frac{N}{TK\log({KT})}}$;
- â€¢ Ser4: Allesiardo et al. [6]ã®ææ¡ˆã«å¾“ã„ã€$\delta=1/T$ï¼ˆ$\delta=1/T$ï¼‰ã€$\epsilon=\frac{1}{KT}$ï¼ˆ$\epsilon=\frac{1}{KT}$ï¼‰ã€ãŠã‚ˆã³$\phi=\sqrt{\frac{N}{TK\log({KT})}}$ï¼ˆ$\phi=\sqrt{\frac{N}{TK\log({KT})}}$ï¼‰ã‚’é¸æŠã—ã¾ã—ãŸã€‚

- â€¢ SW-UCB: as suggested by Garivier and Moulines [23] we selected the sliding-window $\tau=4\sqrt{T\log{T}}$ and the constant $\xi=0.6$;
- â€¢ SW-UCB: Garivierã¨Moulines [23]ã®ææ¡ˆã«å¾“ã„ã€ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦$\tau=4\sqrt{T\log{T}}$ï¼ˆ$\tau=4\sqrt{T\log{T}}$ï¼‰ã¨å®šæ•°$\xi=0.6$ï¼ˆ$\xi=0.6$ï¼‰ã‚’é¸æŠã—ã¾ã—ãŸã€‚

- â€¢ SW-KL-UCB as suggested by Garivier and Moulines [24] we selected the sliding-window $\tau=\sigma^{-4/5}$;
- â€¢ SW-KL-UCB: Garivierã¨Moulines [24]ã®ææ¡ˆã«å¾“ã„ã€ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦$\tau=\sigma^{-4/5}$ï¼ˆ$\tau=\sigma^{-4/5}$ï¼‰ã‚’é¸æŠã—ã¾ã—ãŸã€‚



### Equations for the Abruptly Changing Environment ç’°å¢ƒãŒæ€¥æ¿€ã«å¤‰åŒ–ã™ã‚‹ãŸã‚ã®æ–¹ç¨‹å¼

(159)  
(159)

(160)  
(160)



### Equations for the Smoothly Changing Environment æ»‘ã‚‰ã‹ã«å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã®ãŸã‚ã®æ–¹ç¨‹å¼

(161)
(161)



### Smoothly Changing Experiment for $\sigma=0.001$

The environment is illustrated in Figure 6a. 
ç’°å¢ƒã¯å›³6aã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚

The cumulative regret is depicted in Figure 6b, 
ç´¯ç©å¾Œæ‚”ã¯å›³6bã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚

while the sensitivity analysis is represented in Figure 6c. 
æ„Ÿåº¦åˆ†æã¯å›³6cã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚

6a  
6b  
6c  
(a)  
(b)  
(c)  

Figure 6:  
å›³6ï¼š



## Appendix D TrovÃ² et al. [48] ã®è«–æ–‡ã‹ã‚‰ã®èª¤ã‚Š

Appendix D  
ã“ã®ä»˜éŒ²ã§ã¯ã€TrovÃ² et al. [48] ã§è¦‹ã¤ã‹ã£ãŸæŠ€è¡“çš„ãªèª¤ã‚Šã‚’å ±å‘Šã—ã¾ã™ã€‚  
In this appendix, we report the technical error found in TrovÃ² et al. [48].

Rewriting Equation (18) to Equation (21) from [48]:  
[48] ã®å¼ (18) ã‹ã‚‰å¼ (21) ã¸ã®æ›¸ãæ›ãˆ:

$$
(162)
$$

$$
(163)
$$

$$
(164)
$$

$$
(165)
$$

Notice that the term $\sum_{t\in\mathcal{F}_{\phi}^{\prime}}\mathbb{E}\left[\mathds{1}\left\{T_{i_{\phi}^{*},t,\tau}\leq\bar{n}_{A}\right\}\right]$ is bounded using Lemma 8, implying that the event $\mathds{1}\{\cdot\}$ is:  
é … $\sum_{t\in\mathcal{F}_{\phi}^{\prime}}\mathbb{E}\left[\mathds{1}\left\{T_{i_{\phi}^{*},t,\tau}\leq\bar{n}_{A}\right\}\right]$ ã¯è£œé¡Œ 8 ã‚’ç”¨ã„ã¦æœ‰ç•Œã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã¯ã€äº‹è±¡ $\mathds{1}\{\cdot\}$ ãŒæ¬¡ã®ã‚ˆã†ã«ç¤ºã•ã‚Œã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™:

$$
(166)
$$

However, the separation of the event used by the author (following the line of proof [29]) in Equation (12) to Equation (16) in [48]:  
ã—ã‹ã—ã€è‘—è€…ãŒä½¿ç”¨ã—ãŸäº‹è±¡ã®åˆ†é›¢ï¼ˆè¨¼æ˜ [29] ã«å¾“ã†ï¼‰ã«ãŠã„ã¦ã€å¼ (12) ã‹ã‚‰å¼ (16) ã¸ã® [48]:

$$
(167)
$$

$$
(168)
$$

$$
(169)
$$

$$
(170)
$$

$$
(171)
$$

is such that the event $\{\cdot\}$ is given by:  
ã“ã®ã‚ˆã†ã«ã€äº‹è±¡ $\{\cdot\}$ ã¯æ¬¡ã®ã‚ˆã†ã«ä¸ãˆã‚‰ã‚Œã¾ã™:

$$
(172)
$$

thus making the derived inequality incorrect.  
ã“ã‚Œã«ã‚ˆã‚Šã€å°å‡ºã•ã‚ŒãŸä¸ç­‰å¼ãŒä¸æ­£ç¢ºã«ãªã‚Šã¾ã™ã€‚

The same error is done also in the following equations (Equation 70 to Equation 72 in [48]):  
åŒæ§˜ã®èª¤ã‚ŠãŒæ¬¡ã®å¼ï¼ˆ[48] ã®å¼ 70 ã‹ã‚‰å¼ 72ï¼‰ã«ã‚‚ã‚ã‚Šã¾ã™:

$$
(173)
$$

$$
(174)
$$

$$
(175)
$$

where notice that yet again $\sum_{t\in\mathcal{F}_{\Delta^{C},N}}\mathbb{P}\left(T_{i_{t}^{*},t,\tau}\leq\bar{n}_{A}\right)$ has been wrongly bounded by $\bar{n}_{A}\lceil\frac{N}{\tau}\rceil$.  
ã“ã“ã§å†ã³ã€$\sum_{t\in\mathcal{F}_{\Delta^{C},N}}\mathbb{P}\left(T_{i_{t}^{*},t,\tau}\leq\bar{n}_{A}\right)$ ãŒèª¤ã£ã¦ $\bar{n}_{A}\lceil\frac{N}{\tau}\rceil$ ã«ã‚ˆã£ã¦æœ‰ç•ŒåŒ–ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚



## Appendix E è£œè¶³ãƒ¬ãƒ

In this appendix, we report some results that already exist in the bandit literature and have been used to demonstrate our results.
ã“ã®ä»˜éŒ²ã§ã¯ã€ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆæ–‡çŒ®ã«æ—¢ã«å­˜åœ¨ã™ã‚‹ã„ãã¤ã‹ã®çµæœã‚’å ±å‘Šã—ã€ç§ãŸã¡ã®çµæœã‚’ç¤ºã™ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã—ãŸã€‚

### Lemma 3

Let $X_1,\ldots,X_n$ be independent Bernoulli random variables with $\mathbb{E}[X_i]=p_i$, consider the random variable $X=\frac{1}{n}\sum_{i=1}^{n}X_{i}$, with $\mu=\mathbb{E}[X]$.
$X_1,\ldots,X_n$ ã‚’ç‹¬ç«‹ã—ãŸãƒ™ãƒ«ãƒŒãƒ¼ã‚¤ç¢ºç‡å¤‰æ•°ã¨ã—ã€$\mathbb{E}[X_i]=p_i$ ã¨ã—ã¾ã™ã€‚ãƒ©ãƒ³ãƒ€ãƒ å¤‰æ•° $X=\frac{1}{n}\sum_{i=1}^{n}X_{i}$ ã‚’è€ƒãˆã€$\mu=\mathbb{E}[X]$ ã¨ã—ã¾ã™ã€‚

For any $0<\lambda<1-\mu$ we have:
ä»»æ„ã® $0<\lambda<1-\mu$ ã«å¯¾ã—ã¦ã€æ¬¡ãŒæˆã‚Šç«‹ã¡ã¾ã™ï¼š

### Lemma 4

For all positive integers $\alpha,\beta\in\mathbb{N}$, the following equality holds:
ã™ã¹ã¦ã®æ­£ã®æ•´æ•° $\alpha,\beta\in\mathbb{N}$ ã«å¯¾ã—ã¦ã€æ¬¡ã®ç­‰å¼ãŒæˆã‚Šç«‹ã¡ã¾ã™ï¼š

$$
F_{\alpha,\beta}^{beta}(y)
$$
where $F_{\alpha+\beta-1,y}^{B}(\alpha-1)$ is the cumulative distribution function of a binomial variable with $\alpha+\beta-1$ trials having each probability $y$.
ã“ã“ã§ã€$F_{\alpha+\beta-1,y}^{B}(\alpha-1)$ ã¯ã€å„ç¢ºç‡ãŒ $y$ ã§ã‚ã‚‹ $\alpha+\beta-1$ å›ã®è©¦è¡Œã‚’æŒã¤äºŒé …å¤‰æ•°ã®ç´¯ç©åˆ†å¸ƒé–¢æ•°ã§ã™ã€‚

### Lemma 5

Let $Z$ be a Gaussian random variable with mean $\mu$ and standard deviation $\sigma$, then:
$Z$ ã‚’å¹³å‡ $\mu$ ã¨æ¨™æº–åå·® $\sigma$ ã®ã‚¬ã‚¦ã‚¹ãƒ©ãƒ³ãƒ€ãƒ å¤‰æ•°ã¨ã—ã¾ã™ã€‚æ¬¡ãŒæˆã‚Šç«‹ã¡ã¾ã™ï¼š

$$
(177)
$$

### Lemma 6

Let $Z$ be a Gaussian r.v. with mean $m$ and standard deviation $\sigma$, then:
$Z$ ã‚’å¹³å‡ $m$ ã¨æ¨™æº–åå·® $\sigma$ ã®ã‚¬ã‚¦ã‚¹ãƒ©ãƒ³ãƒ€ãƒ å¤‰æ•°ã¨ã—ã¾ã™ã€‚æ¬¡ãŒæˆã‚Šç«‹ã¡ã¾ã™ï¼š

$$
(178)
$$

### Lemma 7

Let $X_1,\ldots,X_n$ be independent random variables such that $X_{i}\sim Subg(\sigma^{2})$, then for any $a\in\mathbb{R}^{n}$, we have:
$X_1,\ldots,X_n$ ã‚’ç‹¬ç«‹ã—ãŸãƒ©ãƒ³ãƒ€ãƒ å¤‰æ•°ã¨ã—ã€$X_{i}\sim Subg(\sigma^{2})$ ã¨ã—ã¾ã™ã€‚ä»»æ„ã® $a\in\mathbb{R}^{n}$ ã«å¯¾ã—ã¦ã€æ¬¡ãŒæˆã‚Šç«‹ã¡ã¾ã™ï¼š

$$
(179)
$$
and
$$
(180)
$$

Of special interest is the case where $a_i=1/n$ for all $i$ we get that the average $\bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}$ satisfies:
ç‰¹ã«èˆˆå‘³æ·±ã„ã®ã¯ã€ã™ã¹ã¦ã® $i$ ã«å¯¾ã—ã¦ $a_i=1/n$ ã®å ´åˆã§ã‚ã‚Šã€å¹³å‡ $\bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}$ ãŒæ¬¡ã‚’æº€ãŸã—ã¾ã™ï¼š

### Lemma 8

Let $A\subset\mathbb{N}$, and $\tau\in\mathbb{N}$ fixed. Define $a(n)=\sum_{t=n-\tau}^{n-1}\mathds{1}\{t\in A\}$. Then for all $T\in\mathbb{N}$ and $s\in\mathbb{N}$ we have the inequality:
$A\subset\mathbb{N}$ ã¨ã—ã€$\tau\in\mathbb{N}$ ã‚’å›ºå®šã—ã¾ã™ã€‚$a(n)=\sum_{t=n-\tau}^{n-1}\mathds{1}\{t\in A\}$ ã‚’å®šç¾©ã—ã¾ã™ã€‚ã™ã¹ã¦ã® $T\in\mathbb{N}$ ãŠã‚ˆã³ $s\in\mathbb{N}$ ã«å¯¾ã—ã¦ã€æ¬¡ã®ä¸ç­‰å¼ãŒæˆã‚Šç«‹ã¡ã¾ã™ï¼š

$$
(181)
$$

### Lemma 9

Let $j\in\mathbb{N}$, $PB(\underline{\mu}_{i^{*}(t)}(j))$ be a Poisson-Binomial distribution with parameters $\underline{\mu}_{i^{*}(t)}(j)=(\mu_{i^{*}(t),1},\dots,\mu_{i^{*}(t),j})$, and $Bin(j,x)$ be a binomial distribution of $j$ trials and probability of success $0\leq x\leq 1$. Then, it holds that:
$j\in\mathbb{N}$ ã¨ã—ã€$PB(\underline{\mu}_{i^{*}(t)}(j))$ ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\underline{\mu}_{i^{*}(t)}(j)=(\mu_{i^{*}(t),1},\dots,\mu_{i^{*}(t),j})$ ã®ãƒã‚¢ã‚½ãƒ³-äºŒé …åˆ†å¸ƒã¨ã—ã€$Bin(j,x)$ ã‚’ $j$ å›ã®è©¦è¡Œã¨æˆåŠŸç¢ºç‡ $0\leq x\leq 1$ ã®äºŒé …åˆ†å¸ƒã¨ã—ã¾ã™ã€‚æ¬¡ãŒæˆã‚Šç«‹ã¡ã¾ã™ï¼š

### Lemma 10

Let $F_{n,p}^{B}$ be the CDF of a $Bin(n,p)$ distributed random variable, then holds for $m\leq n$ and $q\leq p$:
$F_{n,p}^{B}$ ã‚’ $Bin(n,p)$ åˆ†å¸ƒã®ç´¯ç©åˆ†å¸ƒé–¢æ•°ã¨ã—ã¾ã™ã€‚æ¬¡ãŒæˆã‚Šç«‹ã¡ã¾ã™ $m\leq n$ ãŠã‚ˆã³ $q\leq p$ ã®å ´åˆï¼š

$$
(182)
$$
for all $x$.
ã™ã¹ã¦ã® $x$ ã«å¯¾ã—ã¦ã€‚

### Lemma 11

(i) A $\mathcal{N}(m,\sigma^{2})$ distributed r.v. is stochastically dominated by $\mathcal{N}(m',\sigma^{2})$ if $m' \geq m$.
(i) $\mathcal{N}(m,\sigma^{2})$ åˆ†å¸ƒã®ãƒ©ãƒ³ãƒ€ãƒ å¤‰æ•°ã¯ã€$m' \geq m$ ã®å ´åˆã« $\mathcal{N}(m',\sigma^{2})$ ã«ã‚ˆã£ã¦ç¢ºç‡çš„ã«æ”¯é…ã•ã‚Œã¾ã™ã€‚

(ii) A $Beta(\alpha,\beta)$ random variable is stochastically dominated by $Beta(\alpha',\beta')$ if $\alpha' \geq \alpha$ and $\beta' \leq \beta$.
(ii) $Beta(\alpha,\beta)$ ãƒ©ãƒ³ãƒ€ãƒ å¤‰æ•°ã¯ã€$\alpha' \geq \alpha$ ãŠã‚ˆã³ $\beta' \leq \beta$ ã®å ´åˆã« $Beta(\alpha',\beta')$ ã«ã‚ˆã£ã¦ç¢ºç‡çš„ã«æ”¯é…ã•ã‚Œã¾ã™ã€‚



##### Report Github Issue GitHubã®å•é¡Œå ±å‘Š

LATE
LATE

A
A

E
E

xml
xml



## Instructions for reporting errors ã‚¨ãƒ©ãƒ¼å ±å‘Šã®æ‰‹é †

We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. 
ç§ãŸã¡ã¯è«–æ–‡ã®HTMLãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æ”¹å–„ã—ç¶šã‘ã¦ãŠã‚Šã€ã‚ãªãŸã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã¨ãƒ¢ãƒã‚¤ãƒ«ã‚µãƒãƒ¼ãƒˆã®å‘ä¸Šã«å½¹ç«‹ã¡ã¾ã™ã€‚ 
To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:
HTMLã®ã‚¨ãƒ©ãƒ¼ã‚’å ±å‘Šã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã«ç¤ºã™ã„ãšã‚Œã‹ã®æ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

- Click the "Report Issue" button.
- "Report Issue"ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚
- Open a report feedback form via keyboard, use "Ctrl + ?".
- ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦å ±å‘Šãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ•ã‚©ãƒ¼ãƒ ã‚’é–‹ãã«ã¯ã€ã€ŒCtrl + ?ã€ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
- Make a text selection and click the "Report Issue for Selection" button near your cursor.
- ãƒ†ã‚­ã‚¹ãƒˆã‚’é¸æŠã—ã€ã‚«ãƒ¼ã‚½ãƒ«ã®è¿‘ãã«ã‚ã‚‹ã€ŒReport Issue for Selectionã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚
- You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã‚¢ã‚¯ã‚»ã‚·ãƒ–ãƒ«ãªå ±å‘Šãƒªãƒ³ã‚¯ã‚’ã‚ªãƒ³ã«ã™ã‚‹ã«ã¯Alt+Yã‚’ã€ã‚ªãƒ•ã«ã™ã‚‹ã«ã¯Alt+Shift+Yã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚

Our team has already identified the following issues. 
ç§ãŸã¡ã®ãƒãƒ¼ãƒ ã¯ã™ã§ã«ä»¥ä¸‹ã®å•é¡Œã‚’ç‰¹å®šã—ã¦ã„ã¾ã™ã€‚ 
We appreciate your time reviewing and reporting rendering errors we may not have found yet. 
ç§ãŸã¡ã¯ã€ã¾ã è¦‹ã¤ã‘ã¦ã„ãªã„å¯èƒ½æ€§ã®ã‚ã‚‹ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—å ±å‘Šã™ã‚‹ãŸã‚ã«ã‚ãªãŸãŒè²»ã‚„ã™æ™‚é–“ã«æ„Ÿè¬ã—ã¾ã™ã€‚ 
Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. 
ã‚ãªãŸã®åŠªåŠ›ã¯ã€ã™ã¹ã¦ã®èª­è€…ã®ãŸã‚ã«HTMLãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æ”¹å–„ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚ãªãœãªã‚‰ã€éšœå®³ã¯ç ”ç©¶ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã®éšœå£ã§ã‚ã£ã¦ã¯ãªã‚‰ãªã„ã‹ã‚‰ã§ã™ã€‚ 
Thank you for your continued support in championing open access for all.
ã™ã¹ã¦ã®äººã«ã‚ªãƒ¼ãƒ—ãƒ³ã‚¢ã‚¯ã‚»ã‚¹ã‚’æ¨é€²ã™ã‚‹ãŸã‚ã®ã‚ãªãŸã®ç¶™ç¶šçš„ãªã‚µãƒãƒ¼ãƒˆã«æ„Ÿè¬ã—ã¾ã™ã€‚

Have a free development cycle? Help support accessibility at arXiv! 
é–‹ç™ºã‚µã‚¤ã‚¯ãƒ«ã«ä½™è£•ãŒã‚ã‚Šã¾ã™ã‹ï¼ŸarXivã§ã®ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ãã ã•ã„ï¼ 
Our collaborators at LaTeXML maintain a list of packages that need conversion, and welcome developer contributions.
ç§ãŸã¡ã®å”åŠ›è€…ã§ã‚ã‚‹LaTeXMLã¯ã€å¤‰æ›ãŒå¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã‚’ç¶­æŒã—ã¦ãŠã‚Šã€é–‹ç™ºè€…ã®è²¢çŒ®ã‚’æ­“è¿ã—ã¦ã„ã¾ã™ã€‚
