<!-- 仮タイトル: Udemyのユニット推薦にバンディットアルゴリズムを導入するブログを読んだ! -->

## これは何??

- Udemyさんのユニット推薦にMAB(multi-armed bandit)アルゴリズムを導入するテックブログを読んだメモです!
  - [Building a Multi-Armed Bandit System from the Ground Up: A Recommendations and Ranking Case Study, Part I](https://medium.com/udemy-engineering/building-a-multi-armed-bandit-system-from-the-ground-up-a-recommendations-and-ranking-case-study-b598f1f880e1)
  - [Building a Multi-Armed Bandit System from the Ground Up: A Recommendations and Ranking Case Study, Part II](https://medium.com/udemy-engineering/building-a-multi-armed-bandit-system-from-the-ground-up-a-recommendations-and-ranking-case-study-8f09f65d26b6)

## 導入

- 探索-活用のバランスをとるアプローチが様々なアプリケーションで成功を収めてるため、MAB(multi-armed bandit)アルゴリズムの人気が高まってる。
  - 特に成功してるアプリケーションの一つが、推薦。
- ゼロからMABシステムを本番システムに導入するのって エンジニアリング・運用・データサイエンス全部の観点から大変。
- 今回のテックブログでその実体験をシェアしてる。
  - part1: 理論とデータサイエンス的な話
  - part2: ソフトウェアエンジニアリング的な話

## MABとは?? なぜ推薦にMABを使うのか??

### 問題設定

- MABは、不確実性の元での意思決定問題を解くために使用される **explore-exploit(探索-活用)アルゴリズム**の一種。
- explore-exploit(探索-活用)アルゴリズムってどんな問題設定を解くためのもの??
  - 例: カジノのスロットマシン (かなりイメージしやすくてわかりやすい例でした!:thinking:)
    - あなたはカジノに入り、選択可能なスロットマシンがいくつか提示される。
    - 各スロットマシンにはレバー(アーム)があり、それを引く(プレイ)と、各スロットマシンの事前に定められた報酬確率分布からサンプリングされたランダムな報酬(支払い)を受け取る。
      - 各スロットマシンは異なる報酬確率分布を持ち、これらの分布は最初にプレイを始めた時のあなたは知らない。
    - あなたの目標は、これらのスロットマシンをプレイすることで得られる勝ち金(累積報酬)を最大化すること。
    - あなたは賢いギャンブラーなので、使うつもりのある固定金額を持ってカジノに来た。なので、アームを引く回数は限られている。
    - **もし各スロットマシンの真の報酬確率分布を知っていれば、長期的に勝ち金を最大化する方法は非常に明確**である。
      - シンプルに、報酬期待値が最も高い1つのスロットマシンのアームを継続的にプレイするだけで良いはず。
    - しかし、あなたは最初はこれらの報酬分布について何も知らないため、**限られたアームを引く回数の一部を使って探索し、各アームの期待報酬についてもっと学ぶ**必要がある。また、**これまで得た学びを活用して最も良さそうなアームを引く**こととのバランスも取るよう必要もある。
      - 探索だけを行うと、各スロットマシンの報酬分布に対する高い信頼性を得ることができる。しかし、最適なアームを引くことを知る前に、アームを引ける回数が尽きてしまう。
      - 活用だけを行うと、最適でないスロットマシンを継続的にプレイする可能性が高くなり、それはそれでより高い報酬を得る機会を逃し得る。
    - これが古典的な「探索-活用(explore-exploit)のジレンマ」である。
    - MABアルゴリズムをはじめとした探索-活用アルゴリズムは、このジレンマを解決するため(i.e. 探索と活用の最適なバランスをとり累積報酬の最大化を助けるため)に設計されてる。


### 定義

MABは、不確実性の下で意思決定を行うために使用される探索-活用アルゴリズムの一種。これは簡略化された強化学習アルゴリズムとみなせる。
標準的なMAB問題の設定は、以下のように定義できる:


- $k$ 本のアーム（選択肢）のセット $A = \{a_1, a_2, \ldots, a_k\}$ が与えられた時、目標は、これらのアームをプレイする事で得られる累積報酬を最大化するために、最良のアームを決定すること。
- アーム $a_i$ が時刻 $t$ にプレイされると、そのアームの報酬確率分布からサンプリングされた報酬 $r_{t}$ が得られる。プレイされていないアームに対しては他の報酬は観測されない。このアーム-報酬ペア $(a_i, r_t)$ を**観測(observation)**と呼んで記録する。
- 我々は、最適でないアームをあまり頻繁に表示することなく、各アームの真の期待報酬についてより多くを学ぶために、**次にどのアームを選択するかを最適に決定する戦略**を考案する。戦略は、時刻 $t-1$ までのアーム-報酬ペアの観測履歴を考慮し、時刻 $t$ において次にプレイするアームを選択する。

MAB戦略には多くの異なるアプローチがあるが、それぞれ利点と欠点がある。
ちなみに、**Udemy内で構築したアプリケーションでは、主にThompson Sampling戦略を使用**してるらしい。

### 推薦にMABを使うこと

推薦ドメインは、MABの完全な応用を含んでる。
**一般的な推薦問題において、私たちの目的はユーザに最適なアイテムを推薦することであり、その「最適」とは何らかのユーザフィードバックに基づいて定義される。** 
(前述のMABの問題設定っぽい...!:thinking:)
従って、**推薦問題は以下のようなMAB問題として定式化**できる:

- 推薦するための候補アイテム集合は、プレイ可能なアームの集合 (各アイテムはアーム)。
- 特定の推薦アイテムをユーザに一定の時間表示することは、「そのアームをプレイする」ことに相当する。
- 推薦アイテムが表示された時のユーザフィードバック(ex. クリック、いいね、Udemyの場合はコース登録など)は、観測された報酬に対応する。(これはすなわち、選ばれたアームの未知の報酬分布からサンプリングされた報酬)
- 目標は、探索と活用のバランスを取りながら、ユーザにさまざまなアイテムを動的に推薦することによって、できるだけ早く最高の報酬を得られるアイテムを特定すること。

また、MABには、推薦アプリケーションに優れた特性が多くある。
MABが特に推薦に適してる主な理由は3つ:

- 理由1: 推薦タスクにはopportunity cost(機会コスト?)があるから。
  - (もっとより最適なアイテムをおすすめできたのでは...という機会損失のコスト、という解釈...!:thinking:)
  - MABは、最適でないアームをプレイすることによるコストがある問題設定で真価を発揮する。なぜなら、 MABはそもそも累積regretを最小化するように設計されてるから。
- 理由2: 推薦におけるフィードバックループバイアス問題を克服するのに役立つから。
  - フィードバックループバイアス問題: hoge
  - **MABにおける探索は本質的にこのサイクルを断ち切る**。あまり探索されてないアイテムはMAB的に推薦されやすくなるので、**真に最適でない限り、同じアイテムを繰り返し表示することに陥る可能性が低くなる**
- 理由3: MABは、コールドスタートアイテム問題に自然に対処できるから。
  - フィードバックループバイアス問題に密接に関連しているのがコールドスタートアイテム問題。
    - フィードバックがほとんどない新しいアイテムは、関連するポジティブなシグナルがほとんどないため、推薦される可能性が低くなる。
  - MABの設計上、報酬期待値の推定に自信を持つまでは新しいアイテムを自然に探索するようになってる。なので、**MABは全ての候補アームに公平な機会が与える**。
    - 探索フェーズにて成功と見なされた場合はそのアイテムを活用し、失敗と見なされた場合は表示を停止する。

## MAB導入のデータサイエンス的な話

### ランキング問題をMAB問題として解くこと

伝統的なMABの問題設定は、ユーザに一度に1つのアームが提示され単一のポジションに対して最良のアームを見つけることが目標。(ex. バナー最適化、サムネイル最適化、ボタンの色の最適化, etc.)
(いわゆる、**Single-Play MAB**って呼ばれるやつ...!:thinking:)

一方で、推薦ドメインにはランキング問題もある。
**ここでの重要な違いは、ランキング問題では考慮すべき複数のポジションがあり、そのため技術的には複数のアームに対して同時に報酬を収集できる**ということ。(いわゆる、**Multi-Plays MAB**って呼ばれるやつ...!:thinking:)

Udemyにおけるランキング推薦の一般的な応用の1つは、推薦ユニットランキングである。
推薦ユニットは、特定グループに属するコース達を集めた横カルーセル。(ex. 「Because you Viewed」「Recommended for You」「Popular for ___」など)
(つまり、ページ内での横カルーセル単位のランクづけ、みたい...!:thinking:)。
これらの推薦ユニットがユーザのホームページに表示される順序は、ユーザーの視点とビジネスの視点の両方から非常に重要。

今回、Udemyの推薦ユニットランキング問題を以下のように定式化した:
(この定式化方法、参考になりそう...! 定式化大事だよなぁ...:thinking:)

1. 各推薦ユニットは「アーム」である。
2. 「アームをプレイする」ことは、特定のポジションにおいてユーザに推薦ユニットを固定時間(この事例では15分)表示することを意味する。各ユーザは、表示されたユニットに対して提供されたFBに基づいて「報酬(reward)」の「観測(obeservation)」を生成する。(ユニットのimpressionが発生した場合にのみ報酬が観測される。)
3. 「報酬(reward)」は、特定のポジションにおけるさまざまなユニットのパフォーマンスを比較するために使用したい指標。この事例では、固定時間内にユニット内のコースに対するユーザのクリックと登録の組み合わせを使用する (=確か後述されてたけど、報酬関数はこの2種類のFBの線形結合だった気がする...!:thinking:)

このMABの定式化は十分にシンプルに見える。
しかし、**ランキングの複数のポジションにおけるフィードバックの扱い**についてはまだ検討すべき点が残ってる。
伝統的なMAB設定では、単一のアームのみがプレイされる。
**複数のアームを同時に引くことができるという利点を活かすために、伝統的なMAB設定を修正**する必要がある。(なるほど、利点と見做せるのか...!:thinking:)

このために、今回の事例では以下の2つのフレームワークを検討した:

- **Per-Position Framework**: 各ポジションを独立したMAB問題として扱う。
- **Slate Bandit Framework**: 各ポジションを同時にテストする単一のMAB問題として扱う。

#### 検討したアプローチ1: Per-Position Framework

- hoge

#### 検討したアプローチ2: Slate Bandit Framework

- hoge

### MAB活用におけるDS的な課題

#### 課題1: 収束の達成とその測定

#### 課題2: 方策のオフライン評価

## MAB導入のソフトウェアエンジニアリング的な話

### システムアーキテクチャの全体像

### MAB導入におけるエンジニアリング的な課題

#### 課題1: リアルタイムシステムを開発することの学習コスト

#### 課題2: リアルタイムシステムの運用の大変さ

## 今後の発展の方向性

### 報酬の強化(報酬関数の定義の見直し、拡張みたいな話!)

### Contextual Bandit
