## link ãƒªãƒ³ã‚¯

https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6
https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6

# Topic Modeling with BERT: Leveraging BERT and TF-IDF to create easily interpretable topics. BERT ã«ã‚ˆã‚‹ãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ï¼šBERT ã¨ TF-IDF ã‚’æ´»ç”¨ã—ã¦ã€è§£é‡ˆã—ã‚„ã™ã„ãƒˆãƒ”ãƒƒã‚¯ã‚’ä½œæˆã™ã‚‹ã€‚

Often when I am approached by a product owner to do some NLP-based analyses, I am typically asked the following question:
ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒ»ã‚ªãƒ¼ãƒŠãƒ¼ã‹ã‚‰NLPã«åŸºã¥ãåˆ†æã‚’ä¾é ¼ã•ã‚Œã‚‹ã¨ã€ã—ã°ã—ã°æ¬¡ã®ã‚ˆã†ãªè³ªå•ã‚’ã•ã‚Œã‚‹ï¼š

â€˜Which topic can frequently be found in these documents?â€™
ã“ã‚Œã‚‰ã®æ–‡æ›¸ã«ã¯ã©ã®ãƒˆãƒ”ãƒƒã‚¯ãŒé »ç¹ã«è¦‹ã‚‰ã‚Œã‚‹ã‹ï¼Ÿ

Void of any categories or labels I am forced to look into unsupervised techniques to extract these topics, namely Topic Modeling.
ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚„ãƒ©ãƒ™ãƒ«ãŒãªã„ãŸã‚ã€ã“ã‚Œã‚‰ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã«æ•™å¸«ãªã—æŠ€è¡“ã€ã¤ã¾ã‚Šãƒˆãƒ”ãƒƒã‚¯ãƒ»ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ç›®ã‚’å‘ã‘ã–ã‚‹ã‚’å¾—ãªã„ã€‚

Although topic models such as LDA and NMF have shown to be good starting points, I always felt it took quite some effort through hyperparameter tuning to create meaningful topics.
LDAã‚„NMFã®ã‚ˆã†ãªãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã¯è‰¯ã„å‡ºç™ºç‚¹ã§ã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ãŒã€æ„å‘³ã®ã‚ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã¯ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é€šã—ã¦ã‹ãªã‚Šã®åŠªåŠ›ãŒå¿…è¦ã ã¨ç§ã¯ã„ã¤ã‚‚æ„Ÿã˜ã¦ã„ãŸã€‚

Moreover, I wanted to use transformer-based models such as BERT as they have shown amazing results in various NLP tasks over the last few years.
ã•ã‚‰ã«ã€BERTã®ã‚ˆã†ãªå¤‰æ›å™¨ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ã“ã“æ•°å¹´ã€ã•ã¾ã–ã¾ãªè‡ªç„¶è¨€èªå‡¦ç†ã‚¿ã‚¹ã‚¯ã§ç´ æ™´ã‚‰ã—ã„çµæœã‚’ç¤ºã—ã¦ã„ã‚‹ã®ã§ã€ãã‚Œã‚’ä½¿ã„ãŸã‹ã£ãŸã€‚
Pre-trained models are especially helpful as they are supposed to contain more accurate representations of words and sentences.
äº‹å‰ã«è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã€å˜èªã‚„æ–‡ç« ã‚’ã‚ˆã‚Šæ­£ç¢ºã«è¡¨ç¾ã—ã¦ã„ã‚‹ãŸã‚ã€ç‰¹ã«æœ‰ç”¨ã§ã‚ã‚‹ã€‚

A few weeks ago I saw this great project named Top2Vec\* which leveraged document- and word embeddings to create topics that were easily interpretable.
æ•°é€±é–“å‰ã€Top2Vec*ã¨ã„ã†ç´ æ™´ã‚‰ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’è¦‹ãŸã€‚ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€æ–‡æ›¸ã¨å˜èªã®åŸ‹ã‚è¾¼ã¿ã‚’æ´»ç”¨ã—ã¦ã€è§£é‡ˆã—ã‚„ã™ã„ãƒˆãƒ”ãƒƒã‚¯ã‚’ä½œæˆã—ã¦ã„ãŸã€‚
I started looking at the code to generalize Top2Vec such that it could be used with pre-trained transformer models.
ç§ã¯ã€Top2Vecã‚’ä¸€èˆ¬åŒ–ã—ã€è¨“ç·´æ¸ˆã¿ã®å¤‰æ›ãƒ¢ãƒ‡ãƒ«ã§ä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰ã‚’èª¿ã¹å§‹ã‚ãŸã€‚

The great advantage of Doc2Vec is that the resulting document- and word embeddings are jointly embedding in the same space which allows document embeddings to be represented by nearby word embeddings.
Doc2Vecã®å¤§ããªåˆ©ç‚¹ã¯ã€çµæœã¨ã—ã¦å¾—ã‚‰ã‚Œã‚‹æ–‡æ›¸åŸ‹ã‚è¾¼ã¿ã¨å˜èªåŸ‹ã‚è¾¼ã¿ãŒåŒã˜ç©ºé–“ã«å…±åŒã§åŸ‹ã‚è¾¼ã¾ã‚Œã‚‹ã“ã¨ã§ã‚ã‚Šã€ã“ã‚Œã«ã‚ˆã‚Šæ–‡æ›¸åŸ‹ã‚è¾¼ã¿ã‚’è¿‘å‚ã®å˜èªåŸ‹ã‚è¾¼ã¿ã§è¡¨ç¾ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚
Unfortunately, this proved to be difficult as BERT embeddings are token-based and do not necessarily occupy the same space\*\*.
æ®‹å¿µãªãŒã‚‰ã€BERTåŸ‹ã‚è¾¼ã¿ã¯ãƒˆãƒ¼ã‚¯ãƒ³ãƒ»ãƒ™ãƒ¼ã‚¹ã§ã‚ã‚Šã€å¿…ãšã—ã‚‚åŒã˜ç©ºé–“**ã‚’å æœ‰ã—ãªã„ãŸã‚ã€ã“ã‚Œã¯å›°é›£ã§ã‚ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ãŸã€‚

Instead, I decided to come up with a different algorithm that could use BERT and ğŸ¤— transformers embeddings.
ãã®ä»£ã‚ã‚Šã«ã€ç§ã¯BERTã¨ğŸ¤—å¤‰æ›åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã§ãã‚‹åˆ¥ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è€ƒãˆå‡ºã™ã“ã¨ã«ã—ãŸã€‚
The result is BERTopic, an algorithm for generating topics using state-of-the-art embeddings.
ãã®çµæœãŒBERTopicã§ã‚ã‚Šã€æœ€å…ˆç«¯ã®åŸ‹ã‚è¾¼ã¿ã‚’ç”¨ã„ã¦ãƒˆãƒ”ãƒƒã‚¯ã‚’ç”Ÿæˆã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚ã‚‹ã€‚

The main topic of this article will not be the use of BERTopic but a tutorial on how to use BERT to create your own topic model.
ã“ã®è¨˜äº‹ã®ä¸»ãªãƒˆãƒ”ãƒƒã‚¯ã¯ã€BERTopic ã®ä½¿ç”¨ã§ã¯ãªãã€BERT ã‚’ä½¿ç”¨ã—ã¦ç‹¬è‡ªã®ãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹æ–¹æ³•ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã‚ã‚‹ã€‚

PAPER\*: Angelov, D.(2020).
PAPER*ï¼š Angelov, D.(2020).
Top2Vec: Distributed Representations of Topics.
Top2Vecï¼š ãƒˆãƒ”ãƒƒã‚¯ã®åˆ†æ•£è¡¨ç¾
arXiv preprint arXiv:2008.09470.
arXiv preprint arXiv:2008.09470.

NOTE\*\*: Although you could have them occupy the same space, the resulting size of the word embeddings is quite large due to the contextual nature of BERT.
NOTE* åŒã˜ç©ºé–“ã‚’å æœ‰ã•ã›ã‚‹ã“ã¨ã‚‚ã§ãã‚‹ãŒã€BERTã®æ–‡è„ˆçš„æ€§è³ªã®ãŸã‚ã€çµæœã¨ã—ã¦å˜èªåŸ‹ã‚è¾¼ã¿ ã®ã‚µã‚¤ã‚ºã¯ã‹ãªã‚Šå¤§ãããªã‚‹ã€‚
Moreover, there is a chance that the resulting sentence- or document embeddings will degrade in quality.
ã•ã‚‰ã«ã€çµæœã¨ã—ã¦å¾—ã‚‰ã‚Œã‚‹æ–‡åŸ‹ã‚è¾¼ã¿ã‚„æ–‡æ›¸åŸ‹ã‚è¾¼ã¿ã®å“è³ªãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ã‚‚ã‚ã‚‹ã€‚

## Data & Packages ãƒ‡ãƒ¼ã‚¿ï¼†ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸

For this example, we use the famous 20 Newsgroups dataset which contains roughly 18000 newsgroups posts on 20 topics.
ã“ã®ä¾‹ã§ã¯ã€20ã®ãƒˆãƒ”ãƒƒã‚¯ã«é–¢ã™ã‚‹ç´„18000ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã®æŠ•ç¨¿ã‚’å«ã‚€æœ‰åãª20 Newsgroupsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã€‚
Using Scikit-Learn, we can quickly download and prepare the data:
Scikit-Learnã‚’ä½¿ãˆã°ã€ãƒ‡ãƒ¼ã‚¿ã‚’ç´ æ—©ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦æº–å‚™ã§ãã‚‹ï¼š

If you want to speed up training, you can select the subset train as it will decrease the number of posts you extract.
å­¦ç¿’é€Ÿåº¦ã‚’ä¸Šã’ãŸã„å ´åˆã¯ã€ã‚µãƒ–ã‚»ãƒƒãƒˆå­¦ç¿’ã‚’é¸æŠã™ã‚‹ã¨ã€æŠ½å‡ºã™ã‚‹æŠ•ç¨¿ã®æ•°ãŒæ¸›ã‚Šã¾ã™ã€‚

NOTE: If you want to apply topic modeling not on the entire document but on the paragraph level, I would suggest splitting your data before creating the embeddings.
æ³¨ï¼šæ–‡æ›¸å…¨ä½“ã§ã¯ãªãã€æ®µè½ãƒ¬ãƒ™ãƒ«ã§ãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’é©ç”¨ã—ãŸã„å ´åˆã¯ã€åŸ‹ã‚è¾¼ã¿ã‚’ä½œæˆã™ã‚‹å‰ã«ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

## Embeddings ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°

The very first step we have to do is converting the documents to numerical data.
æœ€åˆã«ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã®ã¯ã€æ–‡æ›¸ã‚’æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã™ã‚‹ã“ã¨ã ã€‚
We use BERT for this purpose as it extracts different embeddings based on the context of the word.
ã“ã®ç›®çš„ã®ãŸã‚ã«ã€å˜èªã®æ–‡è„ˆã«åŸºã¥ã„ã¦ç•°ãªã‚‹åŸ‹ã‚è¾¼ã¿ã‚’æŠ½å‡ºã™ã‚‹BERTã‚’ä½¿ç”¨ã™ã‚‹ã€‚
Not only that, there are many pre-trained models available ready to be used.
ãã‚Œã ã‘ã§ãªãã€å¤šãã®è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒç”¨æ„ã•ã‚Œã¦ãŠã‚Šã€ã™ãã«åˆ©ç”¨ã§ãã‚‹ã€‚

How you generate the BERT embeddings for a document is up to you.
æ–‡æ›¸ã® BERT åŸ‹ã‚è¾¼ã¿ã‚’ã©ã®ã‚ˆã†ã«ç”Ÿæˆã™ã‚‹ã‹ã¯ã€ã‚ãªãŸæ¬¡ç¬¬ã§ã™ã€‚
However, I prefer to use the sentence-transformers package as the resulting embeddings have shown to be of high quality and typically work quite well for document-level embeddings.
ã—ã‹ã—ã€ç§ã¯æ–‡å¤‰æ›ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ä½¿ã†ã“ã¨ã‚’å¥½ã‚€ã€‚ãªãœãªã‚‰ã€çµæœã¨ã—ã¦å¾—ã‚‰ã‚Œã‚‹åŸ‹ã‚è¾¼ã¿ã¯é«˜å“è³ªã§ã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ãŠã‚Šã€ä¸€èˆ¬çš„ã«æ–‡æ›¸ãƒ¬ãƒ™ãƒ«ã®åŸ‹ã‚è¾¼ã¿ã§ã¯éå¸¸ã«ã†ã¾ãæ©Ÿèƒ½ã™ã‚‹ã‹ã‚‰ã§ã‚ã‚‹ã€‚

Install the package with pip install sentence-transformers before generating the document embeddings.
æ–‡æ›¸åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã™ã‚‹å‰ã«ã€pip install sentence-transformersã§ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚
If you run into issues installing this package, then it is worth installing Pytorch first.
ã“ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€ã¾ãšPytorchã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ä¾¡å€¤ãŒã‚ã‚‹ã€‚

Then, run the following code to transform your documents in 512-dimensional vectors:
æ¬¡ã«ã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ã€æ–‡æ›¸ã‚’512æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹ï¼š

We are using Distilbert as it gives a nice balance between speed and performance.
ç§ãŸã¡ã¯ã€ã‚¹ãƒ”ãƒ¼ãƒ‰ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„Distilbertã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
The package has several multi-lingual models available for you to use.
ã“ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã«ã¯ã€ã„ãã¤ã‹ã®å¤šè¨€èªãƒ¢ãƒ‡ãƒ«ãŒç”¨æ„ã•ã‚Œã¦ã„ã‚‹ã€‚

NOTE: Since transformer models have a token limit, you might run into some errors when inputting large documents.
æ³¨ï¼šãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ»ãƒ¢ãƒ‡ãƒ«ã«ã¯ãƒˆãƒ¼ã‚¯ãƒ³ã®åˆ¶é™ãŒã‚ã‚‹ãŸã‚ã€å¤§ããªæ–‡æ›¸ã‚’å…¥åŠ›ã™ã‚‹ã¨ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚
In that case, you could consider splitting documents into paragraphs.
ãã®å ´åˆã€æ–‡æ›¸ã‚’æ®µè½ã”ã¨ã«åˆ†å‰²ã™ã‚‹ã“ã¨ã‚‚è€ƒãˆã‚‰ã‚Œã‚‹ã€‚

## Clustering ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°

We want to make sure that documents with similar topics are clustered together such that we can find the topics within these clusters.
ä¼¼ãŸã‚ˆã†ãªãƒˆãƒ”ãƒƒã‚¯ã‚’æŒã¤æ–‡æ›¸ãŒã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åŒ–ã•ã‚Œã€ãã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ã—ãŸã„ã€‚
Before doing so, we first need to lower the dimensionality of the embeddings as many clustering algorithms handle high dimensionality poorly.
ãã®å‰ã«ã€å¤šãã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ»ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒé«˜æ¬¡å…ƒã‚’ã†ã¾ãæ‰±ãˆãªã„ãŸã‚ã€åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã®æ¬¡å…ƒã‚’ä¸‹ã’ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

UMAP
UMAP

Out of the few dimensionality reduction algorithms, UMAP is arguably the best performing as it keeps a significant portion of the high-dimensional local structure in lower dimensionality.
æ•°å°‘ãªã„æ¬¡å…ƒå‰Šæ¸›ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ä¸­ã§ã€UMAPã¯ã€é«˜æ¬¡å…ƒã®å±€æ‰€æ§‹é€ ã®ã‹ãªã‚Šã®éƒ¨åˆ†ã‚’ä½æ¬¡å…ƒã«ä¿ã¤ã®ã§ã€é–“é•ã„ãªãæœ€ã‚‚æ€§èƒ½ãŒè‰¯ã„ã€‚

Install the package with pip install umap-learn before we lower the dimensionality of the document embeddings.
æ–‡æ›¸ã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã‚’ä¸‹ã’ã‚‹å‰ã«ã€pip install umap-learnã§ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã€‚
We reduce the dimensionality to 5 while keeping the size of the local neighborhood at 15.
å±€æ‰€è¿‘å‚ã®ã‚µã‚¤ã‚ºã‚’15ã«ä¿ã¡ãªãŒã‚‰ã€æ¬¡å…ƒã‚’5ã«æ¸›ã‚‰ã™ã€‚
You can play around with these values to optimize for your topic creation.
ãƒˆãƒ”ãƒƒã‚¯ä½œæˆã«æœ€é©åŒ–ã™ã‚‹ãŸã‚ã«ã€ã“ã‚Œã‚‰ã®å€¤ã‚’å¼„ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
Note that a too low dimensionality results in a loss of information while a too high dimensionality results in poorer clustering results.
æ¬¡å…ƒæ•°ãŒä½ã™ãã‚‹ã¨æƒ…å ±ãŒå¤±ã‚ã‚Œã€æ¬¡å…ƒæ•°ãŒé«˜ã™ãã‚‹ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœãŒæ‚ªããªã‚‹ã“ã¨ã«æ³¨æ„ã€‚

HDBSAN
HDBSAN

After having reduced the dimensionality of the documents embeddings to 5, we can cluster the documents with HDBSCAN.
åŸ‹ã‚è¾¼ã¿æ–‡æ›¸ã®æ¬¡å…ƒæ•°ã‚’5ã¾ã§æ¸›ã‚‰ã—ãŸå¾Œã€HDBSCANã§æ–‡æ›¸ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
HDBSCAN is a density-based algorithm that works quite well with UMAP since UMAP maintains a lot of local structure even in lower-dimensional space.
HDBSCANã¯å¯†åº¦ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚ã‚Šã€UMAPã¨éå¸¸ã«ç›¸æ€§ãŒè‰¯ã„ã€‚UMAPã¯ä½æ¬¡å…ƒç©ºé–“ã§ã‚‚å¤šãã®å±€æ‰€æ§‹é€ ã‚’ç¶­æŒã™ã‚‹ã‹ã‚‰ã ã€‚
Moreover, HDBSCAN does not force data points to clusters as it considers them outliers.
ã•ã‚‰ã«ã€HDBSCANã¯ãƒ‡ãƒ¼ã‚¿ç‚¹ã‚’å¤–ã‚Œå€¤ã¨ã¿ãªã—ã¦å¼·åˆ¶çš„ã«ã‚¯ãƒ©ã‚¹ã‚¿ã«å…¥ã‚Œãªã„ã€‚

Install the package with pip install hdbscan then create the clusters:
pip install hdbscanã§ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ä½œæˆã—ã¾ã™ï¼š

Great! We now have clustered similar documents together which should represent the topics that they consist of.
ç´ æ™´ã‚‰ã—ã„ï¼ã“ã‚Œã§ã€ä¼¼ãŸã‚ˆã†ãªæ–‡æ›¸ãŒã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åŒ–ã•ã‚Œã€ãã‚Œã‚‰ãŒæ§‹æˆã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚’è¡¨ã™ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚
To visualize the resulting clusters we can further reduce the dimensionality to 2 and visualize the outliers as grey points:
å¾—ã‚‰ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’è¦–è¦šåŒ–ã™ã‚‹ãŸã‚ã«ã€ã•ã‚‰ã«æ¬¡å…ƒã‚’2ã«æ¸›ã‚‰ã—ã€ç•°å¸¸å€¤ã‚’ã‚°ãƒ¬ãƒ¼ã®ç‚¹ã¨ã—ã¦è¦–è¦šåŒ–ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼š

It is difficult to visualize the individual clusters due to the number of topics generated (~55).
ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯ã®æ•°ï¼ˆï½55ï¼‰ãŒå¤šã„ãŸã‚ã€å€‹ã€…ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’å¯è¦–åŒ–ã™ã‚‹ã®ã¯é›£ã—ã„ã€‚
However, we can see that even in 2-dimensional space some local structure is kept.
ã—ã‹ã—ã€2æ¬¡å…ƒç©ºé–“ã§ã‚‚å±€æ‰€çš„ãªæ§‹é€ ãŒä¿ãŸã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚

NOTE: You could skip the dimensionality reduction step if you use a clustering algorithm that can handle high dimensionality like a cosine-based k-Means.
æ³¨æ„: ä½™å¼¦ãƒ™ãƒ¼ã‚¹ã®k-Meansã®ã‚ˆã†ãªé«˜æ¬¡å…ƒã‚’æ‰±ãˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€æ¬¡å…ƒå‰Šæ¸›ã‚¹ãƒ†ãƒƒãƒ—ã‚’çœç•¥ã§ãã¾ã™ã€‚

## Topic Creation ãƒˆãƒ”ãƒƒã‚¯ä½œæˆ

What we want to know from the clusters that we generated, is what makes one cluster, based on their content, different from another?
ç”Ÿæˆã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰ç§ãŸã¡ãŒçŸ¥ã‚ŠãŸã„ã®ã¯ã€ãã®å†…å®¹ã«åŸºã¥ã„ã¦ã€ã‚ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒä»–ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¨ä½•ãŒé•ã†ã®ã‹ã€ã¨ã„ã†ã“ã¨ã ã€‚

How can we derive topics from clustered documents?
ã‚¯ãƒ©ã‚¹ã‚¿åŒ–ã•ã‚ŒãŸæ–‡æ›¸ã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯ã‚’å°ãå‡ºã™ã«ã¯ï¼Ÿ

To solve this, I came up with a class-based variant of TF-IDF (c-TF-IDF), that would allow me to extract what makes each set of documents unique compared to the other.
ã“ã‚Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€ç§ã¯TF-IDFã®ã‚¯ãƒ©ã‚¹ãƒ™ãƒ¼ã‚¹ã®å¤‰å½¢ï¼ˆc-TF-IDFï¼‰ã‚’è€ƒãˆå‡ºã—ãŸã€‚

The intuition behind the method is as follows.
ã“ã®æ–¹æ³•ã®èƒŒå¾Œã«ã‚ã‚‹ç›´è¦³ã¯æ¬¡ã®ã‚ˆã†ãªã‚‚ã®ã ã€‚
When you apply TF-IDF as usual on a set of documents, what you are basically doing is comparing the importance of words between documents.
æ–‡æ›¸é›†åˆã«å¯¾ã—ã¦TF-IDFã‚’é€šå¸¸é€šã‚Šé©ç”¨ã™ã‚‹å ´åˆã€åŸºæœ¬çš„ã«è¡Œã£ã¦ã„ã‚‹ã“ã¨ã¯ã€æ–‡æ›¸é–“ã®å˜èªã®é‡è¦åº¦ã‚’æ¯”è¼ƒã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚

What if, we instead treat all documents in a single category (e.g., a cluster) as a single document and then apply TF-IDF? The result would be a very long document per category and the resulting TF-IDF score would demonstrate the important words in a topic.
ãã®ä»£ã‚ã‚Šã«ã€1ã¤ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãªã©ï¼‰ã«å«ã¾ã‚Œã‚‹ã™ã¹ã¦ã®æ–‡æ›¸ã‚’1ã¤ã®æ–‡æ›¸ã¨ã—ã¦æ‰±ã„ã€TF-IDFã‚’é©ç”¨ã—ãŸã‚‰ã©ã†ã ã‚ã†ã‹ã€‚ãã®çµæœã€ã‚«ãƒ†ã‚´ãƒªãƒ¼ã”ã¨ã«éå¸¸ã«é•·ã„æ–‡æ›¸ãŒã§ãã€ãã®çµæœå¾—ã‚‰ã‚Œã‚‹TF-IDFã‚¹ã‚³ã‚¢ã¯ã€ãƒˆãƒ”ãƒƒã‚¯ã«ãŠã‘ã‚‹é‡è¦ãªå˜èªã‚’ç¤ºã™ã“ã¨ã«ãªã‚‹ã ã‚ã†ã€‚

c-TF-IDF
c-TF-IDF

To create this class-based TF-IDF score, we need to first create a single document for each cluster of documents:
ã“ã®ã‚¯ãƒ©ã‚¹ãƒ»ãƒ™ãƒ¼ã‚¹ã®TF-IDFã‚¹ã‚³ã‚¢ã‚’ä½œæˆã™ã‚‹ã«ã¯ã€ã¾ãšæ–‡æ›¸ã®ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã«1ã¤ã®æ–‡æ›¸ã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼š

Then, we apply the class-based TF-IDF:
æ¬¡ã«ã€ã‚¯ãƒ©ã‚¹ãƒ™ãƒ¼ã‚¹ã®TF-IDFã‚’é©ç”¨ã™ã‚‹ï¼š

Where the frequency of each word t is extracted for each class i and divided by the total number of words w.
ã“ã“ã§ã€å„å˜èªã®é »åº¦tã¯ã€å„ã‚¯ãƒ©ã‚¹iã«ã¤ã„ã¦æŠ½å‡ºã•ã‚Œã€å˜èªã®ç·æ•°wã§å‰²ã‚‰ã‚Œã‚‹ã€‚
This action can be seen as a form of regularization of frequent words in the class.
ã“ã®å‹•ä½œã¯ã€ã‚¯ãƒ©ã‚¹å†…ã®é »å‡ºå˜èªã‚’è¦å‰‡åŒ–ã™ã‚‹ä¸€å½¢æ…‹ã¨è¦‹ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
Next, the total, unjoined, number of documents m is divided by the total frequency of word t across all classes n.
æ¬¡ã«ã€çµåˆã•ã‚Œã¦ã„ãªã„ç·æ–‡æ›¸æ•°mã‚’ã€å…¨ã‚¯ãƒ©ã‚¹ã«ã‚ãŸã‚‹å˜èªtã®ç·é »åº¦nã§å‰²ã‚‹ã€‚

Now, we have a single importance value for each word in a cluster which can be used to create the topic.
ã“ã‚Œã§ã€ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®å„å˜èªã«å¯¾ã—ã¦ã€ãƒˆãƒ”ãƒƒã‚¯ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã‚‹å˜ä¸€ã®é‡è¦åº¦å€¤ãŒå¾—ã‚‰ã‚ŒãŸã€‚
If we take the top 10 most important words in each cluster, then we would get a good representation of a cluster, and thereby a topic.
å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§æœ€ã‚‚é‡è¦ãªå˜èªã®ä¸Šä½10å€‹ã‚’å–ã‚Šå‡ºã›ã°ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã€ã²ã„ã¦ã¯ãƒˆãƒ”ãƒƒã‚¯ã‚’ã†ã¾ãè¡¨ç¾ã§ãã‚‹ã ã‚ã†ã€‚

Topic Representation
ãƒˆãƒ”ãƒƒã‚¯ã®è¡¨ç¾

In order to create a topic representation, we take the top 20 words per topic based on their c-TF-IDF scores.
ãƒˆãƒ”ãƒƒã‚¯è¡¨ç¾ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã€c-TF-IDFã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦ãƒˆãƒ”ãƒƒã‚¯ã”ã¨ã«ä¸Šä½20èªã‚’æŠ½å‡ºã™ã‚‹ã€‚
The higher the score, the more representative it should be of its topic as the score is a proxy of information density.
ã‚¹ã‚³ã‚¢ãŒé«˜ã‘ã‚Œã°é«˜ã„ã»ã©ã€æƒ…å ±å¯†åº¦ã®ä»£ç”¨ã¨ãªã‚‹ãŸã‚ã€ãã®ãƒˆãƒ”ãƒƒã‚¯ã‚’ã‚ˆã‚Šä»£è¡¨ã—ã¦ã„ã‚‹ã“ã¨ã«ãªã‚‹ã€‚

We can use topic_sizes to view how frequent certain topics are:
topic_sizesã‚’ä½¿ãˆã°ã€ç‰¹å®šã®ãƒˆãƒ”ãƒƒã‚¯ãŒã©ã‚Œãã‚‰ã„ã®é »åº¦ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ã‚’è¦‹ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼š

The topic name-1 refers to all documents that did not have any topics assigned.
ãƒˆãƒ”ãƒƒã‚¯å-1ã¯ã€ãƒˆãƒ”ãƒƒã‚¯ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã„ãªã„ã™ã¹ã¦ã®æ–‡æ›¸ã‚’æŒ‡ã™ã€‚
The great thing about HDBSCAN is that not all documents are forced towards a certain cluster.
HDBSCANã®ç´ æ™´ã‚‰ã—ã„ã¨ã“ã‚ã¯ã€ã™ã¹ã¦ã®æ–‡æ›¸ãŒç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã‚¿ã«å¼·åˆ¶ã•ã‚Œã‚‹ã‚ã‘ã§ã¯ãªã„ã“ã¨ã ã€‚
If no cluster could be found, then it is simply an outlier.
ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã¯ã€å˜ãªã‚‹å¤–ã‚Œå€¤ã§ã‚ã‚‹ã€‚

We can see that topics 7, 43, 12, and 41 are the largest clusters that we could create.
ãƒˆãƒ”ãƒƒã‚¯7ã€43ã€12ã€41ãŒã€ä½œæˆå¯èƒ½ãªæœ€å¤§ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
To view the words belonging to those topics, we can simply use the dictionarytop_n_words to access these topics:
ã“ã‚Œã‚‰ã®ãƒˆãƒ”ãƒƒã‚¯ã«å±ã™ã‚‹å˜èªã‚’è¡¨ç¤ºã™ã‚‹ã«ã¯ã€å˜ã«è¾æ›¸top_n_wordsã‚’ä½¿ã£ã¦ã“ã‚Œã‚‰ã®ãƒˆãƒ”ãƒƒã‚¯ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚Œã°ã‚ˆã„ï¼š

Looking at the largest four topics, I would say that these nicely seem to represent easily interpretable topics!
æœ€å¤§ã®4ã¤ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’è¦‹ã‚‹ã¨ã€ã“ã‚Œã‚‰ã¯ã†ã¾ãè§£é‡ˆã—ã‚„ã™ã„ãƒˆãƒ”ãƒƒã‚¯ã‚’è¡¨ã—ã¦ã„ã‚‹ã‚ˆã†ã«æ€ãˆã‚‹ï¼

I can see sports, computers, space, and religion as clear topics that were extracted from the data.
ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸæ˜ç¢ºãªãƒˆãƒ”ãƒƒã‚¯ã¨ã—ã¦ã€ã‚¹ãƒãƒ¼ãƒ„ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã€å®‡å®™ã€å®—æ•™ã‚’æŒ™ã’ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

Topic Reduction
ãƒˆãƒ”ãƒƒã‚¯å‰Šæ¸›

There is a chance that, depending on the dataset, you will get hundreds of topics that were created! You can tweak the parameters of HDBSCAN such that you will get fewer topics through its min_cluster_size parameter but it does not allow you to specify the exact number of clusters.
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã£ã¦ã¯ã€ä½•ç™¾ã‚‚ã®ãƒˆãƒ”ãƒƒã‚¯ãŒä½œæˆã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼HDBSCANã®min_cluster_sizeãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒˆãƒ”ãƒƒã‚¯ã‚’å°‘ãªãã™ã‚‹ã‚ˆã†ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã™ãŒã€æ­£ç¢ºãªã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚

A nifty trick that Top2Vec was using is the ability to reduce the number of topics by merging the topic vectors that were most similar to each other.
Top2VecãŒä½¿ã£ã¦ã„ãŸæ°—ã®åˆ©ã„ãŸãƒˆãƒªãƒƒã‚¯ã¯ã€äº’ã„ã«æœ€ã‚‚ä¼¼ã¦ã„ã‚‹ãƒˆãƒ”ãƒƒã‚¯ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ãƒˆãƒ”ãƒƒã‚¯æ•°ã‚’æ¸›ã‚‰ã™æ©Ÿèƒ½ã§ã‚ã‚‹ã€‚

We can use a similar technique by comparing the c-TF-IDF vectors among topics, merge the most similar ones, and finally re-calculate the c-TF-IDF vectors to update the representation of our topics:
ãƒˆãƒ”ãƒƒã‚¯é–“ã§c-TF-IDFãƒ™ã‚¯ãƒˆãƒ«ã‚’æ¯”è¼ƒã—ã€æœ€ã‚‚é¡ä¼¼ã—ã¦ã„ã‚‹ã‚‚ã®ã‚’ãƒãƒ¼ã‚¸ã—ã€æœ€å¾Œã«c-TF-IDFãƒ™ã‚¯ãƒˆãƒ«ã‚’å†è¨ˆç®—ã—ã¦ãƒˆãƒ”ãƒƒã‚¯ã®è¡¨ç¾ã‚’æ›´æ–°ã™ã‚‹ã“ã¨ã§ã€åŒæ§˜ã®æ‰‹æ³•ã‚’ä½¿ã†ã“ã¨ãŒã§ãã‚‹ï¼š

Above, we took the least common topic and merged it with the most similar topic.
ä¸Šè¨˜ã§ã¯ã€æœ€ã‚‚ä¸€èˆ¬çš„ã§ãªã„ãƒˆãƒ”ãƒƒã‚¯ã‚’å–ã‚Šä¸Šã’ã€æœ€ã‚‚é¡ä¼¼ã—ãŸãƒˆãƒ”ãƒƒã‚¯ã¨ãƒãƒ¼ã‚¸ã—ãŸã€‚
By repeating this 19 more times we reduced the number of topics from 56 to 36!
ã“ã‚Œã‚’ã•ã‚‰ã«19å›ç¹°ã‚Šè¿”ã™ã“ã¨ã§ã€ãƒˆãƒ”ãƒƒã‚¯ã®æ•°ã‚’56ã‹ã‚‰36ã«æ¸›ã‚‰ã—ãŸï¼

NOTE: We can skip the re-calculation part of this pipeline to speed up the topic reduction step.
æ³¨ï¼šãƒˆãƒ”ãƒƒã‚¯å‰Šæ¸›ã‚¹ãƒ†ãƒƒãƒ—ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã«ã€ã“ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å†è¨ˆç®—éƒ¨åˆ†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
However, it is more accurate to re-calculate the c-TF-IDF vectors as that would better represent the newly generated content of the topics.
ã—ã‹ã—ã€c-TF-IDFãƒ™ã‚¯ãƒˆãƒ«ã‚’å†è¨ˆç®—ã™ã‚‹æ–¹ãŒã€æ–°ã—ãç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯ã®å†…å®¹ã‚’ã‚ˆã‚Šæ­£ç¢ºã«è¡¨ã™ã“ã¨ãŒã§ãã‚‹ã€‚
You can play around with this by, for example, update every n steps to both speed-up the process and still have good topic representations.
ä¾‹ãˆã°ã€nã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ›´æ–°ã™ã‚‹ã“ã¨ã§ã€å‡¦ç†ã‚’é«˜é€ŸåŒ–ã—ã€ã‹ã¤ãƒˆãƒ”ãƒƒã‚¯ã‚’é©åˆ‡ã«è¡¨ç¾ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

TIP: You can use the method described in this article (or simply use BERTopic) to also create sentence-level embeddings.
ãƒ’ãƒ³ãƒˆï¼šã“ã®è¨˜äº‹ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹æ–¹æ³•ï¼ˆã¾ãŸã¯å˜ã«BERTopicã‚’ä½¿ç”¨ã™ã‚‹ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€æ–‡ãƒ¬ãƒ™ãƒ«ã®åŸ‹ã‚è¾¼ã¿ã‚‚ä½œæˆã§ãã¾ã™ã€‚
The main advantage of this is the possibility to view the distribution of topics within a single document.
ã“ã®ä¸»ãªåˆ©ç‚¹ã¯ã€1ã¤ã®æ–‡æ›¸å†…ã®ãƒˆãƒ”ãƒƒã‚¯ã®åˆ†å¸ƒã‚’è¦‹ã‚‹ã“ã¨ãŒã§ãã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚

Thank you for reading!
èª­ã‚“ã§ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ï¼

If you are, like me, passionate about AI, Data Science, or Psychology, please feel free to add me on LinkedIn or follow me on Twitter.
ç§ã®ã‚ˆã†ã«AIã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã€å¿ƒç†å­¦ã«æƒ…ç†±ã‚’æŒã£ã¦ã„ã‚‹æ–¹ã¯ã€ãŠæ°—è»½ã«LinkedInã§ç§ã‚’è¿½åŠ ã™ã‚‹ã‹ã€Twitterã§ç§ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ã—ã¦ãã ã•ã„ã€‚

All examples and code in this article can be found here:
ã“ã®è¨˜äº‹ã®ã™ã¹ã¦ã®ä¾‹ã¨ã‚³ãƒ¼ãƒ‰ã¯ã“ã“ã«ã‚ã‚‹ï¼š