---
format:
  revealjs:
    # incremental: false
    # theme: [default, custom_lab.scss]
    theme: [default, custom_lab.scss]
    slide-number: true
    logo: https://s3-ap-northeast-1.amazonaws.com/qiita-image-store/0/1697279/dfa905d1c1e242b4e39be182ae21a2b6ac72c0ad/large.png?1655951919
    footer: ⇒ [https://qiita.com/morinota](https://qiita.com/morinota)
from: markdown+emoji

fig-cap-location: bottom

title: Sequential な推薦モデルやSentence BERT等の論文を理解する為に、遅ればせながら(?) Attention Is All You Needを読んだ.
subtitle: n週連続 推薦システム系論文読んだシリーズ 19週目
date: 2023/06/28
author: morinota
title-slide-attributes:
  #   data-background-image: https://i.imgur.com/nTazczH.png
  data-background-size: contain
  data-background-opacity: "0.5"
---

## ざっくり論文概要

- hogehoge

## この論文を読むモチベーション

- hogehoge

# Transformerの各パーツを確認していく.

## Attention function

- hogehoge

## Attention functionのお気持ち実装



## Scaled-dot-product Attention

## Scaled-dot-product Attention のお気持ち実装

## Multi-head Attention

## Multi-head Attention のお気持ち実装

## Positional Encoder

## Positional Encoder のお気持ち実装

# Transformer を組み立てていく

## Transformer Encoder

## Transformer Decoder

## Tranformer のお気持ち実装
