# 1. ラベルが0 or 1の2値の時

## 1.1. MeanReciprocal Rank(MRR)

まずReciprocal Rank(RR)をまとめる。

...といっても非常に単純な評価指標で、「検索結果を上から見ていって最初に○(=1)の文書が現れた順位が$r$のとき、$RR=\frac{1}{r}$」と定義する。

例えば以下のケースでは、検索結果の2位に最初の$y_{q, d} = 1$の文書が現れるため、$RR=\frac{1}{2}$となる。
![](https://cdn-ak.f.st-hatena.com/images/fotolife/s/sz_dr/20181205/20181205233131.png)

Reciprocal Rankは**最初に○(=1)の文書が現れた順位しか気にしない**ので、**他の○の文書の順位はReciprocal Rankの値に影響しない**。
なので、以下のような**奇妙な評価**を行うときがある。

| 順位                    | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 
|-----------------------|---|---|---|---|---|---|---|---|---|----| 
| ランキングモデルAで並べた際の文書のラベル | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1  | 
| ランキングモデルBで並べた際の文書のラベル | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0  | 

ランキングモデルBはAよりも○(=1)の文書を検索結果上位に集めているので、直感的にはランキングモデルBはAよりも優れているはず。
一方で、どちらのランキングモデルも1位に○(=1)の文書が現れているため、共にRR=1となってしまう。

もちろん、検索サービスによってはReciprocal Rankによる評価が正しいケースもあるとは思うが、本当にReciprocal Rankで評価して良いのかは検討した方が良い。

さて、Mean Reciprocal Rank (MRR) ですが、これは単純に「検索キーワード」毎に求められるReciprocal Rankの値を平均した値となる。

## 1.2. Mean Average Precision(MAP)
「Precision@k」→「Average Precision」→「Mean Average Precision」の順にまとめていく。

### Precision@k (P@K)

「検索結果のk番目まで見た時のPrecision」を意味する。

Precisionは、検索結果m個のうちn個実際に○だった場合は、$P@m = \frac{n}{m}$

### Average Precision (AP@K)

「○(=1)の文書が現れる各順位について、Precision@kの値の平均を取る」

$$
AP@K = \sum_{k=1:kが正解のみ}^{K}{P_k}
$$

### Mean Average Precision (MAP@K)

検索キーワード」毎に求められるAverage Precisionの値を平均した値。

$$
MAP@K = \frac{1}{N} \sum_{i=1}^{N}{
    \frac{1}{\min{m_i, K}} AP@K_i
}
$$

# 2. ラベルが多クラスの時

## 2.1. Normalized Discounted Cumulative Gain(NDCG)

# 3. 参考

- https://adventar.org/calendars/3357
- https://www.szdrblog.info/entry/2018/12/06/010959
- レコメンド指標でよく見るMAP@kとは
  - https://qiita.com/tetsuro731/items/718bef2a667e0994eddd
