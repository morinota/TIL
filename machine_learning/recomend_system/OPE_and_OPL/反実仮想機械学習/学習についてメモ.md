## 0章の教師あり学習の基礎

- 教師あり学習では、**同時分布 $p(x, y)$ に従う**特徴量(feature)と呼ばれる多次元確率変数 $x$ と、目的変数(outcome)と呼ばれる確率変数 $y$ の間の関係性を、データに基づいて明らかにしたり予測したりすることが目標。
- そのために、ある特徴量 $x$ が与えられた時に、それに対応する目的変数 $y$ を精度よく予測できる予測関数 $f$ を見つけ出すことを考える。
- より具体的には、次のように定義される期待リスク(expected risk)などによって予測関数 f の正確さを定量化し、それを最適化することを通じて、性格や予測関数$f$を得ることを考える。

### 期待リスク

$$
L(f) := E_{p(x,y)}[l(y, f(x))]
$$

ここで、$l(y, f(x))$ は、予測誤差(yとf(x)の乖離度合い)を評価するための損失関数(loss function)。
期待リスクを最小化する予測関数を、最適な予測関数とよび、$f^* := \argmin_{f} L(f)$ と表記する。

最もよく用いられる損失関数 $l$ の一つに、二乗誤差(squared loss) $l(a,b) = (a - b)^2$ がある。この場合の期待リスクは、次のように表される。

$$
L(f) := E_{p(x,y)}[(y - f(x))^2]
$$

この二乗誤差に基づく期待リスクを変形していく。
まず2乗の展開を行うと...

$$
= E_{p(x,y)}[y^2 - 2yf(x) + f(x)^2]
$$

続いて、「追加の項を足したり引いたりして、都合のいい形に変形できるようにする」テクニックを使うと...

$$
= E_{p(x,y)}[
    % 元々の式
    y^2 - 2yf(x) + f(x)^2
    % テクニックとして追加した項
    + 2yE_{p(y'|x)}[y'] - 2yE_{p(y'|x)}[y'] % 足し引き0
    + 2E_{p(y'|x)}[y'^2] - 2E_{p(y'|x)}[y'^2] % 足し引き0
]
$$

- 実は、上記の足し引きは**分散分解(バイアス・バリアンス分解)につながる有名なテクニック**らしい
  - 具体的には、$E[(y - f(x))^2] = E[(y - E[y|x])^2] + E[(E[y|x] - f(x))^2]$ みたいな式で、誤差を「真の条件付き期待値からのブレ」と「モデルが真の条件付き期待値からズレてる分」に分解して考える、っていう**お決まりの流れ**があるらしい!!

続いて、いい感じに項を並び替えた上で、期待値の線形性を使って分解すると...

$$
= E_{p(x,y)}[y^2 - 2y E_{p(y'|x)}[y'] + (E_{p(y'|x)}[y'])^2]
\\
+ E_{p(x,y)}[(E_{p(y'|x)}[y'])^2 - 2yf(x) + f(x)^2]
\\
+ E_{p(x,y)}[2yE_{p(y'|x)}[y'] - 2(E_{p(y'|x)}[y'])^2]
$$

続いて、「同時分布に関する期待値」を、「条件付き期待値の期待値」に落とし込むテクニックを使って...

- ここで使ってるテクニック
  - 「同時分布の期待値 = まずはxの期待値、続いてyの条件付き期待値をとる」性質を使ってる...! $E_{p(x,y)}[hoge] = E_{p(x)}[E_{p(y|x)}[hoge]]$ みたいな感じで、「同時分布の期待値」を「条件付き期待値の期待値」に落とし込むテクニックが使われてる...!

$$
% 各項について、同時分布に関する期待値が、「条件付き期待値の期待値」という二重の期待値に変形される
= E_{p(x)}[E_{p(y|x)}[y^2 - 2y E_{p(y'|x)}[y'] + (E_{p(y'|x)}[y'])^2]]
\\
+ E_{p(x)}[E_{p(y|x)}[(E_{p(y'|x)}[y'])^2 - 2yf(x) + f(x)^2]]
\\
+ E_{p(x)}[E_{p(y|x)}[2yE_{p(y'|x)}[y'] - 2(E_{p(y'|x)}[y'])^2]]
$$

続いて、各項の期待値の中身を整理していくと...

$$
= E_{p(x)}[E_{p(y|x)}[(y - E_{p(y'|x)}[y'])^2]] % 期待値の中身を因数分解
\\
+ E_{p(x)}[(E_{p(y|x)}[y'])^2 - 2E_{p(y|x)}[y]f(x) + f(x)^2] % 期待値の線形性を使って、条件付き期待値を展開
\\
+ E_{p(x)}[2(E_{p(y|x)}[y])^2 - 2(E_{p(y|x)}[y'])^2] % 期待値の線形性を使って、条件付き期待値を展開
$$

- ここでやってること
  - 第1項: 期待値の中身を因数分解して、**条件付き分散の定義式**の形に! $Var_{p(y|x)}[y] = E_{p(y|x)}[(y - E_{p(y|x)}[y])^2]$
  - 第2項: 期待値の線形性を使って、条件付き期待値を展開。(第2.2項以外はyにとって定数になるのでスッキリするのか...!:thinking:)
    - ちなみに $E_{p(y|x)}[(E_{p(y|x)}[y'])^2] = (E_{p(y|x)}[y'])^2$ になる点に注意!!**理由を簡単に説明すると、E[定数] = 定数 だから**
      - 中身の $E_{p(y'|x)}[y']$ は、「特徴量xを固定された時の、目的変数yの期待値」のこと。**これはxについての関数になり、yの分布に依存しない定数**である!
      - 定数を二乗しても当然定数！
      - よって、$E_{p(y|x)}[定数] = 定数$ になる!
    - 最終的にこの項は「条件付き期待値 - 予測関数」の二乗に落ち着く!
  - 第3項: 期待値の線形性を使って、条件付き期待値を展開。
    - ちなみに $E_{p(y|x)}[2yE_{p(y'|x)}[y']] = 2(E_{p(y|x)}[y])^2$ になる点に注意!!
      - 期待値の線形性 ($E_{p(x)}[Ax] = A E_{p(x)}[x]$) を使って、$E_{p(y'|x)}[y']$ を定数とみなすと上記の結果になる!!:thinking:
    - ちなみに $E_{p(y|x)}[2(E_{p(y'|x)}[y'])^2] = 2(E_{p(y|x)}[y'])^2$ になるのは、前述と同様に $E[定数] = 定数$ なので!!
    - 最終的に、この項は打ち消されて0になる!!

最後に...

$$
= E_{p(x)}[Var_{p(y|x)}[y]] % xで条件付けたyの条件付き分散
\\
+ E_{p(x)}[(E_{p(y|x)}[y] - f(x))^2] % 予測関数f(x)のバイアス
$$

となり、最終的に期待リスク $L(f)$ は、以下の2つの要素に分解されることがわかる。
(期待リスクの分散の式、完全に理解した!!:thinking:)

1. $E_{p(x)}[Var_{p(y|x)}[y]]$ : 「目的変数 $y$ の条件付き分布 $p(y|x)$ に関する分散(の期待値)」。これは、特徴量xが与えられた時の目的変数yのばらつき度合いであり、**目的変数yの予測における、特徴量xの質・有用さ**を定量化したもの。
2. $E_{p(x)}[(E_{p(y|x)}[y] - f(x))^2]$ : 「予測関数 $f(x)$ と、目的変数の条件付き期待値 $E_{p(y|x)}[y]$ の二乗誤差(の期待値)」。これは、**予測関数f(x)の予測精度**を表す。

期待リスクの要素分解からわかること:

- 期待リスクの1つ目の要素を最小化するには、**より良い（同じ$x$を持つデータの$y$のばらつきが小さい）特徴量 $x$ を構成することが求められる**。
- **期待リスクの1つ目の要素は、予測関数 $f$ に非依存である**。つまり、特徴量 $x$ がひとたび定義されてしまえば、予測関数 $f$ の最適化をいくら頑張ったとしても、この1つ目の要素を小さくすることはできない。
- （損失関数 $l$ が二乗誤差のときの）最適な予測関数は $f^*(x) = E_{p(y|x)}[y]$ であり、この時の期待リスクは $L(f^*) = E_{p(x)}[Var_{p(y|x)}[y]]$ になる。

#### 推薦方策の性能についても、期待リスクの要素分解、みたいな考えを定式化したいメモ

### しかし実は、期待リスクは未知である

仮に真の同時分布 $p(x, y)$ がわかっているのならば、最適な予測関数 $f^*$ はちょっとした計算で明らかになるため、教師あり学習の問題は完璧に解けてしまう。

しかし現実には、**同時分布 $p(x,y)$ は未知の情報**である。

代わりに教師あり学習では、予測関数 $f$ を最適化するための情報としてトレーニングデータと呼ばれるデータセットが与えられる。
トレーニングデータは、同時分布 $p(x,y)$ から独立に同分布に(i.i.d.)サンプリングされたデータであり、特徴量 $x$ と目的変数 $y$ のn個の観測の組であり、$D = {(x_i, y_i)}_{i=1}^{n}$ と表記される

理想的には期待リスク $L(f)$ を最適化することで最適な予測関数 $f^*$ を得たいが、真の同時分布 $p(x,y)$ が未知であるため計算できない。
その代わりにトレーニングデータ $D$ に基づいて、**期待リスクを推定する推定量 $\hat{L}(f;D)$ を構築した上で、この推定量を(仕方なく)最小化する**ことで、より良い予測関数 $\hat{f}^*$ を得る (学習する)、という手順に落ち着く。

## 5章のオフ方策学習のメモ

ログデータに基づいて新たな方策を学習する術を持っていなければ、**たとえ正確なオフ方策評価を行う土壌が整っていたとしても（もちろんそれはそれでとても有用ではあるが）、それだけでは方策の性能改善にはつながらない**。

- OPLとOPEを組み合わせた方策改善プロセス
  - 1. データ収集方策によるログデータ $D$ を収集
    - この時、学習用データ $D_{tr}$ と評価用データ $D_{val}$ に分割
  - 2. オフ方策学習
    - 学習用データ $D_{tr}$ に基づいて、オフ方策学習を行い、新たな方策を学習
    - $\hat{\pi} = \argmax_{\pi} \hat{V}(\pi; D_{tr})$
    - ここでの $\hat{V}(\pi; D)$ は、真の目的関数をログデータのみに基づき近似する推定量。
  - 3. オフ方策評価
    - 評価用データ $D_{te}$ に基づいて、方策の事前性能評価。
      - 学習した方策が大きな失敗を招かないものであるか、最低限、データ収集方策の性能を上回るものであるか、などを確認する。
    - $\hat{V}(\hat{\pi}; D_{val})$
    - ここでの $\hat{V}(\hat{\pi}; D)$ は、学習した方策 $\hat{\pi}$ の性能を評価するための推定量。
  - 4. 実環境への実装やオンライン実験による検証
    - 採用されれば $\hat{\pi}$ は次のデータ収集方策 $\pi_{0}$ になる。

実際にOPLの問題を得際には、（教師あり学習と同様に）方策 $\pi$ を何らかの形で**パラメータ化(parameterize)**した上で、方策の性能 $V(\pi)$ を最大化するようなパラメータを求める問題として実装されることが多い。

$$
\theta^* = \argmax_{\theta} V(\pi_{\theta})
$$

方策のパラメータ化の代表例。あくまで代表例なので、方策をパラメータ化する方法は他にもいくらでもある。
（両方ともソフトマックス関数を使って、行動選択確率を表現してるな...!!まあここは別に、決定的方策でも、epsilon-greedyでもいいはず:thinking:）

- 線形方策: $\pi_{\theta}(a|x) = \frac{exp(\theta_{a}^{T}x)}{\sum_{a' \in A} exp(\theta_{a'}^{T}x)}$
- NN方策: $\pi_{\theta}(a|x) = \frac{exp(NN_{\theta}(x, a))}{\sum_{a' \in A} exp(NN_{\theta}(x, a'))}$

### OPLにおける標準的なアプローチ

3つに分類できる?

- 回帰ベース
- 勾配ベース
- 回帰ベースと勾配ベースを融合した第三のアプローチ

#### 回帰ベースのアプローチ

回帰ベースのアプローチでは、最適方策 $\pi^* := \argmax_{\pi} V(\pi)$ と期待報酬関数 $q(x, a)$ に関する次の関係性に着目している。

$$
% q(x,a) が最大の行動を確率1.0で選択、それ以外を確率0.0で選択する方策
\pi^*(a|x) = \begin{cases}
1 & \text{if } a = \argmax_{a' \in A} q(x, a') \\
0 & \text{otherwise}
\end{cases}
$$

すなわち、最適方策は、特徴量 $x$ が与えられた時に、期待報酬関数 $q(x, a)$ が最大となる行動を確率1.0で選択し、それ以外の行動を確率0.0で選択する方策として定義できる(証明は章末問題)

もちろん実際には、真の期待報酬関数 $q(x, a)$ は未知であるため、上式を直接利用して方策を得ることはできない。しかし、**観測可能なログデータ $D$ を用いて期待報酬関数 $q(x, a)$ を正確に近似できるのであれば、近似関数 $\hat{q}(x, a)$ に対して上式を適用することで、より良い方策を得ることができるかも**！
これが回帰ベースのアプローチの基本的な考え方である!

つまり、OPLにおける回帰ベースのアプローチとは、データ収集方策 $\pi_{0}$ によって収集されたログデータ $D$ のみを用いて期待報酬関数 $q(x, a)$ を推定し、それを**方策に変換する手順**のこと!
(これを踏まえると、two-towerモデルなどをはじめ、多くの教師あり学習を使った推薦モデルのアプローチは、回帰ベースのアプローチと言えそう...!:thinking:)

#### 勾配ベースのアプローチ
