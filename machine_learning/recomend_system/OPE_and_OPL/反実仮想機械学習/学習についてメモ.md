## 0章の教師あり学習の基礎

- 教師あり学習では、**同時分布 $p(x, y)$ に従う**特徴量(feature)と呼ばれる多次元確率変数 $x$ と、目的変数(outcome)と呼ばれる確率変数 $y$ の間の関係性を、データに基づいて明らかにしたり予測したりすることが目標。
- そのために、ある特徴量 $x$ が与えられた時に、それに対応する目的変数 $y$ を精度よく予測できる予測関数 $f$ を見つけ出すことを考える。
- より具体的には、次のように定義される期待リスク(expected risk)などによって予測関数 f の正確さを定量化し、それを最適化することを通じて、性格や予測関数$f$を得ることを考える。

### 期待リスク

$$
L(f) := E_{p(x,y)}[l(y, f(x))]
$$

ここで、$l(y, f(x))$ は、予測誤差(yとf(x)の乖離度合い)を評価するための損失関数(loss function)。
期待リスクを最小化する予測関数を、最適な予測関数とよび、$f^* := \argmin_{f} L(f)$ と表記する。

最もよく用いられる損失関数 $l$ の一つに、二乗誤差(squared loss) $l(a,b) = (a - b)^2$ がある。この場合の期待リスクは、次のように表される。

$$
L(f) := E_{p(x,y)}[(y - f(x))^2]
$$

この二乗誤差に基づく期待リスクを変形していく。
まず2乗の展開を行うと...

$$
= E_{p(x,y)}[y^2 - 2yf(x) + f(x)^2]
$$

続いて、「追加の項を足したり引いたりして、都合のいい形に変形できるようにする」テクニックを使うと...

- 実は、これは**分散分解(バイアス・バリアンス分解)につながる有名なテクニック**らしい
  - 具体的には、$E[(y - f(x))^2] = E[(y - E[y|x])^2] + E[(E[y|x] - f(x))^2]$ みたいな式で、誤差を「真の条件付き期待値からのブレ」と「モデルが真の条件付き期待値からズレてる分」に分解して考える、っていう**お決まりの流れ**があるらしい!!

$$
= E_{p(x,y)}[
    % 元々の式
    y^2 - 2yf(x) + f(x)^2
    % テクニックとして追加した項
    + 2yE_{p(y'|x)}[y'] - 2yE_{p(y'|x)}[y'] % 足し引き0
    + 2E_{p(y'|x)}[y'^2] - 2E_{p(y'|x)}[y'^2] % 足し引き0
]
$$

続いて、いい感じに項を並び替えた上で、期待値の線形性を使って分解すると...

$$
= E_{p(x,y)}[y^2 - 2y E_{p(y'|x)}[y'] + (E_{p(y'|x)}[y'])^2]
\\
+ E_{p(x,y)}[(E_{p(y'|x)}[y'])^2 - 2yf(x) + f(x)^2]
\\
+ E_{p(x,y)}[2yE_{p(y'|x)}[y'] - 2(E_{p(y'|x)}[y'])^2]
$$

続いて、...

$$
= E_{p(x)}[E_{p(y|x)}[(y - E_{p(y'|x)}[y'])^2]]
\\
+ E_{p(x)}[(E_{p(y|x)}[y'])^2 - 2E_{p(y|x)}[y]f(x) + f(x)^2]
\\
+ E_{p(x)}[2(E_{p(y|x)}[y])^2 - 2(E_{p(y|x)}[y'])^2] % =
$$

最後に...

$$
= E_{p(x)}[Var_{p(y|x)}[y]] % xで条件付けたyの条件付き分散
\\
+ E_{p(x)}[(E_{p(y|x)}[y] - f(x))^2] % 予測関数f(x)のバイアス
$$

となり、最終的に期待リスク $L(f)$ は、以下の2つの要素に分解されることがわかる。

1. $E_{p(x)}[Var_{p(y|x)}[y]]$ : 「目的変数 $y$ の条件付き分布 $p(y|x)$ に関する分散(の期待値)」。これは、特徴量xが与えられた時の目的変数yのばらつき度合いであり、**目的変数yの予測における、特徴量xの質・有用さ**を定量化したもの。
2. $E_{p(x)}[(E_{p(y|x)}[y] - f(x))^2]$ : 「予測関数 $f(x)$ と、目的変数の条件付き期待値 $E_{p(y|x)}[y]$ の二乗誤差(の期待値)」。これは、**予測関数f(x)の予測精度**を表す。

この分解からわかること:

- 期待リスクの1つ目の要素を最小化するには、**より良い（同じ$x$を持つデータの$y$のばらつきが小さい）特徴量 $x$ を構成することが求められる**。
- **期待リスクの1つ目の要素は、予測関数 $f$ に非依存である**。つまり、特徴量 $x$ がひとたび定義されてしまえば、予測関数 $f$ の最適化をいくら頑張ったとしても、この1つ目の要素を小さくすることはできない。
- （損失関数 $l$ が二乗誤差のときの）最適な予測関数は $f^*(x) = E_{p(y|x)}[y]$ であり、この時の期待リスクは $L(f^*) = E_{p(x)}[Var_{p(y|x)}[y]]$ になる。

###
