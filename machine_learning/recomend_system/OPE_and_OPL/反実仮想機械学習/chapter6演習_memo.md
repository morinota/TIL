## これは何?

2つの演習問題に関してメモします

# 問題1: 方策の長期性能に関するオフライン評価

## 問題設定

- 自分はあるEコマースサービスにおけるクーポン配布施策の最適化を担当するデータサイエンティスト。
  - このサービスにおける重要なKPIは1年間の総利益。
  - 複数あるクーポン配布施策の中で、月一回それぞれのユーザに適したクーポンを配布することで購買意欲を刺激し利益改善につなげる施策を担当している。
  - 現在は2024年3月。現状は同施策の前任者が1年前に開発した方策(=以降、旧方策と呼ぶ)により、毎月のクーポン配布が行われてる。
  - 我々の主な関心事は、旧施策に対して大きな改善をもたらすべく開発した、2024年4月から運用予定の新たなクーポン配布施策(=以降、新方策と呼ぶ)の事前評価を行うこと。
- 詳細:
  - クーポン配布の対象ユーザは、1000人。
  - 配布できるクーポンの種類は、全部で4種類。
  - 主たる目的変数・報酬として、各ユーザから得た月別の利益がある。
    - 利益 = 定価からクーポンによる割引額を引いたもの。すなわちユーザが実際に支払った金額。
  - 新旧方策は以下の通り:
    - 旧方策: 各ユーザごとに(確率1で)同じ種類のクーポンを配布し続ける決定的方策。
    - 新方策: 毎月それまでに配布したクーポンの種類を考慮した上で、新たに配布するクーポンを決定する動的な確率的方策。
  - 旧方策が2023年度(=2023年4月から2024年3月)に週数したログデータを用いることができる。しかし、前担当者は将来行われるオフライン評価のことを考慮しておらず、旧方策は確率的ではない。
  - 2024年4月の1ヶ月間のみ、新旧方策のオンライン実験を行い、各種方策によるデータ収集を行うことが許可されてる。

## まず自力で考えてみる。

以下のstepで検討していく

- 1. 問題の定式化: データ生成過程と推定目標の定義
- 2. 基本推定量の検討
- 3. (必要に応じて)独自の推定量の構築と分析

### step1: 問題の定式化

問題を丁寧に定式化することの重要性の話...!
  - まず問題を定式化し、データ生成過程と推定目標を明確に定義することから始める。
  - ex.) chapter1では、$(x, a, r) \sim p(x) \pi_{0}(a|x) p(r|x, a)$ という過程に従い生成されるログデータ $D = {(x_i, a_i, r_i)}_{i=1}^{n}$ に基づいて、新方策 $V(\pi) = E_{p(x) \pi(a|x) p(r|x, a)}[r]$ の推定を行った。
    - (メモ) $p(x) \pi_{0}(a|x) p(r|x, a)$ っていうのは、同時確率 $p(x, a, r)$ と同義で、同時確率を条件付き確率の積に因数分解したもの...!:thinking:
  - 「**どのような生成過程に基づいて、何を推定したいのか**」という点が明確化されていなければ、それに応じた推定量を定義したり、その性質を議論することはできない。
    - **多くの現場では、データ生成過程や推定目標が意識されず、とりあえずIPS推定量っぽいものを使っておこう、といった対策がなされていることが少なからずあった**。
      - 具体的には「**今使用されてる推定量は、何を推定しようという意図の元で定義されたものですか??**」という質問をしたときに、推定目標を書き下せないことが多くあった。
      - そもそも推定量とは、なんらか未知の推定目標をデータから推測するためのものなので、本来は推定目標やデータ生成過程が明確に定義された後で、初めて推定量の話が出てくるはず...!
    - 活用可能なデータや推定目標は現場ごとに異なるわけなので、それを確認せずに先に進んでしまうと、一見簡単に見えるIPS推定量でさえ気づかぬうちに誤った使い方をしちゃう。
    - -> **「個々の状況に応じて丁寧に問題を定式化できるかどうか」**がその後の方策評価・学習の方向性を決定づける最も重要なステップであり、data scientistの実力を分ける一つの重要なポイント...!!

クーポン配布問題の具体的な定式化のために、必要な記号を導入する:

- ユーザ集合 $u \in U = {1, 2, ..., 1000}$
- それぞれのユーザに付随する特徴量 $x_u$
- 4種類の行動 $a \in A = {1, 2, 3, 4}$
- 方策を条件付きの行動選択分布 $\pi(a|x)$ として導入。
  - 旧方策: $\pi_{0}$, 新方策: $\pi_{1}$ と表記。
- 本ケースでは時間軸も関係してくるので、時点 $t$ という記号を導入する。
  - 年度が始まる2024年4月を起点 $t=0$ とし、以降の月を $t=1, 2, ...$ と表記する。
  - また逆に起点以前の2024年3月、2月、...、2023年4月を $t=-1, -2, ..., -12$ と表記する。
    - 例えばここで、「ユーザ特徴量が時点によって変化する場合、$x_{u, t}$ という表記を用いるべきではないか?」「特徴量の時間変化を考慮する場合、定式化や手法にどのような変化が見込まれるか」「結局のところ、これは考慮すべき重要な点なのか、実務上は無視しても良いほど細かい点なのか」などを常に考えながら定式化することが重要...!! 
    - DS自身が疑問をもち、思考・議論できることが理想!
- ユーザ $u$ からある月 $t$ に得られる利益を $r_{u, t}$ と表記する。(＝報酬!)
- 最後に、「**報酬がどのような分布に従い観測されるか**」想定を立てる。
  - これがまさにこの後の方向性を決める重要なステップであり、またそれぞれの分析者の独自性が現れるポイント! (データ生成過程をどのようにモデル化するか、仮定するか、ってことだよね...!:thinking:)
  - ここでは、ある月 $t$ におけるユーザ $u$ の購買行動は、その月に配布したクーポン $a_{u, t}$ のみならず、それ以前に配布していたクーポンを含めた組み合わせ $a_{u,t-h:t} = (a_{u, t-h}, a_{u, t-h+1}, ..., a_{u, t-1}, a_{u, t})$ に依存する、という想定を置くことにする。
    - ex.) t=0でh=3であれば、あるユーザ $u$ から2024年4月に得られる利益 $r_{u, 0}$ は、そのユーザに対して2024年1月、2月、3月、4月に配布したクーポンの組み合わせ $a_{u, -3:0}$ に影響を受ける、。
  - この想定により、数ヶ月連続でクーポンが配布されたらそれ以上は購買意欲が刺激されない状況 (飽和効果) や以前配布されたクーポンを数ヶ月使わずに保存しておいて後で使う状況 (遅れ効果) などを考慮できるようになる。
  - (このステップがデータ生成過程を定義したってこと?? いや、多分その途中のステップ! 最終的に 観測データ $D$ の定式化まで含めてデータ生成過程を定義するってことだという認識...!:thinking:)

これで問題の定式化に必要な記号を導入できたので、推定目標である方策の性能と、その推定に活用できるログデータ (あ、これが観測データの生成過程ってことか...!) を、問題の特性や事情に合わせて定義する。

#### 推定目標である方策性能の定式化

方策の性能として、方策 $\pi_{w}$ を時点 t からt'まで運用したときに発生する、機体累積利益を以下のように定義する。

$$
V_{t:t'}(\pi_{w}) 
:= \sum_{u \in U} E_{p(a_{u, t:t'}, r_{u, t:t'})}[ \sum_{k=t}^{t'} r_{u, k}] 
\\ 
= \sum_{u \in U} E_{\pi_{w}(a_{u, t:t'} | x_{u}) p(r_{u, t:t'} | x_{u}, a_{u, t-h:t'})}[\sum_{k=t}^{t'} r_{u, k}]
\\
= \sum_{u \in U} \sum_{k=t}^{t'} E_{\pi_{w}(a_{u, k-h:k|x_{u}}) [q_{k}(x_{u}, a_{u, k-h:k})]}
\tag{6.1}
$$


ここで、

- $w \in {0, 1}$
- $q_{t}(x_{u}, a_{u, t-h:t}) := p(r_{u, t} | x_{u}, a_{u, t-h:t})$ は、ユーザ $u$ から月 $t$ に得られる期待利益を表す。
  - (より詳細には、同時分布 p(x_u, a_{u, t-h:t}) に関する、報酬 $r_{u, t}$ の条件付き期待値??) 
  - (ただ今回、x_uは確率変数ではないので、条件付き確率分布 $\pi_{w}(a_{u, k-h:k|x_{u}})$ に関する期待値、というのが適切かも...!:thinking:)

先に定式化した通り、この期待累積報酬は、ユーザ特徴量 $x_{u}$ と、過去 $t-h$ から現在 $t$ までのクーポン配布履歴 $a_{u, t-h:t}$ に依存していることがわかる。(うんうん、「**報酬がどのような分布に従い観測されるか**」の仮定によって、この定式化が変わるってことか...!:thinking:)

今回のケース問題において、我々の主な興味は、ある方策を導入したときに2024年度の1年間に得られる累積利益である。これは式(6.1)を用いると、$V_{0:11}(\pi_{w})$ と表記で切るはず。

- (memo):
  - 記号 $:=$ の意味は、「左辺を右辺で定義する」
  - latexの表記だと、単に $:=$ でも良いし、もしくは $\coloneq$

#### 我々が使用可能なログデータの生成過程の定式化

次に、方策の長期性能　$V_{0:11}(\pi_{w})$ を推定するために、我々が活用できるログデータを記述する。
今回のケース問題で使用可能なログデータは以下の2つ:

- 旧方策 $\pi_{0}$ による2023年度のログデータ
- 2024年4月の1ヶ月間の新旧方策のオンライン実験で収集するログデータ

まず前者。旧方策 $\pi_{0}$ による2023年度中のある月 $t$ に収集したログデータは、導入済みの記号を用いて、以下のように定義できる：

$$
D_{hist}^{t} = {(x_u, a_{u,t}, r_{u, t})}_{u \in U}
~ \Pi p(r_{u,t}|x_{u}, a_{u,t-h:t})
\tag{6.2}
$$

- (ここで注意点: $\sim$ の右側の確率分布について)
  - これまで扱ってきた他の問題だと $\sim$ の右側(=ログデータを生成する確率分布)は、同時分布 $p(x, a, r)$ を条件付き確率の積に因数分解したものだった。でも今回のケースではなぜ変わってる??
  - 今回のケース問題では...
    - ユーザ集合 $u$ やユーザ特徴量 $x_u$ は固定されているので確率変数ではないので、これらに関する分布はデータ生成過程に現れない...!
    - 旧方策は決定的な方策なので、行動 $a_{u,t}$ は単に方策の決定的な出力値 $a_{u,t} = \pi_{0}(x_{u})$ として書けるので、同様にデータ生成過程に現れない...!

式(6.2)により、旧方策が2023年度中に収集した全てのログデータは $D_{hist} = {D_{hist}^{(t)}}_{t=-12}^{-1}$ と表記できる。 

一方で、オンライン実験で収集するログデータはどのように記述できる??

- ここでは、1ヶ月間のオンライン実験で収集されるデータを記述するために、確率変数 $w_{u} \in {0, 1}$ を導入する。
  - この確率変数は、実験中にユーザ $u$ に割り当てられる方策を示す。
  - また、確率分布 $p(w_u)$ で、オンライン実験においてユーザ $u$ に各方策が割り当てられる確率を表す。
    - ex.) 新旧2つの方策を等確率で割り当てる一般的なABテストの場合、$p(w_u = 0) = p(w_u = 1) = 0.5, \forall(u)$ となる。

これまで定義した記号を用いると、2024年4月 (t=0)にオンライン実験で得られるログデータは、以下のように定式化できる。

$$
D_{exp} = D_{exp}^{t=0} 
= {(x_u, w_u, a_{u,0}, r_{u, 0})}_{u \in U} \sim \Pi_{u \in U} p(w_u, a_{u, 0}, r_{u, 0})
\\ 
= {(x_u, w_u, a_{u,0}, r_{u, 0})}_{u \in U}  \sim \Pi_{u \in U} p(w_u) \pi_{w_u}(a_{u, 0} | x_{u}) p(r_{u, 0} | x_{u}, a_{u, -h:0})
\tag{6.3}
$$
(メモ: 技術書には2行目のみ書かれていたが、わかりやすさのために、同時確率の行を挟んでみた...!:thinking:)
(メモ: $\pi_{w_u}(a_{u, 0} | x_{u})$ は $p(a_{u, 0} | x_{u}, w_u)$ と書いても正しいよね...!:thinking:)

- (ここで注意点: 式(６.2)と同じく $\sim$ の右側の確率分布について!)
  - 今回はx_uは確率変数ではないので、データ生成過程には現れない...!
  - 方策の割り当ては確率的なので、$p(w_u)$ データ生成過程に出現してる! (特にx_uなどには影響を受けないので、条件付き確率などではない!)
  - クーポン $a_{u, 0}$ は、割り当てられた方策 $\pi_{w_u}$ によって決定され、新方策は確率的である可能性があるので、これもデータ生成過程に出現してる...!
  - 今回のオンライン実験は2024年4月のみなので、$D_{exp}$ で観測される報酬は $r_{u, 0}$ のみ...!
- 
