## 0章の教師あり学習の基礎

- 教師あり学習では、**同時分布 $p(x, y)$ に従う**特徴量(feature)と呼ばれる多次元確率変数 $x$ と、目的変数(outcome)と呼ばれる確率変数 $y$ の間の関係性を、データに基づいて明らかにしたり予測したりすることが目標。
- そのために、ある特徴量 $x$ が与えられた時に、それに対応する目的変数 $y$ を精度よく予測できる予測関数 $f$ を見つけ出すことを考える。
- より具体的には、次のように定義される期待リスク(expected risk)などによって予測関数 f の正確さを定量化し、それを最適化することを通じて、性格や予測関数$f$を得ることを考える。

### 期待リスク

$$
L(f) := E_{p(x,y)}[l(y, f(x))]
$$

ここで、$l(y, f(x))$ は、予測誤差(yとf(x)の乖離度合い)を評価するための損失関数(loss function)。
期待リスクを最小化する予測関数を、最適な予測関数とよび、$f^* := \argmin_{f} L(f)$ と表記する。

最もよく用いられる損失関数 $l$ の一つに、二乗誤差(squared loss) $l(a,b) = (a - b)^2$ がある。この場合の期待リスクは、次のように表される。

$$
L(f) := E_{p(x,y)}[(y - f(x))^2]
$$

この二乗誤差に基づく期待リスクを変形していく。
まず2乗の展開を行うと...

$$
= E_{p(x,y)}[y^2 - 2yf(x) + f(x)^2]
$$

続いて、「追加の項を足したり引いたりして、都合のいい形に変形できるようにする」テクニックを使うと...

$$
= E_{p(x,y)}[
    % 元々の式
    y^2 - 2yf(x) + f(x)^2
    % テクニックとして追加した項
    + 2yE_{p(y'|x)}[y'] - 2yE_{p(y'|x)}[y'] % 足し引き0
    + 2E_{p(y'|x)}[y'^2] - 2E_{p(y'|x)}[y'^2] % 足し引き0
]
$$

- 実は、上記の足し引きは**分散分解(バイアス・バリアンス分解)につながる有名なテクニック**らしい
  - 具体的には、$E[(y - f(x))^2] = E[(y - E[y|x])^2] + E[(E[y|x] - f(x))^2]$ みたいな式で、誤差を「真の条件付き期待値からのブレ」と「モデルが真の条件付き期待値からズレてる分」に分解して考える、っていう**お決まりの流れ**があるらしい!!

続いて、いい感じに項を並び替えた上で、期待値の線形性を使って分解すると...

$$
= E_{p(x,y)}[y^2 - 2y E_{p(y'|x)}[y'] + (E_{p(y'|x)}[y'])^2]
\\
+ E_{p(x,y)}[(E_{p(y'|x)}[y'])^2 - 2yf(x) + f(x)^2]
\\
+ E_{p(x,y)}[2yE_{p(y'|x)}[y'] - 2(E_{p(y'|x)}[y'])^2]
$$

続いて、「同時分布に関する期待値」を、「条件付き期待値の期待値」に落とし込むテクニックを使って...

- ここで使ってるテクニック
  - 「同時分布の期待値 = まずはxの期待値、続いてyの条件付き期待値をとる」性質を使ってる...! $E_{p(x,y)}[hoge] = E_{p(x)}[E_{p(y|x)}[hoge]]$ みたいな感じで、「同時分布の期待値」を「条件付き期待値の期待値」に落とし込むテクニックが使われてる...!

$$
% 各項について、同時分布に関する期待値が、「条件付き期待値の期待値」という二重の期待値に変形される
= E_{p(x)}[E_{p(y|x)}[y^2 - 2y E_{p(y'|x)}[y'] + (E_{p(y'|x)}[y'])^2]]
\\
+ E_{p(x)}[E_{p(y|x)}[(E_{p(y'|x)}[y'])^2 - 2yf(x) + f(x)^2]]
\\
+ E_{p(x)}[E_{p(y|x)}[2yE_{p(y'|x)}[y'] - 2(E_{p(y'|x)}[y'])^2]]
$$

続いて、各項の期待値の中身を整理していくと...

$$
= E_{p(x)}[E_{p(y|x)}[(y - E_{p(y'|x)}[y'])^2]] % 期待値の中身を因数分解
\\
+ E_{p(x)}[(E_{p(y|x)}[y'])^2 - 2E_{p(y|x)}[y]f(x) + f(x)^2] % 期待値の線形性を使って、条件付き期待値を展開
\\
+ E_{p(x)}[2(E_{p(y|x)}[y])^2 - 2(E_{p(y|x)}[y'])^2] % 期待値の線形性を使って、条件付き期待値を展開
$$

- ここでやってること
  - 第1項: 期待値の中身を因数分解して、**条件付き分散の定義式**の形に! $Var_{p(y|x)}[y] = E_{p(y|x)}[(y - E_{p(y|x)}[y])^2]$
  - 第2項: 期待値の線形性を使って、条件付き期待値を展開。(第2.2項以外はyにとって定数になるのでスッキリするのか...!:thinking:)
    - ちなみに $E_{p(y|x)}[(E_{p(y|x)}[y'])^2] = (E_{p(y|x)}[y'])^2$ になる点に注意!!**理由を簡単に説明すると、E[定数] = 定数 だから**
      - 中身の $E_{p(y'|x)}[y']$ は、「特徴量xを固定された時の、目的変数yの期待値」のこと。**これはxについての関数になり、yの分布に依存しない定数**である!
      - 定数を二乗しても当然定数！
      - よって、$E_{p(y|x)}[定数] = 定数$ になる!
    - 最終的にこの項は「条件付き期待値 - 予測関数」の二乗に落ち着く!
  - 第3項: 期待値の線形性を使って、条件付き期待値を展開。
    - ちなみに $E_{p(y|x)}[2yE_{p(y'|x)}[y']] = 2(E_{p(y|x)}[y])^2$ になる点に注意!!
      - 期待値の線形性 ($E_{p(x)}[Ax] = A E_{p(x)}[x]$) を使って、$E_{p(y'|x)}[y']$ を定数とみなすと上記の結果になる!!:thinking:
    - ちなみに $E_{p(y|x)}[2(E_{p(y'|x)}[y'])^2] = 2(E_{p(y|x)}[y'])^2$ になるのは、前述と同様に $E[定数] = 定数$ なので!!
    - 最終的に、この項は打ち消されて0になる!!

最後に...

$$
= E_{p(x)}[Var_{p(y|x)}[y]] % xで条件付けたyの条件付き分散
\\
+ E_{p(x)}[(E_{p(y|x)}[y] - f(x))^2] % 予測関数f(x)のバイアス
$$

となり、最終的に期待リスク $L(f)$ は、以下の2つの要素に分解されることがわかる。
(期待リスクの分散の式、完全に理解した!!:thinking:)

1. $E_{p(x)}[Var_{p(y|x)}[y]]$ : 「目的変数 $y$ の条件付き分布 $p(y|x)$ に関する分散(の期待値)」。これは、特徴量xが与えられた時の目的変数yのばらつき度合いであり、**目的変数yの予測における、特徴量xの質・有用さ**を定量化したもの。
2. $E_{p(x)}[(E_{p(y|x)}[y] - f(x))^2]$ : 「予測関数 $f(x)$ と、目的変数の条件付き期待値 $E_{p(y|x)}[y]$ の二乗誤差(の期待値)」。これは、**予測関数f(x)の予測精度**を表す。

期待リスクの要素分解からわかること:

- 期待リスクの1つ目の要素を最小化するには、**より良い（同じ$x$を持つデータの$y$のばらつきが小さい）特徴量 $x$ を構成することが求められる**。
- **期待リスクの1つ目の要素は、予測関数 $f$ に非依存である**。つまり、特徴量 $x$ がひとたび定義されてしまえば、予測関数 $f$ の最適化をいくら頑張ったとしても、この1つ目の要素を小さくすることはできない。
- （損失関数 $l$ が二乗誤差のときの）最適な予測関数は $f^*(x) = E_{p(y|x)}[y]$ であり、この時の期待リスクは $L(f^*) = E_{p(x)}[Var_{p(y|x)}[y]]$ になる。

#### 推薦方策の性能についても、期待リスクの要素分解、みたいな考えを定式化したいメモ

### しかし実は、期待リスクは未知である

仮に真の同時分布 $p(x, y)$ がわかっているのならば、最適な予測関数 $f^*$ はちょっとした計算で明らかになるため、教師あり学習の問題は完璧に解けてしまう。

しかし現実には、**同時分布 $p(x,y)$ は未知の情報**である。

代わりに教師あり学習では、予測関数 $f$ を最適化するための情報としてトレーニングデータと呼ばれるデータセットが与えられる。
トレーニングデータは、同時分布 $p(x,y)$ から独立に同分布に(i.i.d.)サンプリングされたデータであり、特徴量 $x$ と目的変数 $y$ のn個の観測の組であり、$D = {(x_i, y_i)}_{i=1}^{n}$ と表記される

理想的には期待リスク $L(f)$ を最適化することで最適な予測関数 $f^*$ を得たいが、真の同時分布 $p(x,y)$ が未知であるため計算できない。
その代わりにトレーニングデータ $D$ に基づいて、**期待リスクを推定する推定量 $\hat{L}(f;D)$ を構築した上で、この推定量を(仕方なく)最小化する**ことで、より良い予測関数 $\hat{f}^*$ を得る (学習する)、という手順に落ち着く。

## 5章のオフ方策学習のメモ

ログデータに基づいて新たな方策を学習する術を持っていなければ、**たとえ正確なオフ方策評価を行う土壌が整っていたとしても（もちろんそれはそれでとても有用ではあるが）、それだけでは方策の性能改善にはつながらない**。

- OPLとOPEを組み合わせた方策改善プロセス
  - 1. データ収集方策によるログデータ $D$ を収集
    - この時、学習用データ $D_{tr}$ と評価用データ $D_{val}$ に分割
  - 2. オフ方策学習
    - 学習用データ $D_{tr}$ に基づいて、オフ方策学習を行い、新たな方策を学習
    - $\hat{\pi} = \argmax_{\pi} \hat{V}(\pi; D_{tr})$
    - ここでの $\hat{V}(\pi; D)$ は、真の目的関数をログデータのみに基づき近似する推定量。
  - 3. オフ方策評価
    - 評価用データ $D_{te}$ に基づいて、方策の事前性能評価。
      - 学習した方策が大きな失敗を招かないものであるか、最低限、データ収集方策の性能を上回るものであるか、などを確認する。
    - $\hat{V}(\hat{\pi}; D_{val})$
    - ここでの $\hat{V}(\hat{\pi}; D)$ は、学習した方策 $\hat{\pi}$ の性能を評価するための推定量。
  - 4. 実環境への実装やオンライン実験による検証
    - 採用されれば $\hat{\pi}$ は次のデータ収集方策 $\pi_{0}$ になる。

実際にOPLの問題を得際には、（教師あり学習と同様に）方策 $\pi$ を何らかの形で**パラメータ化(parameterize)**した上で、方策の性能 $V(\pi)$ を最大化するようなパラメータを求める問題として実装されることが多い。

$$
\theta^* = \argmax_{\theta} V(\pi_{\theta})
$$

方策のパラメータ化の代表例。あくまで代表例なので、方策をパラメータ化する方法は他にもいくらでもある。
（両方ともソフトマックス関数を使って、行動選択確率を表現してるな...!!まあここは別に、決定的方策でも、epsilon-greedyでもいいはず:thinking:）

- 線形方策: $\pi_{\theta}(a|x) = \frac{exp(\theta_{a}^{T}x)}{\sum_{a' \in A} exp(\theta_{a'}^{T}x)}$
- NN方策: $\pi_{\theta}(a|x) = \frac{exp(NN_{\theta}(x, a))}{\sum_{a' \in A} exp(NN_{\theta}(x, a'))}$

### OPLにおける標準的なアプローチ

3つに分類できるみたい。

- 回帰ベース
- 勾配ベース
- 回帰ベースと勾配ベースを融合した第三のアプローチ

### 回帰ベースのアプローチ

回帰ベースのアプローチでは、最適方策 $\pi^* := \argmax_{\pi} V(\pi)$ と期待報酬関数 $q(x, a)$ に関する次の関係性に着目している。

$$
% q(x,a) が最大の行動を確率1.0で選択、それ以外を確率0.0で選択する方策
\pi^*(a|x) = \begin{cases}
1 & \text{if } a = \argmax_{a' \in A} q(x, a') \\
0 & \text{otherwise}
\end{cases}
$$

すなわち、最適方策は、特徴量 $x$ が与えられた時に、期待報酬関数 $q(x, a)$ が最大となる行動を確率1.0で選択し、それ以外の行動を確率0.0で選択する方策として定義できる(証明は章末問題)

もちろん実際には、真の期待報酬関数 $q(x, a)$ は未知であるため、上式を直接利用して方策を得ることはできない。しかし、**観測可能なログデータ $D$ を用いて期待報酬関数 $q(x, a)$ を正確に近似できるのであれば、近似関数 $\hat{q}(x, a)$ に対して上式を適用することで、より良い方策を得ることができるかも**！
これが回帰ベースのアプローチの基本的な考え方!

つまり、OPLにおける回帰ベースのアプローチとは、データ収集方策 $\pi_{0}$ によって収集されたログデータ $D$ のみを用いて期待報酬関数 $q(x, a)$ を推定し、**それを方策に変換する手順**のこと!
(これを踏まえると、two-towerモデルなどをはじめ、**多くの教師あり学習を使った推薦モデルのアプローチは、回帰ベースのアプローチと言えそう**...!:thinking:)

- 回帰ベースのアプローチは、非常に直感的で、教師あり学習に似た思想であり、方策学習の実戦でも多く用いられる手法。現在実装してる意思決定最適化の解き方が、知らぬ間に回帰ベースのアプローチに類するものになっていたというケースも多いはず。

### 勾配ベースのアプローチ

- 回帰ベースの欠点の1つ: **回帰ベースのアプローチはあくまで報酬の予測問題を解いているだけであり、方策学習の問題を直接的に解いているわけではない**
  - 例: 2つの報酬予測モデル $q_{1}(x, a)$ と $q_{2}(x, a)$
    - $q_{2}$ の方が、報酬予測についてはより精度が高い。
    - しかし結果として $q_{1}$ に基づいて構成される方策 $\pi_{regbased1}$ の方が、より最適な行動を選択できて、性能が高い結果となっている。
    - 実際の値は「反実仮想機械学習 p246」を参照
  - この例からわかることは、**より正確な報酬の推定モデルを得ることは、必ずしもより良い性能を発揮する方策の獲得につながるわけではない**ということ。
    - 報酬予測と意思決定最適化は本質的に異なる問題であり、回帰ベースのアプローチは方策学習の問題を直接解いているわけではないので、これは欠点と言える。
- 対して、**勾配ベース (gradient-based) のアプローチは、高性能を発揮する方策をより直接的に学習する方法**、と言える。
  - より具体的には、方策の性能の勾配 $\nabla_{\theta} V(\pi_{\theta})$ に基づいて、方策 $\pi_{\theta}$ の性能が良くなるように以下の通りにパラメータ $\theta$ の更新を繰り返すことで学習を行う。

$$
\theta_{t+1} <- \theta_{t} + \nu \nabla_{\theta} V(\pi_{\theta})
\tag{5.9}
$$

- ここで、
  - $\nu$ は、パラメータ更新時の学習率(leaarning rate)を表すハイパーパラメータ。
  - また、$\nabla_{\theta} V(\pi_{\theta})$ は、**方策勾配(policy gradient)**と呼ばれ、具体的には次の形をしている。

$$
\nabla_{\theta} V(\pi_{\theta}) = \nabla_{\theta} E_{p(x)\pi_{\theta}(a|x)}[q(x, a)] % 勾配にpolicy valueの定義を代入しただけ
$$

- ここから何やかんや変形していくと...

$$
= E_{p(x)\pi_{\theta}(a|x)}[\nabla_{\theta} q(x, a)] % 「期待値の勾配」=「勾配の期待値」の性質を使って変形
\\
= E_{p(x)}[E_{\pi_{\theta}(a|x)}[\nabla_{\theta} q(x, a)]] % 「同時分布の期待値」=「条件付き期待値の期待値」のテクニックを使って変形
\\
= E_{p(x)}[\sum_{a \in A} \nabla_{\theta} q(x, a) \pi_{\theta}(a|x)] % 期待値の定義式より(aは離散変数なので)
\\
= E_{p(x)}[\sum_{a \in A} q(x, a) \nabla_{\theta} \pi_{\theta}(a|x)] % q(x,a)は\thetaに依存しないので、勾配の外に出せる。
\\
= E_{p(x)}[\sum_{a \in A} q(x, a) \pi_{\theta}(a|x) \nabla_{\theta} \log \pi_{\theta}(a|x)] % ログトリックを使って変形!!
\\
= E_{p(x)\pi_{\theta}(a|x)}[q(x, a) \nabla_{\theta} \log \pi_{\theta}(a|x)] % \pi_{\theta}(a|x)が1つ、勾配の外に出たので、期待値の定義式よりEの形式に戻す!
\tag{5.10}
$$

- ここで...
  - 方策(条件付き確率)の勾配の変形 $\nabla_{\theta} \pi_{\theta}(a|x) = \pi_{\theta}(a|x) \nabla_{\theta} \log \pi_{\theta}(a|x)$ は、ログトリック(log-deriative trick)と呼ばれるテクニックであり、**勾配ベースの方策学習における頻出操作**らしい。

- 式(5.9)と式(5.10)を合わせてみると...
  - 勾配ベースのアプローチは、**期待報酬関数 $q(x, a)$ の値が大きい行動 $a$ の選択確率を大きくするように方策パラメータ $\theta$ を更新していく**ことがわかる。
  - (これまだどう解釈したらこの示唆が得られるのかわかってない...!:thinking:)

- しかし、**勾配ベースのアプローチでも、式(5.9)のパラメータ更新則を直接実装することはできない**。
  - -> なぜなら、真の方策勾配 $\nabla_{\theta} V(\pi_{\theta})$ に現れる期待報酬関数 $q(x, a)$ が未知だから!!
- なので代わりに...
  - **データ収集方策 $\pi_{0}$ が生成したログデータ $D$ を用いて真の方策勾配 $\nabla_{\theta} V(\pi_{\theta})$ を推定した上で、（仕方なく）推定された勾配 $\hat{\nabla_{\theta}} V(\pi_{\theta})$ に基づいてパラメータ更新を実装**する。
  - すなわち、勾配ベースアプローチのパラメータ更新則の実装は、次のようになる。
    - (真の方策勾配の代わりに、推定された方策勾配の推定量を使ってパラメータ更新していく...!!:thinking:)

$$
\theta_{t+1} <- \theta_{t} + \nu \hat{\nabla_{\theta}} V(\pi_{\theta})
$$


#### 方策勾配の推定方法としての、IPS推定量

- IPS推定量の考え方を応用して、方策勾配を推定することを考える。
  - 任意のデータ収集方策 $\pi_{0}$ によって収集されたログデータ $D$ が与えられた時、真の方策勾配に対するIPS推定量は以下のようになる。
  - なお式中の $w(x, a):= \pi_{\theta}(a|x) / \pi_{0}(a|x)$ は、学習中の方策とデータ収集方策による行動選択確率の比であり、重要度重みと呼ぶ。

$$
\hat{\nabla_{\theta} V_{IPS}(\pi_{\theta};D)} := \frac{1}{n} \sum_{i=1}^{n} w(x_i, a_i) r_i \nabla_{\theta} \log \pi_{\theta}(a_i|x_i)
\tag{5.12}
$$

- オフ方策評価におけるIPS推定量との対応関係
  - オフ方策学習だと、推定目標が評価方策の性能 $V(\pi)$ から方策勾配 $\nabla_{\theta} V(\pi_{\theta})$ に変わる。
  - -> なので、それに応じて報酬 $r_i$ だけじゃなく、**学習中の方策による行動選択確率の対数の勾配 $\nabla_{\theta} \log \pi_{\theta}(a_i|x_i)$ が推定量の定義に含まれるようになった**のが些細な違い!

#### 方策勾配の推定方法としての、DR推定量

- IPS推定量の問題: 重要度重みに起因して発生するバリアンス!
  - IPS推定量の不偏性を維持しつつバリアンスを減少させる方法 -> メジャーなのがDR推定量!!
- 方策勾配推定におけるDR推定量:
  - データ収集方策 $\pi_{0}$ によって収集されたログデータ $D$ が与えられた時、真の方策勾配に対するDR推定量は以下のようになる。
  - なお式中の...
    -  $\hat{q}(x, a)$ は、期待報酬関数 $q(x, a)$ に対する推定モデル!
       - 例えば回帰ベースのアプローチと同様に、ログデータ $D$ を用いて損失関数を最小化して求めたモデルを使ったりする!
    - $w(x, a)$ はIPS推定量と同様の重要度重み!

$$
\hat{\nabla_{\theta} V_{DR}(\pi_{\theta};D)} := \frac{1}{n} \sum_{i=1}^{n} 
% \left\{
\Big\{
  w(x_i, a_i) (r_i - \hat{q}(x_i, a_i)) \nabla_{\theta} \log \pi_{\theta}(a_i|x_i) 
  \\
  + E_{\pi_{\theta}(a|x)}[\hat{q}(x_i, a) \nabla_{\theta} \log \pi_{\theta}(a|x)]
\Big\}
$$

- オフ方策評価におけるDR推定量との対応関係
  - 基本的には同様のアイデア「ログデータを用いて事前に学習しておいた期待報酬関数に対する推定モデル $\hat{q}(x, a)$ をベースラインとして使うことで、IPS推定量からのバリアンス減少を狙う！」

- ちなみに...「このDR推定量を使うアプローチは回帰ベースのアプローチ??勾配ベースのアプローチ??」 -> 勾配ベースのアプローチと分類すべき、という話。
  - 期待報酬関数に対する推定モデルを使ってるし、重要度重みも使ってるから、回帰ベース & 勾配ベースの融合アプローチなのでは?? と考えることもできそう??
  - しかし、推定モデル $\hat{q}(x, a)$ は単に方策勾配の推定を手助けする役割であり、方策は勾配によるパラメータ更新で学習する点から、DR推定量はあくまで勾配ベースのアプローチとみなせる!

- 書籍での実験
  - 1. 学習データサイズ n に関する実験
    - 縦軸: 学習された方策のデータ収集方策に対する相対的な性能(新たに学習された方策が、データ収集方策の性能をどの程度改善するものか)
    - 横軸: 学習データサイズ n
    - また、回帰ベースの手法のための推定モデル $\hat{q}(x, a)$ や勾配ベースで学習する方策 $\pi_{\theta}$ には、3層のニューラルネットワークを使用。
  - 2. 行動数 |A| に関する実験
    - 縦軸: 上と同じ
    - 横軸: 行動数 |A|
  - 3. ログデータのサイズn毎の学習曲線の比較
    - 縦軸: 上と同じ
    - 横軸: エポック数
    - 学習データのサイズはそれぞれ100, 500, 2000

### 回帰ベースと勾配ベースを融合した第三のアプローチ: POTECアルゴリズム

- 前述した回帰ベースと勾配ベースのアプローチは、十分なログデータが存在する場合は、それぞれの方法でデータ収集方策 $\pi_{0}$ の性能を大きく上回る方策を学習できる。
- しかしOPEでも同様の話題があった良いに、これらの典型的なアプローチには、実践上非常に厄介な課題がある。
  - **データ数 $n$ が少ない場合や、行動数 $|A|$ が大きい場合に方策学習がうまく進まなくなる**、という問題である。
- この場合にも、（データ収集方策 $\pi_{0}$ よりも良い方策を学習できるという意味で）有益な方策を学習するためのアイデアとして提案されているのが、**回帰ベースと勾配ベースを融合した第三のアプローチであるPOTECアルゴリズム**である。
  - POTEC=Policy Optimization via Two-Stage Policy Decomposition
  - 日本語だと「2段階方策分解を用いた方策最適化」とかかな...?
  
POTECアルゴリズムの概要

- 1. まず行動クラスタ $c_a \in C$ を構成する。
  - ここで $c_a$ は、行動 $a$ が属するクラスタを表す。
- 2. 次に、**有望な行動クラスタを特定するための１段階目方策（1st-stage policy）** $\pi_{\theta}^{1st}$ を学習する。
  - これは、特徴量 $x$ で条件付けられたクラスタ上の確率分布 $\pi_{\theta}^{1st}(c|x)$ として定式化できる。
- 3. 最後に、**有望なクラスタ内で最良と思われる行動を特定するための２段階目方策（2nd-stage policy）** $\pi_{\theta}^{2nd}$ を学習する。
  - これは、特徴量 $x$ とクラスタ $c$ で条件付けられた上で行動 $a$ を選択する確率分布 $\pi_{\phi}^{2nd}(a|x, c)$ として定式化できる。

より短くいうと、2段階方策学習のアルゴリズムでは、まずはじめに行動をクラスタリングし、その後各特徴量 $x$ に対して、以下のステップで最終的に選択する行動 $a$ を決定する。

- 1段階目方策により、有望クラスタを特定する: $c \sim \pi_{\theta}^{1st}(\dot|x)$
- 2段階目方策により、有望クラスタ内で最良と思われる行動を特定する: $a \sim \pi_{\phi}^{2nd}(\dot|x, c)$

これは結局のところ、次の**全体方策(overall policy)**に基づき行動を選択していることになる。

$$
\pi_{\theta, \phi}^{overall}(a|x) = E_{\pi_{\theta}^{1st}(c|x)}[\pi_{\phi}^{2nd}(a|x, c)]
\\
= \sum_{c \in C} \pi_{\theta}^{1st}(c|x) \pi_{\phi}^{2nd}(a|x, c) % cが離散変数なので、期待値の定義式より
$$

全体方策 $\pi_{\theta, \phi}^{overall}$ は、1段階目方策に関する、2段階目方策の期待値として定義される。
2段階の推論手順は、この全体方策に基づいて行動を選択すること($a \sim \pi_{\theta, \phi}^{overall}(a|x)$)と同義になる。

つまり、この２段階方策学習の枠組みで実現したいことは、以下のように、**結果としてできあがる全体方策 $\pi_{\theta, \phi}^{overall}$ がより良い性能を発揮するように、１段階目方策と２段階目方策（のパラメータ）を学習すること**。

$$
(\theta^*, \phi^*) = \argmax_{\theta, \phi} V(\pi_{\theta, \phi}^{overall})
$$

２段階方策学習のメリット:

- **全体方策をあえて分解して考えることで、それぞれの方策を学習する際に回帰ベースと勾配ベースのアプローチを使い分けることができ、それぞれの良い点をより際立たせることが可能になる**。
- その結果として、**データ数が少なかったり行動数が多かったとしても、どちらか一つの方法のみに依存する場合よりも良い方策を学習できる可能性が高まる**。

具体的には、1段階目方策 $\pi_{\theta}^{1st}$ は勾配ベースの方法で、2段階目方策 $\pi_{\phi}^{2nd}$ は回帰ベースの方法で学習する、というのsaito2024の論文で提案されてるらしい。
理由:

- 1段階目方策は有望クラスタのみを特定できればよく、クラスタの数は行動の数よりも少ないので、行動数に対するデータ数が多い場合により良い方策を学習できる勾配ベースの方法がより適している。
- 一方で、2段階目方策は個別の行動の良さを見分ける必要があるので、行動数に対するデータ数が少ない状況に比較的頑健な回帰ベースの方法が好まれる。

#### POTECの1段階目方策の学習

まずは、２段階目方策 $\pi_{\phi}^{2nd}$ が与えられた状況で、1段階目方策 $\pi_{\theta}^{1st}$ を勾配ベースの方法で学習することを考える。
ここでは、全体方策 $\pi_{\theta, \phi}^{overall}$ の性能がより良くなるように１段階目方策 $\pi_{\theta}^{1st}$ を学習したいわけなので、1段階目方策のパラメータ更新則は、以下のようになる。

$$
\theta_{t+1} <- \theta_{t} + \nu \nabla_{\theta} V(\pi_{\theta, \phi}^{overall})
$$

ここで、１段階目方策（のパラメータ）に関する方策勾配 $\nabla_{\theta} V(\pi_{\theta, \phi}^{overall})$ は、具体的には次のようになる。

$$
\nabla_{\theta} V(\pi_{\theta, \phi}^{overall}) = \nabla_{\theta} E_{p(x)\pi_{\theta, \phi}^{overall}(a|x)}[q(x, a)] % policy valueの定義式を代入
\\
= E_{p(x)}[\sum_{a \in A} q(x, a) \nabla_{\theta} \pi_{\theta, \phi}^{overall}(a|x)] % 「同時分布の期待値」=「条件付き期待値の期待値」のテクニック & 「期待値の勾配」=「勾配の期待値」& aは離散変数なので期待値の定義式などを使って変形
\\
= E_{p(x)}[\sum_{a \in A} q(x, a) \nabla_{\theta} E_{\pi_{\theta}^{1st}(c|x)}[\pi_{\phi}^{2nd}(a|x, c)]] % 全体方策の定義を代入
\\

$$

ここで、期待報酬関数 $q(x, a)$ の2段階目豊作 $\pi_{\phi}^{2nd}$ に関する期待値に、$q^{\pi_{\phi}^{2nd}}(x, c) := E_{\pi_{\phi}^{2nd}(a|x, c)}[q(x, a)]$ という表記を割り当てている。

上記を踏まえると、**全体方策 $\pi_{\theta, \phi}^{overall}$ の性能をより良いものにするためには、関数　$q^{\pi_{\phi}^{2nd}}(x, c)$ の値が大きいいクラスタ $c$ の選択確率をより大きくするように、1段階目方策 $\pi_{\theta}^{1st}$ のパラメータ $\theta$ を更新すべきである**、ことがわかる。

- ここで導入された関数について補足:
  - 関数の値が大きいクラスタ $c$ をより高確率で選ぶことができれば、全体方策の性能が改善されるので...**クラスタ $c$ の価値を表す関数（クラスタ価値関数）**と言える。
- また関数の定義式より、**クラスタの価値は2段階目方策によって変化するので...1段階目方策が選ぶべき有望クラスタは2段階目方策によって変わる**こともわかる。

#### POTECアルゴリズムのための方策勾配の推定量: POTEC推定量について

勾配ベースのアプローチと同様に、真の方策勾配 $\nabla_{\theta} V(\pi_{\theta, \phi}^{overall})$ を正確に推定できる推定量を設計することを考えたい。
しかしこのように、POTECアルゴリズムでは、**1段階目方策の真の方策勾配が2段階目方策にも依存していることが原因で、重要度重みだけでは方策勾配の不偏推定量を構築できない**（らしい...!:thinking:）
この問題に対応するために、2段階目方策による影響も考慮できるより洗練された方法として、OffCEM推定量の考え方を応用した、次のPOTEC推定量が提案されてる。

$$
\hat_{\nabla_{\theta}} V_{POTEC}(\pi_{\theta, \phi}^{overall})
:= \frac{1}{n} \sum_{i=1}^{n} \{
  \frac{pi_{\theta}^{1st}(c_i|x_i)}{\pi_{0}(c_i|x_i)} (r_{i} - \hat{f}(x_i, a_i)) \nabla_{\theta} \log \pi_{\theta}^{1st}(c_i|x_i) 
  + E_{\pi_{\theta}^{1st}(c|x_i)}[\hat{f}^{\pi_{\phi}^{2nd}}(x_i, c)] \nabla_{\theta} \log \pi_{\theta}^{1st}(c|x_i)
\}
\tag{5.17}
$$

ここで、

- $w(x,c):=\frac{\pi_{\theta}^{1st}(c|x)}{\pi_{0}(c|x)}$ は、1段階目豊作とデータ収集方策によるクラスタ選択確率の比で定義されるクラスタ重要度重み(cluster importance weight)。
- また、$\hat{f}: \mathcal{X} \times \mathcal{A} \to \mathbb{R}$ は、事前に与えられる回帰モデルと呼ばれる関数であり、$\hat{f}^{pi_{\phi}^{2nd}}(x, c) := E_{\pi_{\phi}^{2nd}(a|x, c)}[\hat{f}(x, a)]$ は、回帰モデルの2段階目方策についての期待値である。
- このPOTEC推定量は、オフライン評価のOffCEM推定量に、1段階目方策に関する方策スコア関数 $\nabla_{\theta} \log \pi_{\theta}^{1st}(c|x)$ を適切に加えることで方策勾配に対する推定量として拡張したもの、と解釈できる。
  - -> 3章でも用いた回帰モデルに関する局所正確性の仮定に基づき、POTEC推定量の分析を行える。

#### POTECの2段階目方策の学習

### ランキングにおけるオフ方策学習

- 複数の行動(i.e. アイテム)をうまく順位づけし並び替えるためのランキング方策。
- 記号表記
  - ユーザ情報などの特徴量ベクトル $x \in \mathcal{X}$
  - 商品やニュース記事など1つ1つのアイテム $a \in \mathcal{A}$
  - アイテム集合Aに属するアイテムを並び替えることで構成されるランキング: $\mathbf{a} := (a_1, a_2, ..., a_k) \in \prod(A)^{k}$ というベクトルで表す。
    - ここで $prod(A)^{k}$ は、アイテム集合Aのk個の順列の集合を表す。
    - あるランキング $\mathbf{a}$ におけるk番目のポジションに提示されるアイテムを $a(k)$ と表記する。
  - あるランキング $\mathbf{a}$ に対して観測される報酬を $\mathbf{r} := (r_1, r_2, ..., r_k) \in \mathbb{R}^{k}$ というベクトルで表す。
    - ここで $r_i$ は、i番目のポジションに提示されたアイテム $a(i)$ に対する報酬を表す。
- ランキング方策(i.e. ランキング集合 $\prod(A)^{k}$ 上の条件付き確率分布)
  - = $\pi_{\theta}(\mathbf{a}|x)$ と表記する。
  - ここで $\theta$ は、ランキング方策のパラメータを表す。
  - 条件付き確率分布の意味: ある特徴量ベクトル $x$ で表されるデータ(ユーザ)に対して、$\mathbf{a}$ という特定のランキングが提示される確率を表す。
- ランキング方策の性能: 
  - 「各ポジションで観測される報酬の重み付け和の期待値」で定義されることが通例。

$$
V(\pi) := E_{p(x) \pi(\mathbf{a}|x) p(\mathbf{r}|\mathbf{a}, x)}[\sum_{i=1}^{k} \alpha_i r_i]
\\
= E_{p(x) \pi(\mathbf{a}|x)}[\sum_{i=1}^{k} \alpha_i E_{p(\mathbf{r}|\mathbf{a}, x)}[r_i]]
\\
= E_{p(x) \pi(\mathbf{a}|x)}[\sum_{i=1}^{k} \alpha_i q_i(x, a(i))]
$$

- ここで、
  - $\alpha_i$ は、分析者によって設定されるi番目のポジションの重要度。
  - $q_i(x, a(i)) := E_{p(\mathbf{r}|\mathbf{a}, x)}[r_i]$ は、特徴量xとランキング $\mathbf{a}$ で条件づけた時のk番目のポジションで発生する報酬 $r_i$ の期待値を表す。
