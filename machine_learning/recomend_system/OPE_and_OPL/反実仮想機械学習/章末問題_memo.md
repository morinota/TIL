## 第0章

### 問題0.2

問: 教師あり学習における経験リスクが、期待リスクに対する不偏推定量であること、すなわち $L(f) = E_{p(D)}[\hat{L}_{AVG}(f;D)$ であることを示せ。

期待リスクの定義式:

$$
L(f) := E_{p(x,y)}[l(y, f(x))]
$$

(ここで $l(y, f(x))$ は、予測誤差(yとf(x)の乖離度合い)を評価するための損失関数。)

経験リスク(経験平均に基づくAVG推定量)の定義式:

$$
\hat{L}_{AVG}(f;D) := \frac{1}{n} \sum_{i=1}^{n} l(y_i, f(x_i))
$$

---

まず期待リスクの定義について整理。
期待リスク $L(f) := E_{p(x,y)}[l(y, f(x))]$ は、「真の分布 $p(x,y)$」に従ってサンプル(x,y)を取り出した時の、平均的な損失を表すようなイメージ!

次に経験リスクの定義について整理。
「手元にあるサンプルで計算した平均損失」みたいなイメージ!

では、経験リスクが期待リスクの不偏推定量であることの証明に移る。

重要なポイントは、「観測データ D の中の各サンプル $(x_i, y_i)$ は、いずれも真の分布 $p(x,y)$ に従って**独立に同分布で抽出**される(i.i.d.)」という仮定を使うこと。

まずは経験リスクの期待値を考える。

$$
E_{p(D)}[\hat{L}_{AVG}(f;D)] = E_{p(D)}[\frac{1}{n} \sum_{i=1}^{n} l(y_i, f(x_i))]
$$

期待値の線形性を使って分解すると...

$$
= \frac{1}{n} \sum_{i=1}^{n} E_{p(D)}[l(y_i, f(x_i))]
$$

ここで、上述した重要なポイントより、各サンプル $(x_i, y_i)$ はi.i.d.な抽出に従うので、どのサンプルiでも、その期待値は以下のように同じになる! $E_{p(D)}[l(y_i, f(x_i))] = E_{p(x,y)}[l(y, f(x))] = L(f)$

よって、sum記号の中身が i に依存しないので、外に出せるので...!!

$$
= \frac{1}{n} \sum_{i=1}^{n} L(f) = L(f)
$$

以上より、経験リスク $\hat{L}_{AVG}(f;D)$ は、期待リスク $L(f)$ の不偏推定量であることが示された。
($D \in p(x,y)$ という仮定が前提として成立しない場合は、この証明は成り立たないので注意...!:thinking:)

### 問題0.3

問い: 傾向スコアについて、$e(x) = E_{p(w|x)}[w] = p(w=1|x)$ であることを示せ。

(ここで、$x$ は分布 $p(x)$ に従う特徴量、$w$ は介入の有無を表す２値確率変数とする。)
---

傾向スコア $e(x) := E_{p(w|x)}[w]$ を変形して、$p(w=1|x)$ になることを示す。

まず、期待値の定義式を使って $E_{p(w|x)}[w]$ を書き下すと、wは離散変数なのでsum記号を使って...

$$
E_{p(w|x)}[w] = \sum_{w \in {0, 1}} w \cdot p(w|x)
\\
= 0 \cdot p(w=0|x) + 1 \cdot p(w=1|x)
\\
= p(w=1|x)
$$

以上より、$e(x) = E_{p(w|x)}[w] = p(w=1|x)$ が示された。

### 問題0.4

問い: 全分散の公式(Law of total variance)を示せ。

以下の例は、「x,yの同時分布 $p(x,y)$ に従う確率変数(関数) $f(x,y)$」について書かれたバージョン。(イメージとしては、特徴量 x と目的変数 y? もしくは推薦方策に当てはめるとx=(x,a), y=r とかだろうか...!:thinking:)

$$
Var_{p(x,y)}[f(x,y)] = E_{p(x)}[Var_{p(y|x)}[f(x,y)]] + Var_{p(x)}[E_{p(y|x)}[f(x,y)]]
$$

推薦方策の場合は、具体的には例えば以下のように表現できるはず...!

$$
Var_{p(x,a,r)}[r] = E_{p(x,a)}[Var_{p(r|x,a)}[r]] + Var_{p(x,a)}[E_{p(r|x,a)}[r]]
$$

- 意味
  - 左辺: 全体の分散
  - 右辺第1項: 「(x,a)が決まった状態で、報酬rがどの程度変動するか」を示す分散(の期待値)
  - 右辺第2項: 「(x,a)が変わったときに、報酬rの期待値がどの程度変動するか」を示す分散。

---

まずは $Z:=f(x,y)$ とおいて話を簡単にする。すると、左辺の $Var_{p(x,y)}[f(x,y)]$は、「(x,y)の同時分布 $p(x,y)$ に従う確率変数Zの分散 $Var_{p(x,y)}[Z]$」と書き直せる。従って、示すべき式は...

$$
Var_{p(x,y)}[Z] = E_{p(x)}[Var_{p(y|x)}[Z]] + Var_{p(x)}[E_{p(y|x)}[Z]]
$$

続いて、条件付き期待値を足したり引いたりするテクニックを使って、左辺を整理していく...!
具体的にはまず、

$$
\mu(x) := E_{p(y|x)}[Z]
$$

と定義してみる。これは「xを固定した時の、Zの条件付き期待値」を意味する。
すると、Zは「$\mu(x)$ + そのズレ」に分解できるので...

$$
Z = Z - \mu(x) + \mu(x) % 足し引き0のテクニック
\\
= \mu(x) + (Z - \mu(x)) % 式変形して...
\\
= \mu(x) + \epsilon % ここで、Z - \mu(x) を \epsilon とおいてみる
$$

- この「足し引きの工夫」の狙い(この工夫によって、全分散を2つの項に分解しやすくなる)
  - 「$\mu(x)$ はxに対してのみ依存しており、yからは独立」という性質がある。
  - 「$\epsilon$ は、"Zの条件付き期待値 $\mu(x)$ からのズレ"」という形になるので、その期待値を計算しやすくなる。
    - (つまりp(x)に関しての期待値を取るときに、$E_{p(x)}[\epsilon] = 0$ となりそうだよね...!:thinking:)
- ちなみに...証明に、Zや$\mu(x)$ を導入する利点・欠点
  - 抽象度が上がるので、慣れないうちはちょっと混乱するかも。
  - 一方で、「定番の確率論の計算テクニック」でサクサク進められるようになり、証明の過程を整理しやすくなる。
  - 結局「人による」が、数学的に**汎用の道具立て**を使えるようになるのはメリットが大きいので...!

さてこの $Z = \mu(x) + \epsilon$ を使って、左辺の分散を計算していくと...

$$
Var_{p(x,y)}[Z] = Var_{p(x,y)}[\mu(x) + \epsilon]
$$

分散の性質 ($Var[A + B] = Var[A] + Var[B] + 2Cov[A, B]$) を使って展開すると...

$$
= Var_{p(x,y)}[\mu(x)] + Var_{p(x,y)}[\epsilon] + 2Cov_{p(x,y)}[\mu(x), \epsilon]
$$

ここで、cross termの共分散が0になることを示すと...

- $\mu(x)$ はxに対してのみ依存しておりyからは独立である。
- 一方で、$\epsilon$ は「Zの条件付き期待値 $\mu(x)$ からのズレ」なので、定義上、$E_{p(y|x)}[\epsilon] = 0$ である。よって、同時分布に対する期待値も $E_{p(x,y)}[\epsilon] =E_{p(x)}[E_{p(y|x)}[\epsilon]] = E_{p(x)}[定数0] = 0$ となる。

それらを踏まえると、共分散の値は...

$$
Cov_{p(x,y)}[\mu(x), \epsilon] = E_{p(x,y)}[\mu(x) \cdot \epsilon] - E_{p(x,y)}[\mu(x)] \cdot E_{p(x,y)}[\epsilon]
\\
= E_{p(x,y)}[\mu(x) \cdot \epsilon] - 0 % 前述の通り E_{p(x,y)}[\epsilon]=0なので...!!
\\
= E_{p(x)}[E_{p(y|x)}[\mu(x) \cdot \epsilon]] % 「同時分布に関する期待値」を、「条件付き期待値の期待値」に落とし込むテクニックを使った!
\\
= E_{p(x)}[\mu(x) \cdot E_{p(y|x)}[\epsilon]] % mu(x)はyに依存しない定数なので、条件付き期待値の外に出せる!
\\
= E_{p(x)}[\mu(x) \cdot 0] % 再びE_{p(y|x)}[\epsilon] = 0 なので...
\\
= 0
$$

これにより、左辺の分散の式は、以下の「2項+共分散0」というシンプルな形になる。

$$
= Var_{p(x,y)}[\mu(x)] + Var_{p(x,y)}[\epsilon]
$$

ここからは、まず第1項が「条件付き期待値の分散」になるように整理していく。
というか、抽象化していた $\mu(x)$ を具体的に戻していくと...

$$
Var_{p(x,y)}[\mu(x)] = Var_{p(x)}[E_{p(y|x)}[Z]]
$$

これで、第1項が「条件付き期待値の分散」になったので、あとは第2項が「条件付き分散の期待値」になるように整理すれば証明完了のはず。

分散の定義式 ($Var[W] = E[W^2] - E[W]^2$) を使って展開すると...

$$
Var_{p(x,y)}[\epsilon] = E_{p(x,y)}[\epsilon^2] - E_{p(x,y)}[\epsilon]^2
$$

ここで、前述の通り $E_{p(x,y)}[\epsilon] = 0$ なので、

$$
= E_{p(x,y)}[\epsilon^2] - 0
$$

残った二乗の期待値を、$\epsilon = Z - E_{p(y|x)}[Z]$ という形に戻した上で展開・整理していくと...

$$
= E_{p(x,y)}[(Z - E_{p(y|x)}[Z])^2] % epsilonの定義を戻す
\\
= E_{p(x)}[E_{p(y|x)}[(Z - E_{p(y|x)}[Z])^2]] % 「同時分布に関する期待値」を「条件付き期待値の期待値」の二段階の期待値に落とし込むテクニック!
\\
= E_{p(x)}[Var_{p(y|x)}[Z]] % 条件付き分散の定義式を発見して置き換え!
$$

これで「条件付き分散の期待値」に一致することを確認できた。

以上より、全分散の公式は示された。

## 第一章

### 問題1.1

問い: 報酬rがクリック有無などのbinary変数( $r \in {0, 1}$ )の場合、$q(x,a) = E[r|x,a] = p(r=1|x,a)$ であることを示せ。

---

要するにこの問題は、「報酬が二値変数の場合の、報酬の条件付き期待値は、つまり報酬が1である確率だよね！」ということを示せ、という話。
数式で示すと以下。

$$
E[r|x,a] = \sum_{r \in {0, 1}} r \cdot p(r|x,a)
\\
= 0 \cdot p(r=0|x,a) + 1 \cdot p(r=1|x,a)
$$

前半の項は0だし、後半の項はそのまま  $p(r=1|x,a)$ なので、結局 $E[r|x,a] = p(r=1|x,a)$ となる。

### 問題1.4

問い: 重要度重み $w(x_i,a_i) = \frac{\pi(a_i|x_i)}{\pi_0(a_i|x_i)}$ の和の期待値が、データ数 n に一致すること $E_{p(D)}[\sum_{i=1}^{n} w(x_i, a_i)]=n$ を示せ。

---

$D = {(x_i, a_i, r_i)}_{i=1}^{n}$ だが、報酬 r に関しては今回は影響しないので、$D = {(x_i, a_i)}_{i=1}^{n}$ として考えれば問題ない。

期待値の線形性を使うと...

$$
E_{p(D)}[\sum_{i=1}^{n} w(x_i, a_i)] = \sum_{i=1}^{n} E_{p(D)}[w(x_i, a_i)]
$$

に分解できる。あとは、sumの中身の各項が、それぞれ1になることを示せば良い。
(たぶん要するに、$E_{p(x,a)}[w(x, a)] = 1$ を示すことができればOKのはず...!:thinking:)

各サンプルの期待値を求めると...

$$
E_{p(D)}[w(x_i, a_i)] = E_{x_i \sim p(x), a_i \sim \pi_0(\odot|x_i)}[\frac{\pi(a_i|x_i)}{\pi_0(a_i|x_i)}]
$$

これを書き下すと...

$$
E_{p(x, a)}[\frac{\pi(a|x)}{\pi_0(a|x)}] = \int p(x) \sum_{a_i} \pi_0(a_i|x) \frac{\pi(a_i|x)}{\pi_0(a_i|x)} dx
$$

(行動 a は連続的な場合は sumの部分が $\int$ になるはずだが、アイデアは同じ!)

分子と分母が約分できるので...

$$
= \int p(x) \sum_{a_i} \pi(a_i|x) dx
$$

ここで、方策 $\pi$ は条件付き確率分布として定義してるので、各行動を選択する確率の総和 $\sum_{a_i} \pi(a_i|x) = 1$ となる。なので...

$$
= \int p(x) dx = 1
$$

よって、$E_{p(D)}[w(x_i, a_i)] = 1$ が示されたので、$E_{p(D)}[\sum_{i=1}^{n} w(x_i, a_i)] = n$ が示されたことになる！

### 問題1.5

問い: 本章では、オフ方策評価においてよく用いられる方策の性能として、$V(\pi) = E_{p(x) \pi(a|x) p(r|x, a)}[r]$ を用いたが、これは方策の性能の唯一の定義ではない。方策の性能の定義として $V(\pi) = E_{p(x) \pi(a|x) p(r|x, a)}[r]$ が適切ではない状況の例を挙げ、その場合の方策の性能としてより適切な定義を提案せよ。

---

例として、**リスク回避**が必要な状況を考えてみるとわかりやすい。例えば治療や医療系の意思決定とか、金融トレーディングとかで「**期待報酬(平均リターン)が高いけど、最悪ケースだとめちゃめちゃ痛い目にあう**」みたいな場合もあるはず。そういう場合では単純に $V(\pi) = E_{p(x) \pi(a|x) p(r|x, a)}[r]$ (=平均リターン)を最大化する方策が、実運用的に見て必ずしも良いとは限らないわけである。

なぜ期待値だけじゃだめなの??

- 平均リターンが同じでも、ばらつき(分散やリスク)が全然異なる可能性がある。
- 特に医療では「最悪の副作用をなるべく避けたい」「滅多にないけど致命的な負けパターンを防ぎたい」という観点が大事。
- 金融トレーディングでも「リスクを抑えながら、ある程度のリターンを期待したい」って考え方が普通だから、ただの期待リターンじゃ不十分。

代わりにどんな指標を使う??

1. リスクに応じた報酬評価

例えばリスクを入れ込んだ

$$
V_{risk-sensitive}(\pi) = E[r] - \alpha Var[r]
$$

みたいな "期待値 - 分散ペナルティ"の形とか。

2. 安全性を制約に入れて最適化

「一定以上の安全性を確保した上で期待報酬を最大化したい」といった場合に、

$$
\max_{\pi} E[r] \subjectto P(r < r_{threshold}) < \epsilon
$$

とか、あるいは

$$
\max_{\pi} E[r] \subjectto Var[r] < \epsilon
$$

### 問題1.6

DM推定量の定義は以下。

$$
\hat{V}_{DM}(\pi;D, \hat{q}) = \frac{1}{n} \sum_{i=1}^{n} \sum_{a \in A} \pi(a|x_i) \hat{q}(x_i, a)
$$

問い: DM推定量に用いる期待報酬関数の推定モデル $\hat{q}(x,a)$ を得る方法の一例として、以下の手順を紹介した。

$$
\hat{q}(x,a) = \argmin_{q' \in Q} 1/n \sum_{i=1}^{n} l_{r}(r_i, q'(x_i, a_i))
$$

この手順を、以下で示されるDM推定量のバイアスを参考に、より適切な手順へと修正せよ。

$$
Bias[\hat{V}_{DM}(\pi;D, \hat{q})] = E_{p(x)\pi(a|x)}[\Delta_{q, q^}(x, a)]
$$

---

今の期待報酬関数の推定モデルの学習ステップだと、「観測された $(x_i, a_i, r_i)$ に対してそのままfittingする」っていうのが基本方針になっている。
評価方策での期待値推定により特化させたいので、「評価方策の分布を意識した学習」をするように変えてあげれば、DM推定量のバイアスを減らしやすいのかも...??:thinking:
例えば、評価方策が選びそうな状態-行動ペアに対して、より重視してfittingするように修正すると良さそう。

## 第五章

### 問題5.1

問い: 下式で示される方策 $\pi^*$ が、性能 $V(\pi)$ を最大化する最適方策であることを示せ。

$$
% q(x,a) が最大の行動を確率1.0で選択、それ以外を確率0.0で選択する方策
\pi^*(a|x) = \begin{cases}
1 & \text{if } a = \argmax_{a' \in A} q(x, a') \\
0 & \text{otherwise}
\end{cases}
$$

ここで、$q(x,a)$ は状態-行動ペア $(x,a)$ に対する期待報酬関数である。
なお、方策の性能 $V(\pi)$ は $V(\pi) = E_{p(x) \pi(a|x) p(r|x, a)}[r]$ で定義される。

---

この問題は「コンテキスト $x$ ごとに $q(x,a)$ が最大の行動を確率1でとる方策が、$V(\pi)$ を最大化する最適方策になる」ことを示せばOK。

まず性能 $V(\pi^*)$ を書き下していくと...

$$
V(\pi^*) = E_{p(x) \pi^*(a|x) p(r|x, a)}[r] % 性能の定義式より
\\
= E_{p(x)\pi^*(a|x)}[q(x, a)] % 期待報酬関数の定義を使って
$$

ここで、$\pi^*(a|x)$ は「$q(x,a)$ が最大の行動を確率1で選択」するので、実質的に...

$$
= E_{p(x)}[max_{a \in A} q(x, a)]
$$

つまり、各コンテキスト $x$ において行動 $a$ のうち最も期待報酬が高い値の、$p(x)$ に関する期待値を取ることになる。

状態ごとに期待報酬が最大の行動を必ず選ぶ方策が、期待報酬の観点で最強。サブ最適な行動を選ぶ確率がないから、得られる報酬は常に最大側に寄せられる。なので、任意の方策 $\pi$ に対して以下が必ず成り立つはず...

$$
V(\pi) = E_{p(x) \pi(a|x)}[q(x, a)] \leq E_{p(x)}[max_{a \in A} q(x, a)] = V(\pi^*)
$$

以上より、$\pi^*$ が $V(\pi)$ を最大化する最適方策であることが示された。

### 問題5.2

問い: 推定量 $\hat{\nabla_{\theta} V}_{LPI}(\pi_{\theta};D) := \frac{1}{n} \sum_{i=1}^{n} r_{i} \nabla_{\theta} \log \pi_{\theta}(a_i|x_i)$ が、方策 $\pi_{\theta}$ の真の性能下界を最大化する問題(式5.41)を解くための方策勾配 $E_{p(x)\pi_{0}(a|x)}[q(x,a) \nabla_{\theta} \log \pi_{\theta}(a|x)]$ に対して不変であることを示せ。

式5.41:

$$
\max_{\theta} E_{p(x)\pi_{0}(a|x)}[q(x,a) \log \pi_{\theta}(a|x)]
$$

---

#### 目的関数の確認

問題で与えられてる式 5.41 は、方策勾配法の目的関数の一般形を表している。これを最大化するための勾配を取りたいわけ。

$$
\max_{\theta} E_{p(x)\pi_{0}(a|x)}[q(x,a) \log \pi_{\theta}(a|x)]
$$

上の目的関数を $J(\theta)$ とおくと、その $\theta$ による勾配は...

$$
\nabla_{\theta} J(\theta) = \nabla_{\theta} E_{p(x)\pi_{0}(a|x)}[q(x,a) \log \pi_{\theta}(a|x)]
$$

期待値の勾配は、「期待値の中身の勾配の期待値」と等しいので(定理としてよく使う!)、

$$
= E_{p(x)\pi_{0}(a|x)}[q(x,a) \nabla_{\theta} \log \pi_{\theta}(a|x)]
% ちなみにq(x,a)は\thetaに依存しないので、外に出せる
$$

これが問題文にある「方策勾配」である。

さて、ここで問題の推定量 $\hat{\nabla_{\theta} V}_{LPI}(\pi_{\theta};D)$ の、観測データの分布 $p(D)$ に関する期待値を考えると...

$$
E_{p(D)}[\hat{\nabla_{\theta} V}_{LPI}(\pi_{\theta};D)]
\\
= E_{p(D)}[\frac{1}{n} \sum_{i=1}^{n} r_{i} \nabla_{\theta} \log \pi_{\theta}(a_i|x_i)]
\\
= \frac{1}{n} \sum_{i=1}^{n} E_{p(x)\pi_{0}(a|x)p(r|x,a)}[r_{i} \nabla_{\theta} \log \pi_{\theta}(a_i|x_i)] % 期待値の線形性を使って、sumの中身に展開。
\\
= E_{p(x)\pi_{0}(a|x)p(r|x,a)}[r \nabla_{\theta} \log \pi_{\theta}(a|x)] % 各サンプルはi.i.d.に抽出されるので、sum記号の外に出せる!
\\
= E_{p(x)\pi_{0}(a|x)}[q(x,a) \nabla_{\theta} \log \pi_{\theta}(a|x)] % 「同時分布に関する期待値」を、「条件付き期待値の期待値」に落とし込むテクニック & 期待報酬関数の定義を使って
$$

以上より、LPI推定量の観測データの分布 $p(D)$ に関する期待値は、方策勾配の期待値と等しいことがわかったので、LPI推定量は方策勾配に対して不偏であることが示された。

### 問題5.3
