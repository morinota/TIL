## 第0章

### 問題0.2

問: 教師あり学習における経験リスクが、期待リスクに対する不偏推定量であること、すなわち $L(f) = E_{p(D)}[\hat{L}_{AVG}(f;D)$ であることを示せ。

期待リスクの定義式:

$$
L(f) := E_{p(x,y)}[l(y, f(x))]
$$

(ここで $l(y, f(x))$ は、予測誤差(yとf(x)の乖離度合い)を評価するための損失関数。)

経験リスク(経験平均に基づくAVG推定量)の定義式:

$$
\hat{L}_{AVG}(f;D) := \frac{1}{n} \sum_{i=1}^{n} l(y_i, f(x_i))
$$

---

まず期待リスクの定義について整理。
期待リスク $L(f) := E_{p(x,y)}[l(y, f(x))]$ は、「真の分布 $p(x,y)$」に従ってサンプル(x,y)を取り出した時の、平均的な損失を表すようなイメージ!

次に経験リスクの定義について整理。
「手元にあるサンプルで計算した平均損失」みたいなイメージ!

では、経験リスクが期待リスクの不偏推定量であることの証明に移る。

重要なポイントは、「観測データ D の中の各サンプル $(x_i, y_i)$ は、いずれも真の分布 $p(x,y)$ に従って**独立に同分布で抽出**される(i.i.d.)」という仮定を使うこと。

まずは経験リスクの期待値を考える。

$$
E_{p(D)}[\hat{L}_{AVG}(f;D)] = E_{p(D)}[\frac{1}{n} \sum_{i=1}^{n} l(y_i, f(x_i))]
$$

期待値の線形性を使って分解すると...

$$
= \frac{1}{n} \sum_{i=1}^{n} E_{p(D)}[l(y_i, f(x_i))]
$$

ここで、上述した重要なポイントより、各サンプル $(x_i, y_i)$ はi.i.d.な抽出に従うので、どのサンプルiでも、その期待値は以下のように同じになる! $E_{p(D)}[l(y_i, f(x_i))] = E_{p(x,y)}[l(y, f(x))] = L(f)$

よって、sum記号の中身が i に依存しないので、外に出せるので...!!

$$
= \frac{1}{n} \sum_{i=1}^{n} L(f) = L(f)
$$

以上より、経験リスク $\hat{L}_{AVG}(f;D)$ は、期待リスク $L(f)$ の不偏推定量であることが示された。
($D \in p(x,y)$ という仮定が前提として成立しない場合は、この証明は成り立たないので注意...!:thinking:)

### 問題0.3

問い: 傾向スコアについて、$e(x) = E_{p(w|x)}[w] = p(w=1|x)$ であることを示せ。

(ここで、$x$ は分布 $p(x)$ に従う特徴量、$w$ は介入の有無を表す２値確率変数とする。)
---

傾向スコア $e(x) := E_{p(w|x)}[w]$ を変形して、$p(w=1|x)$ になることを示す。

まず、期待値の定義式を使って $E_{p(w|x)}[w]$ を書き下すと、wは離散変数なのでsum記号を使って...

$$
E_{p(w|x)}[w] = \sum_{w \in {0, 1}} w \cdot p(w|x)
\\
= 0 \cdot p(w=0|x) + 1 \cdot p(w=1|x)
\\
= p(w=1|x)
$$

以上より、$e(x) = E_{p(w|x)}[w] = p(w=1|x)$ が示された。

## 第一章

### 問題1.1

問い: 報酬rがクリック有無などのbinary変数( $r \in {0, 1}$ )の場合、$q(x,a) = E[r|x,a] = p(r=1|x,a)$ であることを示せ。

---

要するにこの問題は、「報酬が二値変数の場合の、報酬の条件付き期待値は、つまり報酬が1である確率だよね！」ということを示せ、という話。
数式で示すと以下。

$$
E[r|x,a] = \sum_{r \in {0, 1}} r \cdot p(r|x,a)
\\
= 0 \cdot p(r=0|x,a) + 1 \cdot p(r=1|x,a)
$$

前半の項は0だし、後半の項はそのまま  $p(r=1|x,a)$ なので、結局 $E[r|x,a] = p(r=1|x,a)$ となる。

### 問題1.4

問い: 重要度重み $w(x_i,a_i) = \frac{\pi(a_i|x_i)}{\pi_0(a_i|x_i)}$ の和の期待値が、データ数 n に一致すること $E_{p(D)}[\sum_{i=1}^{n} w(x_i, a_i)]=n$ を示せ。

---

$D = {(x_i, a_i, r_i)}_{i=1}^{n}$ だが、報酬 r に関しては今回は影響しないので、$D = {(x_i, a_i)}_{i=1}^{n}$ として考えれば問題ない。

期待値の線形性を使うと...

$$
E_{p(D)}[\sum_{i=1}^{n} w(x_i, a_i)] = \sum_{i=1}^{n} E_{p(D)}[w(x_i, a_i)]
$$

に分解できる。あとは、sumの中身の各項が、それぞれ1になることを示せば良い。
(たぶん要するに、$E_{p(x,a)}[w(x, a)] = 1$ を示すことができればOKのはず...!:thinking:)

各サンプルの期待値を求めると...

$$
E_{p(D)}[w(x_i, a_i)] = E_{x_i \sim p(x), a_i \sim \pi_0(\odot|x_i)}[\frac{\pi(a_i|x_i)}{\pi_0(a_i|x_i)}]
$$

これを書き下すと...

$$
E_{p(x, a)}[\frac{\pi(a|x)}{\pi_0(a|x)}] = \int p(x) \sum_{a_i} \pi_0(a_i|x) \frac{\pi(a_i|x)}{\pi_0(a_i|x)} dx
$$

(行動 a は連続的な場合は sumの部分が $\int$ になるはずだが、アイデアは同じ!)

分子と分母が約分できるので...

$$
= \int p(x) \sum_{a_i} \pi(a_i|x) dx
$$

ここで、方策 $\pi$ は条件付き確率分布として定義してるので、各行動を選択する確率の総和 $\sum_{a_i} \pi(a_i|x) = 1$ となる。なので...

$$
= \int p(x) dx = 1
$$

よって、$E_{p(D)}[w(x_i, a_i)] = 1$ が示されたので、$E_{p(D)}[\sum_{i=1}^{n} w(x_i, a_i)] = n$ が示されたことになる！

### 問題1.5

問い: 本章では、オフ方策評価においてよく用いられる方策の性能として、$V(\pi) = E_{p(x) \pi(a|x) p(r|x, a)}[r]$ を用いたが、これは方策の性能の唯一の定義ではない。方策の性能の定義として $V(\pi) = E_{p(x) \pi(a|x) p(r|x, a)}[r]$ が適切ではない状況の例を挙げ、その場合の方策の性能としてより適切な定義を提案せよ。

---

例として、**リスク回避**が必要な状況を考えてみるとわかりやすい。例えば治療や医療系の意思決定とか、金融トレーディングとかで「**期待報酬(平均リターン)が高いけど、最悪ケースだとめちゃめちゃ痛い目にあう**」みたいな場合もあるはず。そういう場合では単純に $V(\pi) = E_{p(x) \pi(a|x) p(r|x, a)}[r]$ (=平均リターン)を最大化する方策が、実運用的に見て必ずしも良いとは限らないわけである。

なぜ期待値だけじゃだめなの??

- 平均リターンが同じでも、ばらつき(分散やリスク)が全然異なる可能性がある。
- 特に医療では「最悪の副作用をなるべく避けたい」「滅多にないけど致命的な負けパターンを防ぎたい」という観点が大事。
- 金融トレーディングでも「リスクを抑えながら、ある程度のリターンを期待したい」って考え方が普通だから、ただの期待リターンじゃ不十分。

代わりにどんな指標を使う??

1. リスクに応じた報酬評価

例えばリスクを入れ込んだ

$$
V_{risk-sensitive}(\pi) = E[r] - \alpha Var[r]
$$

みたいな "期待値 - 分散ペナルティ"の形とか。

2. 安全性を制約に入れて最適化

「一定以上の安全性を確保した上で期待報酬を最大化したい」といった場合に、

$$
\max_{\pi} E[r] \subjectto P(r < r_{threshold}) < \epsilon
$$

とか、あるいは

$$
\max_{\pi} E[r] \subjectto Var[r] < \epsilon
$$
