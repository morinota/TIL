# マッチング市場のオフライン評価が難しい問題を「中間ラベル」を活用して解決する新手法！Wantedlyの実データで効果を実証

## 3行まとめ

- **マッチング市場**（求人サイトやデーティングアプリなど）では、**双方の合意が必要で報酬が希薄**なため、従来のオフポリシー評価（OPE）手法（IPS、DM、DR）は分散が大きくなり信頼性に欠ける
- 本論文では、**第一段階の中間ラベル**（例：スカウト送信）を明示的に活用する新しいOPE推定量「**DiPS**」と「**DPR**」を提案し、バイアス-分散のトレードオフを改善
- Wantedlyの実データとA/Bテストログを用いた実験で、提案手法が既存手法を上回る性能を示し、実際のA/Bテスト結果をより正確に予測できることを実証

## 導入: 本論文のモチベーション

- 推薦システムのオフライン評価の重要性
  - A/Bテストのコストとリスクを回避
  - 新しい推薦アルゴリズムの性能を事前に評価できる重要な技術

- マッチング市場における課題
  - 従来のOPE手法が大きな課題に直面
  - 対象：求人サイト、デーティングアプリ、不動産仲介など

- マッチング市場の特徴と数理的定式化
  - **双方向の合意が必要**：成功したマッチは両者が互いに興味を示したときのみ発生
  - **報酬の希薄性**：成功したマッチの発生確率が非常に低い
  - **大規模なアクション空間**：推薦候補が数百万〜数億規模
  
  - 数学的定式化
    - 企業 $c \in \mathcal{C}$、求職者 $j \in \mathcal{J}$
    - 推薦方策：$\pi(j|c)$ （企業 $c$ に求職者 $j$ を推薦する確率）
    - 3段階の報酬構造：
      - $s$：第一段階報酬（スカウト送信）$s \in \{0,1\}$
      - $r$：第二段階報酬（応答）$r \in \{0,1\}$  
      - $m$：最終マッチ $m = s \cdot r$
    
    - q関数の定義：
      - $q_s(c,j) := \mathbb{E}[s|c,j]$ （スカウト送信確率）
      - $q_r(c,j) := \mathbb{E}[r|c,j,s=1]$ （条件付き応答確率）
      - $q_m(c,j) := q_s(c,j) \cdot q_r(c,j)$ （最終マッチ確率）
    
    - 方策性能の定義：
      $$V(\pi) := \frac{1}{|C|} \sum_{c \in C} \sum_{j \in J} \pi(j|c) \cdot q_m(c,j)$$

- 既存手法の問題
  - 標準的なOPE手法（IPS、DM、DR）は高い分散やバイアスに悩まされる
  - 実用的でなくなってしまう
  - 本記事では、これらの課題を解決する画期的なアプローチを紹介

## マッチング市場における推薦システムの課題

- 従来のOPE手法の限界
  - 求人マッチングの例
    - 企業が求職者を見てスカウトメッセージを送信（第一段階）
    - 求職者がそれに応答して初めてマッチが成立（第二段階）

- 従来手法の数理的問題分析

- IPS推定量の問題点
  - 定式化：
    $$\hat{V}_{\mathrm{IPS}}(\pi;D) = \frac{1}{|C|} \sum_{c \in C} \frac{\pi(j_c|c)}{\pi_0(j_c|c)} m_{c}$$
    
  - **超高分散**の数理的原因：
    $$\text{Var}[\hat{V}_{\mathrm{IPS}}] = \frac{1}{|C|^2} \sum_{c \in C} \mathbb{E}_{\pi(j|c)}\left[w^2(c,j) \cdot \sigma_m^2(c,j)\right] + \text{Var}_{\pi(j|c)}[w(c,j) \cdot q_m(c,j)]$$
    
    ここで $w(c,j) = \frac{\pi(j|c)}{\pi_0(j|c)}$ は重要度重み
    
  - **報酬のノイズ増幅**：
    - ベルヌーイ分布の相対分散：$\frac{\sigma_m^2(c,j)}{q_m(c,j)} = \frac{q_m(c,j)(1-q_m(c,j))}{q_m(c,j)} = 1-q_m(c,j)$
    - $q_m(c,j)$ が小さいほど相対的なノイズが増大

- DM推定量の問題点
  - 定式化：
    $$\hat{V}_{\mathrm{DM}}(\pi;\mathcal{D}) := \frac{1}{|\mathcal{C}|} \sum_{c \in C} \mathbb{E}_{\pi(j|c)}[\hat{q}_{m}(c,j)]$$
    
  - **高バイアス**の原因：
    $$\text{Bias}(\hat{V}_{\mathrm{DM}}) = \frac{1}{|C|} \sum_{c \in C} \mathbb{E}_{\pi(j|c)}[\Delta_{q_m,\hat{q}_m}(c,j)]$$
    
    ここで $\Delta_{q_m,\hat{q}_m}(c,j) = \hat{q}_m(c,j) - q_m(c,j)$ はモデル推定誤差
    
  - **学習困難性**：スパースな正例により $\hat{q}_m(c,j)$ の精度が低下

- DR推定量の問題点
  - 定式化：
    $$\hat{V}_{\mathrm{DR}}(\pi;\mathcal{D}) = \hat{V}_{\mathrm{DM}}(\pi;\mathcal{D}) + \frac{1}{|C|} \sum_{c \in C} \frac{\pi(j_c|c)}{\pi_0(j_c|c)} (m_c - \hat{q}_m(c,j_c))$$
    
  - **分散問題の残存**：IPSの分散項が依然として存在
  - **バイアス問題の持続**：$\hat{q}_m(c,j)$ の精度に依然として依存

## 提案手法：中間ラベルを活用した新しいOPE推定量

- 核心アイデア：第一段階報酬の明示的活用
  - 従来手法の問題
    - 最終的なマッチラベル（m = s × r）のみに着目
  - 提案手法のアプローチ
    - **第一段階の中間ラベル**（例：スカウト送信 s）を明示的に活用

- 重要な洞察
  - 第一段階の報酬（s）は最終マッチ（m）よりも密に観測される
  - 第二段階の条件付き確率（q_r(c,j) = E[r|c,j,s=1]）は相対的に予測しやすい

- DiPS推定量（Direct and Propensity Score）
  - ハイブリッドアプローチの定式化
    $$\hat{V}_{\mathrm{DiPS}}(\pi;\mathcal{D}) := \frac{1}{|C|} \sum_{c \in C} \left(\frac{\pi(j_c|c)}{\pi_0(j_c|c)} \cdot s_c\right) \cdot \hat{q}_r(c,j_c)$$
    
    ここで：
    - $\pi(j_c|c)$：新しい推薦方策による確率
    - $\pi_0(j_c|c)$：ログ方策（データ収集方策）による確率
    - $s_c$：第一段階の報酬（例：スカウト送信）
    - $\hat{q}_r(c,j_c)$：第二段階の条件付き期待報酬の推定モデル
    
  - 戦略的分業
    - **第一段階**：IPSを適用して $q_s(c,j) = \mathbb{E}[s|c,j]$ を不偏推定
    - **第二段階**：回帰モデル $\hat{q}_r(c,j) = \mathbb{E}[r|c,j,s=1]$ で条件付き期待値を予測

- DPR推定量（Direct, Propensity, and doubly Robust）
  - DiPSをさらに強化した定式化
    $$\hat{V}_{\mathrm{DPR}}(\pi;\mathcal{D}) := \hat{V}_{\mathrm{DiPS}}(\pi;\mathcal{D}) + \frac{1}{|C|} \sum_{c \in C} \mathbb{E}_{\pi(j|c)}[\hat{q}_m(c,j) - \hat{q}_s(c,j) \cdot \hat{q}_r(c,j)]$$
    
    ここで：
    - $\hat{q}_m(c,j)$：最終マッチ確率の直接推定モデル
    - $\hat{q}_s(c,j) \cdot \hat{q}_r(c,j)$：DiPSによる分解された推定
    
  - 分散制御メカニズム
    - 制御変数項 $[\hat{q}_m(c,j) - \hat{q}_s(c,j) \cdot \hat{q}_r(c,j)]$ により分散を削減
    - 直接マッチ予測モデルの精度に応じて効果が向上

- 理論的優位性
  - バイアス分析
    - DiPSのバイアス特性：第二段階の報酬推定誤差のみに依存
      $$\text{Bias}(\hat{V}_{\mathrm{DiPS}}(\pi;\mathcal{D})) = \frac{1}{|C|} \sum_{c \in C} \mathbb{E}_{\pi(j|c)} [q_s(c,j) \cdot \Delta_{q_r,\hat{q}_r}(c,j)]$$
      
      ここで $\Delta_{q_r,\hat{q}_r}(c,j) := \hat{q}_r(c,j) - q_r(c,j)$ は第二段階報酬推定誤差
      
    - 理論的優位性
      - この項は、スパースな最終マッチを直接予測するDMのバイアスよりも小さくなることが期待
      - 第一段階はIPSにより不偏推定されるため、バイアスは第二段階の推定精度のみに依存

  - 分散分析  
    - DiPS分散の構造的特徴
      $$\text{Var}[\hat{V}_{\mathrm{DiPS}}(\pi;\mathcal{D})] = \frac{1}{|C|^2} \sum_{c \in C} \left\{\mathbb{E}_{\pi(j|c)}[w^2(c,j) \cdot \sigma_s^2(c,j) \cdot \hat{q}_r^2(c,j)] + \text{Var}_{\pi(j|c)}[w(c,j) \cdot q_s(c,j) \cdot \hat{q}_r(c,j)]\right\}$$
      
      ここで：
      - $w(c,j) = \frac{\pi(j|c)}{\pi_0(j|c)}$：重要度重み
      - $\sigma_s^2(c,j) = \text{Var}[s|c,j]$：第一段階報酬の条件付き分散
      
    - IPSとの分散比較
      - DiPSの分散は第一段階報酬のノイズ $\sigma_s^2(c,j)$ に基づく
      - これはスパースなマッチラベルのノイズ $\sigma_m^2(c,j)$ よりも一般的に小さい
      
      分散削減量の下界：
      $$\text{Var}[\hat{V}_{\mathrm{IPS}}] - \text{Var}[\hat{V}_{\mathrm{DiPS}}] \geq \frac{1}{|C|^2} \sum_{c \in C} \mathbb{E}_{\pi(j|c)}[w^2(c,j) \cdot (\sigma_m^2(c,j) - \sigma_s^2(c,j) \cdot \hat{q}_r^2(c,j))]$$

## 実験結果：理論を実証する強力なエビデンス

- 合成データ実験
  - **複数設定での一貫した優位性**：DiPSとDPRが既存手法を上回るMSE性能
  - **ポリシー選択精度の向上**：Error Rateが大幅に改善
  - **スケーラビリティ**：大規模アクション空間でも安定した性能

- Wantedly実データ実験
  - 特に注目すべきは、**実際のA/Bテストデータセットを用いた検証**

  - 実験設定
    - **データ**：Wantedly Visitの本番A/Bテストログ
    - **評価指標**：実際のA/Bテスト結果の予測精度
    - **比較手法**：IPS、DM、DR vs DiPS、DPR

  - 主要な発見
    - **実用的な予測精度**：提案手法が実際のA/Bテスト結果をより正確に予測
    - **安定性の向上**：推定値の分散が大幅に減少
    - **ビジネス価値**：より信頼性の高いオフライン評価により、A/Bテストの効率が向上

## オフライン学習への拡張

- 提案手法のオフポリシー学習への拡張
  - 理論的拡張性
    - DiPS/DPRはオフポリシー学習（OPL）にも自然に拡張可能
    - 政策勾配推定における新しいアプローチを提供
    
  - DiPS政策勾配推定量
    $$\hat{\nabla}_{\theta} V_{\mathrm{DiPS}}(\pi_\theta) = \frac{1}{|C|} \sum_{c \in C} \frac{\partial \pi_\theta(j_c|c)}{\partial \theta} \cdot \frac{s_c}{\pi_0(j_c|c)} \cdot \hat{q}_r(c,j_c)$$
    
    ここで：
    - $\pi_\theta(j_c|c)$：パラメータ $\theta$ でパラメータ化された推薦方策
    - $\frac{\partial \pi_\theta(j_c|c)}{\partial \theta}$：方策の勾配
    - $\frac{s_c}{\pi_0(j_c|c)}$：第一段階の重要度加重された観測
    
  - 実用的優位性
    - 従来のIPS政策勾配よりも分散が小さい
    - より効果的なマッチング方策の学習が可能
    - 大規模なアクション空間でも安定した学習

## 実装のポイント

- データ要件
  - **第一段階ラベル**：スカウト送信、いいね、閲覧履歴など
  - **第二段階ラベル**：応答、クリック、購入など  
  - **ログ方策**：データ収集時の推薦アルゴリズム

- モデル選択
  - **q̂_r(c,j) 推定**：ロジスティック回帰、LightGBM、ニューラルネットなど
  - **重要度重み推定**：ログ方策が不明な場合は教師あり学習で推定

## おわりに

- 本研究の成果
  - マッチング市場におけるOPEの課題を**中間ラベルの戦略的活用**という新しい視点で解決
  - Wantedlyの実データを用いた実験により、実用性も実証

- 技術的貢献
  - マッチング市場向けOPEの初の体系的定式化
  - 中間ラベルを活用する新しい推定量の提案
  - 理論的性質の厳密な分析

- 実用的価値
  - A/Bテストコストの削減
  - より信頼性の高いオフライン評価
  - マッチング効率の向上

- 開発者への示唆
  - 推薦システムの実務において、**我々開発者が柔軟に中間シグナルや報酬構造を設計することで機械学習という技術をより効果的に使いこなしていきたい**という気持ちになりました

- 今後への期待
  - 最後まで読んでいただき、ありがとうございました！
  - マッチング市場でOPEに取り組んでいる方は、ぜひ第一段階の中間ラベルの活用を検討してみてください

## 参考文献

- [原論文](https://arxiv.org/abs/2507.13608): "Off-Policy Evaluation and Learning for Matching Markets"
- 書籍「反実仮想機械学習」：OPEの基礎理論について詳しく解説
- [Open Bandit Pipeline](https://github.com/st-tech/zr-obp)：OPE実装のためのオープンソースライブラリ
