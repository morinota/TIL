## link ãƒªãƒ³ã‚¯

- https://dl.acm.org/doi/10.1145/3604915.3609488 https://dl.acm.org/doi/10.1145/3604915.3609488

## title ã‚¿ã‚¤ãƒˆãƒ«

Everyoneâ€™s a Winner! On Hyperparameter Tuning of Recommendation Models
èª°ã‚‚ãŒå‹è€…ã  æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã«ã¤ã„ã¦

## abstruct abstruct

The performance of a recommender system algorithm in terms of common offline accuracy measures often strongly depends on the chosen hyperparameters.
ä¸€èˆ¬çš„ãªã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç²¾åº¦æ¸¬å®šã«ãŠã‘ã‚‹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ€§èƒ½ã¯ã€ã—ã°ã—ã°é¸æŠã•ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¼·ãä¾å­˜ã™ã‚‹ã€‚
Therefore, when comparing algorithms in offline experiments, we can obtain reliable insights regarding the effectiveness of a newly proposed algorithm only if we compare it to a number of state-of-the-art baselines that are carefully tuned for each of the considered datasets.
ã—ãŸãŒã£ã¦ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å®Ÿé¨“ã§ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ¯”è¼ƒã™ã‚‹å ´åˆã€æ¤œè¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã”ã¨ã«æ³¨æ„æ·±ãèª¿æ•´ã•ã‚ŒãŸæ•°å¤šãã®æœ€å…ˆç«¯ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒã—ãŸå ´åˆã«ã®ã¿ã€æ–°ã—ãææ¡ˆã•ã‚ŒãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æœ‰åŠ¹æ€§ã«é–¢ã™ã‚‹ä¿¡é ¼ã§ãã‚‹æ´å¯Ÿã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
While this fundamental principle of any area of applied machine learning is undisputed, we find that the tuning process for the baselines in the current literature is barely documented in much of todayâ€™s published research.
å¿œç”¨æ©Ÿæ¢°å­¦ç¿’ã®ã©ã®åˆ†é‡ã«ãŠã„ã¦ã‚‚ã€ã“ã®åŸºæœ¬åŸå‰‡ã¯è­°è«–ã®ä½™åœ°ã®ãªã„ã‚‚ã®ã§ã‚ã‚‹ãŒã€**ç¾åœ¨ç™ºè¡¨ã•ã‚Œã¦ã„ã‚‹ç ”ç©¶ã®å¤šãã§ã¯ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°éç¨‹ãŒã»ã¨ã‚“ã©æ–‡æ›¸åŒ–ã•ã‚Œã¦ã„ãªã„**ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
Ultimately, in case the baselines are actually not carefully tuned, progress may remain unclear.
çµå±€ã®ã¨ã“ã‚ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãŒæ³¨æ„æ·±ãèª¿æ•´ã•ã‚Œãªã‘ã‚Œã°ã€é€²æ­©ã¯ä¸æ˜ç­ãªã¾ã¾ã‹ã‚‚ã—ã‚Œãªã„ã€‚
In this paper, we exemplify through a computational experiment involving seven recent deep learning models how every method in such an unsound comparison can be reported to be outperforming the state-of-the-art.
æœ¬ç¨¿ã§ã¯ã€7ã¤ã®æœ€è¿‘ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸè¨ˆç®—å®Ÿé¨“ã‚’é€šã˜ã¦ã€ã“ã®ã‚ˆã†ãªä¸å¥å…¨ãªæ¯”è¼ƒã®ä¸­ã§ã€ã©ã®ã‚ˆã†ãªæ–¹æ³•ãŒæœ€å…ˆç«¯ã®æŠ€è¡“ã‚’ä¸Šå›ã£ã¦ã„ã‚‹ã¨å ±å‘Šã§ãã‚‹ã‹ã‚’ä¾‹è¨¼ã™ã‚‹ã€‚
Finally, we iterate appropriate research practices to avoid unreliable algorithm comparisons in the future.
æœ€å¾Œã«ã€å°†æ¥çš„ã«ä¿¡é ¼æ€§ã®ä½ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¯”è¼ƒã‚’é¿ã‘ã‚‹ãŸã‚ã«ã€é©åˆ‡ãªç ”ç©¶æ‰‹æ³•ã‚’åå¾©ã™ã‚‹ã€‚

# Introduction ã¯ã˜ã‚ã«

Recommender systems are a highly visible success story of applied machine learning.
ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ãƒ»ã‚·ã‚¹ãƒ†ãƒ ã¯ã€å¿œç”¨æ©Ÿæ¢°å­¦ç¿’ã®æˆåŠŸä¾‹ã¨ã—ã¦éå¸¸ã«æ³¨ç›®ã•ã‚Œã¦ã„ã‚‹ã€‚
Early reports of the value of such systems date back almost 25 years [27].
ã“ã®ã‚ˆã†ãªã‚·ã‚¹ãƒ†ãƒ ã®ä¾¡å€¤ã«é–¢ã™ã‚‹åˆæœŸã®å ±å‘Šã¯ã€ç´„25å¹´å‰ã«ã•ã‹ã®ã¼ã‚‹[27]ã€‚
Today, most major online platforms use such systems to provide personalized item suggestions to their users, nowadays often based on deep learning, see, e.g., [7, 30].
ä»Šæ—¥ã€ã»ã¨ã‚“ã©ã®ä¸»è¦ãªã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ææ¡ˆã‚’æä¾›ã™ã‚‹ãŸã‚ã«ã€ã“ã®ã‚ˆã†ãªã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã€‚
Academic research in this area is flourishing as well, and many new machine learning models or network architectures for the recommendation task are published every year.
ã“ã®åˆ†é‡ã®å­¦è¡“ç ”ç©¶ã‚‚ç››ã‚“ã§ã€æ¨è–¦ã‚¿ã‚¹ã‚¯ã®ãŸã‚ã®æ–°ã—ã„æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ»ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒæ¯å¹´æ•°å¤šãç™ºè¡¨ã•ã‚Œã¦ã„ã‚‹ã€‚
However, there are indications that the progress that is reported to be achieved in academic papers is not as strong as one could expect, Todayâ€™s publication culture more or less mandates that every new published model must significantly improve upon the â€œstate-of-the-artâ€ on various metrics and datasets.
ã—ã‹ã—ã€å­¦è¡“è«–æ–‡ã§å ±å‘Šã•ã‚Œã‚‹é€²æ­©ã¯ã€æœŸå¾…ã•ã‚Œã‚‹ã»ã©å¼·ããªã„ã¨ã„ã†æŒ‡æ‘˜ã‚‚ã‚ã‚‹ã€‚ä»Šæ—¥ã®å‡ºç‰ˆæ–‡åŒ–ã§ã¯ã€ç™ºè¡¨ã•ã‚Œã‚‹æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã¯ã™ã¹ã¦ã€ã•ã¾ã–ã¾ãªæŒ‡æ¨™ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€Œæœ€å…ˆç«¯ã€ã‚’å¤§å¹…ã«æ”¹å–„ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã“ã¨ãŒã€å¤šã‹ã‚Œå°‘ãªã‹ã‚Œç¾©å‹™ä»˜ã‘ã‚‰ã‚Œã¦ã„ã‚‹ã€‚
It was however observed in the related field of information retrieval many years ago that the reported improvements often do not add up [4].
ã—ã‹ã—ã€ä½•å¹´ã‚‚å‰ã«æƒ…å ±æ¤œç´¢ã®é–¢é€£åˆ†é‡ã§ã€å ±å‘Šã•ã‚ŒãŸæ”¹å–„ãŒã—ã°ã—ã°è¾»è¤„ãŒåˆã‚ãªã„ã“ã¨ãŒè¦³å¯Ÿã•ã‚ŒãŸ[4]ã€‚
Similar observations were made in the area of recommender systems [9, 23], as well as in other fields of applied machine learning, e.g., in time series forecasting [21].
ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ãƒ»ã‚·ã‚¹ãƒ†ãƒ [9, 23]ã®åˆ†é‡ã§ã‚‚ã€æ™‚ç³»åˆ—äºˆæ¸¬[21]ãªã©ã®å¿œç”¨æ©Ÿæ¢°å­¦ç¿’ã®ä»–ã®åˆ†é‡ã§ã‚‚ã€åŒæ§˜ã®è¦³å¯ŸãŒãªã•ã‚Œã¦ã„ã‚‹ã€‚
In these and in several other works it turned out that the latest published models are in fact often not outperforming existing models and sometimes conceptually simple or longer-known methods can reach at least similar performance levels, at least in offline evaluations [19, 24].1 There are different factors that contribute to this issue, as discussed in [9, 18, 21].
ã“ã®å•é¡Œã«ã¯ã€[9, 18, 21]ã§è­°è«–ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«ã€ã•ã¾ã–ã¾ãªè¦å› ãŒã‚ã‚‹ã€‚
Besides a certain researcher freedom when it comes to the selection of the baselines, the evaluation protocol and the metrics, one major problem seems to be that the hyperparameters of the selected baselines are often not properly tuned [20, 25], whereas significant effort may go into tuning the newly proposed model.
ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã€è©•ä¾¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®é¸æŠã«é–¢ã—ã¦ã€ç ”ç©¶è€…ãŒã‚ã‚‹ç¨‹åº¦è‡ªç”±ã«ã§ãã‚‹ã“ã¨ã«åŠ ãˆã€1ã¤ã®å¤§ããªå•é¡Œã¯ã€é¸æŠã•ã‚ŒãŸãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã—ã°ã—ã°é©åˆ‡ã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œãªã„ã“ã¨ã§ã‚ã‚‹[20, 25]ã€‚
It is clear, however, that it is impossible to derive any insights from experiments in which not all compared modelsâ€”both the proposed ones and the chosen baselinesâ€”are carefully tuned for each dataset considered in the evaluation.
ã—ã‹ã—ã€ã™ã¹ã¦ã®æ¯”è¼ƒãƒ¢ãƒ‡ãƒ«ï¼ˆææ¡ˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¨é¸æŠã•ã‚ŒãŸãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ä¸¡æ–¹ï¼‰ãŒã€è©•ä¾¡ã§è€ƒæ…®ã•ã‚ŒãŸå„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦æ³¨æ„æ·±ãèª¿æ•´ã•ã‚Œã¦ã„ãªã„å®Ÿé¨“ã‹ã‚‰ã€ã„ã‹ãªã‚‹æ´å¯Ÿã‚‚å°ãå‡ºã™ã“ã¨ãŒã§ããªã„ã“ã¨ã¯æ˜ã‚‰ã‹ã§ã‚ã‚‹ã€‚
With the considerable computational complexity of some modern deep learning models, this systematic tuning process can lead to significant demands in terms of times and resources.
æœ€è¿‘ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»ãƒ¢ãƒ‡ãƒ«ã¯è¨ˆç®—ãŒã‹ãªã‚Šè¤‡é›‘ãªãŸã‚ã€ã“ã®ä½“ç³»çš„ãªãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€æ™‚é–“ã¨ãƒªã‚½ãƒ¼ã‚¹ã®é¢ã§å¤§ããªè¦æ±‚ã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
It is therefore surprising that the documentation of this processâ€”which may easily take weeks to completeâ€”is often only very briefly covered in many papers or not mentioned at all.
ãã‚Œã‚†ãˆã€æ•°é€±é–“ã‚’è¦ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã“ã®ãƒ—ãƒ­ã‚»ã‚¹ã®æ–‡æ›¸åŒ–ãŒã€å¤šãã®è«–æ–‡ã§ã”ãç°¡å˜ã«ã—ã‹å–ã‚Šä¸Šã’ã‚‰ã‚Œã¦ã„ãªã„ã‹ã€ã¾ã£ãŸãè§¦ã‚Œã‚‰ã‚Œã¦ã„ãªã„ã“ã¨ãŒå¤šã„ã®ã¯é©šãã¹ãã“ã¨ã§ã‚ã‚‹ã€‚
In some cases, only one set of optimal hyperparameters is reported, even though more than one dataset is used.
å ´åˆã«ã‚ˆã£ã¦ã¯ã€è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚»ãƒƒãƒˆã¯1ã¤ã—ã‹å ±å‘Šã•ã‚Œã¦ã„ãªã„ã€‚
Sometimes, the hyperparameters for the baselines are taken from the original paper, and no discussion is provided if the same datasets (after pre-processing) were actually used in the original paper.
ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯åŸè‘—è«–æ–‡ã‹ã‚‰å¼•ç”¨ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚‚ã‚ã‚Šã€åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå‰å‡¦ç†å¾Œï¼‰ãŒåŸè‘—è«–æ–‡ã§å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸã‹ã©ã†ã‹ã«ã¤ã„ã¦ã®è€ƒå¯Ÿã¯ãªã„ã€‚
In yet other situations, probably the default parameters of a given public implementation of certain baselines have been used.
ã•ã‚‰ã«åˆ¥ã®çŠ¶æ³ã§ã¯ã€ãŠãã‚‰ãã‚ã‚‹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®å…¬é–‹å®Ÿè£…ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã€‚
Another observation here is that the code of the used baselines and the code that was used to automatically tune the hyperparameters for the experiments is not shared by the authors either.
ã‚‚ã†ä¸€ã¤ã®è¦³å¯Ÿã¯ã€ä½¿ç”¨ã•ã‚ŒãŸãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ã‚³ãƒ¼ãƒ‰ã¨ã€å®Ÿé¨“ã®ãŸã‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è‡ªå‹•çš„ã«èª¿æ•´ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ãŒã€è‘—è€…ãŸã¡ã«ã‚‚å…±æœ‰ã•ã‚Œã¦ã„ãªã„ã“ã¨ã§ã‚ã‚‹ã€‚
With this short essay, we would like to showcase the dangers of what might be common practice in our field.
ã“ã®çŸ­ã„ã‚¨ãƒƒã‚»ã‚¤ã§ã€ç§ãŸã¡ã®åˆ†é‡ã§ã¯å¸¸è­˜ã‹ã‚‚ã—ã‚Œãªã„ã“ã¨ã®å±é™ºæ€§ã‚’ç´¹ä»‹ã—ãŸã„ã€‚
Next, in Section 2, we report the results of the inspection of a set of recent conference papers published at highly relevant outlets for recommender systems in terms of what is reported in the papers regarding hyperparameter tuning of the baselines.
æ¬¡ã«ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³2ã«ãŠã„ã¦ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«é–¢é€£æ€§ã®é«˜ã„å‡ºç‰ˆç¤¾ã§æœ€è¿‘ç™ºè¡¨ã•ã‚ŒãŸä¸€é€£ã®ä¼šè­°è«–æ–‡ã‚’ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«é–¢ã™ã‚‹è«–æ–‡ã§å ±å‘Šã•ã‚Œã¦ã„ã‚‹å†…å®¹ã«é–¢ã—ã¦æ¤œæŸ»ã—ãŸçµæœã‚’å ±å‘Šã™ã‚‹ã€‚
We emphasize that we do not suggest that no proper hyperparameter tuning was actually done in these papers.
æˆ‘ã€…ã¯ã€ã“ã‚Œã‚‰ã®è«–æ–‡ã§é©åˆ‡ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Ÿéš›ã«è¡Œã‚ã‚Œãªã‹ã£ãŸã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã®ã§ã¯ãªã„ã“ã¨ã‚’å¼·èª¿ã™ã‚‹ã€‚
Our observations only refer to the documentation of the process in the papers.
ç§ãŸã¡ã®è¦‹è§£ã¯ã€å„ç´™ã«æ²è¼‰ã•ã‚ŒãŸãƒ—ãƒ­ã‚»ã‚¹ã®æ–‡æ›¸ã«è¨€åŠã—ã¦ã„ã‚‹ã«éããªã„ã€‚
In Section 3, we then report the outcome of an illustrative experiment, in which we tuned a set of recent models and then compared the results to those obtained when using the random parameters for the same models in a recent evaluation framework.
ã‚»ã‚¯ã‚·ãƒ§ãƒ³3ã§ã¯ã€æœ€è¿‘ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã€ãã®çµæœã‚’æœ€è¿‘ã®è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§åŒã˜ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ãƒ³ãƒ€ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸå ´åˆã®çµæœã¨æ¯”è¼ƒã—ãŸã€ä¾‹ç¤ºçš„ãªå®Ÿé¨“ã®çµæœã‚’å ±å‘Šã™ã‚‹ã€‚
Not too surprisingly, the results show that every model can be a winner when we compare its optimized version against non-optimized baselines.
é©šãã“ã¨ã§ã¯ãªã„ãŒã€æœ€é©åŒ–ã•ã‚ŒãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æœ€é©åŒ–ã•ã‚Œã¦ã„ãªã„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒã—ãŸå ´åˆã€ã©ã®ãƒ¢ãƒ‡ãƒ«ã‚‚å‹è€…ã«ãªã‚Šã†ã‚‹ã¨ã„ã†çµæœãŒå‡ºã¦ã„ã‚‹ã€‚
The paper ends with a discussion of implications and ways forward in Section 4.
æœ¬ç¨¿ã®æœ€å¾Œã‚’é£¾ã‚‹ã®ã¯ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³4ã§ã®æ„å‘³åˆã„ã¨ä»Šå¾Œã®å±•é–‹ã§ã‚ã‚‹ã€‚

# A Qulitative Analysis of the State-of-the-Practice å®Ÿè·µã®ç¾çŠ¶ã«é–¢ã™ã‚‹å®šæ€§çš„åˆ†æ

To obtain a picture of the state-of-the-practice in terms of what is reported today in research papers, we scanned the 2022 proceedings of five relevant ACM conference series, namely KDD, RecSys, SIGIR, TheWebConf, WSDM, for papers that report algorithmic improvements.
ä»Šæ—¥ã€ç ”ç©¶è«–æ–‡ã§å ±å‘Šã•ã‚Œã¦ã„ã‚‹å†…å®¹ã¨ã„ã†è¦³ç‚¹ã‹ã‚‰ã€æœ€å…ˆç«¯ã®å®Ÿè·µçŠ¶æ³ã‚’æŠŠæ¡ã™ã‚‹ãŸã‚ã€KDDã€RecSysã€SIGIRã€TheWebConfã€WSDMã¨ã„ã†5ã¤ã®é–¢é€£ã™ã‚‹ACMä¼šè­°ã‚·ãƒªãƒ¼ã‚ºã®2022å¹´ã®ãƒ—ãƒ­ã‚·ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¹ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ”¹å–„ã«ã¤ã„ã¦å ±å‘Šã—ã¦ã„ã‚‹è«–æ–‡ã‚’æ¢ã—ãŸã€‚
Since our experiment in Section 3 focuses on the top-n recommendation task based mainly on user-item interaction matrices, we only considered such papers in our analysis.
ã‚»ã‚¯ã‚·ãƒ§ãƒ³3ã®å®Ÿé¨“ã§ã¯ã€ä¸»ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®ç›¸äº’ä½œç”¨è¡Œåˆ—ã«åŸºã¥ããƒˆãƒƒãƒ—næ¨è–¦ã‚¿ã‚¹ã‚¯ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã‚‹ãŸã‚ã€åˆ†æã§ã¯ã“ã®ã‚ˆã†ãªè«–æ–‡ã®ã¿ã‚’è€ƒæ…®ã—ãŸã€‚
We identified 21 relevant papers, which we list in the online material.
ãã®çµæœã€21ã®é–¢é€£è«–æ–‡ãŒè¦‹ã¤ã‹ã£ãŸã®ã§ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è³‡æ–™ã«æ²è¼‰ã—ãŸã€‚
To avoid highlighting individual research works here we refrain from providing exact citations here for individual observations.
ã“ã“ã§ã¯ã€å€‹ã€…ã®ç ”ç©¶æˆæœã‚’å¼·èª¿ã™ã‚‹ã“ã¨ã‚’é¿ã‘ã‚‹ãŸã‚ã€å€‹ã€…ã®è¦³å¯Ÿçµæœã®æ­£ç¢ºãªå¼•ç”¨ã¯æ§ãˆã‚‹ã€‚
We iterate that our goal is to provide evidence regarding what is documented in the context of hyperparameter tuning, and we are not challenging the reported results in any paper.
æˆ‘ã€…ã®ç›®çš„ã¯ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®æ–‡è„ˆã§æ–‡æ›¸åŒ–ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã«é–¢ã™ã‚‹è¨¼æ‹ ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã‚ã‚Šã€ã©ã®è«–æ–‡ã§ã‚‚å ±å‘Šã•ã‚Œã¦ã„ã‚‹çµæœã«æŒ‘æˆ¦ã—ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªã„ã“ã¨ã‚’ç¹°ã‚Šè¿”ã™ã€‚
We also acknowledge that our analysis is based on a certain selection of conferences series, and things may be different for other publication outlets.
ã¾ãŸã€ç§ãŸã¡ã®åˆ†æã¯ã€ã‚ã‚‹ç‰¹å®šã®ä¼šè­°ã‚·ãƒªãƒ¼ã‚ºã«åŸºã¥ãã‚‚ã®ã§ã‚ã‚Šã€ä»–ã®å‡ºç‰ˆç¤¾ã§ã¯çŠ¶æ³ãŒç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚‚æ‰¿çŸ¥ã—ã¦ã„ã‚‹ã€‚
Still, we believe that our selection of papers is representative for todayâ€™s research practices.
ãã‚Œã§ã‚‚ã€ç§ãŸã¡ãŒé¸ã‚“ã è«–æ–‡ã¯ã€ä»Šæ—¥ã®ç ”ç©¶æ…£è¡Œã‚’ä»£è¡¨ã™ã‚‹ã‚‚ã®ã ã¨ä¿¡ã˜ã¦ã„ã‚‹ã€‚
Finally, we declare that there are similar patterns of shallow reporting of the hyperparameter tuning process in our own previous works as well.
æœ€å¾Œã«ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°éç¨‹ã®æµ…ã„å ±å‘Šã«ã¯ã€æˆ‘ã€…è‡ªèº«ã®éå»ã®ä½œå“ã«ã‚‚åŒæ§˜ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚‹ã“ã¨ã‚’å®£è¨€ã™ã‚‹ã€‚
Our observations of what we find in the papers can be summarized as follows.
ç§ãŸã¡ãŒè«–æ–‡ã§è¦‹ã¤ã‘ãŸã“ã¨ã‚’ã¾ã¨ã‚ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚‹ã€‚
Tuning of the proposed model: On the more positive end, one of the examined works reports the searched ranges for â€œcommonâ€ hyperparameters for such as learning rate, dropout ratio or the coefficient for L2 regularization.
ææ¡ˆãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼š ã‚ˆã‚Šè‚¯å®šçš„ãªé¢ã§ã¯ã€æ¤œè¨ã•ã‚ŒãŸä½œå“ã®1ã¤ãŒã€å­¦ç¿’ç‡ã€ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ã€L2æ­£å‰‡åŒ–ã®ä¿‚æ•°ãªã©ã®ã€Œä¸€èˆ¬çš„ãªã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¢ç´¢ç¯„å›²ã‚’å ±å‘Šã—ã¦ã„ã‚‹ã€‚
Other parameters are however taken from the original papers or using defaults from the provided codes.
ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€åŸè‘—è«–æ–‡ã‹ã‚‰å¼•ç”¨ã™ã‚‹ã‹ã€æä¾›ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã€‚
Furthermore, the optimal values are not reported in the end, and the pointer to the code leads to an empty GitHub repository.
ã•ã‚‰ã«ã€æœ€é©å€¤ã¯æœ€çµ‚çš„ã«ã¯å ±å‘Šã•ã‚Œãšã€ã‚³ãƒ¼ãƒ‰ã¸ã®ãƒã‚¤ãƒ³ã‚¿ã¯ç©ºã®GitHubãƒªãƒã‚¸ãƒˆãƒªã«ã¤ãªãŒã‚‹ã€‚
Another paper reports some of the hyperparameter ranges and some chosen values (also for the baselines), but does not report on how the parameters were found, e.g., by grid search or some other method.
åˆ¥ã®è«–æ–‡ã§ã¯ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¯„å›²ã¨é¸æŠã•ã‚ŒãŸå€¤ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«ã¤ã„ã¦ã‚‚ï¼‰ãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹ãŒã€ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã‚„ä»–ã®æ–¹æ³•ãªã©ã€ã©ã®ã‚ˆã†ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¦‹ã¤ã‘ãŸã‹ã«ã¤ã„ã¦ã¯å ±å‘Šã•ã‚Œã¦ã„ãªã„ã€‚
In this case, only one set of hyperparameter values is reported, even though evaluations were done on three datasets.
ã“ã®å ´åˆã€3ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è©•ä¾¡ãŒè¡Œã‚ã‚ŒãŸã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤ã®ã‚»ãƒƒãƒˆã¯1ã¤ã—ã‹å ±å‘Šã•ã‚Œã¦ã„ãªã„ã€‚
Yet another paper only reports the fixed learning rate that was used for all models and datasets.
ã•ã‚‰ã«åˆ¥ã®è«–æ–‡ã§ã¯ã€ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ä½¿ç”¨ã•ã‚ŒãŸå›ºå®šå­¦ç¿’ç‡ã®ã¿ãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹ã€‚
Furthermore, in this case, the embedding size was kept constant across all compared models â€œfor fair comparisonâ€.
ã•ã‚‰ã«ã“ã®å ´åˆã€ã€Œå…¬å¹³ãªæ¯”è¼ƒã®ãŸã‚ã€ã€ã™ã¹ã¦ã®æ¯”è¼ƒãƒ¢ãƒ‡ãƒ«ã§åŸ‹ã‚è¾¼ã¿ã‚µã‚¤ã‚ºã¯ä¸€å®šã«ä¿ãŸã‚ŒãŸã€‚
In reality, however, embeddings sizes are hyperparameters to tune, and fixing them to one specific value (without much justification) may actually lead to an unfair comparison.
ã—ã‹ã—å®Ÿéš›ã«ã¯ã€ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã®ã‚µã‚¤ã‚ºã¯èª¿æ•´ã™ã¹ããƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚Šã€ï¼ˆã‚ã¾ã‚Šæ­£å½“ãªç†ç”±ã‚‚ãªãï¼‰ç‰¹å®šã®å€¤ã«å›ºå®šã™ã‚‹ã“ã¨ã¯ã€å®Ÿéš›ã«ã¯ä¸å…¬å¹³ãªæ¯”è¼ƒã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
In the end, we only identified two papers among the 21 ones we considered, which reported all optimal hyperparameter sets (proposed model and baseline) for all examined datasets and which documented the hyperparameter ranges and search procedure in more detail.
æœ€çµ‚çš„ã«ã€æ¤œè¨ã—ãŸ21ã®è«–æ–‡ã®ã†ã¡ã€æ¤œè¨ã—ãŸã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ã€ã™ã¹ã¦ã®æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆææ¡ˆãƒ¢ãƒ‡ãƒ«ã¨ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰ã‚’å ±å‘Šã—ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¯„å›²ã¨æ¢ç´¢æ‰‹é †ã‚’ã‚ˆã‚Šè©³ç´°ã«æ–‡æ›¸åŒ–ã—ã¦ã„ã‚‹è«–æ–‡ã‚’2ã¤ã ã‘åŒå®šã—ãŸã€‚
The code for one of the papers was however not shared publicly.
ã—ã‹ã—ã€ã‚ã‚‹è«–æ–‡ã®ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ãªã„ã€‚

Tuning of baseline models: The documentation of how the baselines were tuned and which parameters were finally chosen is even more sparse than for the proposed models.
ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãŒã©ã®ã‚ˆã†ã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã€æœ€çµ‚çš„ã«ã©ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒé¸ã°ã‚ŒãŸã®ã‹ã«ã¤ã„ã¦ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€ææ¡ˆãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚ã•ã‚‰ã«å°‘ãªã„ã€‚
Four authors refer to default parameters of public code or the values that were used in the original papers (even though we notice from the previous paragraph that the quality of documentation can be improved even for the proposed models).
4äººã®è‘—è€…ã¯ã€å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã‚³ãƒ¼ãƒ‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚„ã€å…ƒã®è«–æ–‡ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹å€¤ã‚’å‚ç…§ã—ã¦ã„ã‚‹ï¼ˆå‰ã®æ®µè½ã§ã€ææ¡ˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã‚‚ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è³ªã‚’æ”¹å–„ã§ãã‚‹ã“ã¨ã«æ°—ã¥ã„ãŸã«ã‚‚ã‹ã‹ã‚ã‚‰ãšï¼‰ã€‚
In six papers, no information about the hyperparameters and the tuning process is provided at all.
6ã¤ã®è«–æ–‡ã§ã¯ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã«é–¢ã™ã‚‹æƒ…å ±ã¯ã¾ã£ãŸãæä¾›ã•ã‚Œã¦ã„ãªã„ã€‚
In seven cases, a subset of baseline hyperparameter values is reported, but without listing the ranges or how the values were determined.
7ã¤ã®ã‚±ãƒ¼ã‚¹ã§ã¯ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤ã®ã‚µãƒ–ã‚»ãƒƒãƒˆãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹ãŒã€ãã®ç¯„å›²ã‚„å€¤ã®æ±ºå®šæ–¹æ³•ã¯è¨˜è¼‰ã•ã‚Œã¦ã„ãªã„ã€‚
Again, in four cases only one fixed set of hyperparameter values is reported even though various datasets are used in the experiments.
å®Ÿé¨“ã§ã¯ã•ã¾ã–ã¾ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€4ã¤ã®ã‚±ãƒ¼ã‚¹ã§ã¯1ã¤ã®å›ºå®šã•ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤ã®ã¿ãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹ã€‚
Finally, one paper reports that they used hyperparameters from the original papers if available, and applied grid search for the others.
æœ€å¾Œã«ã€ã‚ã‚‹è«–æ–‡ã§ã¯ã€å…¥æ‰‹å¯èƒ½ã§ã‚ã‚Œã°åŸè‘—è«–æ–‡ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã€ãã‚Œä»¥å¤–ã¯ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã‚’é©ç”¨ã—ãŸã¨å ±å‘Šã—ã¦ã„ã‚‹ã€‚
As mentioned, only two papers report detailed hyperparameter ranges and final values for all datasets.
å‰è¿°ã®ã‚ˆã†ã«ã€ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦è©³ç´°ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¯„å›²ã¨æœ€çµ‚å€¤ã‚’å ±å‘Šã—ã¦ã„ã‚‹è«–æ–‡ã¯2ã¤ã—ã‹ãªã„ã€‚
Shared code: In 12 of the 21 papers, a link to a repository is provided.
å…±æœ‰ã‚³ãƒ¼ãƒ‰ï¼š 21æœ¬ã®è«–æ–‡ã®ã†ã¡12æœ¬ã§ã€ãƒªãƒã‚¸ãƒˆãƒªã¸ã®ãƒªãƒ³ã‚¯ãŒæä¾›ã•ã‚Œã¦ã„ã‚‹ã€‚
In two cases, these URLs were pointing to empty or nonexistent places, leaving us with a ratio of 50% in terms of code sharing2 .
2ã¤ã®ã‚±ãƒ¼ã‚¹ã§ã¯ã€ã“ã‚Œã‚‰ã®URLã¯ã€ç©ºã¾ãŸã¯å­˜åœ¨ã—ãªã„å ´æ‰€ã‚’æŒ‡ã—ã¦ã„ãŸã€‚
For the repositories actually containing code, we found that none of them contains the code of the used baselines.
å®Ÿéš›ã«ã‚³ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ãƒªãƒã‚¸ãƒˆãƒªã«ã¤ã„ã¦ã¯ã€ä½¿ç”¨ã—ãŸãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ã‚³ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‚‚ã®ã¯ãªã„ã“ã¨ãŒã‚ã‹ã£ãŸã€‚
Also none of them contains code for the hyperparameter search, which would allow other researchers to reconstruct, e.g., the considered hyperparameter ranges.
ã¾ãŸã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã®ã‚³ãƒ¼ãƒ‰ã‚‚å«ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€ä»–ã®ç ”ç©¶è€…ãŒæ¤œè¨ã—ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¯„å›²ãªã©ã‚’å†æ§‹ç¯‰ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
Note however that some papers mentioned that hyperparameter tuning was done in a manual process.
ãŸã ã—ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒæ‰‹ä½œæ¥­ã§è¡Œã‚ã‚ŒãŸã¨è¿°ã¹ã¦ã„ã‚‹è«–æ–‡ã‚‚ã‚ã‚‹ã€‚
Discussion: Our analysis of a set of papers published at renowned conferences in 2022 clearly shows that the documentation of the hyperparameter tuning processâ€”both for the proposed models and the baselinesâ€”is in most cases quite incomplete or even missing.
è€ƒå¯Ÿ 2022å¹´ã«æœ‰åãªå­¦ä¼šã§ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ç¾¤ã‚’åˆ†æã—ãŸã¨ã“ã‚ã€ææ¡ˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¨ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ä¸¡æ–¹ã«ã¤ã„ã¦ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã«é–¢ã™ã‚‹æ–‡æ›¸ãŒã€ã»ã¨ã‚“ã©ã®å ´åˆã€æ¥µã‚ã¦ä¸å®Œå…¨ã§ã‚ã‚‹ã‹ã€ã‚ã‚‹ã„ã¯æ¬ è½ã—ã¦ã„ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚
Typically, authors spend about one paragraph in a section typically named â€œImplementation detailsâ€, where this information is packed.
é€šå¸¸ã€è‘—è€…ã¯ã“ã®æƒ…å ±ã‚’è©°ã‚è¾¼ã‚€ãŸã‚ã«ã€ä¸€èˆ¬çš„ã«ã€Œå®Ÿè£…ã®è©³ç´°ã€ã¨åä»˜ã‘ã‚‰ã‚ŒãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ç´„1æ®µè½ã‚’è²»ã‚„ã™ã€‚
In some works, the methodology also seems unclear, e.g., when authors report one single set of hyperparameter values for different datasets or when values from the original works are reused for very different datasets.
ä¾‹ãˆã°ã€è‘—è€…ãŒç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦1ã‚»ãƒƒãƒˆã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤ã‚’å ±å‘Šã—ã¦ã„ã‚‹å ´åˆã‚„ã€åŸè‘—ã®å€¤ã‚’å…¨ãç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å†åˆ©ç”¨ã—ã¦ã„ã‚‹å ´åˆãªã©ã§ã‚ã‚‹ã€‚
Overall, this lack of documentation makes it unclear if it is mainly a issue of reporting (e.g., because of space limitations) or if we are frequently facing methodological issues of competing against non-tuned baselines, as indicated in the literature.
å…¨ä½“çš„ã«ã€ã“ã®æ–‡æ›¸åŒ–ã®æ¬ å¦‚ã¯ã€ä¸»ã«å ±å‘Šã®å•é¡Œãªã®ã‹ï¼ˆä¾‹ãˆã°ã€ã‚¹ãƒšãƒ¼ã‚¹ã®åˆ¶é™ã®ãŸã‚ï¼‰ã€ãã‚Œã¨ã‚‚æ–‡çŒ®ã§æŒ‡æ‘˜ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«ã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ãªã„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®ç«¶åˆã¨ã„ã†æ–¹æ³•è«–çš„ãªå•é¡Œã«é »ç¹ã«ç›´é¢ã—ã¦ã„ã‚‹ã®ã‹ã‚’ä¸æ˜ç¢ºã«ã—ã¦ã„ã‚‹ã€‚

# Experiments å®Ÿé¨“

In this section, we showcase that adopting a research practice of not carefully tuning the hyperparameters of the baselines can indeed lead to arbitrary â€œwinnersâ€ during the hunt for models that outperform the state-of-the-art.
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ³¨æ„æ·±ããƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãªã„ã¨ã„ã†ç ”ç©¶æ…£è¡Œã‚’æ¡ç”¨ã™ã‚‹ã¨ã€æœ€å…ˆç«¯æŠ€è¡“ã‚’å‡Œé§•ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ¢ã™éš›ã«ã€æ£æ„çš„ãªã€Œå‹è€…ã€ã‚’ç”Ÿã¿å‡ºã™å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚

## Comparing tuned and non-tuned models ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»ãƒ¢ãƒ‡ãƒ«ã¨éãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ

### Methodology. æ–¹æ³•è«–

We use the recent Elliot framework [2] as a basis for our evaluation.
è©•ä¾¡ã®åŸºç¤ã¨ã—ã¦ã€æœ€è¿‘ã®ã‚¨ãƒªã‚ªãƒƒãƒˆãƒ»ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯[2]ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
This Python-based framework implements a rich variety of recommendation models and it integrates components for automated hyperparameter search and evaluation.
ã“ã®Pythonãƒ™ãƒ¼ã‚¹ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€è±Šå¯Œãªç¨®é¡ã®æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã—ã€è‡ªå‹•åŒ–ã•ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¤œç´¢ã¨è©•ä¾¡ã®ãŸã‚ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµ±åˆã—ã¦ã„ã‚‹ã€‚
Experiments, including hyperparameter ranges, can be defined through text-based configuration files, which allows for convenient reproducibility.3 Algorithms and Datasets.
ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¯„å›²ã‚’å«ã‚€å®Ÿé¨“ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ˆã£ã¦å®šç¾©ã™ã‚‹ã“ã¨ãŒã§ãã€å†ç¾æ€§ã‚’å®¹æ˜“ã«ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
We focus our experiments on deep learning algorithms, which are the method of choice today.
ã“ã®å®Ÿé¨“ã§ã¯ã€ç¾åœ¨ä¸»æµã¨ãªã£ã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã‚‹ã€‚
We initially considered all such methods implemented in Elliot at the time of the experiment.
ç§ãŸã¡ã¯ã¾ãšã€å®Ÿé¨“æ™‚ç‚¹ã§ã‚¨ãƒªã‚ªãƒƒãƒˆã«å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ã“ã®ã‚ˆã†ãªæ–¹æ³•ã‚’ã™ã¹ã¦è€ƒæ…®ã—ãŸã€‚
As some of the model implementationsâ€” including those proposed in [6],[12], and[28]â€”led to much lower performance levels as reported in the original papers (at an order of magnitude), we omitted them from this experiment and reported the issue to the framework authors.
6]ã€[12]ã€[28]ã§ææ¡ˆã•ã‚ŒãŸã‚‚ã®ã‚’å«ã‚€ã„ãã¤ã‹ã®ãƒ¢ãƒ‡ãƒ«å®Ÿè£…ã¯ã€å…ƒã®è«–æ–‡ã§å ±å‘Šã•ã‚ŒãŸã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«ä½ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒ™ãƒ«ï¼ˆæ¡é•ã„ï¼‰ã«ãªã£ãŸã®ã§ã€æˆ‘ã€…ã¯ã“ã®å®Ÿé¨“ã‹ã‚‰ãã‚Œã‚‰ã‚’é™¤å¤–ã—ã€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®è‘—è€…ã«å•é¡Œã‚’å ±å‘Šã—ãŸã€‚
The following models were included in our experiments, all of which were proposed during the last few years, and which might be considered to represent the state-of-the-art in a research paper.
æˆ‘ã€…ã®å®Ÿé¨“ã«ã¯ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€ã“ã‚Œã‚‰ã¯ã™ã¹ã¦ã“ã“æ•°å¹´ã®é–“ã«ææ¡ˆã•ã‚ŒãŸã‚‚ã®ã§ã€ç ”ç©¶è«–æ–‡ã§ã¯æœ€å…ˆç«¯ã‚’ä»£è¡¨ã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã‚‚ã®ã§ã‚ã‚‹ã€‚
â€¢ Multinomial Likelihood Denoising Autoencoder (MultDAE) [17] â€¢ Multinomial Likelihood Variational Autoencoder (MultVAE) [17] â€¢ Convolutional Matrix Factorization (ConvMF) [15] â€¢ Generalized Matrix Factorization (GMF) [14] â€¢ Neural network based Collaborative Filtering (NeuMF) [14] â€¢ Outer product-based Neural Collaborative Filtering (ONCF) [13], (named ConvNeuMF in Elliot) â€¢ Neural Graph Collaborative Filtering (NGCF) [31] In addition to these models, we include the non-personalized MostPop method in our experiment, which simply recommends the most popular items in the dataset (in terms of the interaction counts) to every user.

- å¤šé …å°¤åº¦ãƒã‚¤ã‚ºé™¤å»ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€(Multinomial Likelihood Denoising Autoencoder: MultDAE) [17] - å¤šé …å°¤åº¦å¤‰åˆ†ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€(Multinomial Likelihood Variational Autoencoder: MultVAE) [17] - ç•³ã¿è¾¼ã¿è¡Œåˆ—åˆ†è§£(Convolutional Matrix Factorization: ConvMF) [15] - ä¸€èˆ¬åŒ–è¡Œåˆ—åˆ†è§£(Generalized Matrix Factorization: GMF) [14] - ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ™ãƒ¼ã‚¹ã®å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°(Neural network based Collaborative Filtering: NeuMF) [14] - å¤–ç©ãƒ™ãƒ¼ã‚¹ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°(Outer product-based Neural Collaborative Filtering: ONCF) [13]ã€ (Elliotã§ã¯ConvNeuMFã¨å‘½å) - ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚°ãƒ©ãƒ•å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°(NGCF) [31] ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã«åŠ ãˆã¦ã€éãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸMostPopæ³•ã‚’å®Ÿé¨“ã«å«ã‚ã€ã“ã‚Œã¯å˜ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æœ€ã‚‚äººæ°—ã®ã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ (ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚«ã‚¦ãƒ³ãƒˆã®è¦³ç‚¹ã‹ã‚‰)ã‚’ã™ã¹ã¦ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æ¨å¥¨ã™ã‚‹ã€‚
  We also used the implementation from the Elliot framework.
  ã‚¨ãƒªã‚ªãƒƒãƒˆãƒ»ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®å®Ÿè£…ã‚‚ä½¿ç”¨ã—ãŸã€‚
  Researchers have a lot of freedom to select datasets and metrics for their experiments, which represents another reason why it is difficult to impossible to determine what actually represents the state-of-the-art.
  ç ”ç©¶è€…ã¯ã€å®Ÿé¨“ã«ä½¿ã†ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚„æ¸¬å®šåŸºæº–ã‚’è‡ªç”±ã«é¸ã¶ã“ã¨ãŒã§ãã‚‹ãŒã€ã“ã‚Œã¯ã€å®Ÿéš›ã«ä½•ãŒæœ€å…ˆç«¯ã§ã‚ã‚‹ã‹ã‚’åˆ¤æ–­ã™ã‚‹ã®ãŒé›£ã—ã„ã€ã‚ã‚‹ã„ã¯ä¸å¯èƒ½ã§ã‚ã‚‹ã‚‚ã†1ã¤ã®ç†ç”±ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
  To avoid any bias in our setup, we strictly followed the experimental setup described in [3], which was also based on the Elliot framework.
  æˆ‘ã€…ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã®åã‚Šã‚’é¿ã‘ã‚‹ãŸã‚ã€ã‚¨ãƒªã‚ªãƒƒãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«åŸºã¥ã[3]ã®å®Ÿé¨“ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«å³å¯†ã«å¾“ã£ãŸã€‚
  Regarding the dataset, we therefore used the exact same datasets and p-core pre-processing procedures.
  å¾“ã£ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é–¢ã—ã¦ã¯ã€å…¨ãåŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨p-coreã®å‰å‡¦ç†æ‰‹é †ã‚’ä½¿ç”¨ã—ãŸã€‚
  Specifically, the datasets include (i) MovieLens-1M (ML-1m), a dataset with movie ratings, (ii) Amazon Digital Music (AMZm), a dataset containing ratings for musical tracks, and (iii) Epinions, a dataset containing binary trust relationships between users of a social network.
  å…·ä½“çš„ã«ã¯ã€(i)MovieLens-1Mï¼ˆML-1mï¼‰ã¯æ˜ ç”»ã®ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€(ii)Amazon Digital Musicï¼ˆAMZmï¼‰ã¯éŸ³æ¥½æ›²ã®ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€(iii)Epinionsã¯ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼é–“ã®ãƒã‚¤ãƒŠãƒªä¿¡é ¼é–¢ä¿‚ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹ã€‚
  For the ML-1M and AMZm datasets, the rating data were transformed in a way that ratings with a value greater than 3 were encoded as positive signals.4 Hyperparameter Tuning Process.
  ML-1Mã¨AMZmã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ã€3ä»¥ä¸Šã®è©•ä¾¡ã‚’æ­£ã®ã‚·ã‚°ãƒŠãƒ«ã¨ã—ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã‚ˆã†ã«ã€è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ã—ãŸã€‚
  We used the Tree Parzen Estimators for hyperparameter tuning, a method that is embedded in Elliot.
  ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯ã€Elliotã«çµ„ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹Tree Parzen Estimatorsã‚’ä½¿ç”¨ã—ãŸã€‚
  In terms of finding suitable ranges for the different hyperparameters, we adopted different strategies to inform our choices.
  ã•ã¾ã–ã¾ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é©åˆ‡ãªç¯„å›²ã‚’è¦‹ã¤ã‘ã‚‹ã¨ã„ã†ç‚¹ã§ã€ç§ãŸã¡ã¯ã•ã¾ã–ã¾ãªæˆ¦ç•¥ã‚’æ¡ç”¨ã—ã¦é¸æŠã‚’è¡Œã£ãŸã€‚
  First, we looked up the original papers to see if the authors reported ranges that they explored.
  ã¾ãšã€åŸè‘—è«–æ–‡ã‚’èª¿ã¹ã€è‘—è€…ãŒèª¿æŸ»ã—ãŸç¯„å›²ã‚’å ±å‘Šã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã—ãŸã€‚
  If this was not the case, we selected ranges that are frequently observed in the literature, e.g., for the learning rate, a common range could be from 0.0001 to 0.01, and typical embedding dimensions in the literature are 64, 128 and 256.
  ãã†ã§ãªã„å ´åˆã¯ã€æ–‡çŒ®ã§é »ç¹ã«è¦³å¯Ÿã•ã‚Œã‚‹ç¯„å›²ã‚’é¸æŠã—ãŸã€‚ä¾‹ãˆã°ã€å­¦ç¿’ç‡ã«ã¤ã„ã¦ã¯ã€ä¸€èˆ¬çš„ãªç¯„å›²ã¯0.0001ã‹ã‚‰0.01ã§ã‚ã‚Šã€æ–‡çŒ®ã«ãŠã‘ã‚‹å…¸å‹çš„ãªåŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã¯64ã€128ã€256ã§ã‚ã‚‹ã€‚
  We set the number of iterations depending on the computational complexity of the tested algorithms.
  ç¹°ã‚Šè¿”ã—ã®å›æ•°ã¯ã€ãƒ†ã‚¹ãƒˆã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è¨ˆç®—é‡ã«å¿œã˜ã¦è¨­å®šã—ãŸã€‚
  The number of iterations ranged from 10 to 50.
  åå¾©å›æ•°ã¯10ï½50å›ã€‚
  The exact ranges and finally chosen values are documented in the online repository.
  æ­£ç¢ºãªç¯„å›²ã¨æœ€çµ‚çš„ã«é¸æŠã•ã‚ŒãŸå€¤ã¯ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ»ãƒªãƒã‚¸ãƒˆãƒªã«æ–‡æ›¸åŒ–ã•ã‚Œã¦ã„ã‚‹ã€‚
  Importantly, we note here that we selected ranges in a way that they are good enough for the purpose of our experiment.
  é‡è¦ãªã®ã¯ã€ç§ãŸã¡ã®å®Ÿé¨“ã®ç›®çš„ã«ã¯ååˆ†ãªç¯„å›²ã‚’é¸ã‚“ã ã¨ã„ã†ã“ã¨ã ã€‚
  This means that we need to find a set of hyperparameter values for each model that is outperforming any other model when using random values.
  ã¤ã¾ã‚Šã€ãƒ©ãƒ³ãƒ€ãƒ ãªå€¤ã‚’ä½¿ç”¨ã—ãŸå ´åˆã«ã€ä»–ã®ã©ã®ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹å„ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤ã®ã‚»ãƒƒãƒˆã‚’è¦‹ã¤ã‘ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
  We therefore do not rule out that better hyperparameters can be found for each model and dataset; and that the ranking of the tuned algorithms may change with other values.
  ã—ãŸãŒã£ã¦ã€å„ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ã€ã‚ˆã‚Šè‰¯ã„ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‹å¯èƒ½æ€§ã‚’æ’é™¤ã™ã‚‹ã‚‚ã®ã§ã¯ãªã„ã€‚
  For the sake of our experiment, we first executed all models with the non-tuned (random) values for the hyperparameters.
  å®Ÿé¨“ã®ãŸã‚ã«ã€ã¾ãšãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã„ãªã„ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãªï¼‰å€¤ã§ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ãŸã€‚
  The random procedure consisted of arbitrarily picking values manually from the given ranges for each hyperparameter.
  ãƒ©ãƒ³ãƒ€ãƒ ãªæ‰‹é †ã¯ã€å„ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦ä¸ãˆã‚‰ã‚ŒãŸç¯„å›²ã‹ã‚‰æ‰‹å‹•ã§ä»»æ„ã«å€¤ã‚’é¸ã¶ã“ã¨ã‹ã‚‰æˆã£ã¦ã„ã‚‹ã€‚
  We chose this procedure because we argue that there is a certain randomness in terms of how hyperparameters for the baselines are chosen in many papers.
  æˆ‘ã€…ã¯ã€å¤šãã®è«–æ–‡ã§ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã©ã®ã‚ˆã†ã«é¸æŠã•ã‚Œã¦ã„ã‚‹ã‹ã¨ã„ã†ç‚¹ã§ã€ã‚ã‚‹ç¨®ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ãŒã‚ã‚‹ã¨ä¸»å¼µã—ã¦ã„ã‚‹ãŸã‚ã€ã“ã®æ‰‹é †ã‚’é¸æŠã—ãŸã€‚
  It can, for example, depend on what worked well for the datasets used in the original paper; or it can be simply based on the hard-coded default values that were left in the published source code by the authors.
  ä¾‹ãˆã°ã€å…ƒã®è«–æ–‡ã§ä½¿ç”¨ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã†ã¾ãæ©Ÿèƒ½ã—ãŸã‚‚ã®ã«ä¾å­˜ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚Œã°ã€è‘—è€…ã«ã‚ˆã£ã¦å…¬é–‹ã•ã‚ŒãŸã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã«æ®‹ã•ã‚ŒãŸãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã«å˜ç´”ã«åŸºã¥ãã“ã¨ã‚‚ã‚ã‚‹ã€‚
  After having obtained the results when using non-tuned hyperparameters, we systematically tuned the hyperparameters for all models on all datasets to obtain the best accuracy values.
  ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ãªã„ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸå ´åˆã®çµæœã‚’å¾—ãŸå¾Œã€æœ€é«˜ã®ç²¾åº¦å€¤ã‚’å¾—ã‚‹ãŸã‚ã«ã€ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç³»çµ±çš„ã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã€‚
  We used the Normalized Discounted Cumulative Gain (NDCG) as the target metric during hyperparameter optimization.3.1.2 Results.
  3.1.2çµæœ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã®éš›ã€æ­£è¦åŒ–å‰²å¼•ç´¯ç©åˆ©å¾—ï¼ˆNDCGï¼‰ã‚’ç›®æ¨™æŒ‡æ¨™ã¨ã—ãŸã€‚
  Table 1 shows the results for all models on the three datasets in terms of the NDCG@10.
  è¡¨1ã¯ã€3ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã‘ã‚‹å…¨ãƒ¢ãƒ‡ãƒ«ã®çµæœã‚’NDCG@10ã§ç¤ºã—ãŸã‚‚ã®ã§ã‚ã‚‹ã€‚
  The ranking of the algorithms when using other metrics such as NDCG or MAP are as usual roughly aligned and can be found in the online repository.
  NDCGã‚„MAPã¨ã„ã£ãŸä»–ã®æŒ‡æ¨™ã‚’ç”¨ã„ãŸå ´åˆã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¯ã€ä¾‹ã«ã‚ˆã£ã¦ã»ã¼ä¸€è‡´ã—ã¦ãŠã‚Šã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ»ãƒªãƒã‚¸ãƒˆãƒªã§è¦‹ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
  The upper part of the table shows the results after systematic hyperparameter tuning, and the lower parts shows the outcomes when running all models with the non-tuned hyperparameters.
  è¡¨ã®ä¸Šéƒ¨ã¯ã€ç³»çµ±çš„ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®çµæœã‚’ç¤ºã—ã€ä¸‹éƒ¨ã¯ã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ãªã„ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ãŸå ´åˆã®çµæœã‚’ç¤ºã™ã€‚
  The numbers in the table correspond to the averages obtained through a five-fold cross-validation procedure.5 The most important observation is that for each dataset, even the worst-performing tuned model is better than the best model with non-tuned hyperparameters.
  è¡¨ä¸­ã®æ•°å€¤ã¯ã€5é‡ã®ã‚¯ãƒ­ã‚¹ãƒ»ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ‰‹é †ã«ã‚ˆã£ã¦å¾—ã‚‰ã‚ŒãŸå¹³å‡å€¤ã«å¯¾å¿œã™ã‚‹ã€‚5 æœ€ã‚‚é‡è¦ãªè¦³å¯Ÿç‚¹ã¯ã€å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ã€æœ€ã‚‚æˆç¸¾ã®æ‚ªã„ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»ãƒ¢ãƒ‡ãƒ«ã§ã•ãˆã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ãªã„ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
  Now this might not sound particularly surprising, given that it is well known that the performance of machine learning models can highly depend on the chosen hyperparameters.
  æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãŒé¸æŠã•ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¤§ããä¾å­˜ã™ã‚‹ã“ã¨ã¯ã‚ˆãçŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ã€‚
  Looking at our results, we in fact find that the difference between tuned and non-tuned models can be of an order of magnitude and more.
  æˆ‘ã€…ã®çµæœã‚’è¦‹ã‚‹ã¨ã€å®Ÿéš›ã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«ã®é–“ã«ã¯ã€1æ¡ä»¥ä¸Šã®å·®ãŒã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
  However, the important point is that even the worst-performing models for each dataset, which have NDCG values that are actually substantially lower than the best performing tuned models, can be reported as being a winner in a comparison in which the hyperparameters of the competitors are not tuned.
  ã—ã‹ã—ã€é‡è¦ãªç‚¹ã¯ã€å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æœ€ã‚‚æˆç¸¾ã®æ‚ªã„ãƒ¢ãƒ‡ãƒ«ã§ã•ãˆã€NDCGã®å€¤ãŒãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸæœ€ã‚‚æˆç¸¾ã®è‰¯ã„ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚å®Ÿéš›ã«ã¯å¤§å¹…ã«ä½ãã€ç«¶åˆã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ãªã„æ¯”è¼ƒã§ã¯å‹è€…ã§ã‚ã‚‹ã¨å ±å‘Šã§ãã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
  In other words, it can be sufficient to outperform an existing method that is actually quite weak to report an improvement over the state-of-the-art that includes as many as seven recent neural methods on three datasets.
  è¨€ã„æ›ãˆã‚Œã°ã€3ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§7ã¤ã‚‚ã®æœ€æ–°ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ‰‹æ³•ã‚’å«ã‚€æœ€å…ˆç«¯æŠ€è¡“ã«å¯¾ã™ã‚‹æ”¹å–„ã‚’å ±å‘Šã™ã‚‹ã«ã¯ã€å®Ÿéš›ã«ã¯ã‹ãªã‚Šå¼±ã„æ—¢å­˜ã®æ‰‹æ³•ã‚’ä¸Šå›ã‚‹ã ã‘ã§ååˆ†ãªã®ã§ã‚ã‚‹ã€‚
  Considering however how little is documented about hyperparameter tuning of baselines in the current literature (see Section 2), there can be major concerns regarding the reliability of the reported rankings and improvements over the state-of-the-art.
  ã—ã‹ã—ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¤ã„ã¦ã€ç¾åœ¨ã®æ–‡çŒ®ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³2å‚ç…§ï¼‰ã§ã¯ã»ã¨ã‚“ã©è¨˜è¿°ã•ã‚Œã¦ã„ãªã„ã“ã¨ã‚’è€ƒãˆã‚‹ã¨ã€å ±å‘Šã•ã‚Œã¦ã„ã‚‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ä¿¡é ¼æ€§ã‚„ã€æœ€å…ˆç«¯æŠ€è¡“ã«å¯¾ã™ã‚‹æ”¹å–„ã«ã¤ã„ã¦ã€å¤§ããªæ‡¸å¿µãŒã‚ã‚‹ã€‚
  In fact, previous reproducibility analyses as reported, e.g., in [9], confirm that methodological issues of various kinds can hamper progress in recommender systems research.
  å®Ÿéš›ã€ä¾‹ãˆã°[9]ã§å ±å‘Šã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ãªéå»ã®å†ç¾æ€§åˆ†æã¯ã€æ§˜ã€…ãªç¨®é¡ã®æ–¹æ³•è«–çš„å•é¡ŒãŒæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶ã®é€²æ­©ã‚’å¦¨ã’ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ã„ã‚‹ã€‚
  Another striking observation here is that the popularity-based MostPop method is performing better than some of the tuned models.
  ã“ã“ã§ã®ã‚‚ã†ä¸€ã¤ã®é¡•è‘—ãªè¦³å¯Ÿã¯ã€äººæ°—ã«åŸºã¥ãMostPopæ³•ãŒã€ã„ãã¤ã‹ã®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚è‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ã—ã¦ã„ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
  A similar phenomenon was also reported in [9].
  åŒæ§˜ã®ç¾è±¡ã¯[9]ã§ã‚‚å ±å‘Šã•ã‚Œã¦ã„ã‚‹ã€‚
  The poor performance of some of the modelsâ€”also compared to the MostPop methodâ€”may to some extent lie in the particular types of modestsized datasets in our experiment.
  MostPopæ³•ã¨æ¯”è¼ƒã—ãŸå ´åˆã€ã„ãã¤ã‹ã®ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãŒä½ã„ã®ã¯ã€å®Ÿé¨“ã«ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç¨®é¡ãŒç‰¹æ®Šã§ã‚ã£ãŸãŸã‚ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
  Or it may be the result of the somewhat restricted hyperparameter tuning process, which was only executed to the extent that was necessary for experiment.
  ã‚ã‚‹ã„ã¯ã€å®Ÿé¨“ã«å¿…è¦ãªç¯„å›²ã§ã®ã¿å®Ÿè¡Œã•ã‚ŒãŸã€ã‚„ã‚„åˆ¶é™ã•ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®çµæœã‹ã‚‚ã—ã‚Œãªã„ã€‚
  We iterate here that the ranking of the tuned models in Table 4 is not important, because we limited our search for hyperparameter ranges to typical values, and we limited the number of tuning iterations for the computationally complex models.
  ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¯„å›²ã‚’å…¸å‹çš„ãªå€¤ã«é™å®šã—ã€è¨ˆç®—ãŒè¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ã¯ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®åå¾©å›æ•°ã‚’åˆ¶é™ã—ãŸãŸã‚ã§ã‚ã‚‹ã€‚
  Exploring alternative or more unusual ranges, as done recently in [24], may help to further improve the performance of the individual models.
  æœ€è¿‘[24]ã§è¡Œã‚ã‚ŒãŸã‚ˆã†ã«ã€ä»£æ›¿çš„ãªã€ã¾ãŸã¯ã‚ˆã‚Šçã—ã„ç¯„å›²ã‚’æ¢ã‚‹ã“ã¨ã¯ã€å€‹ã€…ã®ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ã•ã‚‰ã«å‘ä¸Šã•ã›ã‚‹ã®ã«å½¹ç«‹ã¤ã‹ã‚‚ã—ã‚Œãªã„ã€‚

## Comparison with Other Baselines ä»–ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒ

Besides comparisons with baselines that are not well tuned, another potential methodological issue observed in [9] can lie in the choice of the baselines and the propagation of weak baselines.
ååˆ†ã«èª¿æ•´ã•ã‚Œã¦ã„ãªã„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒã«åŠ ãˆã¦ã€[9]ã§è¦³å¯Ÿã•ã‚ŒãŸã‚‚ã†1ã¤ã®æ½œåœ¨çš„ãªæ–¹æ³•è«–çš„å•é¡Œã¯ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®é¸æŠã¨å¼±ã„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ä¼æ’­ã«ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
Remember that we limited our comparison to recent deep learning models, and reviewers might probably not complain, given the large set of recent baselines.
æ¯”è¼ƒå¯¾è±¡ã‚’æœ€è¿‘ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»ãƒ¢ãƒ‡ãƒ«ã«é™å®šã—ãŸã“ã¨ã‚’æ€ã„å‡ºã—ã¦ã»ã—ã„ã€‚æœ€è¿‘ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®å¤§è¦æ¨¡ãªã‚»ãƒƒãƒˆã‚’è€ƒãˆã‚Œã°ã€ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã¯ãŠãã‚‰ãæ–‡å¥ã‚’è¨€ã‚ãªã„ã ã‚ã†ã€‚
However, there are several indications, also reported in [9] and other works, that simple methods can be competitive as well.
ã—ã‹ã—ã€[9]ãªã©ã§ã‚‚å ±å‘Šã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«ã€å˜ç´”ãªæ–¹æ³•ã§ã‚‚ç«¶äº‰åŠ›ã‚’ç™ºæ®ã§ãã‚‹ã“ã¨ãŒã„ãã¤ã‹ç¤ºå”†ã•ã‚Œã¦ã„ã‚‹ã€‚
The propagation of weak baseline may happen, if we only consider models from one family, e.g., neural models.
å¼±ã„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ä¼æ’­ã¯ã€ä¾‹ãˆã°ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»ãƒ¢ãƒ‡ãƒ«ã®ã‚ˆã†ãªã€ã‚ã‚‹ç³»åˆ—ã«å±ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã ã‘ã‚’è€ƒæ…®ã—ãŸå ´åˆã«èµ·ã“ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
In the analysis in [3], the linear and â€œshallowâ€ EASEğ‘… model [29] was the strongest performing method on the datasets used in our experiments.
3]ã®åˆ†æã§ã¯ã€ç·šå½¢ã§ "æµ…ã„ "EASEÇ”ãƒ¢ãƒ‡ãƒ«[29]ãŒã€æˆ‘ã€…ã®å®Ÿé¨“ã«ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æœ€ã‚‚å¼·åŠ›ãªæ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹æ‰‹æ³•ã§ã‚ã£ãŸã€‚
Also the traditional user-based nearest-neighbor method (userKNN), which was proposed in the context of the GroupLens system [26] from 1994 can lead to competitive results when tuned properly [3].
ã¾ãŸã€1994å¹´ã«GroupLensã‚·ã‚¹ãƒ†ãƒ [26]ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ææ¡ˆã•ã‚ŒãŸä¼çµ±çš„ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ãƒ¼ã‚¹ã®æœ€è¿‘å‚æ³•ï¼ˆuserKNNï¼‰ã¯ã€é©åˆ‡ã«èª¿æ•´ã•ã‚ŒãŸå ´åˆã€ç«¶äº‰åŠ›ã®ã‚ã‚‹çµæœã‚’å°ãã“ã¨ãŒã§ãã¾ã™[3]ã€‚
In fact, had we included EASEğ‘… in our comparison, it would have been on top of the ranking list of the tuned models for all datasets, with NDCG@10 = 0.343 for ML-1M, NDCG@10 = 0.086 for AMZm, and NDCG@10 = 0.164 for the Epinions dataset.
å®Ÿéš›ã€EASEâ†ªLu_1 ã‚’æ¯”è¼ƒã«å«ã‚ã¦ã„ã‚Œã°ã€ML-1Mã§ã¯NDCG@10 = 0.343ã€AMZmã§ã¯NDCG@10 = 0.086ã€Epinionsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯NDCG@10 = 0.164ã¨ãªã‚Šã€ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§èª¿æ•´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒªã‚¹ãƒˆã®ãƒˆãƒƒãƒ—ã«ãªã£ãŸã ã‚ã†ã€‚
In that context it is also worth noting that EASEğ‘… has only one relevant hyperparameter to tune (L2-norm).
ãã®æ„å‘³ã§ã€EASE_445ãŒèª¿æ•´ã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯1ã¤ã ã‘ï¼ˆL2-normï¼‰ã§ã‚ã‚‹ã“ã¨ã‚‚æ³¨ç›®ã«å€¤ã™ã‚‹ã€‚
The performance of EASEğ‘… was also quite good in case we selected this hyperparameter randomly, and a randomly tuned EASEğ‘… model would be ranked about in the middle of the tuned models for all three datasets.
EASE_445ã®æ€§èƒ½ã¯ã€ã“ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠã—ãŸå ´åˆã«ã‚‚éå¸¸ã«å„ªã‚Œã¦ãŠã‚Šã€ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸEASE_445ãƒ¢ãƒ‡ãƒ«ã¯ã€3ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã™ã¹ã¦ã«ãŠã„ã¦ã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ã»ã¼ä¸­å¤®ã«ä½ç½®ã™ã‚‹ã€‚
It remains therefore important to consider simpler, e.g., linear, models, even though they might not be able to learn non-linear, higher-order dependencies in the data that neural models are often claimed to do in the literature.
å¾“ã£ã¦ã€ã‚ˆã‚Šå˜ç´”ãªã€ä¾‹ãˆã°ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œè¨ã™ã‚‹ã“ã¨ã¯é‡è¦ã§ã‚ã‚‹ã€‚ãŸã¨ãˆç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§ã‚ã£ã¦ã‚‚ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»ãƒ¢ãƒ‡ãƒ«ãŒã—ã°ã—ã°æ–‡çŒ®ã§ä¸»å¼µã•ã‚Œã‚‹ã‚ˆã†ãªã€ãƒ‡ãƒ¼ã‚¿ä¸­ã®éç·šå½¢ã§é«˜æ¬¡ã®ä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã¯ã§ããªã„ã‹ã‚‚ã—ã‚Œãªã„ã€‚

# Implications and Conclusions æ„å‘³åˆã„ã¨çµè«–

Our study showcases that almost arbitrary performance rankings for a given set of algorithms can be obtained depending on the level of fine-tuning of the hyperparameters.
æˆ‘ã€…ã®ç ”ç©¶ã¯ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¾®èª¿æ•´ã®ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦ã€ä¸ãˆã‚‰ã‚ŒãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ã»ã¼ä»»æ„ã®æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãŒå¾—ã‚‰ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚
While this is certainly not a huge surprise, the fact that in many published papers insufficient information about the hyperparameter tuning process is provided may raise major concerns regarding the true progress that is achieved.
ç¢ºã‹ã«ã“ã‚Œã¯å¤§ããªé©šãã§ã¯ãªã„ãŒã€å¤šãã®ç™ºè¡¨è«–æ–‡ã«ãŠã„ã¦ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°éç¨‹ã«é–¢ã™ã‚‹æƒ…å ±ãŒä¸ååˆ†ã§ã‚ã‚‹ã¨ã„ã†äº‹å®Ÿã¯ã€é”æˆã•ã‚ŒãŸçœŸã®é€²æ­©ã«é–¢ã—ã¦å¤§ããªæ‡¸å¿µã‚’æŠ±ã‹ã›ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚
A number of previous research efforts both in the area of recommender systems and related fields like information retrieval confirm this issue, e.g., [9, 18, 23].
ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã¨æƒ…å ±æ¤œç´¢ã®ã‚ˆã†ãªé–¢é€£åˆ†é‡ã®ä¸¡æ–¹ã§ã€å¤šãã®å…ˆè¡Œç ”ç©¶ã®åŠªåŠ›ãŒã“ã®å•é¡Œã‚’è£ä»˜ã‘ã¦ã„ã‚‹ã€ä¾‹ãˆã°[9, 18, 23]ã€‚
The additional degrees of freedom that researchers usually have, e.g., in terms of the selection of baselines, metrics, datasets, and pre-processing steps, can further aggravate the problem.
ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã€æ¸¬å®šåŸºæº–ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€å‰å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã®é¸æŠãªã©ã€ç ”ç©¶è€…ãŒé€šå¸¸æŒã£ã¦ã„ã‚‹è‡ªç”±åº¦ãŒã•ã‚‰ã«å¢—ãˆã‚‹ã“ã¨ã¯ã€å•é¡Œã‚’ã•ã‚‰ã«æ‚ªåŒ–ã•ã›ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
Various measures may help to better address these fundamental methodological problems.
ã“ã®ã‚ˆã†ãªæ–¹æ³•è«–ä¸Šã®æ ¹æœ¬çš„ãªå•é¡Œã«ã‚ˆã‚Šã‚ˆãå¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€ã•ã¾ã–ã¾ãªæ–¹ç­–ãŒå½¹ç«‹ã¤ã ã‚ã†ã€‚
First, despite increased awareness in the community, reproducibility is still a major obstacle to achieving progress.
ç¬¬ä¸€ã«ã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ¼ã®æ„è­˜ãŒé«˜ã¾ã£ã¦ã„ã‚‹ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€å†ç¾æ€§ãŒé€²æ­©é”æˆã®å¤§ããªéšœå®³ã¨ãªã£ã¦ã„ã‚‹ã€‚
We may observe that researchers share the code for their newly proposed models more often nowadays, and more attention is also paid to reproducibility in the peer-reviewing process, e.g., through explicit items on review forms.
æœ€è¿‘ã§ã¯ã€ç ”ç©¶è€…ãŒæ–°ã—ãææ¡ˆã—ãŸãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ¼ãƒ‰ã‚’å…±æœ‰ã™ã‚‹ã“ã¨ãŒå¤šããªã‚Šã€æŸ»èª­ãƒ—ãƒ­ã‚»ã‚¹ã«ãŠã„ã¦ã‚‚ã€æŸ»èª­ãƒ•ã‚©ãƒ¼ãƒ ã«æ˜ç¤ºçš„ãªé …ç›®ã‚’è¨­ã‘ã‚‹ãªã©ã€å†ç¾æ€§ã«æ³¨ç›®ãŒé›†ã¾ã£ã¦ã„ã‚‹ã€‚
However, the reproducibility packages are often incompleteâ€”in particular in terms of missing code for the baselines or the tuning processâ€”which makes true reproducibility challenging.
ã—ã‹ã—ã€å†ç¾æ€§ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯ä¸å®Œå…¨ã§ã‚ã‚‹ã“ã¨ãŒå¤šãã€ç‰¹ã«ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚„ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚³ãƒ¼ãƒ‰ãŒæ¬ ã‘ã¦ã„ã‚‹ãŸã‚ã€çœŸã®å†ç¾æ€§ã¯å›°é›£ã§ã‚ã‚‹ã€‚
Various earlier works discuss this issue and propose guidelines and checklists for reproducible research both for general AI, for machine learning, and for recommender systems [8, 11, 22].
æ§˜ã€…ãªå…ˆè¡Œç ”ç©¶ãŒã“ã®å•é¡Œã‚’è­°è«–ã—ã€ä¸€èˆ¬çš„ãªAIã€æ©Ÿæ¢°å­¦ç¿’ã€ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®ä¸¡æ–¹ã«ã¤ã„ã¦ã€å†ç¾å¯èƒ½ãªç ”ç©¶ã®ãŸã‚ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚„ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’ææ¡ˆã—ã¦ã„ã‚‹[8, 11, 22]ã€‚
However, it seems that the research community is only making very slow progress towards a systematic and strict adoption of such checklists and guidelines.
ã—ã‹ã—ã€ã“ã®ã‚ˆã†ãªãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚„ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã®ä½“ç³»çš„ã‹ã¤å³æ ¼ãªæ¡ç”¨ã«å‘ã‘ã¦ã€ç ”ç©¶ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãŒå‰é€²ã™ã‚‹ã®ã¯éå¸¸ã«é…ã„ã‚ˆã†ã ã€‚
In the context of our present work, an important next step would be to make the provision of more detailed information about the hyperparameter tuning process and the sharing of the related software artifacts a mandatory requirement for paper submissions.
æˆ‘ã€…ã®ç ”ç©¶ã®æ–‡è„ˆã§ã¯ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã«é–¢ã™ã‚‹ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ã®æä¾›ã¨ã€é–¢é€£ã™ã‚‹ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã®æˆæœç‰©ã®å…±æœ‰ã‚’ã€è«–æ–‡æŠ•ç¨¿ã®å¿…é ˆè¦ä»¶ã¨ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã‚ã‚ã†ã€‚
The increased use of validated evaluation frameworks such as Elliot is another measure that may help to address the problem.
ã‚¨ãƒªã‚ªãƒƒãƒˆã®ã‚ˆã†ãªæœ‰åŠ¹ãªè©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®åˆ©ç”¨ã‚’å¢—ã‚„ã™ã“ã¨ã‚‚ã€å•é¡Œè§£æ±ºã«å½¹ç«‹ã¤ã‹ã‚‚ã—ã‚Œãªã„ã€‚
Various alternative frameworks exist, which often not only feature a rich number of baseline algorithms, but also include pre-implemented and validated procedures for systematic hyperparameter tuning and model evaluation.
æ§˜ã€…ãªä»£æ›¿ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒå­˜åœ¨ã—ã€ãã‚Œã‚‰ã¯å¤šãã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å‚™ãˆã¦ã„ã‚‹ã ã‘ã§ãªãã€ç³»çµ±çš„ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã®ãŸã‚ã®ã€ã‚ã‚‰ã‹ã˜ã‚å®Ÿè£…ã•ã‚Œæ¤œè¨¼ã•ã‚ŒãŸæ‰‹é †ã‚’å«ã‚“ã§ã„ã‚‹ã“ã¨ãŒå¤šã„ã€‚
While custom evaluation procedures may be required for specific application scenarios, many of todayâ€™s libraries can be used almost off-the-shelf for the most predominant evaluation scenarios, e.g., for traditional matrix-completion problems and top-n recommendation, as well as for alternative scenarios such as session-based recommendation.
ç‰¹å®šã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒŠãƒªã‚ªã§ã¯ã€ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡æ‰‹é †ãŒå¿…è¦ã«ãªã‚‹ã‹ã‚‚ã—ã‚Œãªã„ãŒã€ä»Šæ—¥ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å¤šãã¯ã€å¾“æ¥ã®è¡Œåˆ—è£œå®Œå•é¡Œã‚„ãƒˆãƒƒãƒ—Næ¨è–¦ãªã©ã®æœ€ã‚‚ä¸€èˆ¬çš„ãªè©•ä¾¡ã‚·ãƒŠãƒªã‚ªã‚„ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ãªã©ã®ä»£æ›¿ã‚·ãƒŠãƒªã‚ªã«å¯¾ã—ã¦ã€ã»ã¨ã‚“ã©ãã®ã¾ã¾ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
However, in the majority of works that we reviewed in the context of this work (see Section 2), researchers did not rely on existing frameworks, which also leads to the risk of unsound evaluation procedures.
ã—ã‹ã—ã€æœ¬ç ”ç©¶ã®æ–‡è„ˆã§ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ãŸå¤§åŠã®ç ”ç©¶ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³2å‚ç…§ï¼‰ã§ã¯ã€ç ”ç©¶è€…ã¯æ—¢å­˜ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ä¾å­˜ã—ã¦ãŠã‚‰ãšã€ã“ã‚Œã¯ã¾ãŸã€ä¸å¥å…¨ãªè©•ä¾¡æ‰‹é †ã®ãƒªã‚¹ã‚¯ã«ã¤ãªãŒã‚‹ã€‚
These problems may both stem from a lack of knowledge about proper evaluation methodology, or they may simply be the result of programming errors.
ã“ã‚Œã‚‰ã®å•é¡Œã¯ã€ã„ãšã‚Œã‚‚é©åˆ‡ãªè©•ä¾¡æ–¹æ³•ã«ã¤ã„ã¦ã®çŸ¥è­˜ä¸è¶³ã«èµ·å› ã™ã‚‹ã‚‚ã®ã‹ã‚‚ã—ã‚Œãªã„ã—ã€å˜ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ»ã‚¨ãƒ©ãƒ¼ã®çµæœã‹ã‚‚ã—ã‚Œãªã„ã€‚
The use of existing frameworks should therefore be strongly encouraged, both to avoid such mistakes and to increase reproducibility.
å¾“ã£ã¦ã€ã“ã®ã‚ˆã†ãªå¤±æ•—ã‚’é¿ã‘ã€å†ç¾æ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã«ã‚‚ã€æ—¢å­˜ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®åˆ©ç”¨ã‚’å¼·ãæ¨å¥¨ã™ã¹ãã§ã‚ã‚‹ã€‚
Generally, it seems that increased awareness of this fundamental issue is needed in the community, both on the side of researchers who develop and evaluate new models and on the side of reviewers and journal editors.
ä¸€èˆ¬çš„ã«ã€æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’é–‹ç™ºãƒ»è©•ä¾¡ã™ã‚‹ç ”ç©¶è€…å´ã«ã‚‚ã€æŸ»èª­è€…ã‚„ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ç·¨é›†è€…å´ã«ã‚‚ã€ã“ã®åŸºæœ¬çš„ãªå•é¡Œã«å¯¾ã™ã‚‹èªè­˜ã‚’é«˜ã‚ã‚‹ã“ã¨ãŒå¿…è¦ã§ã‚ã‚‹ã¨æ€ã‚ã‚Œã‚‹ã€‚
One main instrument in this area may lie in improved education of scholars who are entering the field [5].
ã“ã®åˆ†é‡ã®ä¸»è¦ãªæ‰‹æ®µã®ã²ã¨ã¤ã¯ã€ã“ã®åˆ†é‡ã«å‚å…¥ã™ã‚‹ç ”ç©¶è€…ã®æ•™è‚²ã‚’æ”¹å–„ã™ã‚‹ã“ã¨ã«ã‚ã‚‹ã®ã‹ã‚‚ã—ã‚Œãªã„[5]ã€‚
For example, todayâ€™s textbooks on recommender systems, e.g., [1, 10], provide in-depth coverage of how to perform offline evaluation and which metrics may be used.
ä¾‹ãˆã°ã€ä»Šæ—¥ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã«é–¢ã™ã‚‹æ•™ç§‘æ›¸ã€ä¾‹ãˆã°[1, 10]ã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚’ã©ã®ã‚ˆã†ã«è¡Œã†ã‹ã€ãã—ã¦ã©ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã«ã¤ã„ã¦æ·±ãã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹ã€‚
Methodological questions of proper hyperparameter tuning for reliable experimental research results are however not discussed in much detail.
ã—ã‹ã—ã€ä¿¡é ¼ã§ãã‚‹å®Ÿé¨“çš„ç ”ç©¶çµæœã‚’å¾—ã‚‹ãŸã‚ã®é©åˆ‡ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«é–¢ã™ã‚‹æ–¹æ³•è«–çš„ãªå•é¡Œã¯ã€ã‚ã¾ã‚Šè©³ã—ãè­°è«–ã•ã‚Œã¦ã„ãªã„ã€‚
Thus, it is important that in the future these topics are communicated more frequently through various educational channels, including books, lecture materials, or tutorials.
ã—ãŸãŒã£ã¦ã€ä»Šå¾Œã¯æ›¸ç±ã€è¬›ç¾©è³‡æ–™ã€ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ãªã©ã€ã•ã¾ã–ã¾ãªæ•™è‚²ãƒãƒ£ãƒ³ãƒãƒ«ã‚’é€šã˜ã¦ã€ã“ã‚Œã‚‰ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’ã‚ˆã‚Šé »ç¹ã«ä¼ãˆã‚‹ã“ã¨ãŒé‡è¦ã§ã‚ã‚‹ã€‚
Moreover, explicitly asking researchers to include more information about the hyperparameter tuning in their submitted works and considering this aspect in review forms may certainly increase the awareness of the problem.
ã•ã‚‰ã«ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«é–¢ã™ã‚‹ã‚ˆã‚Šå¤šãã®æƒ…å ±ã‚’æŠ•ç¨¿ä½œå“ã«å«ã‚ã‚‹ã‚ˆã†ç ”ç©¶è€…ã«æ˜ç¤ºçš„ã«æ±‚ã‚ãŸã‚Šã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ ã§ã“ã®ç‚¹ã‚’è€ƒæ…®ã—ãŸã‚Šã™ã‚‹ã“ã¨ã§ã€ã“ã®å•é¡Œã«å¯¾ã™ã‚‹èªè­˜ãŒç¢ºå®Ÿã«é«˜ã¾ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
Ultimately, one may speculate to what extent our currently acceptable practice of not reporting in depth about the tuning of the baselines and publication pressureâ€”tuning many models on various datasets can be computationally highly demandingâ€”contribute to the apparent problems in our field.
çµå±€ã®ã¨ã“ã‚ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„å…¬è¡¨åœ§åŠ›ã«ã¤ã„ã¦æ·±ãå ±å‘Šã—ãªã„ã¨ã„ã†ã€ç¾åœ¨å—ã‘å…¥ã‚Œã‚‰ã‚Œã¦ã„ã‚‹æ…£è¡ŒãŒã€ã©ã®ç¨‹åº¦ã€ã“ã®åˆ†é‡ã®æ˜ã‚‰ã‹ãªå•é¡Œã‚’å¼•ãèµ·ã“ã—ã¦ã„ã‚‹ã®ã‹ã‚’æ¨æ¸¬ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
Confirmation bias, where researchers have a predisposition to expect that their own model works better than previous ones, may also play a role.
ç¢ºè¨¼ãƒã‚¤ã‚¢ã‚¹ã¯ã€ç ”ç©¶è€…ãŒè‡ªåˆ†ã®ãƒ¢ãƒ‡ãƒ«ãŒä»¥å‰ã®ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚ã†ã¾ãæ©Ÿèƒ½ã™ã‚‹ã¨æœŸå¾…ã™ã‚‹ç´ å› ã‚’æŒã¤ã‚‚ã®ã§ã€ã“ã‚Œã‚‚ä¸€å½¹è²·ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
In general, we do not expect to see a radical change of the research practices in our field in the near future, also because many issues, e.g., related to reproducibility, are known for many years now.
ä¸€èˆ¬çš„ã«ã€è¿‘ã„å°†æ¥ã€ç§ãŸã¡ã®åˆ†é‡ã«ãŠã‘ã‚‹ç ”ç©¶æ…£è¡ŒãŒæ ¹æœ¬çš„ã«å¤‰ã‚ã‚‹ã“ã¨ã¯ãªã„ã ã‚ã†ã€‚ãªãœãªã‚‰ã€å†ç¾æ€§ãªã©å¤šãã®å•é¡Œã¯ã€ã‚‚ã†ä½•å¹´ã‚‚å‰ã‹ã‚‰çŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ã‹ã‚‰ã§ã‚ã‚‹ã€‚
Through constant educational measures and increased awarenessâ€”also in the form of the showcase study in this present workâ€“we however believe that we will observe more reliable results in recommender systems research in the future.
ã—ã‹ã—ã€ä»Šå›ã®ã‚·ãƒ§ãƒ¼ã‚±ãƒ¼ã‚¹ç ”ç©¶ã®ã‚ˆã†ãªä¸æ–­ã®å•“è’™ã¨æ„è­˜æ”¹é©ã«ã‚ˆã£ã¦ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ç ”ç©¶ã«ãŠã„ã¦ã€ä»Šå¾Œã€ã‚ˆã‚Šç¢ºã‹ãªæˆæœãŒå¾—ã‚‰ã‚Œã‚‹ã¨ä¿¡ã˜ã¦ã„ã‚‹ã€‚
