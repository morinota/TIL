# Predicting Online Performance of News Recommender Systems Through Richer Evaluation Metrics

published date: 16 September 2015,
authors: Andrii Maksai , Florent Garcin , Boi Faltings
url(paper): https://dl.acm.org/doi/10.1145/2792838.2800184
(勉強会発表者: morinota)

---

## どんなもの?

- **オフラインで測定可能なmetricsを利用して、推薦システムのオンライン性能を予測する**事で、コストの掛かるABテストを最小限にする方法を調査する論文.
- acurracy metricsに加えて、diversity, coverage, serendipity等のオフライン観測可能なmetricsを考慮している.
- またオンライン性能予測モデルを用いて、推薦アルゴリズムのハイパーパラメータを自動調整する手法も提案している.(論文の例では、複数の推薦結果のブレンドの際の重み付け設定をself-adjustingしていた...!)
- 実験の結果...
  - オンライン性能の予測には複数のmetrics群が重要であることが示された.(オフライン精度だけでなく...!)
  - カバレッジやセレンディピティといった指標は、CTRなどのオンライン性能の予測や最適化において重要な役割を果たす事が示唆された.
- また、オフラインデータから構築した各アルゴリズムのトレードオフ曲線にこのモデルを適用することで、最適なアルゴリズムとハイパーパラメータを選択することができる.
- その結果、単**にA/Bテストを実施するよりも多くの労力を節約できる**...!

![](https://d3i71xaburhd42.cloudfront.net/96b00351da3e0c281ce8c26b45bbba328b3d5f21/1-Figure1-1.png)

## 導入: 推薦モデルのオフライン精度とオンライン性能の乖離問題

- 推薦システムの歴史の中で、**最も精度(文脈的にはオフライン精度...!)の高い推薦モデルが必ずしもベストではない**ということが早くから認識されていた.
- オフライン精度を基準にアルゴリズムを並べると、サイト運営者が最も気にする指標であるオンラインでのCTRを基準に並べるのと**全く逆の結果になる**事もある、と主張されている[6, 27].
- 推薦アルゴリズムが稼働していない過去の行動から収集したオフラインデータを用いて、推薦のオフライン精度を元にモデルを最適化することは、最適なアルゴリズムを選択する方法として有効でも効率的でもない可能性がある[13, 19]

- アルゴリズムを比較する最も公平な方法は、オンラインでアルゴリズムを公開し、推薦に対するユーザの実際の反応を比較することである.
- そのためには、オンライン環境の存在と熱心なユーザの存在が必要であり、長い時間がかかる.
- **オンライン環境のシミュレーション(オフライン環境での...!)**は、その代替となりうるもの[11, 27].

## 先行研究と比べて何がすごい？

- オンライン性能が、オフライン観測可能なmetricsにどのように依存するかを研究するために回帰モデルを構築したのは本研究が初めてらしい.
- 

## 技術や手法の肝は？

### オンライン性能に影響を与えそうなオフラインmetricsを探索

様々なmetricsをレビューし、オンラインパフォーマンスに最も影響を与えそうなものを見つけ出し、それらの間のトレードオフの存在を示す.
本論文では、5つの異なるグループに分類される17のmetricsを考慮している.

- Accuracy/Error metrics group: オフライン精度的なmetrics.
  - Precision
  - NDPM
  - Kendall's tau
  - Spearman's ρ
  - Success [6],
  - Markedness,
  - Informedness,
  - Matthew’s Correlation [15]
- Diversity metrics group: 推薦リストのアイテム間の非類似性を表すmetrics.
  - “intra-list” diversity:推薦リスト内の多様性
  - temporal diversity: ユーザの異なる訪問時に表示された、推薦アイテムの数に依存する(by Lathia et al[10])
  - "personalization": ユーザペア間の推薦結果の多様性(by Zhou et al[30])
- Novelty metrics group: 推薦アイテムがユーザにとって新しいものであるかを示すmetrics(ex. ユーザーがすでに好きなカテゴリのアイテムを推薦することは、Noveltyが低い推薦と言える)
  - “surprisal”: (by reference[30])
- Coverage metrics group:
  - coverage: これまでに推薦されたことのあるアイテムの割合を示すmetrics.
  - prediction coverage: 推薦結果を作成可能なユーザ数(?)
  - Gini Index: ジニ係数
  - Shannon’s Entropy [17]
- Serendipity metrics group: 推薦アイテムがユーザにとって予想外であると同時にlikeである性質を示す.
  - Serendipity by Ge [7]
  - Serendipity by Murakami [14]

#### metric間の相関関係

Swissinfoデータセット()に対して3種類のアルゴリズムを適用し、metrics間の相関を確認した.

- group内での相関:
  - Accuracy/Error metrics group(NDPM、Kendallのτ、Success、Spearmanのρ、Markedness、Informedness、Matthews相関)は、アルゴリズムが出力した推薦リストについて、**全metricのペア間で0.9以上の相関**が見られた.
  - 他の Diversity group, Coverage group, Serendipity group内でも、それぞれ同様の結果が得られた.
- group間での相関: それぞれのグループの"representative(代表的な)" metricsのペア間の相関を調べた(図2. 推薦リストの長さはk=3)
  - CoverageとSerendipityを除いて、**metricsグループ間で強い相関は見られなかった**.
    - -> これは、**全ての異なるmetrics groupが、推薦結果の異なる特徴を表現している**ことを示しており、したがって、各グループの少なくとも1つの代表metricをパフォーマンスモデルの特徴として使用する必要があることを示している.
  - 散布図にplotされたpointsは3つのクラスターを形成している事が多かったとのこと. (ex. Accuracy group & Diversity group間の相関の図(図2左))
    - このクラスターは、異なるアルゴリズムによる推薦結果に対応.
    - 異なるアルゴリズムによる推薦結果では、metrics group間の関係が異なる可能性があることを示している.

![](https://d3i71xaburhd42.cloudfront.net/96b00351da3e0c281ce8c26b45bbba328b3d5f21/2-Figure2-1.png)

- 推薦リストの長さが、metrics group間の相関関係に与える影響を調査(図3):
  - すべてのmetrics groupペアについて、推薦リストの長さが大きくなるにつれて、まず相関の絶対値が低下し、その後ほとんど変化がないことが確認された.
  - これは、推薦アイテムの数が異なるドメインでは、異なるmetricsの組み合わせが重要である可能性を示している.

![](https://d3i71xaburhd42.cloudfront.net/96b00351da3e0c281ce8c26b45bbba328b3d5f21/2-Figure3-1.png)

#### metrics間のトレードオフ

アルゴリズムは、**その性能に影響を与えるハイパーパラメータ**を持つことが多く、そのような**ハイパーパラメータの値を変えることで、異なる推薦セットを得ることができる**.
ハイパーパラメータを変化させたときにどのように変化するかを観察する.

図4の例は、Yahooデータセットと後述するCT（Context Tree）アルゴリズムのいくつかのバリエーションを用いて得られたもの.

![](https://d3i71xaburhd42.cloudfront.net/96b00351da3e0c281ce8c26b45bbba328b3d5f21/3-Figure4-1.png)

各CTアルゴリズムは、**metrics間(この場合はAccuracyとCoverage)のトレードオフ**を明確に示す曲線を作成した.
また、他のデータセットやアルゴリズムを用いて、これらのトレードオフを観察した.

### オフラインmetricsからオンライン性能を予測する.

選択したmetricsのサブセットを組み合わせて、オンラインパフォーマンスの予測モデルを作成する.

まず、本論文におけるオフライン精度とオンライン精度、クリックスルー率の定義について.

- Offline accuracy(オフライン精度): 評価対象の推薦システムが**未稼働の状態**で発生したユーザの閲覧ログに対して、推薦システムで予測できたクリック数の割合のこと.
- Online accuracy(オンライン精度): 評価対象の推薦システムが**オンライン環境で稼働中の状態**で発生したユーザの閲覧ログに対して、推薦システムで予測できたクリック数の割合のこと.(推薦した際のクリックか否かは問わない)
- Click-through rate (CTR): ユーザが推薦アイテムを見た際にクリックされた割合のこと.

上２つの精度指標の定義について、任意のランダムなユーザについて、ユーザが推薦システムを使わずに訪問したすべてのアイテムが、推薦システムによって**推薦された場合に訪問するアイテムの集合に含まれる**、という仮定に基づいている.(推薦されなかったらそのニュースを読むけど、推薦されたら読まない、という天邪鬼的な事象は無視している...!)
つまり、推薦システムを使用しているときに、推薦システムがないときにも訪問するようなアイテムをユーザが訪問した場合、**そのクリックは推薦システムの効果を測定する際に考慮されるべきではない**ということです.(ん? でもオンライン精度は考慮してしまっている、という事だよね...?! )
このトピックに関するより広範な議論は、Garcin et al.[6].によって紹介されている.(読んだら意図がわかるのかな...)

CTRとオンライン精度は、どちらも推薦システムにとって重要な指標. そこで、それぞれについて回帰モデルを構築する.

#### 特徴量の選択

次に、オンライン性能の回帰モデルに対する特徴選択方法について説明する.
オンラインパフォーマンス指標の予測には、DiversityやCoverageといった複数のmetricsが重要であるという知見を検証するため、**Least Angle Regression（LAR、[4]）**を用いた特徴選択を実施した.

LARは、独立変数yとn個の従属変数$x = (x_1,\cdots ,x_n)^T$の関係の線形モデルを仮定し、$L_{1}$正則化する：

$$
y = \beta^T x + \lambda \sum_{j=1}^{n} |x_{j}|
$$

L1正則化により、βのスパース性が促進される.(不要なパラメータが落とされる様な仕組み...!)

長さΔtの時間間隔におけるmetricsの平均値を predictors(=説明変数?) とし、平均的なCTRをresponses(=目的変数)として、folds間(NNでいうiterationみたいな?)の平均的な順序位置を算出した.
$\Delta t$ を**10分間隔**にしたのは、これより短い間隔では分散が大きく、長い間隔ではデータポイントが少なくなり、有意な結果が得られないからである.
F＝100を使用.(fold=パラメータ推論のサンプリング回数?)

#### オンライン性能の回帰モデル

最後にモデルそのものについて説明する.
前プロセスにて最適なpredictorsを特定した後、重回帰 $y =  \beta |x$ を用いた.
このシンプルなモデルにより、βの係数を、特定のモデルに対する異なるメトリクス間のトレードオフ(=同じCTRを得る場合に、あるmetricsを増やしたらあるmetricsを減らす必要がある、みたいな...??)として、あるいは**メトリクスに関する性能の微分**(=あるmetricが1単位増えたら、オンライン性能は...)として解釈することができる.
単純な線形回帰で得られた結果を、より複雑な手法で得られた結果と比較した.

学習データに関して、オフラインデータ(recsysを使用しないユーザのブラウジング)のログで学習された推薦システムのオフラインmetrics(説明変数側)を算出し、それと同時に、オンライン性能のmetrics(目的変数側)をrecsysを使用している本番環境ウェブサイトから収集する必要がある.

### 推薦アルゴリズムのブレンドの重み付けのself-adjusting

## どうやって有効だと検証した?

### 有効な特徴量(オフラインmetrics)選択の結果

LARによる特徴量選択の手順を、CTアルゴリズム(=推薦アルゴリズム) & Swissinfoデータセット(=データセット)に対して適用した.

今回は各説明変数そのものに興味があるのではなく、**メトリックグループ（Accuracy、Coverage、Diversity、Serendipity、Novelty）の重要性を示したい**ので、各メトリックグループの最初のmetricがモデルに入るまでの平均時間を計算した. table 1はその結果.

![](https://d3i71xaburhd42.cloudfront.net/96b00351da3e0c281ce8c26b45bbba328b3d5f21/5-Table1-1.png)

- **Serendipity、Accuracy、Diversityの3つのグループのメトリクスは、通常、モデルに入る最初の3つ**であった.
  - -> これら3つのグループが、**オンライン性能metricの異なる部分に関係**し、いずれも予測するために重要なmetricsである事を示している.
- DiversityやSerendipityのpredictorsを取り除いた場合\*\*、Cverageの平均初回入力時間が短くなる傾向があった.
  - Diversity、Serendipity、Coverageの3つのグループのうち、2つで十分かもしれないということを示している.
- Accuracy groupのmetricでは、どれを使っても問題なさそう.

あるmetrics group当たり、LARにおいて最速な一つのmetricのみがモデルに入る様にした.

### 回帰モデルの性能

学習データをdatetimeに基づいて、30％、50％、20％のパートに分けた.

- 最初の30%は**アルゴリズム自体のトレーニング**に使用され、回帰モデルには使用されない.
- 次の50%については、アルゴリズムが出力した推薦結果と共に、**オンライン性能予測モデルのトレーニング**に使用された.
- 最後の20%は、**オンライン性能予測モデルの評価**に使用されました.

#### CTR prediction.

表2は、回帰モデルにおけるオフラインmetricsの組み合わせとその予測誤差の結果をまとめたもの.
左の各列は、推薦アルゴリズム毎に結果を平均したもの.
右の各列は、データセット毎に結果を平均したもの.

![](https://camo.qiitausercontent.com/fbaf88b0075e1ce297b5e445d172a65ca59b7e4f/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f32646561373135392d366335622d363730372d646338642d3063646335636632396235352e706e67)

- 4つのmetrics group からそれぞれ1つずつpredictor(説明変数)を用いることで、平均誤差が最も小さくなった. (noveltyは特徴選択の結果が悪く、割愛)
- metricsの全セット(All)の誤差は、おそらくオーバーフィッティングのため、より高い値を示した.(多分17個の説明変数)
- 最初のデータセットにおいては、Diversity が非常に重要だった.
  - metric一つの結果が最も良かった.
  - また、 diversity を含む異なるグループの組み合わせは、そうでない組み合わせよりも良い結果だった.
- ペナルティ付きLR（All+L2）やRBFカーネル付きガウスプロセス（GP+RBF）など、より複雑なモデルはさらに良い結果を示したが、これらは解釈しずらい...
- 結果はデータセット間で一貫しており、**最も優れたmetricsの組み合わせの予測精度は、一定のCTRを仮定したベースラインモデル(Const)を大幅に上回る**.

図5は、２つの推薦アルゴリズムにおいて、テスト期間におけるオンライン性能予測モデルの予測値と実測値を時系列でplotしたもの.

![](https://camo.qiitausercontent.com/269f70a0ec79a22088733bd22b77bf063b3edde2/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f32303931313066382d653631362d383962302d616531352d3862393463393164303065302e706e67)

前述のmetricsが確かにオンライン性能の予測力を持つことがわかる.
予測結果では、オンライン性能metricの経時的な曲線の形状が繰り返され、回帰モデルが**オンライン性能metricの経時的な挙動やおおよその値を予測できる**可能性が高いことが示された.

- また、回帰モデルの係数(parameters)を確認すると、**データセットやアルゴリズムによって各parameterの大小関係が異なる**事がわかった.
  - -> アルゴリズムに関係なく性能を予測するために作られた線形モデルは、各アルゴリズムに特化して訓練されたモデルのセットよりも性能が低下し得る事を示唆している.

#### Online accuracy prediction.

(オンライン性能予測モデルの目的変数をCTRからオンライン精度に置き換えたもの.)
表3は、表2と同様、回帰モデルにおけるオフラインmetricsの組み合わせとその予測誤差の結果をまとめたもの.

![](https://camo.qiitausercontent.com/4c8a1da25a386bf7995170daf2ad5a06ef1081f2/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f61336330386663642d363361622d333361662d333064302d3134326131626238383735392e706e67)

- 予測誤差の傾向はデータセット間であまり一致しなかった.
  - Swissinfoデータセットでは、最良の結果はDiversityを含まない3つのグループから得られた
  - LePointデータセットでは、最良の結果はSerendipityを含まない3つのグループから得られた.

### 推薦アルゴリズムのブレンドの重み付けのself-adjustingの結果

- 今回の実験では、Context TreeとMost Popularの2つの推薦アルゴリズムが与える**推薦結果の線形結合**に基づく4種(重み付け設定が異なる4種)の推薦システムを使用し、ライブのニュースサイトでアルゴリズムを実行した.(Most Popularアルゴリズムによる推薦結果の重み付けを、20%, 40%, 60%, 80%の４種.)

self-adjustingの為に、2つのアルゴリズムによる推薦に近いかどうかを測定する$Z_{CT}$と$Z_{pop}$という潜在的なメトリクスを用意.
self-adjustingに基づき重み付けを変更する事で、本当にCTRが改善するのかどうかを検証した.

図６は、3つのtime frameにおいて、4種の推薦システムによるCTRを記録したもの. 矢印はself-adjustingによって提案された重み付け設定更新の向きを表す.(左向きがmost popularityの重みを減らす、右向きが増やす)

![](https://camo.qiitausercontent.com/9d6e133271d568b8dca988aa4a83cd7eed03039c/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f31333466663762652d336434662d663733632d336565312d3835333036326237656666612e706e67)

第3期、第5期、第7期（日中）では、4種全てが**CTアルゴリズムのウェイトを高めるべきことを示唆**した. (i.e. 日中は、CTアルゴリズムの重み付けを高めるようなself-adjusting結果になった)
CTRを向上させるために示唆された変化は、依然として一貫して正しいものだった.
例えば、Most Popularの重み40%を使用するシステムの回帰係数はプラスで、重みの増加を示唆し、Most Popularの重み60%を使用するアルゴリズムにつながり、この時間枠で実際に高いCTRを得ることができた.

## 議論はある？

## 次に読むべき論文は？

## お気持ち実装
