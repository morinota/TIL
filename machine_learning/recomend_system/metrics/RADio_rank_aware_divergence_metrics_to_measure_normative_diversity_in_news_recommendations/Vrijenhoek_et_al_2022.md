## link

- [pdf](https://arxiv.org/pdf/2209.13520.pdf)

## title

RADio – Rank-Aware Divergence Metrics to Measure Normative Diversity in News Recommendations

## abstract

In traditional recommender system literature, diversity is often seen as the opposite of similarity, and typically defined as the distance between identified topics, categories or word models. However, this is not expressive of the social science’s interpretation of diversity, which accounts for a news organization’s norms and values and which we here refer to as normative diversity. We introduce RADio, a versatile metrics framework to evaluate recommendations according to these normative goals. RADio introduces a rank-aware Jensen Shannon (JS) divergence. This combination accounts for (i) a user’s decreasing propensity to observe items further down a list and (ii) full distributional shifts as opposed to point estimates. We evaluate RADio’s ability to reflect five normative concepts in news recommendations on the Microsoft News Dataset and six (neural) recommendation algorithms, with the help of our metadata enrichment pipeline. We find that RADio provides insightful estimates that can potentially be used to inform news recommender system design.

# Introduction

For centuries, the interplay between journalists and news editors has shaped how news items are created and how they are shown to their readers [82]. With the digitization of society, much has changed: while before, people would typically limit themselves to reading one type of newspaper, they now have a wealth of information available to them at the click of a button [63] – more than anyone could possibly be expected to read or make sense of. News recommender systems can filter the enormous amount of information available to just those news items that are in some way interesting or relevant to their users [8, 52]. The use of news recommender systems is widespread, not just for personalized news recommendations, but also to automatically populate the front page of a news website [53], or present the reader of a particular news article with other articles about the same topic, but from a different perspective [54]. The use of news recommender systems has a wide range of benefits. They can increase engagement [55] and help raise informed citizens [28]. A news recommender system may broaden the horizons of their users by presenting diverse recommendations, including items different from what they are used to or expect seeing. They could even foster tolerance and understanding [29, 66], and counter so-called filter bubbles or echo chambers [52, 58].

To realize the potential benefits of news recommender systems, much attention has been given to generating recommendations that reflect the user’s interests and preferences [39]. However, with news recommenders taking over the role of human editors in news selection, they are becoming gatekeepers in what news is shown to audiences and have thus a democratic role to play in society. As such, their evaluation has different requirements than those of other types of recommender systems [4, 5, 72, 75]. Recent controversies have shown that merely optimizing for click-through rates and engagement may promote sensationalist content [68], and is particularly conducive to the spread of misinformation.1 This observation is not limited to the academic literature – an increasing number of media organizations, both public service and commercial, have acknowledged the difficulties in translating their editorial norms into concrete metrics that can inform recommender system design [9, 32]. News recommender systems exist in a complex space consisting of many different areas and disciplines, each with their own goals and challenges; think of balancing diversity and accuracy [57], nudging [50] or even identifying user preferences [6, 49] and biases [74]. In this paper, we focus on the process of translating normative theory (i.e., what it means for a recommendation to be diverse) into metrics that are usable and understandable for both technical and editorial purposes. We build on the work of Helberger [33], who provides a theoretical foundation for conceptualizing diversity, and of Vrijenhoek et al. [71], who propose a new set of metrics (DART) that reflect this theory. The DART metrics represent a first step towards a normative interpretation of diversity in news recommendations. We identify a number of possible shortcomings in these metrics: there could be more consideration for the theory of metrics and distance functions, generalizability to other normative concepts, unification under one framework, and rankawareness. In this paper, we focus on the mathematical aspects of a rank-aware metric, versatile to different normative concepts and as such addressing these shortcomings. We refer to our framework as the Rank-Aware Divergence metrIcs to measure nOrmative diversity (RADio).

Our contribution consists of a diversity metric that is (i) versatile to any normative concept and expressed as the divergence between two (discrete) distributions; (ii) rank-aware, taking into account the position of an item in a recommendation set; and (iii) mathematically grounded in distributional divergence statistics. We demonstrate the effectiveness of this formulation of the metrics by defining a natural language processing (NLP) metadata enrichment pipeline (e.g., sentiment analysis, named entity recognition) and running it against the MIND dataset [80]. Figure 1 illustrates the operationalization. The pipeline and the code produced for metadata enrichment and metric computation are available online.2 The goal of RADio is not to serve as thresholds or strict guidelines for “diverse recommendations,” but to provide developers of recommender systems with the tools to evaluate their systems on normative principles.

# Related Work

We first highlight recent work on the formal mathematical work on diversity in news recommendation, before citing related work on the normative aspect of diversity. Finally we describe the gap that exists between descriptive and normative diversity.3

## Descriptive (General-Purpose) Diversity

Diversity is a central concept in Information Retrieval literature [17, 62], albeit with a different interpretation than the normative diversity described in the previous section. During the development of news recommender systems, there is currently a large focus on the predictive power of an algorithm. However, this may unduly promote content similar to what a user has interacted with before, and lock them in loops of “more of the same.” To tackle this, “diversity” is introduced, which is typically defined as the “opposite of similarity” [11]. Its goal is to prevent users from being shown the same type of items in their recommendations list and is often expressed as intra-list-diversity (ILD) [11, 13, 19, 23, 24, 38, 48, 70]: mean pairwise dissimilarity between recommended item lists. ILD requires the specification of a distance function between lists, and thus leaves it up to interpretation as to what it means for two lists to be distant. In theory, it could still be interpreted with a metric that accounts for the presence of different sources or viewpoints [25]. However, in practice, diversity is most often implemented as a descriptive distance metric such as cosine similarity between two bag-of-words models or word embeddings [43, 48].

Other popular “beyond-accuracy” metrics related to diversity are novelty (how different is this item from what the user has seen in the past), serendipity (is the user positively surprised by this item), and coverage (what percentage of articles are recommended to at least one user). These metrics can be taken into account at different points in the machine learning pipeline [43, 81]. One can optimize for these descriptive notions of diversity (i) before training, by clustering users based on their profile diversity with JS divergence [27], (ii) directly at training time (e.g., for learning-to-rank [10, 13, 70], collaborative filtering [60], graphs [30, 59] or bandits [21, 84]), (iii) by re-ranking a recommendation set and balance diversity vs. relevance [16] or popularity vs. relevance [15], and (iv) by defining a post-recommendation metric to measure diversity for each recommendation set or at user-level (e.g., the generalist-specialist score [2, 73]). With any of these four methods, a trade-off must be made between the relevance of a recommendation issued to users and the level of descriptive diversity, though there have also been studies indicating that increasing diversity does not necessarily need to negatively affect relevance [48]. Nevertheless, this encouraged recent efforts in training neural-based recommenders that explicitly make a trade-off between accuracy and diversity [61]. Also recently, there have been studies that differentiate between diversity needs of users [83].

## Normative Diversity

Diversity is extensively discussed as a normative concept in literature, and has a role in many different areas of science [46, 65], spanning from ecological diversity to diversity as a proxy for fairness in machine learning systems [51]. While these interpretations of diversity are often related, they do not fully cover the nuances of a diverse news recommender system, the work on which stems from democratic theory and the role of media in society. Following Helberger [33], we define a normatively diverse news recommendation as one that succeeds in informing the user and supports them in fulfilling their role in democratic society. Out of the many theoretical models that exist in literature, Helberger [33] describes four different models from the normative framework of democracy, each with a different view on what it means to properly inform citizens: the Liberal model, which aims to enable personal development and autonomy, the Participatory model, which aims to enable users to fulfill their role as active citizens in a democratic society, the Deliberative model, which aims to foster discussion and debate by equally presenting different viewpoints and opinions in a rational and neutral way, and the Critical model, which aims to challenge the status quo and to inspire the readers to take action against existing injustices in society. For more details regarding the different models, and what a recommender system following each of these models would look like, we refer to Helberger [33]. Which model is followed is a decision that needs to be made by the media organization itself, and should be in line with their norms and values.

Based on these models, the DART metrics [71] take a first step towards normative diversity for recommender systems and reflect the nuances of the different democratic models described above: Calibration, Fragmentation, Activation, Representation and Alternative Voices. Table 1 provides an overview of the DART metrics and their expected value ranges for the different models, and will be further elaborated later in the paper.

## The Gap Between Normative and Descriptive Diversity

The descriptive diversity metrics described in Section 2.1 are generalpurpose and meant to be applicable in all domains of recommendation. However, in their simplicity a large gap can be observed between this interpretation of diversity and the social sciences’ perspective on media diversity that is detailed in Section 2.2. In their comprehensive work on the implementation of media diversity across different domains, Loecherbach et al. [46] note that there is “little to no overlap between concepts and operationalizations (of diversity) used in the different fields interested in media diversity.” As such, a recommendation that would score high on diversity according to traditional information retrieval-based metrics [17, 62], may not be considered to be diverse according to the criteria maintained by newsroom editors. Both Loecherbach et al. [46] and Bernstein et al. [7] call for truly interdisciplinary research in bridging this gap, where Bernstein et al. [7] argue for close collaboration between academia and industry and the foundation of joint labs. This work is a step in that direction, as we provide a versatile and mathematically grounded rank-aware metric that can be used by practitioners to monitor their normative goals.

# Operationalizing Normative Diversity for News Recommendation

With our RADio framework, we further refine the DART metrics that were defined by Vrijenhoek et al. [71] in order to resolve a number of the shortcomings of the metrics’ initial formalizations. In their current form, each of the metrics has different value ranges; for example, Activation has a value range [−1, 1], where a higher score indicates a higher degree of activating content, and Calibration has a range of [0, ∞], where a lower score indicates a better Calibration. These different value ranges reduce the interpretability of the metrics, making them harder to explain and as such less likely to be adopted by news editors. Furthermore, the proposed metrics do not take the position of an article in a recommendation into account. News recommendations are ranked lists of articles that are typically presented to users in such a way that the likelihood of a recommended article to be considered by the user decreases further down the ranking. As such, in the evaluation of the diversity of the recommender system we should also account for the position of an article in the recommendation ranking, rather than considering the set as a whole (e.g. ILD).

Thus, the two major challenges that we seek to address are that (i) scores should be comparable between the metrics and across recommendation systems, and (ii) scoring of both unranked and ranked sets of recommendations should be possible. In this section, we first detail these requirements (Section 3.1), then describe how we reformulate the metrics to each use the same divergence-based approach (Section 3.2). We then add the rank-aware aspect to the metrics (Section 3.3), before applying them to the five concrete DART metrics (Section 3.4).

## Requirements

We first enunciate the classical definition of a distance metric, before specifying three desirable metric criteria for news recommendations. Take a set 𝑋 of random variables and 𝑥, 𝑦, 𝑧 ∈ 𝑋, then a metric 𝐷 is a proper distance measure if 𝐷(𝑥, 𝑦) = 0 ⇔ 𝑥 = 𝑦, 𝐷(𝑥, 𝑦) = 𝐷(𝑦, 𝑥) and𝐷(𝑥, 𝑦) ≤ 𝐷(𝑥, 𝑧) + 𝐷(𝑧, 𝑦). These are respectively the axioms of identity, symmetry and triangle inequality, that express intuitions about concepts of distance [56].

We add that our distance measure should (i) be bounded by [0; 1], for comparisons of different recommendation algorithms (ii) be unified, so as to fairly consider different diversity aspects (as opposed to e.g. using weighted averages or maxima in [18]) and (iii) allow for discrete rank-based distribution sets, to fit the ranked recommendation setting.

## f-Divergence

We model the task of measuring diversity as a comparison between probability distributions: the difference in distribution between the issued recommendations (𝑄) and its context (𝑃). Each diversity metric prescribes its own 𝑄 and 𝑃. The elements in the distribution 𝑄 can be recommendation items (cf. Calibrated Recommendations [64]), but can also be higher-level concepts, such as distributions of topics and viewpoints. The context 𝑃 may refer to either the overall supply of available items, the user profile, such as the reading history or explicitly stated preferences, or the recommendations that were issued to other users (see Figure 1). Intuitively, when 𝑃 is linked to the same user as 𝑄, we measure within user diversity (e.g., towards preventing getting locked in “filter bubbles”). When 𝑃 is linked to another user than 𝑄, we measure diversity across users (e.g., monitoring diversity of viewpoints represented across personalized homepages). In the following, we formalize the role of 𝑃 and 𝑄 in two different metric settings, starting with the simple and common KL divergence metric, before presenting its refinement (JensenShannon divergence) as our preferred metric.

### Kullback-Leibler Divergence.

The concept of relative entropy or KL (Kullback–Leibler) divergence [42] between two probability mass functions 𝑃 and 𝑄 (here, a recommendation and its context) is defined as:

$$
\tag{1}
$$

Often also expressed as $𝐷_{KL}(𝑃, 𝑄) = 𝐻(𝑃, 𝑄)−𝐻(𝑃)$, with $𝐻(𝑃, 𝑄)$ the cross entropy of 𝑃 and 𝑄, and $𝐻(𝑃)$ the entropy of P. Both cross entropy and KL divergence can be thought of as measurements of how far the probability distribution 𝑄 is from the reference probability distribution 𝑃. When $𝑃 = 𝑄$, $𝐷_{KL}(𝑃, 𝑄) = 𝐷_{KL}(𝑃, 𝑃) = 0$, that identity property is not guaranteed by cross entropy alone. This is the main reason to prefer KL divergence over cross entropy. Though KL Divergence satisfies the identity requirement, the symmetry and triangle inequality are not fulfilled. This can be resolved by further refining KL Divergence.

### Jensen–Shannon Divergence.

A succession of steps from KL divergence lead to Jensen-Shannon (JS) divergence. KL divergence was first turned symmetric [37] and then upper bounded [45], to lead to

$$
\tag{2}
$$

When the base 2 logarithm is used, the JS divergence bounds are $0 \leq 𝐷_{JS}(𝑃, 𝑄) \leq 1$. Additionally, Endres and Schindelin [26] show that $\sqrt{𝐷_{JS}}$ is a proper distance which fulfills the identity, symmetry and the triangle inequality properties. When we refer to $𝐷_{JS}$ or $JS$ divergence below, we therefore implicitly refer to the square root of the JS formulation with log base 2. Liese and Vajda [44] defined f-Divergence[$D_f$]: a generic formulation of several divergence metrics. Among them are the JS and KL divergences.4 Further along the text, we use $𝐷_𝑓$ as a shorthand notation for KL and JS divergences. $𝐷_𝑓$ in discrete form is...

$$
\tag{3}
$$

where $f_{KL}(t) = t \log{t}$ and $f_{JS}(t) = \frac{1}{2}[(t+1)\log{\frac{2}{t+1}} +t \log{t}]$.
To avoid misspecified metrics [64], we write $\bar{P}$ and $\bar{Q}$:

$$
\tag{4}
$$

where 𝛼 is a small number close to zero. $\bar{P}$ prevents artificially setting $𝐷_𝑓$ to zero when a category (e.g., a news topic) is represented in 𝑄 and not in 𝑃. In the opposite case (when a category is represented in 𝑃 and not in 𝑄), $\bar{Q}$ avoids zero divisions. In order for the entire probabilistic distributions $\bar{P}$ and $\bar{Q}$ to remain proper statistical distributions, we normalize them to ensure $\sum_{x}\bar{P}(x) = \sum_{x}\bar{Q}(x) = 1$. To avoid notation congestion, 𝑃 and 𝑄 will implicitly refer to $\bar{P}$ and $\bar{Q}$, in the following sections.

## Rank-Aware f-Divergence Metrics

Our ranked recommendation setting (characteristic (iii) above) motivates a further reformulation of our f-Divergence metric. It is well entrenched in Learning To Rank (LTR) literature [67, 85], and by extension in conventional descriptive diversity metrics [13] that a user is a lot less likely to see items further down a recommended ranked list (i.e., diminishing inspection probabilities). Note that the ranking oftentimes reflects relevance to the user, but it is not always the case for news (e.g., editorial layout of a news homepage). We extend our metrics with an optional discount factor for 𝑃 and 𝑄 to weigh down the importance of results lower in the ranked recommendation list. The ranking relevancy metrics Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG) are popular rank-aware metrics for LTR [14, 36], in particular for news recommendation [80]. In line with the LTR literature, we first define the discrete probability distribution of a ranked recommendation set $𝑄^∗$ , given each item 𝑖 in the recommendation list 𝑅:

$$
\tag{5}
$$

where $𝑤_{𝑅𝑖}$ , the weight of a rank for item 𝑖, can be different depending on the discount form. For MMR, $𝑤_{𝑅𝑖} = \frac{1}{Ri}$ , for NDCG, $𝑤_{𝑅𝑖} =  \frac{1}{\log_2{Ri+1}}$ When $𝑤_{𝑅𝑖} = 1$, $𝑄^∗$ is not discounted (i.e., $𝑄^∗ = 𝑄$).

In news recommendation, the sparsity bias plays a predominant role: users will interact with a small fraction of a large item collection, such as scrollable news recommendation websites [40]. We thus opt for weighing based on MRR rather than NDCG, because it applies a heavier discount along the ranking than NDCG. Note that the latter is said to be more suited for query-related rankings, where the user has a particular information need related to a query and thus higher propensity to scroll down a page [14].

The context distribution 𝑃 is discounted in the same manner, when it is a ranked recommendation list. When 𝑃 is a user’s reading history (see Figure 1), the discount on 𝑃 increases with time: articles read recently are weighted higher than articles read longer ago. There are situations when rank-awareness is not applicable, for example when 𝑃 is the entire pool of available articles.5 With rankaware $𝑄^∗$ and optionally rank-aware $𝑃^∗$ , we formulate RADio, our rank-aware f-Divergence metric:

$$
\tag{6}
$$

$𝑄^∗(𝑥)$ and $𝑃^∗(𝑥)$ accommodate for multiple situations: for example, $𝑄^∗(𝑐|𝑅)$ is the rank-aware distribution of news categories 𝑐 over the recommendation set 𝑅. In the following, we specify $𝑃^∗(𝑥 |·)$ and $𝑄^∗(𝑥 |·)$ in accordance to each normative concept of interest for our universal metric.

## Normative Diversity metrics as Rank-Aware f-Divergences

In this section, we describe the RADio formalization of the general f-Divergence formulation above to the five DART metrics. We leave the exact implementation of the metrics in practice for a particular open news recommendation dataset to the next section. More formally, we define the following global parameters:

- 𝑆: The list of news articles the recommender system could make its selection from, also referred to as the “supply.”
- 𝑅: The ranked list of articles in the recommendation set.
- 𝐻: The list of articles in a user’s reading history, ranked by recency.

$𝑅_{i}^u \in {1, 2, 3, \cdots}$ refers to the rank of an item 𝑖 in a ranked list of recommendations for user 𝑢. In this work, metrics are defined for a specific user at a certain point in time, therefore 𝑅 implicitly refers to $𝑅^𝑢$, unless stated otherwise. While this section contains some contextualization of the DART metrics [71], the original paper contains further normative justifications.

### Calibration.

(Equation 7) measures to what extent the recommendations are tailored to a user’s preferences. The user’s preferences are deduced from their reading history (𝐻). Calibration can have two aspects: the divergence of the recommended articles’ categories and complexity. The former is expected to be extracted from news metadata and thus categorical by nature, the latter is a binned (categorical) probabilistic measure extracted via a language model. As such, we compare $𝑃^∗(𝑐|𝐻)$, the rank-aware distribution of categories or complexity score bins 𝑐 over the users’ reading history, and $𝑄^∗(𝑐|𝑅)$ the same in the recommendations issued to the user

### Fragmentation.

(Equation 8) reflects to what extent we can speak of a common public sphere, or whether the users exist in their own bubble. We measure Fragmentation as the divergence between every pair of users’ recommendations. Here we consider$ 𝑃^∗(𝑒|𝑅^𝑢)$ as the rank-aware distribution of news events 𝑒 over the recommendations 𝑅 for user 𝑢, and $𝑄^∗(𝑒|𝑅^𝑣)$ the same but for user 𝑣. KL Divergence is asymmetric (see Section 3.2.1), which means that its outcome differs depending on which user’s recommendation is chosen as the target and which as the reference distribution. To avoid this, we compute the Fragmentation score as the average of KL Divergences with switched parameters. JS divergence is already symmetric and is thus implemented as for the other metrics. In theory, Fragmentation requires a user’s recommendation to be compared to those of all other users. This is not feasible with a sizeable dataset and the requirement of a reasonable compute time. Instead we opt to randomly sample user pairs.

### Activation.

(Equation 9) Most off-the-shelf sentiment analysis tools analyze a text, and return a value (0, 1] when the text expresses a positive emotion, a value [−1, 0) when the expressed sentiment is negative, and 0 if it is completely neutral. The more extreme the value, the stronger the expressed sentiment is. As proposed in [71], we use an article’s absolute sentiment score as an approximation to determine the height of the emotion and therefore the level of Activation expressed in a single article. This then yields a continuous value between 0 and 1. $𝑃(𝑘|𝑆)$ denotes the distribution of (binned) article Activation score 𝑘 within the pool of items that were available at that point (𝑆). $𝑄^∗(𝑘|𝑅)$ expresses the same, but for the binned Activation scores in the rank-aware recommendation distribution.

### Representation.

(Equation 10) aims to approximate a notion of viewpoint diversity (e.g. mentions of political topics or political parties), where the viewpoints are expressed categorically. Here 𝑝 refers to the presence of a particular viewpoint, and $𝑃(𝑝|𝑆)$ is the distribution of these viewpoints within the overall pool of articles, while $𝑄^∗(𝑝|𝑅)$ expresses the rank-aware distribution of viewpoints within the recommendation set.

### Alternative Voices.

(Equation 11) is related to the Representation metric in the sense that it also aims to reflect an aspect of viewpoint diversity. Rather than focusing on the content of the viewpoint, it focuses on the viewpoint holder, and specifically whether they belong to a “protected group” or not. Examples of such protected/unprotected groups could be non-male/male, nonwhite/white, etc.7 This approach is based on the implementation of balanced neighbourhoods in recommender systems [12]. With 𝑚 we refer to the distribution of protected vs. non-protected groups, with $𝑚 \in \text{Minority, Majority}$. $𝑃(𝑚|𝑆)$ and $𝑄^∗(𝑚|𝑆)$ refer to the distribution of these groups in the pool of available articles and rank-aware recommendation distribution respectively. Below is a summary of the formalization of DART with the RADio framework, the notation of which is defined in this section. In the next section, we show how to retrieve the necessary features from an example news dataset:

$$
\tag{11}
$$

# Experimental Setup

TUP In order to demonstrate RADio’s potential effectiveness, we developed an NLP pipeline to retrieve input features to the metrics in Section 3.4 and ran them on a public dataset. It should be noted that this pipeline is an imperfect approximation, and that each metric individually would benefit from more sophisticated methods. The MIND dataset [80] contains the interactions of 1 million randomly sampled and anonymized users with the news items on MSN News between October 12 and November 22 2019. Each interaction contains an impression log, listing which articles were presented to the user, which were clicked on and the user’s reading history. The MIND dataset was published accompanied by a performance comparison on news recommender algorithms trained on this dataset,8 including news-specific neural recommendation methods NPA [78], NAML [77], LSTUR [1] and NRMS [79]. It was shown that these algorithms outperform general-purpose ones [80] or common collaborative filtering models (such as alternating least squares (ALS)), in particular due to the short lifespan of news items [31]. These algorithms are trained on the impression logs in order to predict which items the users are most likely to click on. For the purpose of this paper we will evaluate these neural recommendation methods with the RADio framework (on the DART metrics) and compare their performance with two naive baseline methods, based on a reasonable set of candidates (the original impression log): a random selection, and a selection of the most popular items, where the popularity of the item is approximated by the number of recorded clicks in the dataset.

Since RADio computes the average of all {𝑃, 𝑄} pairs, we retrieve confidence intervals over paired distances too, as illustrated in the sensitivity analyses below. In a traditional model evaluation setting, it would be desirable to generate confidence intervals via different model seeds or cross-validation splits. We refrain from doing this for our metric evaluation as this would introduce a multidimensional confidence interval (e.g., over {𝑃, 𝑄} pairs and over model seeds). We scrape articles via the URLs provided in the MIND dataset. Each article’s metadata is enriched with five methods:

(1) Complexity analysis: Each item is assigned a complexity score based on the Flesch-Kincaid reading ease test [41], implemented in the Python module py-readability-metrics [20]. Complexity is then discretized into bins, to accommodate for the discrete form of $𝐷^*_f$.

(2) Story clustering: The individual news items are clustered into so-called news story chains, which means that stories about the same event will be grouped together. This way, we add a level of analysis between individual news items and higher level categories (see Section 3.4). We use a TF-IDF based unsupervised clustering algorithm based on cosine similarity and a three days moving window, following the setup of Trilling and van Hoof [69].

(3) Sentiment analysis: Using the textBlob open source NLP library we assign each article a sentiment polarity score [47]. Our focus is on the relative neutrality of articles, we thus take the absolute value of the negative / positive polarity score.

(4) Named entity recognition: Using spaCy, we identify the people, organizations and locations mentioned in the text [34], and count their frequency.

(5) Named entity augmentation: For the entities identified in the text in the previous step, we attempt to link them to their Wikidata9 entry through fuzzy name matching, to figure out if they are politicians, or in the case of organizations, political parties.10

We implement RADio with the pipeline above. Table 2 links the numbered list above with the DART metrics. It provides an overview of the different metrics and their respective context distribution 𝑃 over normative concepts. The code for this implementation is available online.11

We evaluate the outcome of our RADio framework for different recommender strategies (LSTUR, NAML, NPA, NRMS, most popular and random), with both KL Divergence and Jensen-Shannon as divergence metrics, with and without discounting for the position in the recommendation and at different ranking cutoffs.

# Results

Having described our methodology and experimental setup around the operationalization of DART metrics, we analyze the results of the experiments on MIND. We separate descriptive analysis of the results in Section 5 from the interpretation of normative interpretation of the metrics in Section 6. We choose to implement RADio with rank-awareness and JS divergence with a rank cutoff @N (the entire ranking list) as our default. After commenting on the overall results, we further motivate that choice with a sensitivity analysis to different hyperparameters. We alter the divergence metric (KL or JS), rank-awareness (with and without a discount) and ranking cutoffs ($@n, with 𝑛 = 1, 2, 5, 10, 20, 𝑁$) for the different recommender models.

Table 3 displays results for RADio with rank-aware JS divergence.12 Higher values imply higher divergence scores, but whether high or low divergence is desired depends on the goal of the recommender system, which we will further elaborate in Section 6. The random recommender scores highest on divergence for all metrics and is also one of the least relevant by definition (see NDCG score). Most popular and random have comparable NDCG results. Popularity scores for the articles are derived from the clicks recorded in the MIND interaction logs, and many articles have zero or only one click recorded. When the candidate list contains exclusively articles with a similar number of clicks this forces the most popular recommender to a random choice, which explains the artificial similarity between most popular and random in terms of the NDCG score. Between the neural recommenders, most scores for LSTUR, NPA, NRMS and NAML are in lower ranges. Note that they produce similar recommendations (see NDCG values and Wu et al. [80]). Some notable differences can be observed when comparing these neural methods to the baselines. For example, we see that the neural recommenders are more Calibrated to the items present in people’s reading history, though the most popular baseline performs marginally better in terms of Calibration of complexity. In the following, we further analyse the entire distribution of individual recommendation list divergences and test the sensitivity of RADio to different settings. Boxplots for all metrics and all recommender strategies are available in the online repository, where we highlight the importance of rank-awareness.

## Sensitivityto the Divergence Metric

JS divergence is our preferred implementation of universal diversity metrics. It is a proper distance metric and bounded between 0 and 1 (see Section 3.2). Figure 2 substantiates that claim empirically, visualizing the sensitivity of RADio to the two described f-Divergence metrics: KL and JS Divergence. Clear differences can be observed in the distributions; KL divergence is skewed towards lower divergence, while JS divergence yields a more centered distribution of values. Additionally, JS divergence applies more contrast between the neural recommender systems and the naive recommendation methods and especially the random baseline. Due to the large sample in MIND, the random baseline is an approximation of a diverse recommendation set, given the candidate articles. In certain cases KL introduces consequential skew in the distribution of individual 𝑃,𝑄 comparison pairs across recommendation models; this does not occur to that extent with JS. Although KL Divergence is a well-known metric that can be found in many applications and is simpler to express mathematically, we found the JS divergence to be a better fit both theoretically and empirically.

## Sensitivity to Rank-awareness

In the original formulation of DART metrics [71], rank-awareness was not considered for most of the defined metrics. In our formalization, rank-awareness is the default. In Figure 3, we visualize the effect of removing the rank-awareness (in blue) on Fragmentation and compare to the original rank-aware Fragmentation (in orange). Rank-awareness allows for better differentiation between methods: LSTUR and “most popular” seem to be similarly distributed without a rank discount. Introducing rank-awareness shifts LSTUR towards a larger divergence, whereas “most popular” remains largely the same.

## Sensitivity @n

One could also consider adding a cut-off point where only the top 𝑛 recommendations are considered for evaluation, the results of which are shown in Figure 4. The figure shows that the effect of rank-awareness becomes stronger with a higher cut-off point, and causes the divergence score to stabilize after roughly 10 recommendations. This is because our MMR rank-awareness strongly discounts values further down the ranking. @20 and @N (no cutoff) are similar for all metrics because MIND rarely contains more than 20 recommendation candidates. Note that when calculating the divergence score for Activation, Representation or Alternative Voices without rank-awareness and without cutoff point, there is no divergence to be reported as recommendation and target distribution are identical in these cases.13

## Normative Evaluation

By comparing divergence scores across different recommender strategies, we can draw conclusions on the way they influence exposure of news to users. This is especially the case when comparing neural methods to the random recommender, which should reflect the characteristics of the overall pool of data. Combining this with DART’s different theoretical models of democracy (summarized in Table 1), one can make informed decisions on which recommender system is better suited to one’s normative stance than others. Imagine, for example, a public service media organization that aims to reflect Participatory norms and values in their recommendations. The Participatory model prescribes low Fragmentation and low Activation, which is shown in the scores of the neural recommenders. This would indicate that those models are more suitable for this organization’s goals. In comparison, imagine a large media organization that wants to dedicate a small section of their website to Critical principles, consisting of one element with recommendations called “A different perspective.” The Critical model calls for a high divergence score in both Representation and Alternative Voices. Given that the random recommender scores best according to these principles, the neural recommenders would not be very suitable for this goal. Nevertheless, the conclusion that a random recommender is suitable for Critical norms and values is moot. Additional steps should be taken to further improve upon these scores: recommendation algorithm developers could tweak the trade-off between different target values in the recommendation, or even explicitly optimize on these metrics.

# Discussion

Choosing an f-Divergence score as the base for our metrics allows us to construct a single base formalization with a clear interpretation amongst all metrics; when the value is 0, the distribution between the recommendations and the chosen context is identical. The larger the measurements, the larger the divergence is. However, it also comes with a number of limitations. For one, f-Divergence does not take the relations between categorical values into account, and the ordering of the categorical values in the input vector is arbitrary. For example, two left-wing political parties in the Representation metric may be more similar than an extremely left-wing and an extremely right-wing party, but this is currently not taken into account. Related to this, in order to make continuous values suitable for our general discrete definition of f-Divergence, they need to be discretized into arbitrarily defined bins. This means that two very similar values may end up in different binsFuture work may propose a different approach for calculating divergence between continuous variables. Regarding the data enrichment pipeline, we identify a number of enhancement points. While some metrics, such as topic Calibration, work with simple data on news topics that is often directly available in a dataset, other metrics require a more sophisticated data enrichment pipeline. The differences in these approaches appear in the results: the metrics with more trivial metadata retrieval setups show clear and distinct patterns for different recommender algorithms, but this is not the case for the more complicated ones. Furthermore, it is not possible to determine the quality of the pipeline, as we do not have a ground truth for evaluation. For future work, we suggest to take the base formalizations as constructed in this paper as a starting point, and work to improve the extraction of the relevant parameters for metrics such as Representation, Alternative Voices and Activation. Especially for the first two, there is already a large body of work that can facilitate this process [3, 22]. Human evaluation, including the input from editorial teams, would then be a promising way to evaluate these three normative metrics, similar to the work in the context of language generation bias [18]. Additionally, more insight needs to be gained on the influence of the choice of dataset. The MIND dataset contains a significant amount of so-called soft news, including articles on lifestyle, sport and entertainment, whereas the DART metrics are mostly applicable to hard news. The influence of the chosen dataset needs to be investigated in more detail, which can then lead to more informed decision-making on the trade-off between diversity and click-through rate, and what can reasonably be expected of a news recommender system.

# Conclusion

In this paper we have made a first attempt at constructing and implementing new evaluation criteria for news recommender systems, with a foundation in normative theory. Based on the DART metrics, first theoretically conceptualized in earlier work, we propose to look at diversity as a divergence score, observing differences between the issued recommendations and a metric-specific target distribution. We proposed RADio, a unified rank-aware f-Divergence metric framework that is mathematically grounded and that fits several possible use cases within the original DART metrics and we hope beyond in future work. We showed that JS divergence was preferred over other divergence metrics. At first mathematically, as JS is a proper distance metric, and empirically, via a sensitivity analysis to different cutoff, rank-awareness and divergence metric regimes. When our approach is adopted in practice, it enables the evaluation of news recommender systems on normative principles beyond user relevance. Finally, we wish to emphasize that the metrics proposed are meant to supplement standard recommender system evaluation metrics, in the same way that current beyond-accuracy metrics do. Most importantly, they are meant to bridge the gap between different disciplines involved in the process of news recommendation and to support more informed discussion between them. We hope for future research to foster interdisciplinary teams, leveraging each fields’ unique skills and specialties.
