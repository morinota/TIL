{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "- https://takuti.me/ja/note/recsys-2021-ab-ndcg/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommender systems evaluation has evolved rapidly in recent years. However, for offline evaluation, accuracy is the de facto standard for assessing the superiority of one method over another, with most research comparisons focused on tasks ranging from rating prediction to ranking metrics for top-n recommendation. Simultaneously, recommendation diversity and novelty have become recognized as critical to users’ perceived utility, with several new metrics recently proposed for evaluating these aspects of recommendation lists. Consequently, the accuracy-diversity dilemma frequently shows up as a choice to make when creating new recommendation algorithms.\n",
    "\n",
    "近年、レコメンダーシステムの評価は急速に発展している。しかし、オフラインでの評価では、ある手法の優位性を評価するための事実上の基準として精度が用いられており、ほとんどの研究は、評価予測からトップNレコメンデーションのためのランキングメトリクスまでのタスクに焦点を当てて比較を行っています。一方、レコメンデーションの多様性と新規性は、ユーザーが有用性を感じる上で重要であると認識されるようになり、最近、これらの側面を評価するための新しい指標がいくつか提案されています。その結果，新しい推薦アルゴリズムを作成する際に，精度と多様性のジレンマがしばしば選択されるようになった．\n",
    "\n",
    "We propose a novel adaptation of a unified metric, derived from one commonly used for search system evaluation, to Recommender Systems. The proposed metric combines topical diversity and accuracy, and we show it to satisfy a set of desired properties that we formulate axiomatically. These axioms are defined as fundamental constraints that a good unified metric should always satisfy. Moreover, beyond the axiomatic analysis, we present an experimental evaluation of the metric with collaborative filtering data. Our analysis shows that the metric respects the desired theoretical constraints and behaves as expected when performing offline evaluation.\n",
    "\n",
    "我々は、検索システムの評価によく用いられる統一的な評価指標を、推薦システムに適用した新しい評価指標を提案する。提案する評価指標は、トピックの多様性と精度を兼ね備え、公理的に定式化された一連の望ましい特性を満たすことを示す。これらの公理は、優れた統一的評価指標が常に満たすべき基本的な制約として定義される。さらに、公理的な分析にとどまらず、協調フィルタリングデータを用いたメトリクスの実験的な評価も行っている。解析の結果、このメトリクスは望ましい理論的制約を尊重し、オフライン評価を行う際に期待通りの振る舞いをすることが示された。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of Recommender Systems (RSs) is a research topic attracting significant attention (e.g. [8, 18, 43]). When evaluating, a sequence of decisions must be taken to determine the metrics. The first decision is between online and offline evaluation. Online evaluation is the gold standard, as it directly assesses user satisfaction. However, online evaluation of RSs imposes challenges compared to the more common offline evaluation [25]. For instance, the personalized nature of the RSs implies the need for more, many times scarce, resources. Moreover, how the system presents recommendations has been demonstrated to have a considerable effect [35]. Therefore, offline evaluation is the most common evaluation framework for RS research. When using offline evaluation, we have to make a second decision: evaluate with error metrics for the rating prediction task [22] or evaluate with ranking metrics for top-n recommendation [14]. Lately, different works have pointed out the advantages of ranking metrics over error metrics regarding the measure of user satisfaction [5, 22, 28].\n",
    "\n",
    "レコメンダーシステム（RS）の評価は、大きな注目を集めている研究テーマである（例えば、[8, 18, 43]）。評価を行う際には、評価指標を決定するために一連の決定を行う必要がある。まず、**オンライン評価かオフライン評価か**の判断がある。オンライン評価はユーザの満足度を直接評価できるため、ゴールドスタンダードである。しかし、RSのオンライン評価には、より一般的なオフライン評価と比較して、課題があります[25]。例えば、RSのパーソナライズされた性質は、より多くの、時には希少なリソースが必要であることを意味します。さらに、システムがどのようにレコメンデーションを提示するかは、かなりの影響を与えることが実証されています [35]。したがって、オフライン評価はRSの研究において最も一般的な評価フレームワークです。オフライン評価を用いる場合，評価予測タスクの誤差評価指標で評価するか[22]，トップN推薦のランキング評価指標で評価するか[14]，という第二の決断が必要である．最近，様々な研究により，ユーザ満足度の測定に関して，エラーメトリクスよりもランキングメトリクスの方が優れていることが指摘されている[5, 22, 28]．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final decision when evaluating the top-n recommendation task on offline collaborative filtering data is to choose what to measure. Here, the most common objective is the accuracy of the recommendations. For that, classical ranking metrics are typically used [43]. More recently, the need for aspects beyond pure recommendation precision has been recognized [22]. Highly related properties such as serendipity, novelty, and diversity have been shown to be crucial in determining user engagement with systems [10]. Therefore new metrics for evaluating recommendation list diversity and novelty have been proposed [8]. This dichotomy forces the researcher to choose which metrics to optimize.\n",
    "\n",
    "オフラインの協調フィルタリングデータでトップNレコメンデーションタスクを評価するときの最終決定は、何を測定するかを選択することである。ここで、最も一般的な目的は推薦の正確さである。そのために、古典的なランキングメトリクスが一般的に使用されます[43]。最近になって，純粋なレコメンデーションの精度を超えた側面の必要性が認識されています [22]．また，**セレンディピティ，新規性，多様性**などの関連性の高い特性は，ユーザーのシステムに対する関与を決定する上で重要であることが示されています [10]．そのため，**レコメンデーションリストの多様性と新規性を評価するための新しい指標**が提案されています[8]．この二律背反により，研究者はどの指標を最適化するか選択を迫られる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper, we try to shed some light on this dilemma by advancing a unified evaluation of RSs. Inspired by extensive work carried out on search results diversification [41], we adapt a unified metric for evaluating top-n recommendations. Our metric considers in its formulation topical (aspect) redundancy and both personalized item and topical relevance. For validating this proposal, we extend work on axiomatic analysis of the properties of different metrics [16], and present experiments on collaborative filtering data. The analysis shows that our metric, αβ-nDCG, satisfies the desired axioms and that it also behaves well under all the experimental scenarios.\n",
    "\n",
    "本論文では、RSの統一的な評価を進めることで、このジレンマに光を当てようとするものである。検索結果の多様化に関する広範な研究[41]に触発され、我々はトップNレコメンデーションを評価するための統一的なメトリックを適応する。この評価指標は、トピック（アスペクト）の冗長性と、パーソナライズされた項目とトピックの関連性の両方を考慮した定式化である。この提案を検証するため，我々は異なるメトリックの特性の公理的分析に関する研究[16]を拡張し，協調フィルタリングデータに関する実験を提示する．解析の結果、我々の提案するαβ-nDCGは望ましい公理を満たし、全ての実験シナリオにおいて良好な振る舞いをすることが示された。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we place our work in the context of related work, and provide background. Section 3 presents our proposal for a unified metric. Then, we perform the axiomatic analysis on Section 4 and the experimental validation on Section 5. Finally, we conclude in Section 6.\n",
    "\n",
    "次のセクションでは、我々の研究を関連研究の文脈に位置づけ、その背景を説明する。セクション3では、統一的な測定基準に関する我々の提案を示す。そして、セクション4で公理的な分析を行い、セクション5で実験的な検証を行う。最後に、セクション6で結論を述べる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The need of offline evaluation metrics beyond accuracy has been pointed out repeatedly by the Recommender Systems community [4, 15, 19, 26]. Several metrics exist for evaluating either accuracy or diversity in RSs. On the one hand, Valcarce et al. [43] recently reviewed the most commonly used accuracy metrics for offline evaluation, showing how precision and nDCG behave best. On the other hand, Castells et al. [8] review the importance of novelty and diversity in RSs and the different existing metrics for those properties. However, limited effort has been put into unifying these, with Vargas et al.’s work probably the most relevant. Vargas and Castells [48] present a framework for analysing metrics regarding item choice, discovery, and relevance. Vargas’ thesis [47] adapted some metrics from search result diversification [41] for RSs diversity evaluation.\n",
    "\n",
    "精度を超えたオフライン評価指標の必要性は、レコメンダーシステムコミュニティで繰り返し指摘されてきた[4, 15, 19, 26]。RSの精度や多様性を評価するための指標はいくつか存在する。一方、Valcarceら[43]は最近、オフライン評価のために最も一般的に使用される精度メトリクスをレビューし、精度とnDCGがどのように最もよく動作するかを示しました。一方、Castellsら[8]は、RSにおける新規性と多様性の重要性と、これらの特性に対する様々な既存の指標をレビューしている。しかし、これらを統一するための努力は限られており、Vargasらの仕事がおそらく最も関連していると思われます。VargasとCastells [48]は、項目の選択、発見、関連性に関するメトリクスを分析するためのフレームワークを提示している。Vargasの論文[47]はRSの多様性評価のために検索結果の多様化[41]からいくつかの指標を適応させた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper, we focus on topical or aspect based diversity [51]. This concept refers to recommended items showing diverse aspects, e.g., different types of products or different genres of movies. This scenario matches the classical search results diversification task [41], where documents may present various aspects (nuggets) for a given information need. In that field, authors carried out extensive work in the joint evaluation of relevance and diversity.\n",
    "\n",
    "本論文では、トピックまたはアスペクトに基づく多様性に注目する[51]。このコンセプトは、例えば、異なるタイプの製品や異なるジャンルの映画など、多様な側面を示す推奨項目を指す。このシナリオは、文書が与えられた情報ニーズに対して様々なアスペクト（ナゲット）を提示する古典的な検索結果の多様化タスク[41]に一致する。この分野では、著者らは関連性と多様性の共同評価について広範な研究を行った。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In search settings, Clarke et al. [12] introduced the α-nDCG metric to evaluate aspect diversity. This metric adapts normalized Discounted Cumulative Gain (nDCG) [24], with different aspects (nuggets) also considered. Agrawal et al. [1] proposed a simple intent-aware adaptation of ranking metrics as a weighted average of isolated local aspect evaluation. Following this, Clarke et al. [13] presented NRBP as an extension of RBP for the diversification scenario. NRBP combines the strengths of the intent-aware metrics [1], RBP [31] and α-nDCG [12]. In document ranking, some authors also argue that the appearance of non-relevant documents should penalize the metric value, as for instance with Expected Utility (EU) [49]. Finally, we note work by Amigó et al. [3] on analyzing an extensive set of metrics for the search results diversification task. In particular, they present Rank-Biased Utility (RBU), a metric informed by a set of desired axioms that the authors defined for the search diversification scenario.\n",
    "\n",
    "検索設定において、Clarkeら[12]はアスペクト多様性を評価するために、α-nDCGメトリックを導入した。この指標は正規化割引累積利得（nDCG）[24]を適応したもので、異なるアスペクト（ナゲット）も考慮されている。また，Agrawal ら[1]は，局所的なアスペクト評価の加重平均とし て，ランキングメトリクスの単純な意図考慮型適応を提案した．これに続き，Clarkeら[13]は多様化シナリオのためのRBPの拡張としてNRBPを提示した．NRBPは、意図を考慮した評価指標[1]、RBP[31]、α-nDCG[12]の長所を兼ね備えている。また、文書ランキングでは、例えばExpected Utility (EU) [49]のように、非関連文書の出現がメトリック値にペナルティを与えるべきであると主張する著者もいる。最後に、検索結果の多様化タスクのための広範なメトリックのセットを分析するAmigóらによる研究[3]に注目します。特に、彼らはRank-Biased Utility（RBU）を提示しており、これは著者らが検索結果の多様化シナリオのために定義した一連の望ましい公理によって情報を得たメトリックです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation of our proposed metric, we rely on two different strategies. First, we use axiomatics [16] to analyze the properties of the metric we present. The theoretical characterization of different research pieces has been extensively used in the past, both in document search and RSs. For instance, axioms have been used for the characterization of ranking models [32, 36, 42] or smoothing methods [21, 45]. Moreover, they were also used to characterize user expectations [29] and, therefore, metric behaviour [2, 3, 17]. Second, regarding our experimental part, we will evaluate mainly three different aspects of the metric (1) rank correlation (2) discriminative power and (3) robustness to incompleteness. These have been widely used both in RSs and search evaluation. For instance, Valcarce et al. [44] study the discriminative power and robustness to incompleteness in the RSs scenario, and Sakai and Song [39] demonstrate the importance of analyzing the discriminative power of metrics in search result diversification.\n",
    "\n",
    "提案するメトリックを評価するために，我々は2つの異なる戦略に頼っている．まず、我々が提示するメトリックの特性を分析するために、公理学[16]を用いる。異なる研究課題の理論的特徴付けは，文書検索とRSの両方において過去に広く用いられてきた．例えば、公理はランキングモデル[32, 36, 42]や平滑化手法[21, 45]の特徴付けに用いられてきた。また，ユーザの期待値[29]や，それによるメトリック行動[2, 3, 17]の特徴付けにも利用されている．次に、実験的な部分に関して、我々は主にメトリックの3つの異なる側面（1）順位相関（2）識別力（3）不完全性に対する頑健性）を評価する予定である。これらは、RSや検索評価において広く用いられている。例えば、Valcarce ら[44]は RSs シナリオにおける識別力と不完全性に対する頑健性を研究し、Sakai と Song [39] は検索結果の多様化におけるメトリックの識別力を分析することの重要性を示している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Unified Metric for Diversity and Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The α-nDCG metric was formulated in the Information Retrieval community and further adapted by Vargas [47] for RSs. We now reformulate α-nDCG to consider both topical diversity and accuracy specific to top-n recommendation. Formally, a top-n recommender produces a list of ranked item suggestions $i = (i_1, \\cdots, i_n)$  selected from the whole set of items $L$  not belonging to the user profile $L_u$, , which is the list of items with preferences already expressed by the user $u$.\n",
    "\n",
    "α-nDCGは情報検索コミュニティで定式化され、さらにVargas [47]によってRSに適応された。我々は今、α-nDCGをトップN推薦に特有の話題の多様性と精度の両方を考慮するように再定式化する。形式的には、top-nレコメンダーは、ユーザープロファイル$L_u$に属さないアイテム集合$L$全体から選ばれたランク付けされたアイテム提案リスト$i = (i_1, \\cdots, i_n)$、すなわち、ユーザー$u$によってすでに表明された好みを持つアイテムリストを作成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\alpha$- nDCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\alpha \\beta$-nDCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b22ec1e2b6afadea401017b5d017bed2a24cc1c17b91f841eb85f29e785129cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
