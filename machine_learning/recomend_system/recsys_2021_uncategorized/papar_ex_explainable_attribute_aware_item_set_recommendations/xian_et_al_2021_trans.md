## link 

- https://assets.amazon.science/d3/ad/9af131bd49b8a0697c6bd763a1cf/ex3-explainable-attribute-aware-item-set-recommendations.pdf httpsã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

- https://www.wantedly.com/companies/wantedly/post_articles/350652 httpsã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

## title ã‚¿ã‚¤ãƒˆãƒ«

EX3 : Explainable Attribute-aware Item-set Recommendations
EX3 : èª¬æ˜å¯èƒ½ãªå±æ€§è€ƒæ…®å‹ã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆå‹§å‘Š

## abstract ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆ

Existing recommender systems in the e-commerce domain primarily focus on generating a set of relevant items as recommendations; however, few existing systems utilize underlying item attributes as a key organizing principle in presenting recommendations to users.
é›»å­å•†å–å¼•åˆ†é‡ã«ãŠã‘ã‚‹æ—¢å­˜ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã¯ã€é–¢é€£ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆã‚’ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã—ã¦ç”Ÿæˆã™ã‚‹ã“ã¨ã«ä¸»çœ¼ã‚’ç½®ã„ã¦ã„ã‚‹ã€‚ã—ã‹ã—ã€æ—¢å­˜ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æç¤ºã™ã‚‹éš›ã®é‡è¦ãªçµ„ç¹”åŸç†ã¨ã—ã¦ã€ã‚¢ã‚¤ãƒ†ãƒ ã®åŸºæœ¬å±æ€§ã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹ã‚‚ã®ã¯ã»ã¨ã‚“ã©ãªã„ã€‚
Mining important attributes of items from customer perspectives and presenting them along with item sets as recommendations can provide users more explainability and help them make better purchase decision.
ã—ã‹ã—ã€æ—¢å­˜ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ ã®å±æ€§ã‚’é‡è¦ãªçµ„ç¹”åŸç†ã¨ã—ã¦åˆ©ç”¨ã™ã‚‹ã‚‚ã®ã¯ã»ã¨ã‚“ã©ãªã„ã€‚é¡§å®¢ã®è¦–ç‚¹ã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ ã®é‡è¦ãªå±æ€§ã‚’æŠ½å‡ºã—ã€ã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆã¨ã¨ã‚‚ã«æ¨è–¦ã¨ã—ã¦æç¤ºã™ã‚‹ã“ã¨ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«èª¬æ˜å¯èƒ½æ€§ã‚’æä¾›ã—ã€ã‚ˆã‚Šè‰¯ã„è³¼è²·æ±ºå®šã‚’æ”¯æ´ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
In this work, we generalize the attribute-aware item-set recommendation problem, and develop a new approach to generate sets of items (recommendations) with corresponding important attributes (explanations) that can best justify why the items are recommended to users.
æœ¬ç ”ç©¶ã§ã¯ã€å±æ€§è€ƒæ…®å‹ã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆæ¨è–¦å•é¡Œã‚’ä¸€èˆ¬åŒ–ã—ã€ã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆï¼ˆæ¨è–¦ï¼‰ã¨ã€ãã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ¦ãƒ¼ã‚¶ã«æ¨è–¦ã™ã‚‹ç†ç”±ã‚’æœ€ã‚‚ã‚ˆãèª¬æ˜ã§ãã‚‹é‡è¦ãªå±æ€§ï¼ˆèª¬æ˜ï¼‰ã‚’å¯¾å¿œã¥ã‘ã¦ç”Ÿæˆã™ã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’é–‹ç™ºã™ã‚‹ã€‚
In particular, we propose a system that learns important attributes from historical user behavior to derive item set recommendations, so that an organized view of recommendations and their attribute-driven explanations can help users more easily understand how the recommendations relate to their preferences.
ç‰¹ã«ã€éå»ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ã‹ã‚‰é‡è¦ãªå±æ€§ã‚’å­¦ç¿’ã—ã€ã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆã®æ¨è–¦ã‚’å°ãå‡ºã™ã‚·ã‚¹ãƒ†ãƒ ã‚’ææ¡ˆã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ¨è–¦ã¨ãã®å±æ€§ã«åŸºã¥ã„ãŸèª¬æ˜ã‚’æ•´ç†ã—ã¦è¡¨ç¤ºã™ã‚‹ã“ã¨ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯æ¨è–¦ã¨è‡ªåˆ†ã®å¥½ã¿ã®é–¢é€£æ€§ã‚’ã‚ˆã‚Šå®¹æ˜“ã«ç†è§£ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚
Our approach is geared towards real world scenarios: we expect a solution to be scalable to billions of items, and be able to learn item and attribute relevance automatically from user behavior without human annotations.
ç§ãŸã¡ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯å®Ÿä¸–ç•Œã®ã‚·ãƒŠãƒªã‚ªã‚’å¯¾è±¡ã¨ã—ã¦ãŠã‚Šã€ä½•åå„„ã‚‚ã®ã‚¢ã‚¤ãƒ†ãƒ ã«æ‹¡å¼µå¯èƒ½ã§ã€äººé–“ã®æ³¨é‡ˆãªã—ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ ã‚„å±æ€§ã®é–¢é€£æ€§ã‚’è‡ªå‹•çš„ã«å­¦ç¿’ã§ãã‚‹ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ãŒæ±‚ã‚ã‚‰ã‚Œã¦ã„ã‚‹ã€‚
To this end, we propose a multi-step learning-based framework called Extract-Expect-Explain (EX3 ), which is able to adaptively select recommended items and important attributes for users.
ã“ã®ãŸã‚ã€æˆ‘ã€…ã¯Extract-Expect-Explain (EX3) ã¨å‘¼ã°ã‚Œã‚‹å¤šæ®µéšå­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€ãƒ¦ãƒ¼ã‚¶ãŒæ¨å¥¨ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã‚„é‡è¦ãªå±æ€§ã‚’é©å¿œçš„ã«é¸æŠã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
We experiment on a large-scale real-world benchmark and the results show that our model outperforms state-of-the-art baselines by an 11.35% increase on NDCG with adaptive explainability for item set recommendation.
æˆ‘ã€…ã¯å¤§è¦æ¨¡ãªå®Ÿä¸–ç•Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å®Ÿé¨“ã‚’è¡Œã„ã€ãã®çµæœã€æˆ‘ã€…ã®ãƒ¢ãƒ‡ãƒ«ã¯é …ç›®ã‚»ãƒƒãƒˆæ¨è–¦ã®ãŸã‚ã®é©å¿œçš„èª¬æ˜å¯èƒ½æ€§ã‚’æŒã¤NDCGã«ãŠã„ã¦ã€æœ€æ–°æŠ€è¡“ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’11.35%ä¸Šå›ã‚‹æ€§èƒ½ã‚’æŒã¤ã“ã¨ã‚’ç¤ºã™ã€‚

# Introduction ã¯ã˜ã‚ã«

Recommender systems have been widely deployed in modern e-commerce websites, helping users overcome overwhelming selection issues in large catalogs and contributing large business impact [9, 20, 32].
ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã¯ã€ç¾ä»£ã®é›»å­å•†å–å¼•ã‚µã‚¤ãƒˆã«åºƒãå°å…¥ã•ã‚Œã¦ãŠã‚Šã€å¤§è¦æ¨¡ãªã‚«ã‚¿ãƒ­ã‚°ã«ãŠã‘ã‚‹åœ§å€’çš„ãªé¸æŠã®å•é¡Œã‚’å…‹æœã—ã€å¤§ããªãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã«è²¢çŒ®ã—ã¦ã„ã‚‹ [9, 20, 32]ã€‚
Many existing recommender systems in industry focus on generating a set of relevant items based on a set of pivot
ã“ã®ã‚ˆã†ãªãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã¯ï¼Œãƒ¦ãƒ¼ã‚¶ãŒè†¨å¤§ãªã‚«ã‚¿ãƒ­ã‚°ã‹ã‚‰å•†å“ã‚’é¸æŠã™ã‚‹éš›ã®å•é¡Œã‚’è§£æ±ºã—ï¼Œãƒ“ã‚¸ãƒã‚¹ã«å¤§ããªå½±éŸ¿ã‚’ä¸ãˆã‚‹ï¼

Throughout the paper, we study the explainable attribute-aware item-set recommendation problem by learning an item-to-item-set mapping guided by attribute differences.
æœ¬è«–æ–‡ã§ã¯ã€å±æ€§ã®é•ã„ã«ã‚ˆã£ã¦å°ã‹ã‚Œã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€èª¬æ˜å¯èƒ½ãªå±æ€§ã‚’è€ƒæ…®ã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆæ¨è–¦å•é¡Œã‚’ç ”ç©¶ã™ã‚‹ã€‚
Formally, given a â€œpivotâ€ item, our goal is to generate ğ¾ sets of items (recommendations), each of which is associated with an important attribute (explanation) to justify why the items are recommended to users.
å½¢å¼çš„ã«ã¯ã€ã€Œãƒ”ãƒœãƒƒãƒˆã€ã‚¢ã‚¤ãƒ†ãƒ ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€æˆ‘ã€…ã®ç›®æ¨™ã¯ã€å„ã‚¢ã‚¤ãƒ†ãƒ ãŒãƒ¦ãƒ¼ã‚¶ã«æ¨å¥¨ã•ã‚Œã‚‹ç†ç”±ã‚’æ­£å½“åŒ–ã™ã‚‹ãŸã‚ã®é‡è¦ãªå±æ€§ï¼ˆèª¬æ˜ï¼‰ã¨é–¢é€£ä»˜ã‘ã‚‰ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ï¼ˆæ¨å¥¨ï¼‰ã®áµƒã‚»ãƒƒãƒˆ ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
We aim to not only generate relevant item recommendations, but also provide corresponding explanations based on those important item attributes whose value changes will affect user purchase decision.
æˆ‘ã€…ã¯ã€é©åˆ‡ãªã‚¢ã‚¤ãƒ†ãƒ ã®æ¨è–¦ã‚’ç”Ÿæˆã™ã‚‹ã ã‘ã§ãªãã€å€¤ã®å¤‰åŒ–ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³¼è²·æ„æ€æ±ºå®šã«å½±éŸ¿ã‚’ä¸ãˆã‚‹é‡è¦ãªã‚¢ã‚¤ãƒ†ãƒ ã®å±æ€§ã«åŸºã¥ã„ã¦ã€å¯¾å¿œã™ã‚‹èª¬æ˜ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã‚‹ã€‚
Unlike existing work [3] that focuses primarily on making understandable substitute recommendations, we attempt to help users broaden their consideration set by presenting them with differentiated options by attribute type.
æ—¢å­˜ã®ç ”ç©¶[3]ãŒä¸»ã«ç†è§£ã—ã‚„ã™ã„ä»£æ›¿å“ã®æ¨è–¦ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸã®ã¨ã¯ç•°ãªã‚Šã€æˆ‘ã€…ã¯å±æ€§ã‚¿ã‚¤ãƒ—ã«ã‚ˆã£ã¦å·®åˆ¥åŒ–ã•ã‚ŒãŸé¸æŠè‚¢ã‚’æç¤ºã™ã‚‹ã“ã¨ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¤œè¨ç¯„å›²ã‚’åºƒã’ã‚‹æ‰‹åŠ©ã‘ã‚’ã™ã‚‹ã“ã¨ã‚’è©¦ã¿ã‚‹ã€‚
Additionally, different from generating explanations based on userâ€“item and itemâ€“attribute interactions [3], we propose to infer important attributes directly from usersâ€™ historical behaviors, providing a framework to understand how users reason about recommendations when making decisions.
ã•ã‚‰ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚¤ãƒ†ãƒ ã‚„ã‚¢ã‚¤ãƒ†ãƒ -å±æ€§ã®ç›¸äº’ä½œç”¨ã«åŸºã¥ãèª¬æ˜ã®ç”Ÿæˆ[3]ã¨ã¯ç•°ãªã‚Šã€æˆ‘ã€…ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éå»ã®è¡Œå‹•ã‹ã‚‰ç›´æ¥é‡è¦ãªå±æ€§ã‚’æ¨æ¸¬ã™ã‚‹ã“ã¨ã‚’ææ¡ˆã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ„æ€æ±ºå®šã‚’è¡Œã†éš›ã«ã©ã®ã‚ˆã†ã«æ¨è–¦ã‚’ç†ç”±ã¥ã‘ã‚‹ã‹ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®æ çµ„ã¿ã‚’æä¾›ã™ã‚‹ã€‚
To the best of our knowledge, we are the first to approach the explainable item-set recommendations via behavior-oriented important attribute identification in e-commerce domain.
æˆ‘ã€…ã®çŸ¥ã‚‹é™ã‚Šã€é›»å­å•†å–å¼•é ˜åŸŸã«ãŠã„ã¦ã€è¡Œå‹•æŒ‡å‘ã®é‡è¦å±æ€§æ¨å®šã«ã‚ˆã‚‹èª¬æ˜å¯èƒ½ãªã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆæ¨è–¦ã«ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã—ãŸã®ã¯æˆ‘ã€…ãŒåˆã‚ã¦ã§ã‚ã‚‹ã€‚

The main idea in solving this problem is to first learn important attributes based on usersâ€™ historical behaviors, and then generate corresponding item recommendations.
ã“ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®ä¸»ãªã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã€ã¾ãšãƒ¦ãƒ¼ã‚¶ã®éå»ã®è¡Œå‹•ã‹ã‚‰é‡è¦ãªå±æ€§ã‚’å­¦ç¿’ã—ã€ãã‚Œã«å¯¾å¿œã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ æ¨è–¦ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
Note that learning important attributes can benefit many other applications beyond item-set recommendations alone.
é‡è¦ãªå±æ€§ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆã®æ¨è–¦ã ã‘ã§ãªãã€ä»–ã®å¤šãã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«åˆ©ç›Šã‚’ã‚‚ãŸã‚‰ã™ã“ã¨ã«æ³¨æ„ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
Modeling behavior-oriented attribute importance from usersâ€™ historical actions rather than manual identification is a critical component to conduct explainable recommendations.
èª¬æ˜å¯èƒ½ãªæ¨è–¦ã‚’è¡Œã†ãŸã‚ã«ã¯ã€æ‰‹å‹•ã§è­˜åˆ¥ã™ã‚‹ã®ã§ã¯ãªãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éå»ã®è¡Œå‹•ã‹ã‚‰è¡Œå‹•æŒ‡å‘ã®å±æ€§é‡è¦åº¦ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ã“ã¨ãŒé‡è¦ãªè¦ç´ ã§ã‚ã‚‹ã€‚
It saves time-consuming effort in manual labeling and provides a more robust way to model user preference.
ã“ã‚Œã¯ã€æ‰‹å‹•ã§ã®ãƒ©ãƒ™ãƒ«ä»˜ã‘ã«ã‹ã‹ã‚‹æ™‚é–“ã‚’ç¯€ç´„ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å—œå¥½ã‚’ã‚ˆã‚Šå¼·å›ºã«ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹æ–¹æ³•ã§ã‚ã‚‹ã€‚
Once important attributes are derived, we can utilize them to build user profiles, e.g., identifying usersâ€™ preferred size, color, flavor, etc, which can be used in generating personalized recommendations.
é‡è¦ãªå±æ€§ãŒå°ãå‡ºã•ã‚Œã‚‹ã¨ã€ãã‚Œã‚’åˆ©ç”¨ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ä¾‹ãˆã°ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã®ã‚µã‚¤ã‚ºã€è‰²ã€å‘³ãªã©ã‚’ç‰¹å®šã—ã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®ç”Ÿæˆã«åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã‚ã‚‹ã€‚
We can also perform brief item summarization based on important attributes, and the proposed method can also be easily extended to involve more contextual information (e.g., usersâ€™ sequential actions) to provide customized item summarization [40].
ã¾ãŸã€é‡è¦ãªå±æ€§ã«åŸºã¥ã„ã¦ç°¡å˜ãªã‚¢ã‚¤ãƒ†ãƒ ã®è¦ç´„ã‚’è¡Œã†ã“ã¨ãŒã§ãã‚‹ã€‚ææ¡ˆæ‰‹æ³•ã¯ã€ã‚ˆã‚Šå¤šãã®æ–‡è„ˆæƒ…å ±ï¼ˆä¾‹ãˆã°ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é€£ç¶šã—ãŸè¡Œå‹•ï¼‰ã‚’å«ã‚€ã‚ˆã†ã«ç°¡å˜ã«æ‹¡å¼µã™ã‚‹ã“ã¨ãŒã§ãã€ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã®è¦ç´„ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒã§ãã‚‹[40]ã€‚
We can further leverage the behavior-driven important attributes to advance query rewriting techniques in the item search domain, by attending to those terms that are closely related to itemsâ€™ important attributes.
ã•ã‚‰ã«ã€è¡Œå‹•é§†å‹•å‹ã®é‡è¦å±æ€§ã‚’æ´»ç”¨ã—ã€ã‚¢ã‚¤ãƒ†ãƒ ã®é‡è¦å±æ€§ã¨å¯†æ¥ã«é–¢é€£ã™ã‚‹ç”¨èªã«æ³¨ç›®ã™ã‚‹ã“ã¨ã§ã€ã‚¢ã‚¤ãƒ†ãƒ æ¤œç´¢é ˜åŸŸã«ãŠã‘ã‚‹ã‚¯ã‚¨ãƒªæ›¸ãæ›ãˆæŠ€è¡“ã‚’ç™ºå±•ã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

To this end, we propose a multi-step framework called Extract-Expect-Explain (EX3 ) to approach the explainable item-set recommendation problem.
ãã“ã§ã€èª¬æ˜å¯èƒ½ãªé …ç›®é›†åˆæ¨è–¦å•é¡Œã«ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã™ã‚‹ãŸã‚ã«ã€Extract-Expect-Explain (EX3) ã¨å‘¼ã°ã‚Œã‚‹å¤šæ®µéšã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã™ã‚‹ã€‚
Our EX3 framework takes as input a pivot
ã“ã®EX3ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã¯ã€å…¥åŠ›ã¨ã—ã¦ãƒ”ãƒœãƒƒãƒˆ

To guarantee the robustness and scalability in real world environment, EX3 is carefully designed to overcome several inherent challenges.
EX3ã¯ã€å®Ÿç’°å¢ƒã§ã®å …ç‰¢æ€§ã¨æ‹¡å¼µæ€§ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã«ã€ã„ãã¤ã‹ã®èª²é¡Œã‚’å…‹æœã™ã‚‹ã‚ˆã†ã«æ…é‡ã«è¨­è¨ˆã•ã‚Œã¦ã„ã‚‹ã€‚
(1) The foremost challenge is how to dynamically recommend items and attributes that provide comprehensive information contributed to usersâ€™ purchase decision.
(1) æœ€ã‚‚é‡è¦ãªèª²é¡Œã¯ã€ãƒ¦ãƒ¼ã‚¶ã®è³¼è²·æ„æ€æ±ºå®šã«è²¢çŒ®ã™ã‚‹åŒ…æ‹¬çš„ãªæƒ…å ±ã‚’æä¾›ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã‚„å±æ€§ã‚’ã©ã®ã‚ˆã†ã«å‹•çš„ã«æ¨è–¦ã™ã‚‹ã‹ã§ã‚ã‚‹ã€‚
In this work, we propose to train EX3 with user behavior signals in the distant supervision manner, and leverage attribute value difference and historical purchase signals to capture user-behavior driven important attributes.
æœ¬ç ”ç©¶ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ã®è¡Œå‹•ä¿¡å·ã‚’é éš”ç›£è¦–æ–¹å¼ã§å­¦ç¿’ã•ã›ã€å±æ€§å€¤ã®å·®ã¨éå»ã®è³¼è²·ä¿¡å·ã‚’åˆ©ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ä¸»å°ã®é‡è¦ãªå±æ€§ã‚’æ•æ‰ã™ã‚‹ã“ã¨ã‚’ææ¡ˆã™ã‚‹ã€‚
We believe that the important attributes are those whose value changes will critically affect usersâ€™ purchase decision, e.g., size for shoes, roast type for coffee.
é‡è¦ãªå±æ€§ã¨ã¯ã€ä¾‹ãˆã°ã€é´ã®ã‚µã‚¤ã‚ºã‚„ã‚³ãƒ¼ãƒ’ãƒ¼ã®ç„™ç…åº¦ãªã©ã€ãã®å€¤ã®å¤‰åŒ–ãŒãƒ¦ãƒ¼ã‚¶ã®è³¼è²·æ„æ€æ±ºå®šã«æ±ºå®šçš„ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‚‚ã®ã§ã‚ã‚‹ã¨æˆ‘ã€…ã¯è€ƒãˆã¦ã„ã‚‹ã€‚
(2) In real-world environment, we are always facing data challenges, especially on the attribute missing
(2)å®Ÿä¸–ç•Œã§ã¯å¸¸ã«ãƒ‡ãƒ¼ã‚¿ã®å•é¡Œã«ç›´é¢ã—ã€ç‰¹ã«å±æ€§ã®æ¬ è½ãŒå•é¡Œã¨ãªã£ã¦ã„ã‚‹ã€‚

- We highlight the importance of jointly considering important attributes and relevant items in achieving the optimal user experience in explainable recommendations. èª¬æ˜å¯èƒ½ãªãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ãŠã„ã¦æœ€é©ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«ã€é‡è¦ãªå±æ€§ã¨é–¢é€£ã™ã‚‹é …ç›®ã‚’å…±åŒã§è€ƒæ…®ã™ã‚‹ã“ã¨ã®é‡è¦æ€§ã‚’å¼·èª¿ã™ã‚‹ã€‚

- We propose a novel three-step framework, EX3 , to approach the explainable attribute-aware item-set recommendation problem along with couples of novel components. The whole framework is carefully designed towards large-scale real-world scenario. æˆ‘ã€…ã¯ã€èª¬æ˜å¯èƒ½ãªå±æ€§è€ƒæ…®å‹ã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆæ¨è–¦å•é¡Œã«ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã™ã‚‹ãŸã‚ã®æ–°ã—ã„3ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€EX3ã‚’ã€æ–°ã—ã„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚«ãƒƒãƒ—ãƒªãƒ³ã‚°ã¨ã¨ã‚‚ã«ææ¡ˆã™ã‚‹ã€‚ ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€å¤§è¦æ¨¡ãªå®Ÿä¸–ç•Œã®ã‚·ãƒŠãƒªã‚ªã‚’æƒ³å®šã—ã¦æ…é‡ã«è¨­è¨ˆã•ã‚Œã¦ã„ã‚‹ã€‚

- We extensively conduct experiments on the real-world benchmark for item-set recommendations. The results show that EX3 achieves 11.35% better NDCG than state-of-the-art baselines, as well as better explainability in terms of important attribute ranking. æˆ‘ã€…ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆæ¨è–¦ã®å®Ÿä¸–ç•Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹å®Ÿé¨“ã‚’åºƒç¯„ã«è¡Œã£ãŸã€‚ ãã®çµæœã€EX3ã¯æœ€æ–°ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒã—ã¦11.35%ã®NDCGã®å‘ä¸Šã‚’é”æˆã—ã€é‡è¦å±æ€§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®èª¬æ˜å¯èƒ½æ€§ã‚‚å‘ä¸Šã™ã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸã€‚

# Preliminary äºˆå‚™

In this section, we start with the introduction of relevant concepts and formulation of the explainable attribute-aware item-set recommendation problem.
æœ¬ç¯€ã§ã¯ã€ã¾ãšã€èª¬æ˜å¯èƒ½ãªå±æ€§è€ƒæ…®å‹é …ç›®é›†åˆæ¨è–¦å•é¡Œã®æ¦‚å¿µã¨å®šå¼åŒ–ã«ã¤ã„ã¦è¿°ã¹ã‚‹ã€‚
Then, we introduce how to approach this problem via distant supervision.
ãã—ã¦ã€é éš”ç›£è¦–ã«ã‚ˆã‚‹ã“ã®å•é¡Œã¸ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ–¹æ³•ã‚’ç´¹ä»‹ã™ã‚‹ã€‚

## Problem Formulation. Problem Formulation (å•é¡Œã®å®šå¼åŒ–)

Let P be the universal set of items and A be the set of all available attributes. We define the attribute value to be a function ğ‘£ : P Ã— A â†¦â†’ Cğ‘‘ğ‘£ that maps an item and an attribute to a sequence of characters, where C denotes a set of predefined characters and ğ‘‘ğ‘£ is the maximum length of the sequence.1 An item ğ‘ âˆˆ P is said to have value ğ‘£(ğ‘, ğ‘) on attribute ğ‘ âˆˆ A if ğ‘£(ğ‘, ğ‘) â‰  âˆ…. Accordingly, the attribute-value pairs of an item ğ‘ on multiple attributes are defined as ğ´ğ‘ = {(ğ‘1, ğ‘£1), . . . , (ğ‘
ğ´ğ‘

Definition 1 (Problem Definition).
å®šç¾© 1 (å•é¡Œã®å®šç¾©).
Given the pivot item ğ‘ âˆˆ P with attribute-value pairs ğ´ğ‘, and the number of groups, ğ¾, the goal is to output ğ¾ ordered explainable groups ğºğ‘(1) , . . .
å±æ€§å€¤ã®çµ„â†ªL_1D45â†©ã‚’æŒã¤ãƒ”ãƒœãƒƒãƒˆãƒ»ã‚¢ã‚¤ãƒ†ãƒ ğ‘âˆˆPã€ãŠã‚ˆã³ã‚°ãƒ«ãƒ¼ãƒ—ã®æ•°áµƒãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€ç›®æ¨™ã¯ã€ğ‘ğ¾ã®é †åºä»˜ãèª¬æ˜å¯èƒ½ã‚°ãƒ«ãƒ¼ãƒ—áµƒï¼ˆ1ï¼‰ , ... ... ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
,ğºğ‘(ğ¾) such that the user utility (e.g., purchase) of displaying ğ¾ such groups is maximized.
ğ¾ã‚’è¡¨ç¤ºã™ã‚‹ã“ã¨ã«ã‚ˆã‚‹ãƒ¦ãƒ¼ã‚¶ã®åŠ¹ç”¨ï¼ˆä¾‹ãˆã°ã€è³¼å…¥ï¼‰ãŒæœ€å¤§ã«ãªã‚‹ã‚ˆã†ãªáµ„(áµƒ)ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚

Intuitively, the goal of the problem is to recommend ğ¾ groups of items with attributes such that the likelihood of these recommended items being clicked or purchased is maximized after users compare them with the pivot item and view the displayed attribute-based justifications.
ç›´æ„Ÿçš„ã«ã¯ã€å•é¡Œã®ç›®çš„ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒ”ãƒœãƒƒãƒˆé …ç›®ã¨æ¯”è¼ƒã—ã€è¡¨ç¤ºã•ã‚ŒãŸå±æ€§ã«åŸºã¥ãæ­£å½“æ€§ã‚’è¦‹ãŸå¾Œã€ã“ã‚Œã‚‰ã®æ¨å¥¨é …ç›®ãŒã‚¯ãƒªãƒƒã‚¯ã¾ãŸã¯è³¼å…¥ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒæœ€å¤§ã¨ãªã‚‹ã‚ˆã†ãªå±æ€§ã‚’æŒã¤é …ç›®ç¾¤áµƒã‚’æ¨å¥¨ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
In other words, it is required to generate important attributes given different pivot and candidate items so that they are useful to users, e.g., â€œscreen resolutionâ€ is relatively more important than â€œheightâ€ for a TV item.
ã¤ã¾ã‚Šã€ãƒ†ãƒ¬ãƒ“ã§ã‚ã‚Œã°ã€Œé«˜ã•ã€ã‚ˆã‚Šã‚‚ã€Œç”»é¢è§£åƒåº¦ã€ã®æ–¹ãŒç›¸å¯¾çš„ã«é‡è¦ã§ã‚ã‚‹ãªã©ã€ç•°ãªã‚‹ãƒ”ãƒœãƒƒãƒˆé …ç›®ã¨å€™è£œé …ç›®ã‚’ä¸ãˆã¦ã€ãƒ¦ãƒ¼ã‚¶ã«ã¨ã£ã¦æœ‰ç”¨ãªé‡è¦å±æ€§ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹ã€‚
Note that the explainable item set recommendation can be considered to be a item-to-item-set recommendation problem in e-commerce shopping scenario, and we assume user context information is not available in this work.
ãªãŠã€èª¬æ˜å¯èƒ½ãªã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆæ¨è–¦ã¨ã¯ã€é›»å­å•†å–å¼•ã«ãŠã‘ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆã¸ã®æ¨è–¦å•é¡Œã¨è€ƒãˆã‚‹ã“ã¨ãŒã§ãã€æœ¬ç ”ç©¶ã§ã¯ãƒ¦ãƒ¼ã‚¶ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ãŒåˆ©ç”¨ã§ããªã„ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã‚‹ã€‚
The challenges of this problem are threefold.
ã“ã®å•é¡Œã®èª²é¡Œã¯3ã¤ã‚ã‚‹ã€‚

- How to automatically identify important attributes without supervision and aggregate relevant items into the corresponding groups for recommendation? ã©ã®ã‚ˆã†ã«ã—ã¦ã€ç›£è¦–ãªã—ã«è‡ªå‹•çš„ã«é‡è¦ãªå±æ€§ã‚’ç‰¹å®šã—ã€é–¢é€£ã™ã‚‹é …ç›®ã‚’å¯¾å¿œã™ã‚‹ã‚°ãƒ«ãƒ¼ãƒ—ã«é›†ç´„ã—ã¦æ¨è–¦ã™ã‚‹ã®ã‹ï¼Ÿ

- How to make the model robust to the data issues including missing attributes and noisy and arbitrary values? æ¬ æå±æ€§ã‚„ãƒã‚¤ã‚ºã®å¤šã„æ£æ„çš„ãªå€¤ãªã©ã®ãƒ‡ãƒ¼ã‚¿å•é¡Œã«å¯¾ã—ã¦ã€ã©ã®ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒã‚¹ãƒˆåŒ–ã™ã‚‹ã‹ï¼Ÿ

- How to effectively reduce the search space of seeking similar items for item set recommendation and make the model scalable to large real-world dataset? é …ç›®ã‚»ãƒƒãƒˆæ¨è–¦ã®ãŸã‚ã®é¡ä¼¼é …ç›®æ¢ç´¢ç©ºé–“ã‚’åŠ¹ç‡çš„ã«ç¸®å°ã—ã€å®Ÿä¸–ç•Œã®å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿç¾ã™ã‚‹ã«ã¯ã©ã†ã™ã‚Œã°ã‚ˆã„ã‹ï¼Ÿ

## Distant Supervision. Distant Supervision (é éš”ç›£ç£)ã€‚

In order to capture the comparable relationship among various items, we consider three common user behavior signals to construct datasets to provide distant supervision [11, 20, 30]: co-purchase (Bcp), co-view (Bcv) and purchase-after-view (Bpv) between items, where Bcp, Bcv, Bpv âŠ† P Ã— P denote how items are co-purchased, co-viewed and view-then-purchased together.
ã“ã“ã§ã€Bcp, Bcv, Bpv âŠ† P Ã— P ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ ãŒã©ã®ã‚ˆã†ã«å…±åŒè³¼å…¥ã€å…±åŒè¦–è´ã€è¦–è´å¾Œè³¼å…¥ã•ã‚ŒãŸã‹ã‚’è¡¨ã™ã€‚
From the above definition, one can notice that Bpv offers an opportunity to simulate usersâ€™ shopping behaviors.
ä¸Šè¨˜ã®å®šç¾©ã‹ã‚‰ã€Bpvã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è²·ã„ç‰©è¡Œå‹•ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹æ©Ÿä¼šã‚’æä¾›ã™ã‚‹ã“ã¨ã«æ°—ã¥ãã“ã¨ãŒã§ãã‚‹ã€‚
When users view an item and then purchase another one in a short period of time (e.g., within the same session), it is reasonable to assume that users are making comparison between relevant items.
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã‚’é–²è¦§ã—ãŸå¾Œã€çŸ­æ™‚é–“ã§ï¼ˆä¾‹ãˆã°ã€åŒã˜ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã§ï¼‰åˆ¥ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’è³¼å…¥ã™ã‚‹å ´åˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯é–¢é€£ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ é–“ã®æ¯”è¼ƒã‚’è¡Œã£ã¦ã„ã‚‹ã¨è€ƒãˆã‚‹ã®ãŒå¦¥å½“ã§ã—ã‚‡ã†ã€‚
Through empirical analysis on Amazon Mechanical Turk (MTurk), we observe that item pairs within Bpv have more than 80% similarities, which verifies our assumption that users are comparing similar items before purchase.
Amazon Mechanical Turk (MTurk) ã§ã®å®Ÿè¨¼åˆ†æã‚’é€šã˜ã¦ã€Bpvå†…ã®ã‚¢ã‚¤ãƒ†ãƒ ãƒšã‚¢ã¯80%ä»¥ä¸Šã®é¡ä¼¼æ€§ã‚’æŒã¤ã“ã¨ãŒè¦³å¯Ÿã•ã‚Œã€ã“ã‚Œã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè³¼å…¥å‰ã«é¡ä¼¼ã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’æ¯”è¼ƒã—ã¦ã„ã‚‹ã¨ã„ã†æˆ‘ã€…ã®ä»®å®šã‚’æ¤œè¨¼ã—ã¦ã„ã¾ã™ã€‚
In order to further improve the relevance from raw behavior signals to build up distant supervision with high quality, by further combing Bcp and Bcv, we conduct several annotation experiments via MTurk and observe that B = Bcv âˆ© Bpv âˆ’ Bcp, which contains items pairs in both Bcv and Bpv but not in Bcp, gives us the best relevance signals and mimics usersâ€™ shopping actions on Bpv.
Bcpã¨Bcvã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€é«˜å“è³ªã®é éš”ç›£è¦–ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã«ã€ç”Ÿã®è¡Œå‹•ä¿¡å·ã‹ã‚‰ã®é–¢é€£æ€§ã‚’ã•ã‚‰ã«æ”¹å–„ã™ã‚‹ãŸã‚ã«ã€æˆ‘ã€…ã¯MTurkã‚’ä»‹ã—ã¦ã„ãã¤ã‹ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“ã‚’è¡Œã„ã€B = Bcvâˆ©Bpv - Bcpã¯ã€Bcvã¨Bpvä¸¡æ–¹ã«ã‚ã‚‹ãŒBcpã«ã¯ãªã„ã‚¢ã‚¤ãƒ†ãƒ ãƒšã‚¢ã‚’å«ã‚€ã€æœ€é«˜ã®é–¢é€£ä¿¡å·ã¨Bpvä¸Šã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’çœŸä¼¼ã—ã¦ä¸ãˆã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
Throughout the paper, we will use this way to construct datasets for model learning and offline evaluation on multiple item categories.
æœ¬è«–æ–‡ã§ã¯ã€ã“ã®æ–¹æ³•ã‚’ç”¨ã„ã¦ã€è¤‡æ•°ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚«ãƒ†ã‚´ãƒªã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

# Proposed Method ææ¡ˆã•ã‚ŒãŸæ–¹æ³•

In this section, we first formulate an optimization-based method for the explainable attribute-aware item-set recommendation problem and pose several potential issues of this solution in industrial scenario.
æœ¬ç¯€ã§ã¯ã€ã¾ãšã€èª¬æ˜å¯èƒ½ãªå±æ€§ã‚’è€ƒæ…®ã—ãŸé …ç›®é›†åˆæ¨è–¦å•é¡Œã«å¯¾ã™ã‚‹æœ€é©åŒ–ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã‚’å®šå¼åŒ–ã—ã€ç”£æ¥­ã‚·ãƒŠãƒªã‚ªã«ãŠã‘ã‚‹ã“ã®ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã®æ½œåœ¨çš„ãªå•é¡Œç‚¹ã‚’ã„ãã¤ã‹æŒ‡æ‘˜ã™ã‚‹ã€‚
Then, we propose a novel learning-based framework called Extract-Expect-Explain (EX3 ) as a feasible and scalable alternative.
æ¬¡ã«ã€æ‹¡å¼µå¯èƒ½ã§å®Ÿç¾å¯èƒ½ãªä»£æ›¿æ¡ˆã¨ã—ã¦ã€Extract-Expect-Explain (EX3) ã¨å‘¼ã°ã‚Œã‚‹æ–°ã—ã„å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®æ çµ„ã¿ã‚’ææ¡ˆã™ã‚‹ã€‚

An Optimization-based Method.
æœ€é©åŒ–ã«åŸºã¥ãæ–¹æ³•ã€‚

Suppose we have a utility function ğ‘¢(ğ‘, ğ‘, ğ‘) that estimates how likely users will click (or purchase) a recommended item ğ‘ âˆˆ P after comparing it with the pivot item ğ‘ âˆˆ P on attribute ğ‘ âˆˆ A, i.e., ğ‘¢ : P Ã— P Ã— A â†¦â†’ [0, 1].
å±æ€§ ğ‘ ï¹‘ ã§ã‚ã‚‹æ¨å¥¨ã‚¢ã‚¤ãƒ†ãƒ  áµ„ ã‚’ãƒ”ãƒœãƒƒãƒˆã‚¢ã‚¤ãƒ†ãƒ  ğ‘ ã¨æ¯”è¼ƒã—ãŸå¾Œã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã©ã®ç¨‹åº¦ã‚¯ãƒªãƒƒã‚¯ï¼ˆã¾ãŸã¯è³¼å…¥ï¼‰ã™ã‚‹ã‹ã‚’æ¨å®šã™ã‚‹åŠ¹ç”¨é–¢æ•° ğ‘¢ï¼ˆğ‘ï¼Œâ†ªLl_21D45E ï¼‰ã€ã™ãªã‚ã¡áµ„ : P Ã— P Ã— Aâ‡”â†’ [0ï¼Œ1] ãŒã‚ã‚‹ã¨ã™ã‚‹ã€‚
We can formulate an optimization problem for explainable item set recommendation as follows.
èª¬æ˜å¯èƒ½ãªé …ç›®é›†åˆæ¨è–¦ã®æœ€é©åŒ–å•é¡Œã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«å®šå¼åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
Given a pivot item ğ‘, ğ‘š candidate items {ğ‘1, . . .
ãƒ”ãƒœãƒƒãƒˆé …ç›®â†ªLl45â†©ã€å€™è£œé …ç›®á‘ã€ ....
, ğ‘ğ‘š}
, á‘á‘š}ãŒä¸ãˆã‚‰ã‚Œã‚‹ã€‚
âŠ† P and ğ‘› attributes {ğ‘1, . . .
âŠ† P ã¨ ğ‘› å±æ€§ {áµ„, ... .
, ğ‘ğ‘›}
, ğ‘ğ‘›}ã§ã‚ã‚‹ã€‚
âŠ† A, we aim to find an assignment ğ‘‹ âˆˆ {0, 1} ğ‘šÃ—ğ‘› that maximizes the overall utilities subject to some constraints:
âŠ† Aã€ã„ãã¤ã‹ã®åˆ¶ç´„æ¡ä»¶ä¸‹ã§å…¨ä½“ã®åŠ¹ç”¨ã‚’æœ€å¤§åŒ–ã™ã‚‹å‰²ã‚Šå½“ã¦ğ‘‹ âˆˆ {0, 1} ğ‘šÃ—ğ‘› ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã™ã‚‹ã€‚

$$
\tag{1}
$$

where ğ‘‹ğ‘–ğ‘— = 1 means the item ğ‘ğ‘– is assigned to the explainable group ğºğ‘ğ‘— with attribute ğ‘ğ‘— , and otherwise ğ‘‹ğ‘–ğ‘— = 0.
ã“ã“ã§ã€ğ‘‹ğ‘–ğ‘— = 1ã¯ã‚¢ã‚¤ãƒ†ãƒ ğ‘ãŒå±æ€§ğºã§èª¬æ˜å¯èƒ½ãªã‚°ãƒ«ãƒ¼ãƒ—ğ‘—ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã‚‹ã“ã¨ã‚’æ„å‘³ã—ã€ãã‚Œä»¥å¤–ã¯ğ‘‹_1D44E = 0ã§ã‚ã‚‹ã€‚
The group capacity constraint restricts the max number of items assigned in each group with an upperbound ğ·grp âˆˆ N, while the item diversity constraint limits the occurrence of each item in overall recommendations with upperbound ğ·div âˆˆ N. The problem defined in Eq. 1 can be deemed as the weighted bipartite b-matching problem [22], which can be solved by modern LP solvers.
ã‚°ãƒ«ãƒ¼ãƒ—å®¹é‡åˆ¶ç´„ã¯ã€å„ã‚°ãƒ«ãƒ¼ãƒ—ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®æœ€å¤§æ•°ã‚’ä¸Šé™â†ªL_1D437â†©grpâˆˆNã§åˆ¶é™ã—ã€ã‚¢ã‚¤ãƒ†ãƒ å¤šæ§˜æ€§åˆ¶ç´„ã¯ã€å…¨ä½“ã®æ¨è–¦ã«ãŠã‘ã‚‹å„ã‚¢ã‚¤ãƒ†ãƒ ã®å‡ºç¾ã‚’ä¸Šé™â†ªL_1D437â†©divâˆˆNã§åˆ¶é™ã™ã‚‹ã€‚å¼1ã§å®šç¾©ã—ãŸå•é¡Œã¯ã€é‡ã¿ä»˜ãäºŒéƒ¨å¼bãƒãƒƒãƒãƒ³ã‚°å•é¡Œ [22] ã¨ã¿ãªã™ã“ã¨ãŒã§ãã€æœ€æ–°ã®LPã‚½ãƒ«ãƒã«ã‚ˆã£ã¦è§£ãã“ã¨ãŒã§ãã‚‹ã€‚
Once the ğ‘› sets of item assignments are derived from ğ‘‹, we can easily select top-ğ¾ groups with any heuristic method based on group-level utility, e.g., the average of all item-attribute utilities in the group.
ğ‘‹ã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ å‰²ã‚Šå½“ã¦ã®ğ‘›ã‚»ãƒƒãƒˆãŒå°å‡ºã•ã‚Œã‚‹ã¨ã€ã‚°ãƒ«ãƒ¼ãƒ—ãƒ¬ãƒ™ãƒ«ã®åŠ¹ç”¨ã€ä¾‹ãˆã°ã€ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®å…¨ã¦ã®ã‚¢ã‚¤ãƒ†ãƒ -å±æ€§ã®åŠ¹ç”¨ã®å¹³å‡å€¤ã«åŸºã¥ã„ã¦ã€ä»»æ„ã®ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯æ‰‹æ³•ã§ãƒˆãƒƒãƒ—-1D43ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å®¹æ˜“ã«é¸æŠã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

However, there are two major issues with this method.
ã—ã‹ã—ã€ã“ã®æ–¹æ³•ã«ã¯2ã¤ã®å¤§ããªå•é¡ŒãŒã‚ã‚‹ã€‚
First, the optimization in Eq. 1 cannot be efficiently solved when ğ‘š is very large and let alone take all items in P as input.
ç¬¬ä¸€ã«ã€å¼ 1 ã®æœ€é©åŒ–ã¯ã€â†ªLl_145A ãŒéå¸¸ã«å¤§ããã€ã¾ã—ã¦ã‚„ P ã®å…¨é …ç›®ã‚’å…¥åŠ›ã¨ã™ã‚‹å ´åˆã«ã¯ã€åŠ¹ç‡çš„ã«è§£ãã“ã¨ãŒã§ããªã„ã€‚
Second, the utility ğ‘¢(ğ‘, ğ‘, ğ‘) is not directly available from distant user behavior signal (e.g. view-then-purchase) because users will not explicitly express which attributes are important to them to compare the items.
ç¬¬äºŒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ¯”è¼ƒã™ã‚‹ãŸã‚ã«ã©ã®å±æ€§ãŒé‡è¦ã‹ã‚’æ˜ç¤ºçš„ã«è¡¨ç¾ã—ãªã„ãŸã‚ã€åŠ¹ç”¨ğ‘¢(áµ…, ğ‘)ã¯é ãã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ä¿¡å·ï¼ˆä¾‹ï¼šé–²è¦§-è³¼å…¥ï¼‰ã‹ã‚‰ç›´æ¥åˆ©ç”¨ã™ã‚‹ã“ã¨ã¯ã§ããªã„ã€‚
Meanwhile, attribute frequency is also not a good indicator for ğ‘¢(ğ‘, ğ‘, ğ‘) due to the common data issue of large amount of missing attribute values.
ä¸€æ–¹ã€å±æ€§é »åº¦ã‚‚ã€å±æ€§å€¤ã®æ¬ æãŒå¤šã„ã¨ã„ã†ä¸€èˆ¬çš„ãªãƒ‡ãƒ¼ã‚¿ã®å•é¡Œã‹ã‚‰ã€ğ‘¢(áµ…, áµ„)ã®è‰¯ã„æŒ‡æ¨™ã¨ã¯ãªã‚‰ãªã„ã€‚

To this end, we propose a learning based multi-step framework called Extract-Expect-Explain (EX3 ).
ã“ã®ãŸã‚ã€æˆ‘ã€…ã¯Extract-Expect-Explain (EX3 ) ã¨å‘¼ã°ã‚Œã‚‹å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®å¤šæ®µéšãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã™ã‚‹ã€‚
As illustrated in Fig. 1, the first Extract step aims to reduce the search space of candidate items by learning item embeddings with distant supervision and approximating coarse-grained item similarity.
å›³1ã«ç¤ºã™ã‚ˆã†ã«ã€æœ€åˆã®Extractã‚¹ãƒ†ãƒƒãƒ—ã¯ã€é éš”ç›£è¦–ã«ã‚ˆã‚‹ã‚¢ã‚¤ãƒ†ãƒ åŸ‹ã‚è¾¼ã¿å­¦ç¿’ã¨ç²—è¦–åŒ–ã‚¢ã‚¤ãƒ†ãƒ é¡ä¼¼åº¦ã®è¿‘ä¼¼ã«ã‚ˆã‚Šã€å€™è£œã‚¢ã‚¤ãƒ†ãƒ ã®æ¢ç´¢ç©ºé–“ã‚’ç¸®å°ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã™ã‚‹ã€‚
Next, the Expect step aims to estimate the utility function ğ‘¢(ğ‘, ğ‘, ğ‘) by decomposing it into two parts: fine-grained item relevance and attribute importance.
æ¬¡ã«ã€Expectã‚¹ãƒ†ãƒƒãƒ—ã¯åŠ¹ç”¨é–¢æ•°ğ‘¢(â†ªLl_1D45E, ğ‘)ã‚’ç´°ç²’åº¦ã®ã‚¢ã‚¤ãƒ†ãƒ é–¢é€£åº¦ã¨å±æ€§é‡è¦åº¦ã®2ã¤ã«åˆ†è§£ã—ã¦æ¨å®šã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã‚‹ã€‚
The last Explain step leverages the outputs from two previous steps to solve the optimization problem and derive the ğ¾ explainable groups for item set recommendations.
æœ€å¾Œã®èª¬æ˜ã‚¹ãƒ†ãƒƒãƒ—ã¯ã€å‰ã®2ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å‡ºåŠ›ã‚’æ´»ç”¨ã—ã¦æœ€é©åŒ–å•é¡Œã‚’è§£ãã€ã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆæ¨è–¦ã®ãŸã‚ã® áµƒ èª¬æ˜å¯èƒ½ãªã‚°ãƒ«ãƒ¼ãƒ—ã‚’å°ãå‡ºã™ã‚‚ã®ã§ã‚ã‚‹ã€‚

## Extract-Step æŠ½å‡ºã‚¹ãƒ†ãƒƒãƒ—

In this step, we aim to learn an item encoder ğœ™ : P â†¦â†’ R ğ‘‘ğ‘ that maps each item in P to ğ‘‘ğ‘ -dimensional space such that the items with relationships in B are closer in the latent space.
ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€P ã®å„ã‚¢ã‚¤ãƒ†ãƒ ã‚’ á‘‘ -æ¬¡å…ƒç©ºé–“ã«å†™åƒã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ ğœ™ : P â‡‘â†’ R á‘ ã‚’å­¦ç¿’ã—ã€B ã®é–¢ä¿‚ã‚’æŒã¤ã‚¢ã‚¤ãƒ†ãƒ ãŒæ½œåœ¨ç©ºé–“ã«ãŠã„ã¦ã‚ˆã‚Šè¿‘ããªã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã™ã‚‹ã€‚
The latent item vectors generated by ğœ™ can be subsequently used as pretrained item embeddings in downstream steps and extracting coarse-grained similar candidates with respect to pivot items.
â†ªLl_1D719 ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸæ½œåœ¨çš„ãªé …ç›®ãƒ™ã‚¯ãƒˆãƒ«ã¯ã€ãã®å¾Œã€ä¸‹æµã®ã‚¹ãƒ†ãƒƒãƒ—ã§äº‹å‰å­¦ç¿’ã•ã‚ŒãŸé …ç›®åŸ‹ã‚è¾¼ã¿ã¨ã—ã¦ä½¿ç”¨ã§ãã€ãƒ”ãƒœãƒƒãƒˆé …ç›®ã«é–¢ã—ã¦ç²—è¦–åŒ–ã•ã‚ŒãŸé¡ä¼¼å€™è£œã‚’æŠ½å‡ºã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã‚ã‚‹ã€‚

Specifically, each item ğ‘ âˆˆ P is initialized with either a one-hot vector or a raw feature vector extracted from metadata such as item title and category. Then, it is fed to the item encoder ğœ™, which is modeled as a multilayer perceptron (MLP) with non-linear activation function. In order to capture relatedness among items, we assume that each item is similar to its related items in B and is distinguishable from other unrelated items. As illustrated in Fig. 1(a), let ğ‘ğ‘ = {ğ‘ğ‘–
(ğ‘, ğ‘ğ‘–) âˆˆ B} be the related items for an item ğ‘ âˆˆ P. We define a metric function ğ‘“ (ğ‘, ğ‘ğ‘ ) to measure the distance between the item and its related items:

$$
\tag{2}
$$

where ğœ† is the base distance to distinguish ğ‘ and ğ‘ğ‘ , and â„(Â·) denotes an aggregation function over item set ğ‘ğ‘ , which encodes ğ‘ğ‘ into the same ğ‘‘ğ‘ -dimensional space as ğœ™ (ğ‘).
ã“ã“ã§ã€â†ªLl_1D706 ã¯ â†ªLl_1D45D ã¨åŒºåˆ¥ã™ã‚‹ãŸã‚ã®åŸºåº•è·é›¢ã€ğ‘(-) ã¯ã‚¢ã‚¤ãƒ†ãƒ é›†åˆğ‘ ä¸Šã®é›†ç´„é–¢æ•°ã§ã€ğ‘ã¨åŒã˜áµ -D445D ç©ºé–“ã« áµ ã‚’åŒ…å«ã™ã‚‹ã€‚
In this work, we define â„(Â·) to be a weighted sum over item embeddings via dot-product attention:
æœ¬ç ”ç©¶ã§ã¯ã€â„(-) ã‚’ç‚¹ç©æ³¨ç›®ã«ã‚ˆã‚‹é …ç›®åŸ‹ã‚è¾¼ã¿ã«å¯¾ã™ã‚‹åŠ é‡å’Œã¨å®šç¾©ã™ã‚‹ã€‚

$$
\tag{3}
$$

We assign a positive label ğ‘¦ + = 1 for each pair of (ğ‘, ğ‘ğ‘ ). For non-trivial learning to distinguish item relatedness, for each item ğ‘, we also randomly sample
ğ‘ğ‘

$$
\tag{4}
$$

where ğœ– is the margin distance.
ã“ã“ã§ã€ğœ–ã¯ãƒãƒ¼ã‚¸ãƒ³è·é›¢ã§ã‚ã‚‹ã€‚

Once the item encoder ğœ™ is trained, for each pivot item ğ‘ âˆˆ P, we can retrieve a set of ğ‘š (
ğ‘ğ‘

##  Expect-Step ã‚¨ã‚¯ã‚¹ãƒšã‚¯ãƒˆã‚¹ãƒ†ãƒƒãƒ—

The goal of this step is to learn the utility function ğ‘¢(ğ‘, ğ‘, ğ‘) to estimate how likely a candidate item ğ‘ will be clicked or purchased by users after being compared with pivot item ğ‘ on attribute ğ‘.
ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ç›®çš„ã¯ã€å±æ€§ áµ„ã§ãƒ”ãƒœãƒƒãƒˆãƒ»ã‚¢ã‚¤ãƒ†ãƒ  áµ„ã¨æ¯”è¼ƒã•ã‚ŒãŸå¾Œã€å€™è£œã‚¢ã‚¤ãƒ†ãƒ  áµ†(áµ…, áµ…)ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚¯ãƒªãƒƒã‚¯ã¾ãŸã¯è³¼å…¥ã•ã‚Œã‚‹ç¢ºç‡ã‚’æ¨å®šã™ã‚‹ãŸã‚ã®åŠ¹ç”¨é–¢æ•° ğ‘¢ ã‚’å­¦ã¶ã“ã¨ã§ã‚ã‚‹ã€‚
For simplicity of modeling, we assume that the utility function can be decomposed into two parts:
ãƒ¢ãƒ‡ãƒ«ã‚’ç°¡å˜ã«ã™ã‚‹ãŸã‚ã«ã€åŠ¹ç”¨é–¢æ•°ã¯2ã¤ã®éƒ¨åˆ†ã«åˆ†è§£ã§ãã‚‹ã¨ä»®å®šã™ã‚‹ã€‚

$$
\tag{5}
$$

where ğ‘” : [0, 1] Ã— [0, 1] â†¦â†’ [0, 1] is a binary operation. The first term ğ‘¢rel(ğ‘, ğ‘) reveals the fine-grained item relevance, or equivalently, the likelihood of item ğ‘ being clicked by users after compared with pivot ğ‘ (no matter which attributes are considered). The second term ğ‘¢att(ğ‘
ğ‘, ğ‘) indicates the importance of displaying attribute ğ‘ to users when they compare items ğ‘ and ğ‘. It is natural to learn these two functions if well-curated datasets are available. However, practically, even though the item relevance can be simulated from distant user behavior signals, e.g., Bpv view-then-purchased, the groundtruth of important attributes still remain unknown. This is because users will not explicitly express the usefulness of item attributes when they do online shopping, which leads to the challenge of how to infer the attribute importance without supervision. In addition, the data issue of missing attributes and noisy values is quite common since it costs much time and effort to manually align all the attributes of items. That is to say each item may contain arbitrary number of attributes and their values may contain arbitrary content and data types.

To overcome the issues, we propose a novel neural model named Attribute Differentiating Network (ADN) to jointly approximate ğ‘¢rel and ğ‘¢att.
ã“ã®å•é¡Œã‚’å…‹æœã™ã‚‹ãŸã‚ã«ã€æˆ‘ã€…ã¯ğ‘¢ã¨ğ‘¢ã‚’å…±åŒã§è¿‘ä¼¼ã™ã‚‹å±æ€§å¾®åˆ†ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆADNï¼‰ã¨åä»˜ã‘ãŸæ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã™ã‚‹ã€‚
Formally, it takes as input a pivot itemğ‘ and a candidate item ğ‘ along with the corresponding attribute-value pairs ğ´ğ‘, ğ´ğ‘ (e.g., ğ´ğ‘ = {(ğ‘1, ğ‘£(ğ‘, ğ‘1)), . . .
å½¢å¼çš„ã«ã¯ã€ãƒ”ãƒœãƒƒãƒˆã‚¢ã‚¤ãƒ†ãƒ Å…ã¨å€™è£œã‚¢ã‚¤ãƒ†ãƒ Å…ã‚’ã€å¯¾å¿œã™ã‚‹å±æ€§å€¤ãƒšã‚¢Å…ï¼ˆä¾‹ãˆã°ã€ğ´, á‘ï¼‰ã€ï½›ï¼ˆá‘‘,ğ‘ï¼‰, ....
, (ğ‘ğ‘›, ğ‘£(ğ‘, ğ‘ğ‘›))}), and simultaneously outputs an item relevance score ğ‘ŒË† ğ‘ âˆˆ [0, 1] and attribute importance scores ğ‘¦Ë†ğ‘,ğ‘— âˆˆ [0, 1] for attribute ğ‘ğ‘— (ğ‘— = 1, . . . , ğ‘›).
, (ğ‘, ğ‘)}) ã¨ã€å±æ€§ áµ„(áµ„Ë† ğ‘)ã«å¯¾ã™ã‚‹é …ç›®é–¢é€£æ€§ã‚¹ã‚³ã‚¢ğ‘Œâˆˆ [0, 1] ã¨å±æ€§é‡è¦åº¦ã‚¹ã‚³ã‚¢ğ‘—âˆˆ [0, 1] ã‚’åŒæ™‚å‡ºåŠ›ã™ã‚‹ï¼ˆÅ…ï¼1, ...,ğ‘›ï¼‰ã€‚

### Network Overview. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¦‚è¦

As illustrated in Fig. 2, ADN consists of three components: a value-difference module to capture the difference levels of attribute values of two items, an attention-based attribute scorer to implicitly predict the attribute contribution, and a relevance predictor that estimates the fine-grained relevance of two items.
å›³2ã«ç¤ºã™ã‚ˆã†ã«ã€ADNã¯2ã¤ã®é …ç›®ã®å±æ€§å€¤ã®å·®åˆ†ãƒ¬ãƒ™ãƒ«ã‚’æ‰ãˆã‚‹ä¾¡å€¤å·®åˆ†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€å±æ€§å¯„ä¸åº¦ã‚’æš—é»™çš„ã«äºˆæ¸¬ã™ã‚‹æ³¨ç›®ãƒ™ãƒ¼ã‚¹å±æ€§ã‚¹ã‚³ã‚¢ãƒ©ã€2ã¤ã®é …ç›®ã®ãã‚ç´°ã‹ã„é–¢é€£æ€§ã‚’æ¨å®šã™ã‚‹é–¢é€£æ€§äºˆæ¸¬å™¨ã®3ã¤ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‹ã‚‰æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚
Specifically, two input items are first respectively vectorized by the encoder ğœ™ from the Extract step.
å…·ä½“çš„ã«ã¯ã€ã¾ãš2ã¤ã®å…¥åŠ›é …ç›®ã¯Extractã‚¹ãƒ†ãƒƒãƒ—ã§å¾—ã‚‰ã‚ŒãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ€áµ±ã«ã‚ˆã£ã¦ãã‚Œãã‚Œãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚Œã‚‹ã€‚
The derived item embeddings are then mapped into low-dimensional space via linear transformation, i.e. xğ‘ğ‘ = ğ‘Šğ‘ [ğœ™ (ğ‘);ğœ™ (ğ‘)], where [;] denotes the concatenation and ğ‘Šğ‘ is the learnable parameters.
ã™ãªã‚ã¡ã€xÅ… = â†ªL_145Dâ†© [Å… (â†ªL_1D44Aâ†©);á‘] ã§ã‚ã‚Šã€[;]ã¯é€£çµã‚’ã€â†ªL_145Dâ†©ã¯å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¡¨ã™ã€‚
Then, each attribute-value tuple (ğ‘ğ‘— , ğ‘£(ğ‘, ğ‘ğ‘—), ğ‘£(ğ‘, ğ‘ğ‘—)) is encoded by the value-difference module into a vector denoted by xğ‘£ğ‘— .
æ¬¡ã«ã€å„å±æ€§-å€¤ã‚¿ãƒ—ãƒ«(áµ„ ğ‘—, ğ‘£, ğ‘—)ã¯ã€å€¤å·®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ã‚ˆã£ã¦xğ‘£ğ‘—ã¨è¡¨ã•ã‚Œã‚‹ãƒ™ã‚¯ãƒˆãƒ«ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã€‚
All these vectors xğ‘£1 , . . .
ã“ã‚Œã‚‰ã®ãƒ™ã‚¯ãƒˆãƒ« xğ‘£1 , ... ...
, xğ‘£ğ‘› together with xğ‘ğ‘ will be further fed to the attention-based attribute scorer to produce attribute importance scores ğ‘¦Ë†ğ‘,1, . . .
xğ‘£ã€xğ‘ã¯ã•ã‚‰ã«æ³¨æ„ãƒ™ãƒ¼ã‚¹ã®å±æ€§ã‚¹ã‚³ã‚¢ãƒ©ãƒ¼ã«ä¾›çµ¦ã•ã‚Œã€å±æ€§é‡è¦åº¦ã‚¹ã‚³ã‚¢ğ‘¦Ë†ğ‘, ... ...ãŒç”Ÿæˆã•ã‚Œã‚‹ã€‚
,ğ‘¦Ë†ğ‘,ğ‘› as well as an aggregated vector zğ‘£ about value-difference information on all attributes.
ğ‘¦Ë†áµ…ã¨ã€ã™ã¹ã¦ã®å±æ€§ã®ä¾¡å€¤å·®æƒ…å ±ã«é–¢ã™ã‚‹é›†ç´„ãƒ™ã‚¯ãƒˆãƒ«zğ‘£ãŒç”Ÿæˆã•ã‚Œã‚‹ã€‚
The relevance predictor finally yields ğ‘ŒË† ğ‘ based on the joint of xğ‘ğ‘ and zğ‘£ .
é–¢é€£æ€§äºˆæ¸¬å™¨ã¯æœ€çµ‚çš„ã«ã€xÅ…ã¨zá‘£ã®çµåˆã«åŸºã¥ã„ã¦ã€Å…ã‚’ç”Ÿæˆã™ã‚‹ã€‚

### Value-Difference Module. Value-Difference Moduleã®ç•¥ã€‚

As shown in Fig. 2(b), we represent each attribute ğ‘ğ‘— as a one-hot vector and then embed it into ğ‘‘ğ‘-dimensional space via linear transformation, i.e., ağ‘— = ğ‘Šğ‘ğ‘ğ‘— , with learnable parameters ğ‘Šğ‘.
å›³ 2(b)ã«ç¤ºã™ã‚ˆã†ã«ã€å„å±æ€§áµ„ã‚’ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦è¡¨ç¾ã—ã€ç·šå½¢å¤‰æ›ã€ã™ãªã‚ã¡ã€ağ‘‘ = â†ªL_1D44Eâ†©ã€å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿áµ„ã‚’ç”¨ã„ã¦ğ‘æ¬¡å…ƒç©ºé–“ã¸åŸ‹ã‚è¾¼ã‚€ã€‚
Since the value ğ‘£(ğ‘, ğ‘ğ‘—) of item ğ‘ and attribute ğ‘ğ‘— can be of arbitrary type, inspired by character-level CNN, we treat it as a sequence of characters and each character is embedded into a ğ‘‘ğ‘ -dimensional vector via linear transformation with parameters ğ‘Šğ‘ .
é …ç›® áµ… ã¨å±æ€§ áµ„ ã®å€¤ ğ‘£ ã¯ä»»æ„ã®å‹ãŒå¯èƒ½ãªã®ã§ã€æ–‡å­—ãƒ¬ãƒ™ãƒ«CNNã«ãƒ’ãƒ³ãƒˆã‚’å¾—ã¦ã€ã“ã‚Œã‚’æ–‡å­—åˆ—ã¨ã—ã¦æ‰±ã„ã€å„æ–‡å­—ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ áµ„ ã®ç·šå½¢å¤‰æ›ã«ã‚ˆã‚Šğ‘æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã«åŸ‹ã‚è¾¼ã‚€ã“ã¨ã«ã—ãŸã€‚
Suppose the length of character sequence is at most ğ‘›ğ‘ .
æ–‡å­—åˆ—ã®é•·ã•ãŒæœ€å¤§ã§ã‚‚ğ‘›ğ‘ã§ã‚ã‚‹ã¨ã™ã‚‹ã€‚
We can represent the value ğ‘£(ğ‘, ğ‘ğ‘—) as a matrix vğ‘ ğ‘— âˆˆ R ğ‘›ğ‘Ã—ğ‘‘ğ‘ .
å€¤ ğ‘£(ğ‘, ğ‘—)ã¯è¡Œåˆ— vğ‘âˆˆ R ğ‘›ğ‘Ã—ğ‘‘ã¨ã—ã¦è¡¨ã™ã“ã¨ãŒã§ãã‚‹ï¼
Then, we adopt convolutional layers to encode the character sequence as follows:
ãã—ã¦ã€æ–‡å­—åˆ—ã®ç¬¦å·åŒ–ã«ç•³ã¿è¾¼ã¿å±¤ã‚’æ¡ç”¨ã—ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ã™ã‚‹ã€‚

$$
\tag{6}
$$

where conv(Â·) denotes the 1D convolution layer and maxpool(Â·) is the 1D max pooling layer.
ã“ã“ã§conv(-)ã¯1æ¬¡å…ƒç•³ã¿è¾¼ã¿å±¤ã€maxpool(-)ã¯1æ¬¡å…ƒæœ€å¤§ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚
The output xğ‘–ğ‘— âˆˆ R ğ‘‘ğ‘ is the latent representation of arbitrary value ğ‘£ğ‘–ğ‘— .
å‡ºåŠ› xğ‘–á‘—âˆˆ R á‘‘ ã¯ä»»æ„ã®å€¤á‘£ğ‘–á‘—ã®æ½œåœ¨çš„è¡¨ç¾ã§ã‚ã‚‹ã€‚
To capture value difference on attribute ğ‘ğ‘— between items ğ‘, ğ‘, we further encode the attribute vector ağ‘— and the value vectors xğ‘ğ‘— and xğ‘ ğ‘— via an MLP:
é …ç›® á‘, Å… é–“ã®å±æ€§ â†ªLl_1457 ã®ä¾¡å€¤å·®ã‚’æ‰ãˆã‚‹ãŸã‚ã«ã€ã•ã‚‰ã«å±æ€§ãƒ™ã‚¯ãƒˆãƒ« aâ†ªLl_1457 ã¨ä¾¡å€¤ãƒ™ã‚¯ãƒˆãƒ« xá‘, xÅ… ã‚’MLPã§ç¬¦å·åŒ–ã—ãŸã‚‚ã®ã§ã‚ã‚‹ã€‚

$$
\tag{7}
$$

where xğ‘£ğ‘— is supposed to encode the value-difference information between values ğ‘£(ğ‘, ğ‘ğ‘—) and ğ‘£(ğ‘, ğ‘ğ‘—) on attribute ğ‘ğ‘— .
ã“ã“ã§ã€xá‘£ã¯å±æ€§á‘—ã®å€¤á‘£(áµ…, ğ‘“)ã¨ğ‘á‘—ã®å€¤å·®æƒ…å ±ã‚’ã‚³ãƒ¼ãƒ‰åŒ–ã™ã‚‹ã‚‚ã®ã¨ã•ã‚Œã‚‹ã€‚

### Attention-based Attribute Scorer. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ãƒˆã‚¹ã‚³ã‚¢ãƒ©ãƒ¼ã€‚

Since our goal is to detect important attributes with respect to the pair of items, we further entangle each value-difference vector xğ‘£ğ‘— of attribute ğ‘ğ‘— conditioned on item vector xğ‘ğ‘ as follows:
æˆ‘ã€…ã®ç›®çš„ã¯é …ç›®ã®ãƒšã‚¢ã«é–¢ã—ã¦é‡è¦ãªå±æ€§ã‚’æ¤œå‡ºã™ã‚‹ã“ã¨ãªã®ã§ã€é …ç›®ãƒ™ã‚¯ãƒˆãƒ«xá‘ã‚’æ¡ä»¶ã¨ã™ã‚‹å±æ€§á‘—ã®å„å€¤å·®ãƒ™ã‚¯ãƒˆãƒ«xá‘“ã‚’ã•ã‚‰ã«ä»¥ä¸‹ã®ã‚ˆã†ã«çµ¡ã‚ã‚‹ã€‚

$$
\tag{8}
$$

where another MLPğ‘ is employed to generate the item-conditioned value-difference vector wğ‘ ğ‘— .
ã“ã“ã§ï¼Œåˆ¥ã®MLPáµ…ãŒé …ç›®æ¡ä»¶ä»˜ãä¾¡å€¤å·®ãƒ™ã‚¯ãƒˆãƒ«wğ‘—ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«æ¡ç”¨ã•ã‚Œã‚‹ï¼

Natually, we can use attention mechanism to aggregate ğ‘› item-conditioned attribute vectors wğ‘1, . . .
é€šå¸¸ã€æ³¨ç›®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ç”¨ã„ã¦ã€ğ‘›å€‹ã®é …ç›®æ¡ä»¶ä»˜ãå±æ€§ãƒ™ã‚¯ãƒˆãƒ«wl_1D, ... ...ã‚’é›†ç´„ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
, wğ‘ğ‘› for better representation and automatic detection of important attributes.
, wl_145Dâ†©ğ‘›ã‚’ã‚ˆã‚Šè‰¯ã„è¡¨ç¾ã¨é‡è¦ãªå±æ€§ã®è‡ªå‹•æ¤œå‡ºã®ãŸã‚ã«ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
However, directly applying existing attention mechanism here will encounter several issues.
ã—ã‹ã—ã€ã“ã“ã§æ—¢å­˜ã®æ³¨ç›®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ç›´æ¥é©ç”¨ã™ã‚‹ã¨ã€ã„ãã¤ã‹ã®å•é¡ŒãŒç™ºç”Ÿã™ã‚‹ã€‚
First, the learned attention weights may have bias on frequent attributes.
ã¾ãšã€å­¦ç¿’ã•ã‚ŒãŸæ³¨æ„ã®é‡ã¿ã¯é »åº¦ã®é«˜ã„å±æ€§ã«åã‚ŠãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
That is higher weights may not necessarily indicate attribute importance, but only because they are easily to acquire and hence occur frequently in datasets.
ã¤ã¾ã‚Šã€é«˜ã„é‡ã¿ã¯å¿…ãšã—ã‚‚å±æ€§ã®é‡è¦æ€§ã‚’ç¤ºã—ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªãã€å–å¾—ãŒå®¹æ˜“ã§ã‚ã‚‹ãŸã‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é »ç¹ã«ç¾ã‚Œã‚‹ã¨ã„ã†ã ã‘ã®ç†ç”±ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
Second, attribute cardinality varies from items to items due to the issue of missing attribute values, so model performance is not supposed to only rely on a single attribute, i.e. distributing large weight on one attribute.
ç¬¬äºŒã«ã€å±æ€§å€¤ã®æ¬ è½ã®å•é¡Œã‹ã‚‰ã€å±æ€§ã®ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã¯é …ç›®ã”ã¨ã«ç•°ãªã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯ä¸€ã¤ã®å±æ€§ã«ã®ã¿ä¾å­˜ã™ã‚‹ã€ã¤ã¾ã‚Šã€ä¸€ã¤ã®å±æ€§ã«å¤§ããªé‡ã¿ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã¯æƒ³å®šã•ã‚Œãªã„ã€‚
To this end, we propose the Random-masking Attention Block (RAB) to alleviate the issues.
ãã“ã§ã€ã“ã®å•é¡Œã‚’ç·©å’Œã™ã‚‹ãŸã‚ã«ã€Random-masking Attention Block (RAB)ã‚’ææ¡ˆã™ã‚‹ã€‚
Specifically, given item vector xğ‘ğ‘ and ğ‘› item-conditioned value-difference vectors wğ‘1, . . .
å…·ä½“çš„ã«ã¯ã€é …ç›®ãƒ™ã‚¯ãƒˆãƒ«xá‘ã¨ğ‘›å€‹ã®é …ç›®æ¡ä»¶ä»˜ãå€¤å·®ãƒ™ã‚¯ãƒˆãƒ«wá‘1, ... ...ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€RABã¯é …ç›®æ¡ä»¶ä»˜ãå€¤å·®ãƒ™ã‚¯ãƒˆãƒ«wá‘1, ... ...ã«é‡ã¿ã‚’ä¸ãˆã‚‹ã€‚
, wğ‘ğ‘›, the RAB block is defined as follows.
, wl_1D45ğ‘› ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€RAB ãƒ–ãƒ­ãƒƒã‚¯ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã‚‹ã€‚

$$
\tag{9}
$$

$$
\tag{10}
$$

$$
\tag{11}
$$

where ğœ‚ğ‘— is a random mask that has value ğ›¾ with probability ğ‘“ ğ‘Ÿğ‘’ğ‘ğ‘— (frequency of attribute ğ‘ğ‘— ) in training and value 1 otherwise. It is used to alleviate the influence by imbalanced attribute frequencies. ğœğ‘— is known as the temperature in softmax and is set as (1 + ğ‘“ ğ‘Ÿğ‘’ğ‘ğ‘—) by default, which is used to shrink the attention on the attribute assigned with large weight. The RAB block can be regarded as a variant of the scaled dot-product attention by incorporating randomness of attribute frequencies and item-conditioned information. The attention weights {ğ‘¦Ë†ğ‘,ğ‘— }ğ‘— âˆˆ [ğ‘›] are used to approximate attribute importance ğ‘¢att(ğ‘ğ‘—
ğ‘, ğ‘). The output zğ‘£ encodes the aggregated information contributed by all attributes.

### Relevance Predictor. é–¢é€£æ€§äºˆæ¸¬è£…ç½®ã€‚

We adopt a linear classifier model to predict the relevance of two items based on the item vector as well as encoded attribute-value vector:
é …ç›®ãƒ™ã‚¯ãƒˆãƒ«ã¨ç¬¦å·åŒ–ã•ã‚ŒãŸå±æ€§å€¤ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã‚‚ã¨ã«ã€2ã¤ã®é …ç›®ã®é–¢é€£æ€§ã‚’äºˆæ¸¬ã™ã‚‹ç·šå½¢åˆ†é¡å™¨ãƒ¢ãƒ‡ãƒ«ã‚’æ¡ç”¨ã—ãŸã€‚

$$
\tag{12}
$$

We treat the problem as a binary classification with the objective function defined as follows:
ã“ã®å•é¡Œã‚’ã€ç›®çš„é–¢æ•°ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã—ãŸ2å€¤åˆ†é¡ã¨ã—ã¦æ‰±ã†ã€‚

$$
\tag{13}
$$

Note that pairwise ranking loss can also easily be extended here and the choice of a better ranking loss function is beyond the scope of this paper.
ãªãŠã€ã“ã“ã§ã¯ãƒšã‚¢ãƒ¯ã‚¤ã‚ºãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ­ã‚¹ã‚‚ç°¡å˜ã«æ‹¡å¼µã§ãã€ã‚ˆã‚Šè‰¯ã„ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ­ã‚¹é–¢æ•°ã®é¸æŠã¯æœ¬è«–æ–‡ã®ç¯„å›²å¤–ã§ã‚ã‚‹ã€‚

Once the model is trained, we can obtain the relevance score ğ‘¢rel(ğ‘, ğ‘) â‰ˆ ğ‘ŒË† ğ‘ that implies whether candidate item ğ‘ is relevant to query item ğ‘, and the attribute importance score ğ‘¢att(ğ‘ğ‘—
ğ‘, ğ‘) â‰ˆ ğ‘¦Ë†ğ‘,ğ‘— (ğ‘— = 1, . . . , ğ‘›) indicating how important each attribute ğ‘ğ‘— is to users when they compare items ğ‘ and ğ‘. We adopt a simple binary operation ğ‘”(ğ‘¢rel(ğ‘, ğ‘), ğ‘¢att(ğ‘ğ‘—

## Explain-Step Explain-Step

In this step, the goal is to present ğ¾ explainable groups ğºğ‘(1) , . . .
ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€å…¨ä½“ã®åŠ¹ç”¨ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ãªèª¬æ˜å¯èƒ½ãªã‚°ãƒ«ãƒ¼ãƒ— áµƒ(1) , ... .
,ğºğ‘(ğ¾) such that the whole utility is maximized.
áµƒ(áµƒ)ã¯å…¨ä½“ã®åŠ¹ç”¨ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ãªã‚‚ã®ã§ã‚ã‚‹ã€‚
The complete inference algorithm is described in Alg.
å®Œå…¨ãªæ¨è«–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯Alg.1ã«è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ã€‚
1.
1.
Specifically, it first extracts a small subset of similar candidate items ğ¶ğ‘ with respect to the pivot item ğ‘.
å…·ä½“çš„ã«ã¯ã€ã¾ãšãƒ”ãƒœãƒƒãƒˆé …ç›®â†ªLl_1D45E ã«å¯¾ã—ã¦é¡ä¼¼ã®å€™è£œé …ç›®ğ¶ã®å°ã•ãªéƒ¨åˆ†é›†åˆã‚’æŠ½å‡ºã™ã‚‹ã€‚
For each pair of ğ‘ and ğ‘ğ‘– âˆˆ Cğ‘, it computes the relevance score of two items as well as the importance scores of attributes.
Å…ã¨á‘ã®å„çµ„âˆˆCÅ…ã«å¯¾ã—ã¦ã€2ã¤ã®ã‚¢ã‚¤ãƒ†ãƒ ã®é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã¨å±æ€§ã®é‡è¦åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹ã€‚
Then, the LP problem defined in Eq. 1 is solved to obtain the assignments of candidate items on attribute-based groups.
æ¬¡ã«ã€å¼1ã§å®šç¾©ã•ã‚Œã‚‹LPå•é¡Œã‚’è§£ã„ã¦ã€å±æ€§ãƒ™ãƒ¼ã‚¹ã®ã‚°ãƒ«ãƒ¼ãƒ—ã¸ã®å€™è£œã‚¢ã‚¤ãƒ†ãƒ ã®å‰²ã‚Šå½“ã¦ã‚’å¾—ã‚‹ã€‚
For each group, the algorithm takes the score from the most significant item as the heuristic score for group-level ranking.
å„ã‚°ãƒ«ãƒ¼ãƒ—ã«å¯¾ã—ã¦ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯æœ€ã‚‚é‡è¦ãªé …ç›®ã®ã‚¹ã‚³ã‚¢ã‚’ã‚°ãƒ«ãƒ¼ãƒ—ãƒ¬ãƒ™ãƒ«ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ãŸã‚ã®ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã‚¹ã‚³ã‚¢ã¨ã—ã¦ç”¨ã„ã‚‹ã€‚
Finally, the top ğ¾ groups are generated as the recommendation with attribute-based explanations.
æœ€å¾Œã«ã€ä¸Šä½ Ğ¾ã‚°ãƒ«ãƒ¼ãƒ—ãŒå±æ€§ã«åŸºã¥ãèª¬æ˜ä»˜ãã®æ¨è–¦æ–‡ã¨ã—ã¦ç”Ÿæˆã•ã‚Œã‚‹ã€‚
Note that we adopt template-based generation approach to generate the natural language explanation based on attributes, which is not the focus in this paper.
ãªãŠã€å±æ€§ã«åŸºã¥ãè‡ªç„¶è¨€èªèª¬æ˜ã®ç”Ÿæˆã«ã¯ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã®ç”Ÿæˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ãŒã€ã“ã‚Œã¯æœ¬è«–æ–‡ã®ç„¦ç‚¹ã§ã¯ãªã„ã€‚

## Implementation Detail å®Ÿæ–½å†…å®¹

In the Extract-step, the raw features of each item consist in n-gram features extracted from itemsâ€™ titles, key words and categories.
æŠ½å‡ºã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€å„ã‚¢ã‚¤ãƒ†ãƒ ã®ç”Ÿã®ç‰¹å¾´ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚¿ã‚¤ãƒˆãƒ«ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸn-gramç‰¹å¾´ã§æ§‹æˆã•ã‚Œã‚‹ã€‚
The feature extractor ğœ™ consists of 3 fully-connected layers of sizes 1024, 1024, 128 with ReLU [24] as nonlinear activation function.
ç‰¹å¾´æŠ½å‡ºå™¨ ğœ™ ã¯ã‚µã‚¤ã‚º 1024, 1024, 128 ã® 3 ã¤ã®å®Œå…¨é€£çµå±¤ã‹ã‚‰ãªã‚Šã€éç·šå½¢æ´»æ€§åŒ–é–¢æ•°ã¨ã—ã¦ ReLU [24] ã‚’ç”¨ã„ã¦ã„ã‚‹ã€‚
Margin parameters ğœ† = 1.0 and ğœ– = 1.0.
ãƒãƒ¼ã‚¸ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯áœ† = 1.0 ã¨ ğœ– = 1.0ã§ã‚ã‚‹ã€‚
The model is trained with Adam optimizer with learning rate 0.001 and batch size 128.
ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€å­¦ç¿’ç‡0.001ã€ãƒãƒƒãƒã‚µã‚¤ã‚º128ã®Adam optimizerã§å­¦ç¿’ã•ã‚Œã‚‹ã€‚

In the Expect-step, the network parameters are as follows. ğ‘Šğ‘ âˆˆ R 256Ã—64 , ğ‘Šğ‘ âˆˆ R
A

In the Explain-step, we set both ğ·grp and ğ·att to be 5 by default. Candidate set size
ğ¶ğ‘

# Experiments å®Ÿé¨“

In this section, we comprehensively evaluate the performance of the proposed method EX3 in terms of both recommendation and attribute ranking on a real-world benchmark.
æœ¬ç¯€ã§ã¯ã€ææ¡ˆæ‰‹æ³•EX3ã®æ¨è–¦æ€§èƒ½ã¨å±æ€§ãƒ©ãƒ³ã‚¯ä»˜ã‘æ€§èƒ½ã‚’å®Ÿä¸–ç•Œã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ç·åˆçš„ã«è©•ä¾¡ã™ã‚‹ã€‚

## Experimental Setup Experimental Setup

### Dataset. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

We take experiments on a real-world industrial dataset collected from Amazon.com including 7 subcategories:
Amazon.comã‹ã‚‰åé›†ã—ãŸã€7ã¤ã®ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚’å«ã‚€å®Ÿä¸–ç•Œã®ç”£æ¥­ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å®Ÿé¨“ã‚’è¡Œã†ã€‚
Battery, Coffee, Incontinence Protector, Laundry Detergent, Shampoo, Toilet Paper and Vitamin.
é›»æ± ã€ã‚³ãƒ¼ãƒ’ãƒ¼ã€å¤±ç¦é˜²æ­¢å‰¤ã€æ´—æ¿¯æ´—å‰¤ã€ã‚·ãƒ£ãƒ³ãƒ—ãƒ¼ã€ãƒˆã‚¤ãƒ¬ãƒƒãƒˆãƒšãƒ¼ãƒ‘ãƒ¼ã€ãƒ“ã‚¿ãƒŸãƒ³ã§ã‚ã‚‹ã€‚
Following distant supervision manner mentioned in Section 2, each subset can be regarded as an individual benchmark.
ã‚»ã‚¯ã‚·ãƒ§ãƒ³2ã§è¿°ã¹ãŸé éš”ç›£è¦–ã®æ–¹æ³•ã«å¾“ã„ã€å„ã‚µãƒ–ã‚»ãƒƒãƒˆã¯å€‹ã€…ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã¿ãªã™ã“ã¨ãŒã§ãã‚‹ã€‚
To enable fast experiments, we randomly sample products from each product category and select their corresponding attributes to construct the datasets.
é«˜é€Ÿãªå®Ÿé¨“ã‚’å¯èƒ½ã«ã™ã‚‹ãŸã‚ã€å„è£½å“ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰è£½å“ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«æŠ½å‡ºã—ã€å¯¾å¿œã™ã‚‹å±æ€§ã‚’é¸æŠã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
The statistics of these datasets are summarized in Table 1.
ã“ã‚Œã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆé‡ã¯è¡¨1ã«ã¾ã¨ã‚ã‚‰ã‚Œã¦ã„ã‚‹ã€‚
Similar metadata can also be found in [20, 21].
åŒæ§˜ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯[20, 21]ã«ã‚‚è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã€‚
Due to the large-scale product pool, we generate candidate products for each query product via the proposed Extract-Step, which leads to around 30 similar candidate products per query.
å¤§è¦æ¨¡ãªå•†å“ãƒ—ãƒ¼ãƒ«ã®ãŸã‚ã€ææ¡ˆã™ã‚‹Extract-Stepã«ã‚ˆã‚Šå„ã‚¯ã‚¨ãƒªå•†å“ã«å¯¾ã—ã¦å€™è£œå•†å“ã‚’ç”Ÿæˆã—ã€1ã‚¯ã‚¨ãƒªã‚ãŸã‚Šç´„30ã®é¡ä¼¼å€™è£œå•†å“ã‚’ç”Ÿæˆã™ã‚‹ã€‚
Our model and all the baselines are trained and evaluated based on the extracted candidates.
æˆ‘ã€…ã®ãƒ¢ãƒ‡ãƒ«ã¨å…¨ã¦ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¯ã€æŠ½å‡ºã•ã‚ŒãŸå€™è£œã«åŸºã¥ã„ã¦å­¦ç¿’ãƒ»è©•ä¾¡ã•ã‚Œã‚‹ã€‚
We randomly split the dataset into training set (80%), validation set (10%) and test set (10%).
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å­¦ç¿’ã‚»ãƒƒãƒˆ(80%)ã€æ¤œè¨¼ã‚»ãƒƒãƒˆ(10%)ã€ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆ(10%)ã«ãƒ©ãƒ³ãƒ€ãƒ ã«åˆ†å‰²ã—ã€å­¦ç¿’ã¨è©•ä¾¡ã‚’è¡Œã†ã€‚

### Baselines & Metrics. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼†ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹

We compare our method with following baselines.
æœ¬æ‰‹æ³•ã‚’ä»¥ä¸‹ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒã™ã‚‹ã€‚

- Relevance is the method that computes item similarity based on item embeddings learned in Extract step. é–¢é€£æ€§ã¨ã¯ã€Extractã‚¹ãƒ†ãƒƒãƒ—ã§å­¦ç¿’ã—ãŸé …ç›®åŸ‹ã‚è¾¼ã¿ã‚’å…ƒã«ã€é …ç›®ã®é¡ä¼¼æ€§ã‚’è¨ˆç®—ã™ã‚‹æ–¹æ³•ã§ã‚ã‚‹ã€‚

- BPR [26] is the Bayesian personalized ranking method for making recommendations, which is modified to item-to-item prediction in this work. BPR [26]ã¯ã€æ¨è–¦ã‚’è¡Œã†ãŸã‚ã®ãƒ™ã‚¤ã‚ºå‹ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºãƒ‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ‰‹æ³•ã§ã‚ã‚Šã€æœ¬ç ”ç©¶ã§ã¯é …ç›®é–“äºˆæ¸¬ã«ä¿®æ­£ã—ãŸã‚‚ã®ã§ã‚ã‚‹ã€‚

- ACCM [27] is a CF-based and CB-based recommendation approach that leverages attribute to enrich the representation of items. We adapt this method to our item-to-item recommendation. ACCM [27]ã¯ã€CFãƒ™ãƒ¼ã‚¹ãŠã‚ˆã³CBãƒ™ãƒ¼ã‚¹ã®æ¨è–¦æ‰‹æ³•ã§ã‚ã‚Šã€é …ç›®ã®è¡¨ç¾ã‚’è±Šã‹ã«ã™ã‚‹ãŸã‚ã«å±æ€§ã‚’åˆ©ç”¨ã™ã‚‹ã‚‚ã®ã§ã‚ã‚‹ã€‚ æˆ‘ã€…ã¯ã“ã®æ‰‹æ³•ã‚’ã‚¢ã‚¤ãƒ†ãƒ é–“æ¨è–¦ã«é©ç”¨ã™ã‚‹ã€‚

- A2CF [3] is the state-of-the-art attribute-based recommendation model that outputs substitutes for pivot items. A2CF [3]ã¯ã€ãƒ”ãƒœãƒƒãƒˆã‚¢ã‚¤ãƒ†ãƒ ã®ä»£æ›¿å“ã‚’å‡ºåŠ›ã™ã‚‹ã€æœ€å…ˆç«¯ã®å±æ€§ãƒ™ãƒ¼ã‚¹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã€‚

- EX3 is our approach proposed in Expect step. EX3ã¯ã€Expect stepã§ææ¡ˆã—ãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™ã€‚

For fair comparison, we generate the a candidate set of 30 items for each pivot from the Extract step.
æ¯”è¼ƒã®ãŸã‚ã€Extract ã®æ®µéšã§å„ãƒ”ãƒœãƒƒãƒˆã«å¯¾å¿œã™ã‚‹ 30 å€‹ã®ã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰ãªã‚‹å€™è£œã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚
All the baselines are evaluated based on the candidate set and also leverage the pretrained item embeddings as input if necessary.
ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¯ã™ã¹ã¦ã“ã®å€™è£œã‚»ãƒƒãƒˆã«åŸºã¥ã„ã¦è©•ä¾¡ã•ã‚Œã€å¿…è¦ã«å¿œã˜ã¦äº‹å‰ã«å­¦ç¿’ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ åŸ‹ã‚è¾¼ã¿ã‚‚å…¥åŠ›ã¨ã—ã¦åˆ©ç”¨ã•ã‚Œã‚‹ã€‚

We adopt NDCG@10, Recall@10, Precision@10 as the metrics to evaluate the top-N recommendation performance.
TOP-Nã®æ¨è–¦æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹æŒ‡æ¨™ã¨ã—ã¦ã€NDCG@10ã€Recall@10ã€Precision@10ã‚’æ¡ç”¨ã—ãŸã€‚

## Top-N Recommendation Performance (Expect-Step) ãƒˆãƒƒãƒ—Nãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ€§èƒ½(Expect-Step)

In this experiment, we first evaluate the recommendation performance output by the Expect step, which produces the same results as traditional recommendations.
æœ¬å®Ÿé¨“ã§ã¯ã€ã¾ãšExpectã‚¹ãƒ†ãƒƒãƒ—ã§å‡ºåŠ›ã•ã‚Œã‚‹æ¨è–¦æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ã€‚Expectã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€å¾“æ¥ã®æ¨è–¦ã¨åŒã˜çµæœãŒå¾—ã‚‰ã‚Œã‚‹ã€‚
Specifically, given a pivot item, both our method and all other baselines outputs top 10 recommendations from 30 candidates generated by Extract step.
å…·ä½“çš„ã«ã¯ã€ãƒ”ãƒœãƒƒãƒˆã‚¢ã‚¤ãƒ†ãƒ ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€æœ¬æ‰‹æ³•ã¨ä»–ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¯å…±ã«Extractã‚¹ãƒ†ãƒƒãƒ—ã§ç”Ÿæˆã•ã‚ŒãŸ30å€‹ã®å€™è£œã®ä¸­ã‹ã‚‰ä¸Šä½10å€‹ã®æ¨è–¦ã‚’å‡ºåŠ›ã™ã‚‹ã€‚
The goal of this experiment is to verify if our model can output more relevant items than others.
ã“ã®å®Ÿé¨“ã®ç›®çš„ã¯ã€æˆ‘ã€…ã®ãƒ¢ãƒ‡ãƒ«ãŒä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚é–¢é€£æ€§ã®é«˜ã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’å‡ºåŠ›ã§ãã‚‹ã‹ã©ã†ã‹ã‚’æ¤œè¨¼ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚

The results are reported in Table 2.
ãã®çµæœã‚’è¡¨ 2 ã«ç¤ºã™ã€‚
We observe that our model EX3 consistently outperforms all baselines across all datasets on all metrics.
æˆ‘ã€…ã®ãƒ¢ãƒ‡ãƒ«EX3ã¯ã€å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã„ã¦ã€å…¨ã¦ã®æŒ‡æ¨™ã§ä¸€è²«ã—ã¦ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚‹ã€‚
For instance, our model achieves NDCG of 0.8177, Recall of 0.9667 and Precision of 0.1953, which are higher than the results produced by the best baseline A2CF by a large margin.
ä¾‹ãˆã°ã€æˆ‘ã€…ã®ãƒ¢ãƒ‡ãƒ«ã¯NDCG 0.8177ã€Recall 0.9667ã€Precision 0.1953ã‚’é”æˆã—ã€æœ€è‰¯ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³A2CFã®çµæœã‚ˆã‚Šã‚‚å¤§ããªå·®ã‚’ã¤ã‘ã¦é«˜ã„å€¤ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
It is interesting to see that our model shows significant improvements on the item ranking performance, resulting at least 11.35% improvement in NDCG in Overall dataset and 10.36%â€“56.06% improvements across 7 subdomains.
èˆˆå‘³æ·±ã„ã“ã¨ã«ã€æˆ‘ã€…ã®ãƒ¢ãƒ‡ãƒ«ã¯ã‚¢ã‚¤ãƒ†ãƒ ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®æ€§èƒ½ã«è‘—ã—ã„æ”¹å–„ã‚’ç¤ºã—ã€Overall datasetã§ã¯å°‘ãªãã¨ã‚‚11.35%ã®NDCGã®æ”¹å–„ã€7ã¤ã®ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã¯10.36%-56.06%ã®æ”¹å–„ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
In addition, we notice that for datasets Coffee and Incontinence Protector, the recommendation performance of all models are better than the overall (average) performance.
ã¾ãŸï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆCoffeeã¨Incontinence Protectorã§ã¯ï¼Œã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®æ¨è–¦æ€§èƒ½ãŒå…¨ä½“ï¼ˆå¹³å‡ï¼‰æ€§èƒ½ã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚‹ï¼
For example, our model achieves NDCG of 0.8716 and 0.8660 respectively, which are higher than Overall NDCG of 0.8177.
ä¾‹ãˆã°ã€æˆ‘ã€…ã®ãƒ¢ãƒ‡ãƒ«ã¯ãã‚Œãã‚Œ0.8716ã¨0.8660ã®NDCGã‚’é”æˆã—ã€å…¨ä½“ã®NDCGã§ã‚ã‚‹0.8177ã‚ˆã‚Šé«˜ã„å€¤ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
Other models share similar trends.
ä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚‚åŒæ§˜ã®å‚¾å‘ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
This indicates that the cases in these two datasets are easier to learn to capture user behavior.
ã“ã‚Œã¯ã€ã“ã®2ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚±ãƒ¼ã‚¹ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã‚’æ‰ãˆã‚‹ãŸã‚ã®å­¦ç¿’ãŒå®¹æ˜“ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

##  Model Robustness to Missing Attributes å±æ€§æ¬ è½ã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®é ‘å¥æ€§

We further show our model is robust to missing attributes in inference data with the proposed masking attention.
ã•ã‚‰ã«ã€ææ¡ˆã™ã‚‹ãƒã‚¹ã‚­ãƒ³ã‚°ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ç”¨ã„ã¦ã€æˆ‘ã€…ã®ãƒ¢ãƒ‡ãƒ«ãŒæ¨è«–ãƒ‡ãƒ¼ã‚¿ã«ãŠã‘ã‚‹å±æ€§ã®æ¬ è½ã«å¯¾ã—ã¦ãƒ­ãƒã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚
Specifically, we randomly drop 10%, 20%, 30%, 40% and 50% attributes in the test set and evaluate the top-N recommendation performance of our model with and without the proposed attention mechanism.
å…·ä½“çš„ã«ã¯ã€ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆä¸­ã®10%ã€20%ã€30%ã€40%ã€50%ã®å±æ€§ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å‰Šé™¤ã—ã€ææ¡ˆã™ã‚‹æ³¨ç›®æ©Ÿæ§‹ã‚’ç”¨ã„ãŸå ´åˆã¨ç”¨ã„ãªã„å ´åˆã®æˆ‘ã€…ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒƒãƒ—Næ¨è–¦æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ã€‚
All other settings remain the same.
ä»–ã®è¨­å®šã¯å…¨ã¦åŒã˜ã§ã‚ã‚‹ã€‚
As shown in Fig. 3, our model w
å›³3ã«ç¤ºã™ã‚ˆã†ã«ã€æˆ‘ã€…ã®ãƒ¢ãƒ‡ãƒ«w

## Attribute Ranking Performance (Explain-Step) å±æ€§ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ï¼ˆExplain-Stepï¼‰

### Effectiveness of Attribute Ranking. å±æ€§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®åŠ¹æœ

In this experiment, we evaluate the performance of the proposed Explain-Step in identifying important attributes.
æœ¬å®Ÿé¨“ã§ã¯ã€ææ¡ˆã™ã‚‹Explain-StepãŒé‡è¦ãªå±æ€§ã‚’ç‰¹å®šã™ã‚‹éš›ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ã€‚
We specifically consider following three baselines.
å…·ä½“çš„ã«ã¯ã€ä»¥ä¸‹ã®3ã¤ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’æ¤œè¨ã™ã‚‹ã€‚

- Random is a simple grouping algorithm by randomly assigning items into attribute-based groups as long as the corresponding value exists. Then the groups are ordered in the way same as Alg. 1 (line 13â€“14). ãƒ©ãƒ³ãƒ€ãƒ ã¯ã€å¯¾å¿œã™ã‚‹å€¤ãŒå­˜åœ¨ã™ã‚‹é™ã‚Šã€å±æ€§ã«åŸºã¥ã„ãŸã‚°ãƒ«ãƒ¼ãƒ—ã«ãƒ©ãƒ³ãƒ€ãƒ ã«é …ç›®ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã§ã€å˜ç´”ãªã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã‚’è¡Œã†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚ã‚‹ã€‚ ãã—ã¦ã€å„ã‚°ãƒ«ãƒ¼ãƒ—ã¯Alg.1ã¨åŒã˜æ–¹æ³•ã§é †åºä»˜ã‘ã•ã‚Œã‚‹ã€‚ 1ã¨åŒã˜æ–¹æ³•ã§é †åºä»˜ã‘ã•ã‚Œã‚‹ï¼ˆ13-14è¡Œç›®ï¼‰ã€‚

- Greedy is an iterative algorithm by always picking the pair of item and attribute with larger utility value è²ªæ¬²ã¨ã¯ã€å¸¸ã«åŠ¹ç”¨å€¤ã®å¤§ãã„é …ç›®ã¨å±æ€§ã®çµ„ã‚’é¸ã¶åå¾©ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚ã‚‹

- EX3 is our proposed method of the Explain-Step. EX3ã¯ã€ç§ãŸã¡ãŒææ¡ˆã™ã‚‹Explain-Stepã®æ–¹æ³•ã§ã™ã€‚

Note that all compared methods differ in the grouping ways but take the same utility function as input, which is generated by the Expect-step for fair comparison.
ãªãŠã€æ¯”è¼ƒã—ãŸæ‰‹æ³•ã¯ã™ã¹ã¦ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã®æ–¹æ³•ãŒç•°ãªã‚‹ãŒã€å…¬æ­£ãªæ¯”è¼ƒã®ãŸã‚ã«Expectã‚¹ãƒ†ãƒƒãƒ—ã§ç”Ÿæˆã•ã‚ŒãŸåŒã˜åŠ¹ç”¨é–¢æ•°ã‚’å…¥åŠ›ã¨ã—ã¦ã„ã‚‹ã€‚
To quantify the attribute ranking performance, we randomly sample around 1000 cases and ask human evaluators to manually score each attribute in a 5-point scale given a pivot item and a set of candidate items.
å±æ€§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®æ€§èƒ½ã‚’å®šé‡åŒ–ã™ã‚‹ãŸã‚ã«ã€ç´„1000ã‚±ãƒ¼ã‚¹ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€äººé–“ã®è©•ä¾¡è€…ã«ã€ãƒ”ãƒœãƒƒãƒˆé …ç›®ã¨å€™è£œé …ç›®ã®ã‚»ãƒƒãƒˆã‚’ä¸ãˆã¦å„å±æ€§ã‚’5ç‚¹æº€ç‚¹ã§æ‰‹å‹•ã§æ¡ç‚¹ã—ã¦ã‚‚ã‚‰ã†ã€‚
Then we can calculate the average and the normalized score of the predicted important attributes by each model.
ãã—ã¦ã€å„ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦äºˆæ¸¬ã•ã‚ŒãŸé‡è¦ãªå±æ€§ã®å¹³å‡å€¤ã¨æ­£è¦åŒ–ã•ã‚ŒãŸã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
The results are reported in Table 3.
ãã®çµæœã‚’è¡¨3ã«å ±å‘Šã™ã‚‹ã€‚

We observe that our method EX3 gives the better performance in important attribute ranking compared with two baselines.
ãã®çµæœã€æˆ‘ã€…ã®æ‰‹æ³•EX3ã¯ã€2ã¤ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒã—ã¦ã€é‡è¦å±æ€§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§ã‚ˆã‚Šè‰¯ã„æ€§èƒ½ã‚’ç¤ºã™ã“ã¨ãŒã‚ã‹ã£ãŸã€‚
One interesting fact is that the Greedy algorithm is actually an approximation algorithm for the optimization problem Eq. 1, which interprets that its performance is slightly worse than ours.
èˆˆå‘³æ·±ã„ã“ã¨ã«ã€Greedyã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯å¼1ã®æœ€é©åŒ–å•é¡Œã®è¿‘ä¼¼ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚ã‚Šã€æˆ‘ã€…ã®æ‰‹æ³•ã‚ˆã‚Šè‹¥å¹²æ€§èƒ½ãŒæ‚ªã„ã¨è§£é‡ˆã•ã‚Œã‚‹ã€‚

### Adaptive attribute ranking. é©å¿œçš„ãªå±æ€§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€‚

In addition, we show that for the same pivot item, our model will rank attributes differently if the candidates are different.
ã¾ãŸã€åŒã˜ãƒ”ãƒœãƒƒãƒˆé …ç›®ã§ã‚ã£ã¦ã‚‚ã€å€™è£œãŒç•°ãªã‚Œã°å±æ€§ã®ãƒ©ãƒ³ã‚¯ä»˜ã‘ã‚‚ç•°ãªã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚
We showcase an example in Table 4 to demonstrate this characteristics of our model.
ã“ã®ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´ã‚’ç¤ºã™ãŸã‚ã«ã€è¡¨ 4 ã«ä¾‹ã‚’ç¤ºã™ã€‚
Given a shampoo product with ID â€œB000YG1INIâ€ as pivot item 3 , whose attributes are listed in the second column, we feed two sets of candidate items to our model that is able to generate two different attribute rankings as shown in the upper and lower parts of the table.
IDãŒ "B000YG1INI "ã®ã‚·ãƒ£ãƒ³ãƒ—ãƒ¼å•†å“ã‚’ãƒ”ãƒœãƒƒãƒˆã‚¢ã‚¤ãƒ†ãƒ 3ã¨ã—ã€ãã®å±æ€§ã‚’2åˆ—ç›®ã«è¨˜è¼‰ã—ãŸå ´åˆã€2ã‚»ãƒƒãƒˆã®å€™è£œã‚¢ã‚¤ãƒ†ãƒ ã‚’æˆ‘ã€…ã®ãƒ¢ãƒ‡ãƒ«ã«ä¾›çµ¦ã™ã‚‹ã¨ã€è¡¨ã®ä¸Šä¸‹ã«ç¤ºã™ã‚ˆã†ã«ã€2ç¨®é¡ã®å±æ€§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
It is interesting to see that the model is able to rank attributes based on value differences and diversity.
èˆˆå‘³æ·±ã„ã®ã¯ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ãŒä¾¡å€¤ã®å·®ã¨å¤šæ§˜æ€§ã«åŸºã¥ã„ã¦å±æ€§ã‚’ãƒ©ãƒ³ã‚¯ä»˜ã‘ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
Take â€œbrandâ€ attribute as example.
ä¾‹ãˆã°ã€ã€Œãƒ–ãƒ©ãƒ³ãƒ‰ã€å±æ€§ã‚’ä¾‹ã«ã¨ã‚Šã¾ã™ã€‚
In the first case (upper table), â€œbrandâ€ is ranked in the second place and considered as a relatively important attributes when users compare different shampoo products.
æœ€åˆã®ã‚±ãƒ¼ã‚¹ï¼ˆä¸Šè¡¨ï¼‰ã§ã¯ã€ã€Œãƒ–ãƒ©ãƒ³ãƒ‰ã€ã¯2ä½ã«ãƒ©ãƒ³ã‚¯ã•ã‚Œã¦ãŠã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ§˜ã€…ãªã‚·ãƒ£ãƒ³ãƒ—ãƒ¼è£½å“ã‚’æ¯”è¼ƒã™ã‚‹éš›ã«æ¯”è¼ƒçš„é‡è¦ãªå±æ€§ã§ã‚ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¦ã„ã‚‹ã€‚
In contrast, in the second case (lower table), â€œbrandâ€ is ranked lower because all the candidates have the same brand â€œDesert Essenceâ€ and it is less informative for users to enhance their shopping experience.
ä¸€æ–¹ã€2ç•ªç›®ã®ã‚±ãƒ¼ã‚¹ï¼ˆä¸‹è¡¨ï¼‰ã§ã¯ã€ã™ã¹ã¦ã®å€™è£œãŒåŒã˜ãƒ–ãƒ©ãƒ³ãƒ‰ã€ŒDesert Essenceã€ã‚’æŒã£ã¦ãŠã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ä½“é¨“ã‚’é«˜ã‚ã‚‹ãŸã‚ã®æƒ…å ±é‡ãŒå°‘ãªã„ãŸã‚ã€ã€Œãƒ–ãƒ©ãƒ³ãƒ‰ã€ã®é †ä½ã¯ä½ããªã£ã¦ã„ã¾ã™ã€‚

## Ablation Study ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶

We first show the recommendation performance under different masking ratios (ğœ‚) in the proposed attention mechanism.
ã¾ãšã€ææ¡ˆã™ã‚‹æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«ãŠã„ã¦ã€ç•°ãªã‚‹ãƒã‚¹ã‚­ãƒ³ã‚°æ¯”ç‡(á´¥)ã®ä¸‹ã§ã®æ¨è–¦æ€§èƒ½ã‚’ç¤ºã™ã€‚
Specifically, we adopt different values of ğœ‚ to train the model of Expect step, e.g. ğœ‚ = 0, 0.1, . . .
å…·ä½“çš„ã«ã¯ã€Expectã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã€ç•°ãªã‚‹å€¤ã®áœ‚ã‚’æ¡ç”¨ã™ã‚‹ã€ä¾‹ãˆã°ã€áœ‚ = 0, 0.1, . .
, 0.9, 1.0.
, 0.9, 1.0.
Note that ğœ‚ = 0 means the attribute is completely dropped while ğœ‚ = 1 means there is no attribute dropping.
áœ‚ = 0ã¯å±æ€§ãŒå®Œå…¨ã«å‰Šé™¤ã•ã‚Œã‚‹ã“ã¨ã‚’æ„å‘³ã—ã€áœ‚ = 1ã¯å±æ€§ãŒå‰Šé™¤ã•ã‚Œãªã„ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã“ã¨ã«æ³¨æ„ã•ã‚ŒãŸã„ã€‚
We report the top-N recommendation performance under various ğœ‚â€™s in Fig. 4 (a, b).
å›³ 4 (a, b) ã¯ã€æ§˜ã€…ãª áœ‚ ã®ä¸‹ã§ãƒˆãƒƒãƒ— N ã®æ¨è–¦æ€§èƒ½ã‚’å ±å‘Šã—ãŸã‚‚ã®ã§ã‚ã‚‹ã€‚
We observe that the masking ratios influence on ranking performance (NDCG) and the model achieves the best performance when ğœ‚ = 0.3.
ãƒã‚¹ã‚­ãƒ³ã‚°æ¯”ç‡ã¯ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ï¼ˆNDCGï¼‰ã«å½±éŸ¿ã‚’ä¸ãˆã€ãƒ¢ãƒ‡ãƒ«ã¯ğœ‚=0.3ã®ã¨ãã«æœ€è‰¯ã®æ€§èƒ½ã‚’é”æˆã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
For precision, we find that the performance does not vary a lot, but still show similar trends as the NDCG, i.e. ğœ‚ = 0.4 leads to the relatively better performance.
ç²¾åº¦ã«é–¢ã—ã¦ã¯ã€æ€§èƒ½ã¯ã‚ã¾ã‚Šå¤‰åŒ–ã—ãªã„ãŒã€NDCGã¨åŒæ§˜ã®å‚¾å‘ã‚’ç¤ºã—ã€ã™ãªã‚ã¡ã€áœ‚ = 0.4ãŒæ¯”è¼ƒçš„è‰¯ã„æ€§èƒ½ã«ã¤ãªãŒã‚‹ã“ã¨ãŒåˆ†ã‹ã‚‹ã€‚

Next, we evaluate the influence of different values of temperatures (ğœ) in the attention mechanism.
æ¬¡ã«ã€æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«ãŠã‘ã‚‹æ¸©åº¦ï¼ˆï¼‰ã®å€¤ã®é•ã„ã®å½±éŸ¿ã‚’è©•ä¾¡ã™ã‚‹ã€‚
Specifically, we experiment with two ways of imposing temperatures over softmax function.
å…·ä½“çš„ã«ã¯ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã«æ¸©åº¦ã‚’èª²ã™2ã¤ã®æ–¹æ³•ã‚’å®Ÿé¨“ã™ã‚‹ã€‚
The first one relies on the predefined attribute frequencies, i.e. ğœ = (1 + ğ‘“ ğ‘Ÿğ‘’ğ‘ğ‘–) ğ‘› with ğ‘› = 1, 2.
1ã¤ç›®ã¯äº‹å‰ã«å®šç¾©ã•ã‚ŒãŸå±æ€§é »åº¦ã€ã™ãªã‚ã¡ğœ = (1 + ğ‘“) ğ‘›ã§ğ‘Ÿ = 1, 2ã«ä¾å­˜ã™ã‚‹ã‚‚ã®ã§ã‚ã‚‹ã€‚
The other one uses the fixed value of ğœ = 1, 1.5, 2, 10.
ã‚‚ã†1ã¤ã¯å›ºå®šå€¤ğœ = 1, 1.5, 2, 10ã‚’ç”¨ã„ã‚‹ã‚‚ã®ã§ã‚ã‚‹ã€‚
All other training settings remain the same.
ä»–ã®å­¦ç¿’è¨­å®šã¯å…¨ã¦åŒã˜ã§ã‚ã‚‹ã€‚
The results of top N recommendation are reported in Fig. 4 (c, d).
ãƒˆãƒƒãƒ—Næ¨è–¦ã®çµæœã‚’å›³4 (c, d)ã«å ±å‘Šã™ã‚‹ã€‚
We can see that the default choice of ğœ = 1 + ğ‘“ ğ‘Ÿğ‘’ğ‘ğ‘– leads to the best performance in both NDCG and precision.
ğœ = 1 + ğ‘“ ğ‘’ğ‘ğ‘–ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é¸æŠã¯NDCGã¨ç²¾åº¦ã®ä¸¡æ–¹ã§æœ€é«˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ã‚‚ãŸã‚‰ã™ã“ã¨ãŒåˆ†ã‹ã‚‹ã€‚
Besides, note that when ğœ = 1, it is equivalent to the original softmax function.
ãã®ä¸Šã€â†ªLl_1F = 1ã®æ™‚ã€å…ƒã®ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã¨ç­‰ä¾¡ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã€‚
Our model with the default ğœ shows superior performance over such setup, which indicates the effectiveness of the proposed attention mechanism.
ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚’ç”¨ã„ãŸæˆ‘ã€…ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ã“ã®ã‚ˆã†ãªè¨­å®šã‚ˆã‚Šã‚‚å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ã€ææ¡ˆã™ã‚‹æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®æœ‰åŠ¹æ€§ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

## Online Simulation and Experiments 

In this experiment, we evaluate the overall performance of the group-form explainable item set recommendation.
æœ¬å®Ÿé¨“ã§ã¯ã€ã‚°ãƒ«ãƒ¼ãƒ—å½¢å¼ã®èª¬æ˜å¯èƒ½ãªé …ç›®ã‚»ãƒƒãƒˆæ¨è–¦ã®ç·åˆçš„ãªæ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ã€‚
Before serving the proposed method to real users, we generate a batch of explainable item set recommendations in an offline mode and leverage human annotators to help us evaluate the recommendation quality.
ææ¡ˆæ‰‹æ³•ã‚’å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ã«æä¾›ã™ã‚‹å‰ã«ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§èª¬æ˜å¯èƒ½ãªã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆæ¨è–¦ã®ãƒãƒƒãƒã‚’ç”Ÿæˆã—ã€äººé–“ã®ã‚¢ãƒãƒ†ãƒ¼ã‚¿ãƒ¼ã«æ¨è–¦å“è³ªã®è©•ä¾¡ã‚’ä¾é ¼ã™ã‚‹ã€‚
For each of 7 product categories, we sample top 50 most popular pivot products from our recommendation dataset and ask the annotators to evaluate whether the attribute-based explainable recommendations can help users make better purchase decision.
7ã¤ã®å•†å“ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ã€æ¨è–¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ä¸Šä½50ã®äººæ°—å•†å“ã‚’æŠ½å‡ºã—ã€å±æ€§ã«åŸºã¥ãèª¬æ˜å¯èƒ½ãªæ¨è–¦ãŒãƒ¦ãƒ¼ã‚¶ã®ã‚ˆã‚Šè‰¯ã„è³¼è²·æ„æ€æ±ºå®šã«å½¹ç«‹ã¤ã‹ã©ã†ã‹ã‚’ã‚¢ãƒãƒ†ãƒ¼ã‚¿ãƒ¼ã«è©•ä¾¡ã—ã¦ã‚‚ã‚‰ã£ã¦ã„ã‚‹ã€‚
Note that the evaluation metric contains two-fold interactive measurement on both product relevance and attribute importance, as the ranked important attribute list should depend on what products are recommended to users.
ã“ã®è©•ä¾¡æŒ‡æ¨™ã¯ã€å•†å“ã®é–¢é€£æ€§ã¨å±æ€§ã®é‡è¦æ€§ã®2é‡ã®è©•ä¾¡ã‹ã‚‰æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚
Through human evaluation, we obtain over 80% acceptance rate on high-quality item set recommendations with over 86% accuracy on comparable product recommendation performance, which is 2x higher than using raw Bpv data for recommendation.
äººé–“ã«ã‚ˆã‚‹è©•ä¾¡ã§ã¯ã€80%ä»¥ä¸Šã®é«˜å“è³ªãªã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆæ¨è–¦ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã€86%ä»¥ä¸Šã®ç²¾åº¦ã§åŒç­‰ã®å•†å“æ¨è–¦ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã—ãŸã€‚ã“ã‚Œã¯ã€æ¨è–¦ã«ç”Ÿã®Bpvãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã‚Šã‚‚2å€é«˜ã„ç²¾åº¦ã§ã™ã€‚
We also conduct online A
ã¾ãŸã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³A

# Related Work é–¢é€£ä½œå“

In this section, we discuss the related work regarding explainable recommendation and item relationship mining.
æœ¬ç¯€ã§ã¯ã€èª¬æ˜å¯èƒ½ãªæ¨è–¦ã¨é …ç›®é–¢ä¿‚ãƒã‚¤ãƒ‹ãƒ³ã‚°ã«é–¢ã™ã‚‹é–¢é€£ç ”ç©¶ã«ã¤ã„ã¦è¿°ã¹ã‚‹ã€‚

## Explainable Recommendation. Explainable Recommendation (èª¬æ˜å¯èƒ½ãªæ¨å¥¨äº‹é …)

In the era of e-commerce, recommender systems have been widely used to provide users with relevant item suggestions.
é›»å­å•†å–å¼•ã®æ™‚ä»£ã«ãŠã„ã¦ï¼Œãƒ¦ãƒ¼ã‚¶ã«é–¢é€£ã™ã‚‹å•†å“ã‚’ææ¡ˆã™ã‚‹ãŸã‚ã«ï¼Œæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ãŒåºƒãåˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹ï¼
Most of existing methods are based on collaborative filtering [16], matrix factorization [17] and neural recommendation model [34].
æ—¢å­˜ã®æ‰‹æ³•ã®å¤šãã¯ã€å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° [16]ã€è¡Œåˆ—åˆ†è§£ [17]ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ¨è–¦ãƒ¢ãƒ‡ãƒ« [34] ã«åŸºã¥ã„ã¦ã„ã‚‹ã€‚
Recently, to further improve user experience of recommender systems [18], great research efforts have been promoted to explainable recommendation problems [6, 7, 35].
è¿‘å¹´ï¼Œæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¦ãƒ¼ã‚¶ã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹ã‚’ã•ã‚‰ã«å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«[18]ï¼Œèª¬æ˜å¯èƒ½ãªæ¨è–¦å•é¡Œã«å¯¾ã™ã‚‹å¤§ããªç ”ç©¶åŠªåŠ›ãŒé€²ã‚ã‚‰ã‚Œã¦ã„ã‚‹[6, 7, 35]ï¼
One common way to generate explanation for recommendation is to leverage knowledge graphs [5, 15, 31, 33, 39].
æ¨è–¦ã®ãŸã‚ã®èª¬æ˜ã‚’ç”Ÿæˆã™ã‚‹ä¸€èˆ¬çš„ãªæ–¹æ³•ã¨ã—ã¦ã€çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚’æ´»ç”¨ã™ã‚‹æ–¹æ³•ãŒã‚ã‚‹[5, 15, 31, 33, 39]ã€‚
For example, Xian et al. [32] propose to leverage reinforcement learning on knowledge graph to provide behavior-based explanation for product recommendations, while Zhao et al. [37] also employs reinforcement learning but propose a different demonstration-based knowledge graph reasoning framework for explainable recommendation.
ä¾‹ãˆã°ã€Xianã‚‰[32]ã¯çŸ¥è­˜ã‚°ãƒ©ãƒ•ã®å¼·åŒ–å­¦ç¿’ã‚’åˆ©ç”¨ã—ã¦ã€å•†å“æ¨è–¦ã®ãŸã‚ã®è¡Œå‹•ãƒ™ãƒ¼ã‚¹ã®èª¬æ˜ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’ææ¡ˆã—ã€Zhaoã‚‰[37]ã‚‚å¼·åŒ–å­¦ç¿’ã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹ãŒã€èª¬æ˜å¯èƒ½ãªæ¨è–¦ã®ãŸã‚ã®ç•°ãªã‚‹ãƒ‡ãƒ¢ãƒ™ãƒ¼ã‚¹ã®çŸ¥è­˜ã‚°ãƒ©ãƒ•æ¨è«–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚
Besides knowledge graph, sentiment
çŸ¥è­˜ã‚°ãƒ©ãƒ•ã®ä»–ã«ã€æ„Ÿæƒ…

## Item Relationship Mining. ã‚¢ã‚¤ãƒ†ãƒ é–¢ä¿‚ ãƒã‚¤ãƒ‹ãƒ³ã‚°

As our work is around item-to-item-set recommendation, we will also discuss existing work on item relationship mining.
æœ¬è«–æ–‡ã§ã¯ã€é …ç›®é–“æ¨è–¦ã‚’å¯¾è±¡ã¨ã—ã¦ã„ã‚‹ãŸã‚ã€é …ç›®é–“é–¢ä¿‚ãƒã‚¤ãƒ‹ãƒ³ã‚°ã«é–¢ã™ã‚‹æ—¢å­˜ã®ç ”ç©¶ã«ã¤ã„ã¦ã‚‚è¨€åŠã™ã‚‹ã€‚
Identifying relationships among items is a fundamental component of many realworld recommender systems [20, 30].
ã‚¢ã‚¤ãƒ†ãƒ é–“ã®é–¢ä¿‚ã‚’ç‰¹å®šã™ã‚‹ã“ã¨ã¯ã€å¤šãã®ç¾å®Ÿä¸–ç•Œã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®åŸºæœ¬è¦ç´ ã§ã‚ã‚‹ [20, 30]ã€‚
Linden et al. [19] designs an item-to-item collaborative filtering to generate similar item recommendation for Amazon.com.
Linden ã‚‰[19]ã¯ Amazon.com ã®é¡ä¼¼å“æ¨è–¦ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ã‚¢ã‚¤ãƒ†ãƒ é–“å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¨­è¨ˆã—ã¦ã„ã‚‹ã€‚
Zhang et al. [38] discuss the impact of substitute and complement relationship between items on recommendations.
Zhang ã‚‰[38]ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ é–“ã®ä»£æ›¿ãƒ»è£œå®Œé–¢ä¿‚ãŒæ¨è–¦ã«ä¸ãˆã‚‹å½±éŸ¿ã«ã¤ã„ã¦è­°è«–ã—ã¦ã„ã‚‹ã€‚
Similar efforts [1, 12] have been put to target at explicitly modeling relationship between items for recommendations.
ã¾ãŸï¼Œãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ãŠã‘ã‚‹ã‚¢ã‚¤ãƒ†ãƒ é–“ã®é–¢ä¿‚ã‚’æ˜ç¤ºçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ã“ã¨ã‚‚ï¼ŒåŒæ§˜ã®å–ã‚Šçµ„ã¿[1,12]ã«ã‚ˆã£ã¦ç›®æ¨™ã¨ã•ã‚Œã¦ã„ã‚‹ï¼
Representative examples include Sceptre [20], which proposes a topic modeling method to infer networks of products, and PMSC [30], which incorporates path constraints in item pairwise relational modeling.
ä»£è¡¨çš„ãªä¾‹ã¨ã—ã¦ã€å•†å“ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ¨å®šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒªãƒ³ã‚°æ‰‹æ³•ã‚’ææ¡ˆã—ãŸSceptre [20]ã‚„ã€é …ç›®ã®ãƒšã‚¢ãƒ¯ã‚¤ã‚ºãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒŠãƒ«ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ãƒ‘ã‚¹åˆ¶ç´„ã‚’çµ„ã¿è¾¼ã‚“ã PMSC [30]ãŒã‚ã‚‹ã€‚
He et al. [13] design a framework to use visual features to identify compatibility relationship between clothes and jewelry.
Heã‚‰[13]ã¯ã€è¡£æœã¨å®é£¾å“ã®äº’æ›æ€§é–¢ä¿‚ã‚’è­˜åˆ¥ã™ã‚‹ãŸã‚ã«è¦–è¦šçš„ç‰¹å¾´ã‚’åˆ©ç”¨ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’è¨­è¨ˆã—ã¦ã„ã‚‹ã€‚
These methods seek to distinguish substitutes, complements and compatibilities, but fail to provide any clear explanation on why these items are substitutable and comparable.
ã“ã‚Œã‚‰ã®æ–¹æ³•ã¯ã€ä»£æ›¿å“ã€è£œå®Œå“ã€äº’æ›å“ã‚’åŒºåˆ¥ã—ã‚ˆã†ã¨ã™ã‚‹ã‚‚ã®ã§ã‚ã‚‹ãŒã€ãªãœã“ã‚Œã‚‰ã®å“ç›®ãŒä»£æ›¿å¯èƒ½ã§ã‚ã‚Šã€æ¯”è¼ƒå¯èƒ½ã§ã‚ã‚‹ã®ã‹ã«ã¤ã„ã¦æ˜ç¢ºãªèª¬æ˜ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒã§ããªã„ã€‚

# Conclusion çµè«–

In this work, we study the important problem of explainable attribute-aware item-set recommendation.
æœ¬ç ”ç©¶ã§ã¯ã€èª¬æ˜å¯èƒ½ãªå±æ€§ã‚’è€ƒæ…®ã—ãŸé …ç›®é›†åˆæ¨è–¦ã¨ã„ã†é‡è¦ãªå•é¡Œã‚’ç ”ç©¶ã™ã‚‹ã€‚
We propose a multi-step learning-based framework called Extract-Expect-Explain (EX3 ) to approach the problem by first extracting coarse-grained candidate sets of items with respect to the pivot to reduce the search space of similar items (Extract-step), followed by a joint prediction of pairwise item relevance and attribute importance (Expect-step), which are subsequently fed to a constrained optimization solver to generate the group-form recommendations with explanations (Explain-step).
ã¾ãšã€é¡ä¼¼é …ç›®ã®æ¢ç´¢ç©ºé–“ã‚’ç¸®å°ã™ã‚‹ãŸã‚ã«ã€ãƒ”ãƒœãƒƒãƒˆã«é–¢ã™ã‚‹ç²—è¦–åŒ–ã•ã‚ŒãŸå€™è£œé …ç›®ç¾¤ã‚’æŠ½å‡ºã—ï¼ˆExtract-stepï¼‰ã€æ¬¡ã«ã€ãƒšã‚¢ãƒ¯ã‚¤ã‚ºé …ç›®é–¢é€£æ€§ã¨å±æ€§é‡è¦åº¦ã®å…±åŒäºˆæ¸¬ï¼ˆExpect-stepï¼‰ã‚’è¡Œã„ã€ãã®å¾Œã€èª¬æ˜ä»˜ãã‚°ãƒ«ãƒ¼ãƒ—å½¢å¼æ¨è–¦ã‚’ç”Ÿæˆã™ã‚‹ï¼ˆExplain-stepï¼‰åˆ¶ç´„ä»˜ãæœ€é©è§£æ³•ã«ä¾›çµ¦ã—ã¦å•é¡Œã«ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã™ã‚‹ã€EX3 ï¼ˆExract-Expect-Explain) ã¨å‘¼ã¶ãƒãƒ«ãƒã‚¹ãƒ†ãƒƒãƒ—å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®æ çµ„ã¿ã‚’ææ¡ˆã™ã‚‹ã€‚
The experiments are conducted on a real-world large-scale dataset and the results demonstrate that our proposed model achieves over 10% higher NDCG than state-of-the-art baselines in the explainable recommendation domain.
å®Ÿä¸–ç•Œã®å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦å®Ÿé¨“ã‚’è¡Œã£ãŸçµæœã€ææ¡ˆæ‰‹æ³•ã¯èª¬æ˜å¯èƒ½ãªæ¨è–¦é ˜åŸŸã«ãŠã„ã¦ã€æœ€æ–°ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒã—ã¦10%ä»¥ä¸Šé«˜ã„NDCGã‚’é”æˆã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚
Moreover, our proposed method can adaptively generate attribute-based explanations for various products, and the resulting explainable item-set recommendations are also shown to be effective in large-scale online experiments.
ã•ã‚‰ã«ã€ææ¡ˆæ‰‹æ³•ã¯æ§˜ã€…ãªå•†å“ã«å¯¾ã—ã¦å±æ€§ã«åŸºã¥ã„ãŸèª¬æ˜ã‚’é©å¿œçš„ã«ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã€ãã®çµæœå¾—ã‚‰ã‚ŒãŸèª¬æ˜å¯èƒ½ãªã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆæ¨è–¦ãŒå¤§è¦æ¨¡ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å®Ÿé¨“ã«ãŠã„ã¦ã‚‚æœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚
There are several promising areas that we consider for future work, such as leveraging the learnt important attributes for query rewriting and product categorization.
ã¾ãŸã€å­¦ç¿’ã—ãŸé‡è¦ãªå±æ€§ã‚’ã‚¯ã‚¨ãƒªæ›¸ãæ›ãˆã‚„å•†å“åˆ†é¡ã«åˆ©ç”¨ã™ã‚‹ãªã©ã€ä»Šå¾Œã®èª²é¡Œã¨ã—ã¦æœ‰æœ›ãªé ˜åŸŸãŒã„ãã¤ã‹è€ƒãˆã‚‰ã‚Œã‚‹ã€‚