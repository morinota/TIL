## 0.1. link ãƒªãƒ³ã‚¯

https://arxiv.org/pdf/2305.05585.pdf
https://arxiv.org/pdf/2305.05585.pdf

## 0.2. title ã‚¿ã‚¤ãƒˆãƒ«

Improving Implicit Feedback-Based Recommendation through Multi-Behavior Alignment
ãƒãƒ«ãƒè¡Œå‹•ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã«ã‚ˆã‚‹æš—é»™ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ããƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®æ”¹å–„

## 0.3. abstruct abstruct

Recommender systems that learn from implicit feedback often use large volumes of a single type of implicit user feedback, such as clicks, to enhance the prediction of sparse target behavior such as purchases.
æš—é»™ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰å­¦ç¿’ã™ã‚‹ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã¯ã€è³¼å…¥ã®ã‚ˆã†ãªç–ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¡Œå‹•(=æœ€é©åŒ–ã•ã›ãŸã„è¡Œå‹•?)ã®äºˆæ¸¬ã‚’å¼·åŒ–ã™ã‚‹ãŸã‚ã«ã€ã‚¯ãƒªãƒƒã‚¯ã®ã‚ˆã†ãªå˜ä¸€ã®ã‚¿ã‚¤ãƒ—ã®æš—é»™ã®ãƒ¦ãƒ¼ã‚¶ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å¤§é‡ã«ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒå¤šã„ã€‚
Using multiple types of implicit user feedback for such target behavior prediction purposes is still an open question.
ã“ã®ã‚ˆã†ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¡Œå‹•äºˆæ¸¬ã®ç›®çš„ã§ã€**è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®æš—é»™çš„ãƒ¦ãƒ¼ã‚¶ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯ã€ã¾ã æœªè§£æ±ºã®å•é¡Œ**ã§ã‚ã‚‹ã€‚
Existing studies that attempted to learn from multiple types of user behavior often fail to: (i) learn universal and accurate user preferences from different behavioral data distributions, and (ii) overcome the noise and bias in observed implicit user feedback.
è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã‹ã‚‰å­¦ç¿’ã—ã‚ˆã†ã¨ã™ã‚‹æ—¢å­˜ã®ç ”ç©¶ã¯ã€ã—ã°ã—ã°ä»¥ä¸‹ã®äº‹ã«å¤±æ•—ã™ã‚‹ï¼š(i)ç•°ãªã‚‹è¡Œå‹•ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‹ã‚‰æ™®éçš„ã§æ­£ç¢ºãªãƒ¦ãƒ¼ã‚¶å—œå¥½ã‚’å­¦ç¿’ã™ã‚‹ã€(ii)è¦³æ¸¬ã•ã‚ŒãŸæš—é»™çš„ãªãƒ¦ãƒ¼ã‚¶ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«ãŠã‘ã‚‹ãƒã‚¤ã‚ºã¨ãƒã‚¤ã‚¢ã‚¹ã‚’å…‹æœã™ã‚‹ã€‚
To address the above problems, we propose multi-behavior alignment (MBA), a novel recommendation framework that learns from implicit feedback by using multiple types of behavioral data.
ä¸Šè¨˜ã®å•é¡Œç‚¹ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€æˆ‘ã€…ã¯ã€è¤‡æ•°ç¨®é¡ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦æš—é»™ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰å­¦ç¿’ã™ã‚‹æ–°ã—ã„æ¨è–¦ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚‹**multi-behavior alignment(MBA)**(alignmentã¯èª¿æ•´??:thinking:)ã‚’ææ¡ˆã™ã‚‹ã€‚
We conjecture that multiple types of behavior from the same user (e.g., clicks and purchases) should reflect similar preferences of that user.
åŒã˜ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®è¡Œå‹•ï¼ˆä¾‹ãˆã°ã€ã‚¯ãƒªãƒƒã‚¯ã‚„è³¼å…¥ï¼‰ã¯ã€ãã®ãƒ¦ãƒ¼ã‚¶ã®é¡ä¼¼ã—ãŸå—œå¥½ã‚’åæ˜ ã—ã¦ã„ã‚‹ã¯ãšã ã¨æ¨æ¸¬ã•ã‚Œã‚‹ã€‚
To this end, we regard the underlying universal user preferences as a latent variable.
ã“ã®ç›®çš„ã®ãŸã‚ã«ã€ç§ãŸã¡ã¯**æ ¹åº•ã«ã‚ã‚‹æ™®éçš„ãªãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã‚’æ½œåœ¨å¤‰æ•°ã¨ã¿ãªã™**ã€‚
The variable is inferred by maximizing the likelihood of multiple observed behavioral data distributions and, at the same time, minimizing the Kullbackâ€“Leibler divergence (KL-divergence) between user models learned from auxiliary behavior (such as clicks or views) and the target behavior separately.
ã“ã®(æ½œåœ¨)å¤‰æ•°ã¯ã€è¦³æ¸¬ã•ã‚ŒãŸè¤‡æ•°ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®å°¤åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹ã¨åŒæ™‚ã«ã€è£œåŠ©è¡Œå‹•(clickã‚„viewãªã©)ã‹ã‚‰å­¦ç¿’ã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¢ãƒ‡ãƒ«ã¨å¯¾è±¡è¡Œå‹•ã¨ã®é–“ã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹(Kullback-Leibler divergence)ã‚’å€‹åˆ¥ã«æœ€å°åŒ–ã™ã‚‹ã“ã¨ã§æ¨è«–ã•ã‚Œã‚‹ã€‚
MBA infers universal user preferences from multi-behavior data and performs data denoising to enable effective knowledge transfer.
**MBAã¯ã€è¤‡æ•°ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ™®éçš„ãªãƒ¦ãƒ¼ã‚¶å—œå¥½ã‚’æ¨æ¸¬ã—ã€åŠ¹æœçš„ãªçŸ¥è­˜ä¼é”ã‚’å¯èƒ½ã«ã™ã‚‹ãŸã‚ã«ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚¤ã‚ºé™¤å»ã‚’è¡Œã†**ã€‚
We conduct experiments on three datasets, including a dataset collected from an operational e-commerce platform.
é›»å­å•†å–å¼•ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‹ã‚‰åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å«ã‚€ã€3ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å®Ÿé¨“ã‚’è¡Œã£ãŸã€‚
Empirical results demonstrate the effectiveness of our proposed method in utilizing multiple types of behavioral data to enhance the prediction of the target behavior.
å®Ÿè¨¼çµæœã¯ã€è¤‡æ•°ç¨®é¡ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‚’åˆ©ç”¨ã—ã¦å¯¾è±¡è¡Œå‹•ã®äºˆæ¸¬ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ææ¡ˆæ‰‹æ³•ã®æœ‰åŠ¹æ€§ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

# 1. Introduction ã¯ã˜ã‚ã«

Recommender systems aim to infer user preferences from observed user-item interactions and recommend items that match those preferences.
ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã¯ã€è¦³å¯Ÿã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®ç›¸äº’ä½œç”¨ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã‚’æ¨æ¸¬ã—ã€ãã®å—œå¥½ã«åˆã£ãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’æ¨è–¦ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã‚‹ã€‚
Many operational recommender systems are trained from implicit user feedback [14, 16].
å¤šãã®é‹ç”¨ã•ã‚Œã¦ã„ã‚‹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¯ã€æš—é»™ã®ãƒ¦ãƒ¼ã‚¶ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰å­¦ç¿’ã•ã‚Œã‚‹[14, 16]ã€‚
Recommender systems that learn from implicit user feedback are typically trained on a single type of implicit user behavior, such as clicks.
æš—é»™çš„ãªãƒ¦ãƒ¼ã‚¶ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰å­¦ç¿’ã™ã‚‹ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã¯ã€é€šå¸¸ã€ã‚¯ãƒªãƒƒã‚¯ãªã©ã®å˜ä¸€ã‚¿ã‚¤ãƒ—ã®æš—é»™çš„ãªãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã§å­¦ç¿’ã•ã‚Œã‚‹ã€‚(ãã†ãªã®...??)
However, in real-world scenarios, multiple types of user behavior are logged when a user interacts with a recommender system.
ã—ã‹ã—ã€å®Ÿä¸–ç•Œã®ã‚·ãƒŠãƒªã‚ªã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ãŒãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ãƒ»ã‚·ã‚¹ãƒ†ãƒ ã¨ç›¸äº’ä½œç”¨ã™ã‚‹éš›ã«ã€**è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ãŒè¨˜éŒ²ã•ã‚Œã‚‹**ã€‚(ã†ã‚“ã†ã‚“...!)
For example, users may click, add to a cart, and purchase items on an e-commerce platform [31].
ä¾‹ãˆã°ã€ãƒ¦ãƒ¼ã‚¶ã¯eã‚³ãƒãƒ¼ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ä¸Šã§å•†å“ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã€ã‚«ãƒ¼ãƒˆã«å…¥ã‚Œã€è³¼å…¥ã™ã‚‹ã“ã¨ãŒã§ãã‚‹[31]ã€‚
Simply learning recommenders from a single type of behavioral data such as clicks can lead to a misunderstanding of a userâ€™s real user preferences since the click data is noisy and can easily be corrupted due to bias [5], and thus lead to suboptimal target behavior (e.g., purchases) predictions.
ã‚¯ãƒªãƒƒã‚¯ã®ã‚ˆã†ãªå˜ä¸€ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å˜ç´”ã«ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã¯ã€**ã‚¯ãƒªãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã¯ãƒã‚¤ã‚ºãŒå¤šãã€ãƒã‚¤ã‚¢ã‚¹ã«ã‚ˆã£ã¦å®¹æ˜“ã«ç ´æã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹**ãŸã‚[5]ã€ãƒ¦ãƒ¼ã‚¶ã®å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶å—œå¥½ã‚’èª¤è§£ã™ã‚‹ã“ã¨ã«ã¤ãªãŒã‚Šã€ãã®çµæœã€æœ€é©ã§ãªã„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¡Œå‹•(è³¼å…¥ãªã©)ã®äºˆæ¸¬ã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
Meanwhile, only considering purchase data tends to lead to severe cold-start problems [26, 41, 48] and data sparsity problems [23, 27].
ä¸€æ–¹ã€è³¼å…¥ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’è€ƒæ…®ã™ã‚‹ã¨ã€æ·±åˆ»ãªã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆå•é¡Œ[26, 41, 48]ã‚„ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ†ã‚£å•é¡Œ[23, 27]ã‚’å¼•ãèµ·ã“ã™å‚¾å‘ãŒã‚ã‚‹ã€‚

### 1.0.1. Using multiple types of behavioral data.è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹ã€‚

How can we use multiple types of auxiliary behavioral data (such as clicks) to enhance the prediction of sparse target user behavior (such as purchases) and thereby improve recommendation performance? Some prior work [2, 12] has used multi-task learning to train recommender systems on both target behavior and multiple types of auxiliary behavior.
è¤‡æ•°ç¨®é¡ã®è£œåŠ©è¡Œå‹•ãƒ‡ãƒ¼ã‚¿(ã‚¯ãƒªãƒƒã‚¯ãªã©)ã‚’ä½¿ã£ã¦ã€ã‚¹ãƒ‘ãƒ¼ã‚¹ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•(è³¼å…¥ãªã©)ã®äºˆæ¸¬ã‚’å¼·åŒ–ã—ã€æ¨è–¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ã«ã¯ã©ã†ã™ã‚Œã°ã„ã„ã®ã ã‚ã†ã‹ï¼Ÿã„ãã¤ã‹ã®å…ˆè¡Œç ”ç©¶[2, 12]ã§ã¯ã€multi-taskå­¦ç¿’(=clickäºˆæ¸¬å•é¡Œã¨conversionäºˆæ¸¬å•é¡Œã‚’å­¦ç¿’ã•ã›ã‚‹ã€ã¿ãŸã„ãª??:thinking:)ã‚’ç”¨ã„ã¦ã€target behavior(ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¡Œå‹•)ã¨è¤‡æ•°ç¨®é¡ã® auxiliary behavior(è£œåŠ©è¡Œå‹•)ã®ä¸¡æ–¹ã«ã¤ã„ã¦æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã€‚
Building on recent advances in graph neural networks, Jin et al.[18] encode target behavior and multiple types of auxiliary behavior into a heterogeneous graph and perform convolution operations on the constructed graph for recommendation.
Jinã‚‰[18]ã¯ã€ã‚°ãƒ©ãƒ•ãƒ»ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€è¿‘ã®é€²æ­©ã«åŸºã¥ãã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¡Œå‹•ã¨è¤‡æ•°ç¨®é¡ã®è£œåŠ©è¡Œå‹•ã‚’ç•°ç¨®ã‚°ãƒ©ãƒ•ã«ç¬¦å·åŒ–ã—ã€æ¨è–¦ã®ãŸã‚ã«æ§‹ç¯‰ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ã«å¯¾ã—ã¦ç•³ã¿è¾¼ã¿æ¼”ç®—ã‚’è¡Œã†ã€‚
In addition, recent research tries to integrate the micro-behavior of useritem interactions into representation learning in the sequential and session-based recommendation [25, 44, 46].
ã•ã‚‰ã«ã€æœ€è¿‘ã®ç ”ç©¶ã§ã¯ã€é€æ¬¡æ¨è–¦ã‚„ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ã«ãŠã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®ç›¸äº’ä½œç”¨ã®ãƒŸã‚¯ãƒ­ãªè¡Œå‹•ã‚’è¡¨ç¾å­¦ç¿’(??)ã«çµ±åˆã™ã‚‹ã“ã¨ãŒè©¦ã¿ã‚‰ã‚Œã¦ã„ã‚‹[25, 44, 46]ã€‚
These publications focus on mining user preferences from user-item interactions, which is different from our task of predicting target behavior from multiple types of user behavior.
ã“ã‚Œã‚‰ã®å‡ºç‰ˆç‰©ã¯ã€ãƒ¦ãƒ¼ã‚¶ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®ç›¸äº’ä½œç”¨ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã‚’ãƒã‚¤ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãŠã‚Šã€è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã‹ã‚‰ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¡Œå‹•ã‚’äºˆæ¸¬ã™ã‚‹æˆ‘ã€…ã®ã‚¿ã‚¹ã‚¯ã¨ã¯ç•°ãªã‚‹ã€‚

### 1.0.2. Limitations of current approaches. ç¾åœ¨ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®é™ç•Œã€‚

Prior work on using multiple types of behavioral data to improve the prediction of the target behavior in a recommendation setting has two main limitations.
ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šã«ãŠã„ã¦ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¡Œå‹•ã®äºˆæ¸¬ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹å…ˆè¡Œç ”ç©¶ã«ã¯ã€ä¸»ã«2ã¤ã®é™ç•ŒãŒã‚ã‚‹ã€‚

The first limitation concerns the gap between data distributions of different types of behavior.
æœ€åˆã®é™ç•Œã¯ã€ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®è¡Œå‹•ã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒé–“ã®ã‚®ãƒ£ãƒƒãƒ—ã«é–¢ã™ã‚‹ã‚‚ã®ã§ã‚ã‚‹ã€‚
This gap impacts the learning of universal and effective user preferences.
ã“ã®ã‚®ãƒ£ãƒƒãƒ—ã¯ã€æ™®éçš„ã§åŠ¹æœçš„ãªãƒ¦ãƒ¼ã‚¶å—œå¥½ã®å­¦ç¿’ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã€‚
For example, users may have clicked on but not purchased items, resulting in different positive and negative instance distributions across auxiliary and target behaviors.
ä¾‹ãˆã°ã€ãƒ¦ãƒ¼ã‚¶ã¯ã‚¢ã‚¤ãƒ†ãƒ ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ãŸãŒè³¼å…¥ã—ãªã‹ã£ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã€ãã®çµæœã€è£œåŠ©è¡Œå‹•ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¡Œå‹•ã§æ­£ã¨è² ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åˆ†å¸ƒãŒç•°ãªã‚‹ã€‚
Existing work typically learns separate user preferences for different types of behavior and then combines those preferences to obtain an aggregate user representation.
æ—¢å­˜ã®ç ”ç©¶ã§ã¯ã€é€šå¸¸ã€ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®è¡Œå‹•ã«å¯¾ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã‚’å€‹åˆ¥ã«å­¦ç¿’ã—ã€æ¬¡ã«ãã‚Œã‚‰ã®å—œå¥½ã‚’çµ„ã¿åˆã‚ã›ã¦é›†ç´„çš„ãªãƒ¦ãƒ¼ã‚¶è¡¨ç¾ã‚’å¾—ã‚‹ã€‚
We argue that:
æˆ‘ã€…ã¯æ¬¡ã®ã‚ˆã†ã«ä¸»å¼µã™ã‚‹ï¼š
(i) user preferences learned separately based on different types of behavior may not consistently lead to the true user preferences,
(i)ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®è¡Œå‹•ã«åŸºã¥ã„ã¦åˆ¥ã€…ã«å­¦ç¿’ã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶å—œå¥½ã¯ã€ä¸€è²«ã—ã¦çœŸã®ãƒ¦ãƒ¼ã‚¶å—œå¥½ã«ã¤ãªãŒã‚‰ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ã€
and (ii) multiple types of user behavior should reflect similar user preferences; in other words, there should be an underlying universal set of user preferences under different types of behavior of the same user.
(ii)è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã¯ã€é¡ä¼¼ã—ãŸãƒ¦ãƒ¼ã‚¶å—œå¥½ã‚’åæ˜ ã™ã‚‹ã¯ãšã§ã‚ã‚‹ã€è¨€ã„æ›ãˆã‚Œã°ã€åŒã˜ãƒ¦ãƒ¼ã‚¶ã®ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®è¡Œå‹•ã®ä¸‹ã§ã€æ ¹åº•ã«æ™®éçš„ãªãƒ¦ãƒ¼ã‚¶å—œå¥½ã®ã‚»ãƒƒãƒˆãŒå­˜åœ¨ã™ã‚‹ã¯ãšã§ã‚ã‚‹ã€‚

The second limitation concerns the presence of noise and bias in auxiliary behavioral data, which impacts knowledge extraction and transfer.
2ã¤ç›®ã®é™ç•Œã¯ã€è£œåŠ©çš„ãªè¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã«ãƒã‚¤ã‚ºã‚„ãƒã‚¤ã‚¢ã‚¹ãŒå­˜åœ¨ã—ã€ãã‚ŒãŒçŸ¥è­˜ã®æŠ½å‡ºã‚„ä¼é”ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
A basic assumption of recommendations based on implicit feedback is that observed interactions between users and items reflect positive user preferences, while unobserved interactions are considered negative training instances.
æš—é»™ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ããƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®åŸºæœ¬çš„ãªä»®å®šã¯ã€ãƒ¦ãƒ¼ã‚¶ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®é–“ã®è¦³å¯Ÿã•ã‚ŒãŸç›¸äº’ä½œç”¨ã¯ãƒã‚¸ãƒ†ã‚£ãƒ–ãªãƒ¦ãƒ¼ã‚¶ã®å¥½ã¿ã‚’åæ˜ ã—ã€è¦³å¯Ÿã•ã‚Œã¦ã„ãªã„ç›¸äº’ä½œç”¨ã¯ãƒã‚¬ãƒ†ã‚£ãƒ–ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¨ã¿ãªã•ã‚Œã‚‹ã¨ã„ã†ã“ã¨ã§ã‚ã‚‹ã€‚
However, this assumption seldom holds in reality.
ã—ã‹ã—ã€ã“ã®ä»®å®šãŒç¾å®Ÿã«æˆã‚Šç«‹ã¤ã“ã¨ã¯ã‚ã£ãŸã«ãªã„ã€‚
A click may be triggered by popularity bias [5], which does not reflect a positive preference.
ã‚¯ãƒªãƒƒã‚¯ã¯äººæ°—ãƒã‚¤ã‚¢ã‚¹[5]ã«ã‚ˆã£ã¦å¼•ãèµ·ã“ã•ã‚Œã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚
And an unobserved interaction may be attributed to a lack of exposure [6].
ã¾ãŸã€è¦³å¯Ÿã•ã‚Œãªã„ç›¸äº’ä½œç”¨ã¯ã€exposure(éœ²å‡º)ã®ä¸è¶³ã«èµ·å› ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹[6]ã€‚
Hence, simply incorporating noisy or biased behavioral data may lead to sub-optimal recommendation performance.
ã—ãŸãŒã£ã¦ã€å˜ã«ãƒã‚¤ã‚ºã®å¤šã„ã€ã‚ã‚‹ã„ã¯åã£ãŸè¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šå…¥ã‚Œã‚‹ã ã‘ã§ã¯ã€æ¨è–¦ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒæœ€é©åŒ–ã•ã‚Œãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

### 1.0.3. Motivation.

![figure1]()

(è«–æ–‡ã‚’èª­ã‚“ã æ„Ÿã˜ã§ã¯ã“ã®æ•£å¸ƒå›³ã¯ã€CFã§å¾—ã‚‰ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ åŸ‹ã‚è¾¼ã¿ã‚’2æ¬¡å…ƒç©ºé–“ã«æŠ•å½±ã—ãŸã‚‚ã®ã€‚è‡ªç¤¾ãƒ‡ãƒ¼ã‚¿ã§CBã£ã½ã„è¨˜äº‹åŸ‹ã‚è¾¼ã¿ã§ã‚‚è©¦ã›ã‚‹ã‹ã‚‚...!:thinking:)

Our assumption is that multiple types of behavior from the same user (e.g., clicks and purchases) should reflect similar preferences of that user.
ç§ãŸã¡ã®ä»®å®šã¯ã€**åŒã˜ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®è¡Œå‹•ï¼ˆä¾‹ãˆã°ã€ã‚¯ãƒªãƒƒã‚¯ã¨è³¼å…¥ï¼‰ã¯ã€ãã®ãƒ¦ãƒ¼ã‚¶ã®åŒæ§˜ã®å—œå¥½ã‚’åæ˜ ã™ã‚‹ã¯ãšã§ã‚ã‚‹**ã¨ã„ã†ã“ã¨ã§ã™ã€‚
To illustrate this assumption, consider Figure 1, which shows distributions of items that two users (ğ‘¢1 and ğ‘¢2) interacted with (clicks ğ‘ and purchases ğ‘), in the Beibei and Taobao datasets (described in Section 4.2 below).
ã“ã®ä»®å®šã‚’èª¬æ˜ã™ã‚‹ãŸã‚ã«ã€2äººã®ãƒ¦ãƒ¼ã‚¶($u_1$ ã¨ $u_2$)ãŒç›¸äº’ä½œç”¨ã—ãŸã‚¢ã‚¤ãƒ†ãƒ (clicks $c$ ã¨ è³¼å…¥ $p$)ã®åˆ†å¸ƒã‚’ç¤ºã™å›³1ã‚’è€ƒãˆã¦ã¿ã‚ˆã†ã€‚
For both users, the items they clicked or purchased are relatively close.
ã©ã¡ã‚‰ã®ãƒ¦ãƒ¼ã‚¶ã«ã¨ã£ã¦ã‚‚ã€**ã‚¯ãƒªãƒƒã‚¯ã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã‚„è³¼å…¥ã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã¯æ¯”è¼ƒçš„è¿‘ã„**ã€‚
These observations motivate our hypothesis that multiple types of user behavior reflect similar user preferences, which is vital to improve the recommendation performance further.
ã“ã‚Œã‚‰ã®è¦³å¯Ÿçµæœã¯ã€è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ãŒé¡ä¼¼ã—ãŸãƒ¦ãƒ¼ã‚¶å—œå¥½ã‚’åæ˜ ã—ã¦ã„ã‚‹ã¨ã„ã†æˆ‘ã€…ã®ä»®èª¬ã‚’å‹•æ©Ÿã¥ã‘ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€æ¨è–¦æ€§èƒ½ã‚’ã•ã‚‰ã«å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ä¸å¯æ¬ ã§ã‚ã‚‹ã€‚

### 1.0.4. Proposed method.

To address the problem of learning from multiple types of auxiliary behavioral data and improve the prediction of the target behavior (and hence recommendation performance), we propose a training framework called multi-behavior alignment (MBA).
è¤‡æ•°ç¨®é¡ã®autxiliary behaviorãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ã€target behaviorã®äºˆæ¸¬(ã²ã„ã¦ã¯æ¨è–¦æ€§èƒ½)ã‚’å‘ä¸Šã•ã›ã‚‹ã¨ã„ã†å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€æˆ‘ã€…ã¯**multi-behavior alignment(MBA)ã¨å‘¼ã°ã‚Œã‚‹å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**ã‚’ææ¡ˆã™ã‚‹ã€‚
MBA aligns user preferences learned from different types of behavior.
MBAã¯ã€ã•ã¾ã–ã¾ãªã‚¿ã‚¤ãƒ—ã®è¡Œå‹•ã‹ã‚‰å­¦ã‚“ã ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã‚’èª¿æ•´ã™ã‚‹ã€‚
The key assumption behind MBA is that multiple types of behavior from the same user reflect similar underlying user preferences.
MBAã®èƒŒå¾Œã«ã‚ã‚‹é‡è¦ãªä»®å®šã¯ã€åŒã˜ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®è¡Œå‹•ã¯ã€åŒæ§˜ã®æ ¹æœ¬çš„ãªãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã‚’åæ˜ ã—ã¦ã„ã‚‹ã¨ã„ã†ã“ã¨ã§ã‚ã‚‹ã€‚

To address the data distribution limitation mentioned above, we utilize KL-divergence to measure the discrepancy between user models learned from multiple types of auxiliary behavior and target behavior, and then conduct knowledge transfer by minimizing this discrepancy to improve the recommendation performance.
ä¸Šè¨˜ã®ã‚ˆã†ãªãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®åˆ¶ç´„(=auxiliary behaviorã¯ãƒã‚¤ã‚ºã¨ãƒã‚¤ã‚¢ã‚¹ãŒå¤§ãã„, target behaviorã¯ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ãŒé«˜ã„ã€ã£ã¦è©±?:thinking:)ã«å¯¾ã—ã¦ã€KL-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’åˆ©ç”¨ã—ã€è¤‡æ•°ç¨®é¡ã®è£œåŠ©è¡Œå‹•ã‹ã‚‰å­¦ç¿’ã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¢ãƒ‡ãƒ«ã¨å¯¾è±¡è¡Œå‹•(ã‹ã‚‰å­¦ç¿’ã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¢ãƒ‡ãƒ«?)ã¨ã®ä¸ä¸€è‡´åº¦ã‚’æ¸¬å®šã—ã€ã“ã®ä¸ä¸€è‡´åº¦ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã§çŸ¥è­˜ç§»è»¢ã‚’è¡Œã„ã€æ¨è–¦æ€§èƒ½ã®å‘ä¸Šã‚’å›³ã‚‹ã€‚

For the second limitation mentioned above (concerning noise and bias in behavioral data), MBA regards the underlying universal user preferences as a latent variable.
å‰è¿°ã®2ã¤ç›®ã®é™ç•Œ(è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã«ãŠã‘ã‚‹ãƒã‚¤ã‚ºã¨ãƒã‚¤ã‚¢ã‚¹ã«é–¢ã—ã¦)ã«ã¤ã„ã¦ã¯ã€MBAã¯ã€æ ¹åº•ã«ã‚ã‚‹æ™®éçš„ãªãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã‚’æ½œåœ¨å¤‰æ•°ã¨ã¿ãªã—ã¦ã„ã‚‹ã€‚
The variable is then inferred by maximizing the likelihood of multiple types of observed behavioral data while minimizing the discrepancy between models trained on different types of behavioral data.
ãã—ã¦ã€è¦³æ¸¬ã•ã‚ŒãŸè¤‡æ•°ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã®å°¤åº¦ã‚’æœ€å¤§åŒ–ã—ã¤ã¤ã€**ç•°ãªã‚‹è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«é–“ã®ä¸ä¸€è‡´ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨**(=çœŸã®å—œå¥½ãŒã€å„è¡Œå‹•ã®ä¸­é–“ã«ã‚ã‚‹ã‚ˆã†ãªæƒ³å®š??:thinking:)ã§ã€å¤‰æ•°ã‚’æ¨è«–ã™ã‚‹ã€‚
In this manner, MBA denoises multiple types of behavioral data and enables more effective knowledge transfer across multiple types of user behavior.
ã“ã®ã‚ˆã†ã«ã€MBAã¯è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚¤ã‚ºé™¤å»ã—ã€è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã«ã‚ãŸã£ã¦ã‚ˆã‚ŠåŠ¹æœçš„ãªçŸ¥è­˜ä¼é”ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚

To demonstrate the effectiveness of the proposed method, we conduct extensive experiments on two open benchmark datasets and one dataset collected from an operational e-commerce platform.
ææ¡ˆæ‰‹æ³•ã®æœ‰åŠ¹æ€§ã‚’å®Ÿè¨¼ã™ã‚‹ãŸã‚ã€2ã¤ã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã€é‹ç”¨ä¸­ã®eã‚³ãƒãƒ¼ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‹ã‚‰åé›†ã—ãŸ1ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å¤§è¦æ¨¡ãªå®Ÿé¨“ã‚’è¡Œã£ãŸã€‚
Experimental results show that the proposed MBA framework outperforms related state-of-the-art baselines.
å®Ÿé¨“çµæœã¯ã€ææ¡ˆã™ã‚‹MBAãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒã€é–¢é€£ã™ã‚‹æœ€æ–°ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

### 1.0.5. Main contributions.

Our main contributions are as follows:
æˆ‘ã€…ã®ä¸»ãªè²¢çŒ®ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã‚ã‚‹ï¼š

- We argue that multiple types of auxiliary and target behavior should reflect similar user preferences, and we propose to infer the true user preferences from multiple types of behavioral data. æˆ‘ã€…ã¯ã€è¤‡æ•°ç¨®é¡ã®è£œåŠ©è¡Œå‹•ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¡Œå‹•ã¯é¡ä¼¼ã—ãŸãƒ¦ãƒ¼ã‚¶å—œå¥½ã‚’åæ˜ ã™ã‚‹ã¯ãšã§ã‚ã‚‹ã¨ä¸»å¼µã—ã€è¤‡æ•°ç¨®é¡ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çœŸã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å—œå¥½ã‚’æ¨è«–ã™ã‚‹ã“ã¨ã‚’ææ¡ˆã™ã‚‹ã€‚

- We propose a learning framework MBA to jointly perform data denoising and knowledge transfer across multiple types of behavioral data to enhance target behavior prediction and hence improve the recommendation performance.æœ¬ç ”ç©¶ã§ã¯ã€è¤‡æ•°ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ã‚ºé™¤å»ã¨çŸ¥è­˜ä¼é”ã‚’å…±åŒã§è¡Œã†ã“ã¨ã§ã€å¯¾è±¡è¡Œå‹•ã®äºˆæ¸¬ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã€ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯MBAã‚’ææ¡ˆã™ã‚‹ã€‚

- We conduct experiments on three datasets to demonstrate the effectiveness of the MBA method. MBAæ³•ã®æœ‰åŠ¹æ€§ã‚’å®Ÿè¨¼ã™ã‚‹ãŸã‚ã€3ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å®Ÿé¨“ã‚’è¡Œã£ãŸã€‚One of these datasets is collected from an operational e-commerce platform, and includes clicks and purchase behavior data.ã“ã‚Œã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®1ã¤ã¯ã€é‹ç”¨ä¸­ã®eã‚³ãƒãƒ¼ã‚¹ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‹ã‚‰åé›†ã•ã‚ŒãŸã‚‚ã®ã§ã€ã‚¯ãƒªãƒƒã‚¯æ•°ã¨è³¼è²·è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã€‚Experimental results show **state-of-the-art** recommendation performance of the proposed MBA method.å®Ÿé¨“çµæœã¯ã€ææ¡ˆã•ã‚ŒãŸMBAæ‰‹æ³•ã®æœ€å…ˆç«¯ã®æ¨è–¦æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã  -->

# 2. Related Work é–¢é€£ä½œå“

We review prior work on multi-behavior recommendation and on denoising methods for recommendation from implicit feedback.
æœ¬ç¨¿ã§ã¯ã€multi-behavioæ¨è–¦ã«é–¢ã™ã‚‹å…ˆè¡Œç ”ç©¶ã¨ã€æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ã®æ¨è–¦ã®ãŸã‚ã®ãƒã‚¤ã‚ºé™¤å»æ³•ã«é–¢ã™ã‚‹å…ˆè¡Œç ”ç©¶ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹ã€‚(ä¸¡æ–¹ã¨ã‚‚å®Ÿå‹™çš„ã«èˆˆå‘³ã‚ã‚‹...!)

## 2.1. Multi-behavior recommendation

Unlike conventional implicit feedback recommendation models [15, 21], which train a recommender on a single type of user behavior (e.g., clicks), multi-behavior recommendation models use multiple types of auxiliary behavior data to enhance the recommendation performance on target behavior [1, 7, 12, 18, 33, 37, 39].
å¾“æ¥ã®æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ¨è–¦ãƒ¢ãƒ‡ãƒ«[15, 21]ãŒã€å˜ä¸€ã®ã‚¿ã‚¤ãƒ—ã®ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ï¼ˆä¾‹ãˆã°ã‚¯ãƒªãƒƒã‚¯ï¼‰ã«åŸºã¥ã„ã¦æ¨è–¦è€…ã‚’è¨“ç·´ã™ã‚‹ã®ã¨ã¯ç•°ãªã‚Šã€è¤‡æ•°è¡Œå‹•æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã¯ã€è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®è£œåŠ©è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¡Œå‹•ã«é–¢ã™ã‚‹æ¨è–¦æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹[1, 7, 12, 18, 33, 37, 39]ã€‚
Recent studies use multi-task learning to perform joint optimization on learning auxiliary behavior and target behavior.
æœ€è¿‘ã®ç ”ç©¶ã§ã¯ã€**ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã‚’ç”¨ã„ã¦ã€auxiliary behaviorã¨target behaviorã®å­¦ç¿’ã«é–¢ã™ã‚‹å…±åŒæœ€é©åŒ–**ã‚’è¡Œã†ã€‚
For example, Gao et al.[12] propose a multi-task learning framework to learn user preferences from multi-behavior data based on a pre-defined relationship between different behavior.
ä¾‹ãˆã°ã€Gaoã‚‰[12]ã¯ã€ç•°ãªã‚‹è¡Œå‹•é–“ã®äº‹å‰ã«å®šç¾©ã•ã‚ŒãŸé–¢ä¿‚(=behaviorã®ç¨®é¡ã”ã¨ã®é‡ã¿ä»˜ã‘ã¨ã‹??)ã«åŸºã¥ã„ã¦ã€è¤‡æ•°ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã‚’å­¦ç¿’ã™ã‚‹ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚
Since different behavioral interactions between users and items can form a heterogeneous graph, recent studies also focus on using graph neural network (GNN) to mine the correlations among different types of behavior.
ãƒ¦ãƒ¼ã‚¶ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®é–“ã®ç•°ãªã‚‹è¡Œå‹•ã®ç›¸äº’ä½œç”¨ã¯ã€ç•°ç¨®ã‚°ãƒ©ãƒ•ã‚’å½¢æˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ãŸã‚ã€æœ€è¿‘ã®ç ”ç©¶ã§ã¯ã€ã‚°ãƒ©ãƒ•ãƒ»ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯(GNN)ã‚’ä½¿ç”¨ã—ã¦ã€ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®è¡Œå‹•é–“ã®ç›¸é–¢é–¢ä¿‚ã‚’ãƒã‚¤ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã«ã‚‚ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã‚‹ã€‚
For example, Wang et al.[33] uses the auxiliary behavior data to build global item-to-item relations and further improve the recommendation performance of target behavior.
ä¾‹ãˆã°ã€Wangã‚‰[33]ã¯ã€è£œåŠ©è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦å¤§åŸŸçš„ãªã‚¢ã‚¤ãƒ†ãƒ é–“é–¢ä¿‚ã‚’æ§‹ç¯‰ã—ã€å¯¾è±¡è¡Œå‹•ã®æ¨è–¦æ€§èƒ½ã‚’ã•ã‚‰ã«å‘ä¸Šã•ã›ã¦ã„ã‚‹ã€‚
Jin et al.[18] propose a graph convolutional network (GCN) based model on capturing the diverse influence of different types of behavior and the various semantics of different types of behavior.
Jinã‚‰[18]ã¯ã€ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®è¡Œå‹•ã®å¤šæ§˜ãªå½±éŸ¿ã¨ã€ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®è¡Œå‹•ã®å¤šæ§˜ãªã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹ã‚’æ‰ãˆã‚‹ä¸Šã§ã€ã‚°ãƒ©ãƒ•ç•³ã¿è¾¼ã¿ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆGCNï¼‰ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚
Xia et al.[39] incorporate multi-behavior signals through graph-based meta-learning.
Xiaã‚‰[39]ã¯ã€ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹ã®ãƒ¡ã‚¿å­¦ç¿’ã«ã‚ˆã£ã¦ã€è¤‡æ•°ã®è¡Œå‹•ã‚·ã‚°ãƒŠãƒ«ã‚’çµ„ã¿è¾¼ã‚“ã§ã„ã‚‹ã€‚
Chen et al.[1] regard the multi-behavior recommendation task as a multirelationship prediction task and train the recommender with an efficient non-sampling method.
Chenã‚‰[1]ã¯ã€multi-behavioræ¨è–¦ã‚¿ã‚¹ã‚¯ã‚’å¤šé–¢ä¿‚äºˆæ¸¬ã‚¿ã‚¹ã‚¯(?)ã¨ã¿ãªã—ã€åŠ¹ç‡çš„ãªéã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ³•ã‚’ç”¨ã„ã¦æ¨è–¦å™¨ã‚’å­¦ç¿’ã™ã‚‹ã€‚
Additionally, some studies apply contrastive learning or a variational autoencoder (VAE) to improve the multi-behavior recommender.
ã•ã‚‰ã«ã€å¯¾ç…§å­¦ç¿’ã‚„å¤‰åˆ†ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆVAEï¼‰ã‚’é©ç”¨ã—ã¦ã€è¤‡æ•°è¡Œå‹•ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚’æ”¹å–„ã™ã‚‹ç ”ç©¶ã‚‚ã‚ã‚‹ã€‚
Xuan et al.[42] propose a knowledge graph enhanced contrastive learning framework to capture multi-behavioral dependencies better and solve the data sparsity problem of the target behavior, and Ma et al.[24] propose a VAEbased model to conduct multi-behavior recommendation.
Xuanã‚‰[42]ã¯ã€è¤‡æ•°è¡Œå‹•ã®ä¾å­˜é–¢ä¿‚ã‚’ã‚ˆã‚Šã‚ˆãæ‰ãˆã€å¯¾è±¡è¡Œå‹•ã®ãƒ‡ãƒ¼ã‚¿ç–æ€§ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€**çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚’å¼·åŒ–ã—ãŸcontractive learningãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆ**ã—ã¦ãŠã‚Šã€Maã‚‰[24]ã¯ã€è¤‡æ•°è¡Œå‹•ã®æ¨è–¦ã‚’è¡Œã†ãŸã‚ã«VAEãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚

Another related research field is based on micro-behaviors [25, 44, 46], which utilize the micro-operation sequence in the process of user-item interactions to capture user preferences and predict the next item.
**åˆ¥ã®é–¢é€£ã™ã‚‹ç ”ç©¶åˆ†é‡ã¯ã€micro-behaviors(?)ã«åŸºã¥ãã‚‚ã®[25, 44, 46]**ã§ã€ãƒ¦ãƒ¼ã‚¶ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®ç›¸äº’ä½œç”¨ã®éç¨‹ã«ãŠã‘ã‚‹ micro-operationã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆ©ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã‚’æŠŠæ¡ã—ã€next-itemã‚’äºˆæ¸¬ã™ã‚‹ã‚‚ã®ã§ã‚ã‚‹ã€‚
For example, Yuan et al.[44] focus on â€œsequential patternsâ€ and â€œdyadic relational patternsâ€ in micro-behaviors, and then use an extended self-attention network to mine the relationship between micro-behavior and user preferences.
ä¾‹ãˆã°ã€Yuanã‚‰[44]ã¯ã€ãƒŸã‚¯ãƒ­è¡Œå‹•ã«ãŠã‘ã‚‹ã€Œé€æ¬¡çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã¨ã€Œãƒ€ã‚¤ã‚¢ãƒ‰çš„é–¢ä¿‚ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã«æ³¨ç›®ã—ã€æ‹¡å¼µè‡ªå·±æ³¨æ„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ã¦ã€micro-behaviorsã¨ãƒ¦ãƒ¼ã‚¶å—œå¥½ã®é–¢ä¿‚ã‚’ãƒã‚¤ãƒ‹ãƒ³ã‚°ã—ã¦ã„ã‚‹ã€‚
This work focuses on mining user preferences from the micro-operation sequence.
ã“ã®ç ”ç©¶ã§ã¯ã€ãƒã‚¤ã‚¯ãƒ­æ“ä½œã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã‚’ãƒã‚¤ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã‚‹ã€‚

However, existing studies still neglect the different data distributions across multiple types of user behavior, and thus fail to learn accurate and universal user preferences.
ã—ã‹ã—ã€æ—¢å­˜ã®ç ”ç©¶ã§ã¯ã€è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã«ã‚ãŸã‚‹ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒãŒä¾ç„¶ã¨ã—ã¦ç„¡è¦–ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€æ­£ç¢ºã§æ™®éçš„ãªãƒ¦ãƒ¼ã‚¶å—œå¥½ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ããªã„ã€‚
Besides, prior work does not consider the noisy signals of user implicit feedback data, resulting in ineffective knowledge extraction and transfers.
ã•ã‚‰ã«ã€**å…ˆè¡Œç ”ç©¶ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ã®æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚¤ã‚ºä¿¡å·ã‚’è€ƒæ…®ã—ã¦ã„ãªã„ãŸã‚**ã€åŠ¹æœçš„ãªçŸ¥è­˜æŠ½å‡ºã¨è»¢é€ãŒã§ããªã„ã€‚

## 2.2. Recommendation denoising

Existing recommender systems are usually trained with implicit feedback since it is much easier to collect than explicit ratings [28].
æ—¢å­˜ã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¯ã€æ˜ç¤ºçš„ãªè©•ä¾¡ã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«åé›†ã—ã‚„ã™ã„ãŸã‚ã€é€šå¸¸ã€æš—é»™çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã§è¨“ç·´ã•ã‚Œã¦ã„ã‚‹[28]ã€‚
Recently, some research [17, 32, 36] has pointed out that implicit feedback can easily be corrupted by different factors, such as various kinds of bias [5] or usersâ€™ mistaken clicks.
æœ€è¿‘ã€ã„ãã¤ã‹ã®ç ”ç©¶[17, 32, 36]ã¯ã€**æš—é»™ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯ã€æ§˜ã€…ãªç¨®é¡ã®ãƒã‚¤ã‚¢ã‚¹[5]ã‚„ãƒ¦ãƒ¼ã‚¶ã®èª¤ã‚¯ãƒªãƒƒã‚¯ãªã©ã€æ§˜ã€…ãªè¦å› ã«ã‚ˆã£ã¦å®¹æ˜“ã«ç ´æã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹**ã“ã¨ã‚’æŒ‡æ‘˜ã—ã¦ã„ã‚‹ã€‚
Therefore, there have been efforts aimed at alleviating the noisy problem of implicit recommendation.
ãã®ãŸã‚ã€**implicit recommendation**ã¨ã„ã†ãƒã‚¤ã‚¸ãƒ¼ãªå•é¡Œã‚’ç·©å’Œã™ã‚‹ãŸã‚ã®åŠªåŠ›ãŒãªã•ã‚Œã¦ããŸã€‚
These efforts include sample selection methods [8â€“ 11, 36, 43], re-weighting methods [3, 30, 32, 32, 35], methods using additional information [19, 22, 45], and methods designing specific denoising architectures [4, 13, 38, 40].
ã“ã‚Œã‚‰ã®å–ã‚Šçµ„ã¿ã«ã¯ã€ã‚µãƒ³ãƒ—ãƒ«é¸æŠæ³•[8-11, 36, 43](=ã“ã‚Œã¯MNARãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹under samplingæ‰‹æ³•ã‚‚å«ã¾ã‚Œã‚‹??:thinking:)ã€å†é‡ã¿ä»˜ã‘æ³•[3, 30, 32, 35] (=ã“ã‚Œã¯IPSé‡ã¿ä»˜ã‘ã¨ã‹ãŒå«ã¾ã‚Œã‚‹??:thinking:)ã€è¿½åŠ æƒ…å ±ã‚’åˆ©ç”¨ã™ã‚‹æ–¹æ³•[19, 22, 45]ã€ç‰¹å®šã®ãƒã‚¤ã‚ºé™¤å»ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è¨­è¨ˆã™ã‚‹æ–¹æ³•[4, 13, 38, 40](=ã“ã‚Œã¯denoising sequential recommenderçš„ãªã‚„ã¤??:thinking:)ãªã©ãŒã‚ã‚‹ã€‚

Sample selection methods aim to design more effective samplers for model training.
ã‚µãƒ³ãƒ—ãƒ«é¸æŠæ³•ã¯ã€ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã«ã€ã‚ˆã‚ŠåŠ¹æœçš„ãªã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã‚‹ã€‚
For example, Gantner et al.[11] consider popular but un-interacted items as items that are highly likely to be negative ones, while Ding et al.[8] consider clicked but not purchased items as likely to be negative samples.
ä¾‹ãˆã°ã€Gantnerã‚‰[11]ã¯ã€äººæ°—ãŒã‚ã‚‹ãŒã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã®ãªã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒã‚¬ãƒ†ã‚£ãƒ–ãªã‚¢ã‚¤ãƒ†ãƒ ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã‚¢ã‚¤ãƒ†ãƒ ã¨ã—ã¦ãŠã‚Šã€Dingã‚‰[8]ã¯ã€ã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸãŒè³¼å…¥ã•ã‚Œãªã‹ã£ãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒã‚¬ãƒ†ã‚£ãƒ–ãªã‚µãƒ³ãƒ—ãƒ«ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã‚¢ã‚¤ãƒ†ãƒ ã¨ã—ã¦ã„ã‚‹ã€‚

Re-weighting methods typically identify noisy samples as instances with higher loss values and then assign lower weights to them.
å†é‡ã¿ä»˜ã‘æ³•ã¯ä¸€èˆ¬çš„ã«ã€ãƒã‚¤ã‚ºã®å¤šã„ã‚µãƒ³ãƒ—ãƒ«ã‚’æå¤±å€¤ã®é«˜ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¨ã—ã¦è­˜åˆ¥ã—ã€ãã‚Œã‚‰ã«ä½ã„é‡ã¿ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã€‚
For example, Wang et al.[32] discard the large-loss samples with a dynamic threshold in each iteration.
ä¾‹ãˆã°ã€Wangã‚‰[32]ã¯ã€å„åå¾©ã«ãŠã„ã¦ã€å‹•çš„é–¾å€¤ã§å¤§æå¤±ã‚µãƒ³ãƒ—ãƒ«ã‚’ç ´æ£„ã—ã¦ã„ã‚‹ã€‚
Wang et al.[35] utilize the differences between model predictions as the denoising signals.
Wangã‚‰[35]ã¯ã€ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬é–“ã®å·®ã‚’ãƒã‚¤ã‚ºé™¤å»ä¿¡å·ã¨ã—ã¦åˆ©ç”¨ã—ã¦ã„ã‚‹ã€‚

Additional information such as dwell time [19], gaze pattern [45] and auxiliary item features [22] can also be used to denoise implicit feedback.
æ»ç•™æ™‚é–“[19]ã€è¦–ç·šãƒ‘ã‚¿ãƒ¼ãƒ³[45]ã€è£œåŠ©é …ç›®ç‰¹å¾´[22]ãªã©ã®è¿½åŠ æƒ…å ±ã‚‚ã€æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ãƒã‚¤ã‚ºé™¤å»ã«ä½¿ç”¨ã§ãã‚‹ã€‚

Methods designing specific denoising architectures improve the robustness of recommender systems by designing special modules.
ç‰¹å®šã®ãƒã‚¤ã‚ºé™¤å»ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è¨­è¨ˆã™ã‚‹æ–¹æ³•ã¯ã€ç‰¹åˆ¥ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ã§æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®é ‘å¥æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚
Wu et al.[38] use self-supervised learning on user-item interaction graphs to improve the robustness of graph-based recommendation models.
Wuã‚‰[38]ã¯ã€ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®é ‘å¥æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®ç›¸äº’ä½œç”¨ã‚°ãƒ©ãƒ•ã®è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã€‚
Gao et al.[13] utilize the self-labeled memorized data as denoising signals to improve the robustness of recommendation models.
Gaoã‚‰[13]ã¯ã€æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®é ‘å¥æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€è‡ªå·±ãƒ©ãƒ™ãƒ«åŒ–ã•ã‚ŒãŸè¨˜æ†¶ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚¤ã‚ºé™¤å»ä¿¡å·ã¨ã—ã¦åˆ©ç”¨ã—ã¦ã„ã‚‹ã€‚

Unlike the work listed above, which does not consider multiple types of user behavior, in this work, we focus on extracting underlying user preferences from (potentially) corrupted multi-behavior data and then conducting knowledge transfer to improve the recommendation performance.
è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã‚’è€ƒæ…®ã—ãªã„ä¸Šè¨˜ã®ç ”ç©¶ã¨ã¯ç•°ãªã‚Šã€ã“ã®ç ”ç©¶ã§ã¯ã€**(æ½œåœ¨çš„ã«)ç ´æã—ãŸè¤‡æ•°ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ ¹æœ¬çš„ãªãƒ¦ãƒ¼ã‚¶å—œå¥½ã‚’æŠ½å‡º**ã—ã€æ¨è–¦æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«çŸ¥è­˜ç§»è»¢ã‚’è¡Œã†ã“ã¨ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

# 3. Method ãƒ¡ã‚½ãƒƒãƒ‰

In this section, we detail our proposed MBA framework for multibehavior recommendation.
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ç§ãŸã¡ãŒææ¡ˆã™ã‚‹ãƒãƒ«ãƒè¡Œå‹•æ¨è–¦ã®ãŸã‚ã®MBAãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã™ã‚‹ã€‚
We first introduce notations and the problem formulation in Section 3.1.After that, we describe how to perform multi-behavior alignment on noisy implicit feedback in Section 3.2.Finally, training details are given in Section 3.3.
ãã®å¾Œã€3.2ç¯€ã§ãƒã‚¤ã‚ºã®å¤šã„æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«å¯¾ã—ã¦è¤‡æ•°è¡Œå‹•ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’è¡Œã†æ–¹æ³•ã‚’èª¬æ˜ã—ã€æœ€å¾Œã«3.3ç¯€ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®è©³ç´°ã‚’è¿°ã¹ã‚‹ã€‚

## 3.1. Notation and problem formulation

We write $u \in U$ and $i \in I$ for a user and an item, where U and I indicate the user set and the item set, respectively.
Uã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼é›†åˆã€Iã¯ã‚¢ã‚¤ãƒ†ãƒ é›†åˆã‚’è¡¨ã™ã€‚
Without loss of generality, we regard click behavior as the auxiliary behavior and purchase behavior as the target behavior.
ä¸€èˆ¬æ€§ã‚’æãªã‚ãªã„ç¯„å›²ã§ã€**ã‚¯ãƒªãƒƒã‚¯è¡Œå‹•ã‚’auxiliary behaviorã¨ã—ã€è³¼è²·è¡Œå‹•ã‚’target behaviorã¨ã™ã‚‹**ã€‚
We write $R_f \in \mathbb{R}^{|U| \times |I|}$ for the observed purchase behavior data.
ã“ã“ã§ã€è¦³å¯Ÿã•ã‚ŒãŸè³¼è²·è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‚’ $R_f \in \mathbb{R}^{|U| \times |I|}$ ã¨è¡¨è¨˜ã™ã‚‹ã€‚
Specifically, each item ğ‘Ÿ ğ‘“ ğ‘¢,ğ‘– âˆˆ Rğ‘“ is set to 1 if there is a purchase behavior between user ğ‘¢ and item ğ‘–; otherwise ğ‘Ÿ ğ‘“ ğ‘¢,ğ‘– is set as 0.
å…·ä½“çš„ã«ã¯ã€å„ã‚¢ã‚¤ãƒ†ãƒ  $r^{f}_{u,i}$ ã¯ã€ãƒ¦ãƒ¼ã‚¶ $u$ ã¨ã‚¢ã‚¤ãƒ†ãƒ  $i$ ã®é–“ã«è³¼è²·è¡Œå‹•ãŒã‚ã‚Œã°1ã¨ã•ã‚Œã€ãã†ã§ãªã‘ã‚Œã°0ã¨ã•ã‚Œã‚‹ã€‚
Similarly, we denote Rğ‘” âˆˆ R |U |Ã— |I | as the observed click behavior data, where each ğ‘Ÿ ğ‘” ğ‘¢,ğ‘– âˆˆ Rğ‘” is set as 1 if there is a click behavior between user ğ‘¢ and item ğ‘–; otherwise ğ‘Ÿ ğ‘” ğ‘¢,ğ‘– = 0.
åŒæ§˜ã«ã€$R_g \in \mathbb{R}^{|U| \times |I|}$ ã‚’è¦³æ¸¬ã•ã‚ŒãŸã‚¯ãƒªãƒƒã‚¯è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã¨ã™ã‚‹ã€‚
We use ğ‘ƒ (Rğ‘“ ) and ğ‘ƒ (Rğ‘”) to denote the user preference distribution learned from Rğ‘“ and Rğ‘”, respectively.
$R_f$ ã¨ $R_g$ ã‹ã‚‰å­¦ç¿’ã•ã‚ŒãŸ**ãƒ¦ãƒ¼ã‚¶å—œå¥½åˆ†å¸ƒ**ã‚’è¡¨ã™ãŸã‚ã«ã€ãã‚Œãã‚Œ $P(R_f)$ ã¨ $P(R_g)$ ã‚’ä½¿ç”¨ã™ã‚‹ã€‚

We assume that there is an underlying latent true user preference matrix $R_t$ with $r^{t}_{u,i}$ as the true preference of user ğ‘¢ over item ğ‘–.
$R_t$ ã‚’æ½œåœ¨çš„ãªçœŸã®ãƒ¦ãƒ¼ã‚¶å—œå¥½ã®è¡Œåˆ—ã¨ã™ã‚‹ã€‚
The probabilistic distribution of Rğ‘¡ is denoted as ğ‘ƒ (Rğ‘¡).
Rğ‘¡ã®ç¢ºç‡åˆ†å¸ƒã‚’ğ‘ƒ (Rğ‘¡)ã¨è¡¨ã™ã€‚
Both the data observation of Rğ‘“ and Rğ‘” is motivated by the latent universal true user preference distribution ğ‘ƒ (Rğ‘¡) plus different kinds of noises or biases.
**$R_f$ ã¨ $R_g$ ã®ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿è¦³æ¸¬ã¯ã€æ½œåœ¨çš„ã§æ™®éçš„ãªçœŸã®ãƒ¦ãƒ¼ã‚¶å—œå¥½åˆ†å¸ƒ $P(R_t)$ ã¨ã€ç•°ãªã‚‹ç¨®é¡ã®ãƒã‚¤ã‚ºã‚„ãƒã‚¤ã‚¢ã‚¹ã«ã‚ˆã£ã¦å‹•æ©Ÿã¥ã‘ã‚‰ã‚Œã¦ã„ã‚‹**ã€‚(ãµã‚€ãµã‚€...!)
Formally, we assume that ğ‘ƒ (Rğ‘¡) follows a Bernoulli distribution and can be approximated by a target recommender model ğ‘¡ğœƒ with ğœƒ as the parameters:
å½¢å¼çš„ã«ã¯ã€$P(R_t)$ ã¯ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤åˆ†å¸ƒã«å¾“ã†ã¨ä»®å®šã—ã€ $\theta$ ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã™ã‚‹ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¨è–¦ãƒ¢ãƒ‡ãƒ« $t_{\theta}$ ã§è¿‘ä¼¼ã§ãã‚‹:

$$
r^{t}_{u,i} \sim Bernoulli(t_{\theta}(u,i))
\tag{1}
$$

Since the true user preferences ğ‘Ÿ ğ‘¡ ğ‘¢,ğ‘– are intractable, we need to introduce the learning signals from the observed ğ‘Ÿ ğ‘“ ğ‘¢,ğ‘– and ğ‘Ÿ ğ‘” ğ‘¢,ğ‘– to infer ğ‘Ÿ ğ‘¡ ğ‘¢,ğ‘–.
çœŸã®ãƒ¦ãƒ¼ã‚¶å—œå¥½ $r^{t}$ ã¯å®Ÿè¡Œä¸å¯èƒ½ã§ã‚ã‚‹ãŸã‚ã€è¦³æ¸¬ã•ã‚ŒãŸ $r^{f}$ ã¨$r^{g}$ ã‹ã‚‰å­¦ç¿’ä¿¡å·ã‚’å°å…¥ã—ã¦ $r^{t}$ ã‚’æ¨è«–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚(æ·»å­—u, iã‚’çœç•¥ã—ãŸ)
As a result, we introduce the following models to depict the correlations between the observed user implicit feedback (i.e., ğ‘Ÿ ğ‘“ ğ‘¢,ğ‘– and ğ‘Ÿ ğ‘” ğ‘¢,ğ‘– ) and the latent true user preferences ğ‘Ÿ ğ‘¡ ğ‘¢,ğ‘–:
ãã®çµæœã€**è¦³æ¸¬ã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶æš—é»™ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯(i.e. $r^{f}$ ã¨ $r^{g}$) ã¨æ½œåœ¨çš„ãªçœŸã®ãƒ¦ãƒ¼ã‚¶å—œå¥½ $r^{t}$ ã¨ã®ç›¸é–¢ã‚’è¡¨ã™**ãŸã‚ã«ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’å°å…¥ã™ã‚‹:

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

$$
r^{f}_{u,i} | r^{t}_{u,i} = 0 \sim Bernouli(h^{f}_{\phi}(u,i)) \\
r^{f}_{u,i} | r^{t}_{u,i} = 1 \sim Bernouli(h^{f}_{\varphi}(u,i)) \\
r^{g}_{u,i} | r^{t}_{u,i} = 0 \sim Bernouli(h^{g}_{\phi'}(u,i)) \\
r^{g}_{u,i} | r^{t}_{u,i} = 1 \sim Bernouli(h^{g}_{\varphi'}(u,i)) \\
\tag{2}
$$

where $h^{f}_{\phi}(u,i)$ and $h^{f}_{\varphi}(u,i)$ are parameterized by ğœ™ and ğœ‘ in the observed purchase behavior data, respectively, while $h^{g}_{\phi'}(u,i)$ and $h^{g}_{\varphi'}(u,i)$ are parameterized by ğœ™ â€² and ğœ‘ â€² in the observed click behavior data respectively.
ã“ã“ã§ã€$h^{f}_{\phi}(u,i)$ ã¨ $h^{f}_{\varphi}(u,i)$ ã¯ã€ãã‚Œãã‚Œã€è¦³å¯Ÿã•ã‚ŒãŸè³¼è²·è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã«ã¦ğœ™ã¨ğœ‘ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚Œã‚‹ã€‚
ä¸€æ–¹ã€$h^{g}_{\phi'}(u,i)$ ã¨ $h^{g}_{\varphi'}(u,i)$ ã¯è¦³æ¸¬ã•ã‚ŒãŸã‚¯ãƒªãƒƒã‚¯è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã®ğœ™ã¨ğœ‘ã§ãã‚Œãã‚Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚Œã‚‹ã€‚

The target of our task is formulated as follows: given the observed multi-behavior user implicit feedback, i.e., Rğ‘“ and Rğ‘”, we aim to train the latent true user preference model ğ‘¡ğœƒ , and then use ğ‘¡ğœƒ to improve the prediction performance on target behavior.
æˆ‘ã€…ã®ã‚¿ã‚¹ã‚¯ã®ç›®æ¨™ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®šå¼åŒ–ã•ã‚Œã‚‹ï¼š è¦³å¯Ÿã•ã‚ŒãŸè¤‡æ•°è¡Œå‹•ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æš—é»™ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã€ã™ãªã‚ã¡ $R_f$ ã¨ $R_g$ ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€æ½œåœ¨çš„ãªçœŸã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å—œå¥½ãƒ¢ãƒ‡ãƒ« $t_{\theta}$ ã‚’è¨“ç·´ã—(=ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‡ºåŠ›ã™ã‚‹ãƒ¢ãƒ‡ãƒ«)ã€æ¬¡ã« $t_{\theta}$ ã‚’ç”¨ã„ã¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¡Œå‹•ã«é–¢ã™ã‚‹äºˆæ¸¬æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚
More precisely, during model inference, we introduce both ğ‘ƒ (Rf ) and ğ‘ƒ (Rt) to perform the target behavior recommendation and use a hyperparameter ğ›½ to balance the ğ‘ƒ (Rt) and ğ‘ƒ (Rf ), which is formulated as:
ã‚ˆã‚Šæ­£ç¢ºã«ã¯ã€ãƒ¢ãƒ‡ãƒ«æ¨è«–æ™‚ã«ã€targer behavioræ¨è–¦ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ç‚ºã« $P(R_f)$ ã¨ $P(R_t)$ ã®ä¸¡æ–¹ã‚’å°å…¥ã—ã€$P(R_f)$ ã¨ $P(R_t)$ã®ãƒãƒ©ãƒ³ã‚¹ã‚’ã¨ã‚‹ãŸã‚ã«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\beta$ ã‚’ä½¿ç”¨ã™ã‚‹ã€‚æ•°å¼ã«ã™ã‚‹ã¨ä»¥ä¸‹:

$$
score = \beta P(R_t) + (1 - \beta)P(R_f)
\tag{3}
$$

(ã“ã“ã§ã€target behavior ã®åˆ†å¸ƒ $P(R_f)$ ã ã‘ã‚’ä½¿ã‚ãªã„ã®ã¯ã€target ehaviorã®åˆ†å¸ƒã‚‚å¿…ãšã—ã‚‚çœŸã®ãƒ¦ãƒ¼ã‚¶å—œå¥½ã¨ä¸€è‡´ã—ã¦ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‹ã‚‰ã‹ãª...!:thinking:)
We select items with the highest score as the target behavior recommendation results.
æœ€ã‚‚ã‚¹ã‚³ã‚¢ã®é«˜ã„itemã‚’target behavioræ¨è–¦ã®çµæœã¨ã—ã¦é¸æŠã™ã‚‹ã€‚

## 3.2. Multi-behavior alignment on noisy data ãƒã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹

The key motivation for MBA is that multiple types of user behavior should reflect similar user preferences.
MBAã®ä¸»ãªå‹•æ©Ÿã¯ã€**è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ãŒã€é¡ä¼¼ã—ãŸãƒ¦ãƒ¼ã‚¶å—œå¥½ã‚’åæ˜ ã—ã¦ã„ã‚‹ã¯ãšã **ã¨ã„ã†ã“ã¨ã ã€‚
Hence, Eq.4 is expected to be achieved with the convergence of the training models:
ã—ãŸãŒã£ã¦ã€å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®åæŸã¨ã¨ã‚‚ã«ã€å¼.4ãŒé”æˆã•ã‚Œã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã‚‹:

$$
P(R_f) \approx P(R_g) \approx P(R_t)
\tag{4}
$$

(P(R_f)ã¨ã‹ã¯ã„ãšã‚Œã‚‚ã€ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦æ¨è«–ã•ã‚Œã‚‹ã‚„ã¤ã ã‚ˆã­...!:thinking:)
Therefore, ğ‘ƒ (Rğ‘“ ) and ğ‘ƒ (Rğ‘¡) should have a relatively small KLdivergence, which is formulated as follows:
ã—ãŸãŒã£ã¦ã€ P(R_f) ã¨ $P(R_t)$ ã¯æ¯”è¼ƒçš„å°ã•ãªKLdivergenceã‚’æŒã¤ã¹ãã§ã‚ã‚Šã€ã“ã‚Œã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®šå¼åŒ–ã•ã‚Œã‚‹:

$$
KL[P(R_f) || P(R_t)] = E_{P(R_f)}[log P(R_f) - log P(R_t)]
\tag{5}
$$

(KL divergenceã®å¼ã€‚ãã†ãã†ã€KL-divã¯JS-divã¨é•ã£ã¦ã€å¯¾ç§°æ€§ã¨ã‹ãªã„ã‚“ã ã£ãŸã€‚)

Similarly, we also have the KL-divergence between ğ‘ƒ (Rğ‘”) and ğ‘ƒ (Rğ‘¡):
P(R_g)ã¨P(R_t)ã®é–“ã®KL-divã‚‚åŒæ§˜:

$$
KL[P(R_g) || P(R_t)] = E_{P(R_g)}[log P(R_g) - log P(R_t)]
\tag{6}
$$

However, naively minimizing the above KL-divergence is not feasible since it overlooks the data distribution gaps and correlations between multiple types of behavior.
ã—ã‹ã—ã€**ä¸Šè¨˜ã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’ç´ æœ´ã«æœ€å°åŒ–ã™ã‚‹ã“ã¨ã¯ã€ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®ã‚®ãƒ£ãƒƒãƒ—ã‚„è¤‡æ•°ç¨®é¡ã®è¡Œå‹•é–“ã®ç›¸é–¢ã‚’è¦‹è½ã¨ã—ã¦ã—ã¾ã†ãŸã‚ã€å®Ÿè¡Œä¸å¯èƒ½ã§ã‚ã‚‹**ã€‚
To address this issue, we use Bayesâ€™ theorem to rewrite ğ‘ƒ (Rğ‘¡) as follows:
ã“ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€ãƒ™ã‚¤ã‚ºã®å®šç†ã‚’ç”¨ã„ã¦ çœŸã®å—œå¥½åˆ†å¸ƒ $P(R_t)$ ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«æ›¸ãæ›ãˆã‚‹:
(ãƒ™ã‚¤ã‚ºã®å®šç†ã£ã¦ã€åŒæ™‚ç¢ºç‡ãŒ2é€šã‚Šã®æ›¸ãæ–¹ãŒã§ãã‚‹äº‹ã‹ã‚‰å°å‡ºã§ãã‚‹ã‚„ã¤ã ã‚ˆã­...! ex. $P(R_t, R_f) = P(R_t)\cdot P(R_f|R_t) = P(R_f)\cdot P(R_t|R_f)$ ã“ã‚Œã‚’ä½¿ã†ã¨å¼7ã®ã‚ˆã†ã«ãªã‚‹)

$$
P(R_t) = \frac{P(R_f) P(R_t|R_f)}{P(R_f|R_t)}
= \frac{P(R_g) P(R_t|R_g)}{P(R_g|R_t)}
\tag{7}
$$

By substituting the right part of Eq.7 into Eq.5 and rearranging erms, we obtain the following equation:
å¼7ã®å³è¾ºã‚’å¼5ã«ä»£å…¥ã—ã€ä¸¡å´ã®å¼ã‚’ä¸¦ã¹æ›¿ãˆã‚‹ã¨ã€ä»¥ä¸‹ã®å¼ãŒå¾—ã‚‰ã‚Œã‚‹:

$$
\tag{8}
$$

Since ğ¾ğ¿[ğ‘ƒ (Rğ‘“ ) âˆ¥ğ‘ƒ (Rğ‘¡ | Rğ‘”)] â‰¥ 0, the left side of Eq.8 is an approximate lower bound of the logarithm log ğ‘ƒ (Rğ‘”).
ğ¾ğ¿[ğ‘ƒ (Rğ‘“)] âˆ¥ (Rğ‘ƒ | Rğ‘“) â‰¥ 0ãªã®ã§ã€å¼.8ã®å·¦è¾ºã¯å¯¾æ•°log ğ‘ƒ (Rğ‘”)ã®è¿‘ä¼¼ä¸‹ç•Œã¨ãªã‚‹ã€‚
The bound is satisfied if, and only if, ğ‘ƒ (Rğ‘“ ) perfectly recovers ğ‘ƒ (Rğ‘¡ | Rğ‘”), which means ğ‘ƒ (Rğ‘“ ) trained on the observed target behavior can perfectly approximates the true user preference distribution captured from the auxiliary behavior data.
ã“ã®å¢ƒç•Œã¯ã€$P(R_f)$ ãŒ $P(R_t|R_g)$ ã‚’å®Œå…¨ã«å¾©å…ƒã™ã‚‹(=ä¸¡åˆ†å¸ƒãŒå®Œå…¨ã«ä¸€è‡´ã™ã‚‹) å ´åˆã«ã®ã¿æº€ãŸã•ã‚Œã¾ã™ã€‚ã“ã‚Œã¯ã€è¦³å¯Ÿã•ã‚ŒãŸtarget behaviorã®ã‚‚ã¨ã§å­¦ç¿’ã•ã‚ŒãŸ $P(R_f)$ ãŒã€è£œåŠ©è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—ã•ã‚ŒãŸçœŸã®ãƒ¦ãƒ¼ã‚¶é¸å¥½åˆ†å¸ƒã‚’å®Œå…¨ã«è¿‘ä¼¼ã§ãã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚
The above condition is in line with the main motivation of the MBA, i.e., different behavior data should reflect similar user preferences.
ä¸Šè¨˜ã®æ¡ä»¶ã¯ã€MBAã®ä¸»ãªå‹•æ©Ÿã«æ²¿ã£ãŸã‚‚ã®ã§ã‚ã‚‹ã€‚ã¤ã¾ã‚Šã€ç•°ãªã‚‹è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã¯ã€é¡ä¼¼ã—ãŸãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã‚’åæ˜ ã™ã¹ãã§ã‚ã‚‹ã€‚

We see that the left side of Eq.8 is based on the expectation over ğ‘ƒ (Rğ‘“ ), which means that we are trying to train ğ‘ƒ (Rğ‘“ ) with the given corrupted auxiliary behavior data Rğ‘” (i.e., the term ğ¸ğ‘ƒ (Rğ‘“ ) [log ğ‘ƒ (Rğ‘” | Rğ‘¡)]) and then to transmit the information from ğ‘ƒ (Rğ‘“ ) to ğ‘ƒ (Rğ‘¡) via the term ğ¾ğ¿[ğ‘ƒ (Rğ‘“ ) âˆ¥ğ‘ƒ (Rğ‘¡)].
å¼.8ã®å·¦è¾ºãŒ $P(R_f)$ ã«å¯¾ã™ã‚‹æœŸå¾…å€¤ã«åŸºã¥ã„ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚ã“ã‚Œã¯ã€ä¸ãˆã‚‰ã‚ŒãŸç ´æã—ãŸè£œåŠ©è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ $R_g$ (i.e. hogehoge)ã‚’ç”¨ã„ã¦ $P(R_f)$ ã‚’è¨“ç·´ã—ã€ãã—ã¦ $P(R_f)$ ã‹ã‚‰ $P(R_t)$ ã« $KL[P(R_f)||P(R_t)]$ ã¨ã„ã†é …ã‚’çµŒç”±ã—ã¦æƒ…å ±ã‚’ä¼é”ã—ã‚ˆã†ã¨ã—ã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚
Such a learning process is ineffective for learning the true user preference distribution ğ‘ƒ (Rğ‘¡) and the target recommender model ğ‘¡ğœƒ .
ã“ã®ã‚ˆã†ãªå­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€çœŸã®ãƒ¦ãƒ¼ã‚¶å—œå¥½åˆ†å¸ƒ $P(R_t)$ ã¨å¯¾è±¡ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ãƒ»ãƒ¢ãƒ‡ãƒ« $t_{\theta}$ ã®å­¦ç¿’ã«ã¯åŠ¹æœãŒã‚ã‚Šã¾ã›ã‚“ã€‚
To overcome the above issue, according to Eq.4, when the training process has converged, the preference distributions ğ‘ƒ (Rğ‘“ ) and ğ‘ƒ (Rğ‘¡) would be close to each other.
ä¸Šè¨˜ã®å•é¡Œã‚’å…‹æœã™ã‚‹ãŸã‚ã«ã€å¼.4ã«ã‚ˆã‚Œã°ã€å­¦ç¿’éç¨‹ãŒåæŸã—ãŸã¨ãã€é¸å¥½åˆ†å¸ƒ $P(R_f)$ ã¨ $P(R_t)$ ã¯äº’ã„ã«è¿‘ããªã‚‹ã€‚
As a result, we can change the expectation over ğ‘ƒ (Rğ‘“ ) to the expectation over ğ‘ƒ (Rğ‘¡) to learn ğ‘ƒ (Rğ‘¡) more effectively.
ãã®çµæœã€$P(R_f)$ ã«å¯¾ã™ã‚‹æœŸå¾…å€¤ã‚’ $P(R_t)$ ã«å¯¾ã™ã‚‹æœŸå¾…å€¤ã«å¤‰æ›´ã™ã‚‹ã“ã¨ã§ã€$P(R_t)$ ã‚’ã‚ˆã‚ŠåŠ¹æœçš„ã«å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚(ï¼’ã¤ã®åˆ†å¸ƒã¯è¿‘ãã«ã‚ã‚‹ã¯ãšã ã‹ã‚‰ã£ã¦ã“ã¨??:thinking:)
So we modify the left side of Eq.8 as:
ãã“ã§ã€å¼8ã®å·¦è¾ºã‚’æ¬¡ã®ã‚ˆã†ã«ä¿®æ­£ã™ã‚‹:
(å·¦è¾ºã‚’ä¿®æ­£ã—ãŸã®ã§ã€$=$ ãŒ $\approx$ ã«å¤‰ã‚ã£ã¦ã‚‹...!)

$$
\tag{9}
$$

Similarly, if we substitute the middle part of Eq.7 into Eq.6 and perform similar derivations, we can obtain:
åŒæ§˜ã«ã€å¼7ã®ä¸­å¤®éƒ¨åˆ†ã‚’å¼6ã«ä»£å…¥ã—ã€åŒæ§˜ã®å°å‡ºã‚’è¡Œãˆã°ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚‹ï¼š
(å¼9ã¨åŒæ§˜ã«ã€$P(R_f)$ ã®ç®‡æ‰€ã‚’ $P(R_t)$ ã«è¿‘ä¼¼ã—ãŸã®ã§ã€$=$ ãŒ $\approx$ ã«å¤‰ã‚ã£ã¦ã‚‹...!)

$$
\tag{10}
$$

The left side of Eq.10 is an approximate lower bound of log ğ‘ƒ (Rğ‘“ ).
å¼10ã®å·¦è¾ºã¯log ğ‘ƒ (R_1D453 )ã®è¿‘ä¼¼ä¸‹ç•Œã§ã‚ã‚‹ã€‚
The bound is satisfied only if ğ‘ƒ (Rğ‘”) perfectly recovers ğ‘ƒ (Rğ‘¡ | Rğ‘“ ), which means ğ‘ƒ (Rğ‘”) trained on the observed auxiliary behaviors can perfectly approximate the true user preference distribution captured from the target behavior data.
ã“ã®å¢ƒç•Œã¯ $P(R_g)$ ãŒ $P(R_t|R_f)$ ã‚’å®Œå…¨ã«å¾©å…ƒã™ã‚‹å ´åˆã«ã®ã¿æº€ãŸã•ã‚Œã¾ã™ã€‚ã“ã‚Œã¯ã€è¦³å¯Ÿã•ã‚ŒãŸè£œåŠ©è¡Œå‹•ã«å¯¾ã—ã¦å­¦ç¿’ã•ã‚ŒãŸğ‘ƒï¼ˆRğ‘”ï¼‰ãŒã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—ã•ã‚ŒãŸçœŸã®ãƒ¦ãƒ¼ã‚¶å—œå¥½åˆ†å¸ƒã‚’å®Œå…¨ã«è¿‘ä¼¼ã§ãã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚
Such condition further verifies the soundness of MBA, i.e., multiple types of user behavior are motivated by similar underlying user preferences.
ã“ã®ã‚ˆã†ãªæ¡ä»¶ã¯ã€MBAã®å¥å…¨æ€§ã‚’ã•ã‚‰ã«æ¤œè¨¼ã™ã‚‹ã€‚ã¤ã¾ã‚Šã€è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã¯ã€æ ¹åº•ã«ã‚ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å—œå¥½ãŒé¡ä¼¼ã—ã¦ã„ã‚‹ã“ã¨ãŒå‹•æ©Ÿã¨ãªã£ã¦ã„ã‚‹ã€‚(ã“ã®æ–‡ã¾ãŸã§ã¦ããŸ...!)
Combining the left side of both Eq.9 and Eq.10 we obtain the loss function as:
å¼9ã¨å¼10ã®å·¦è¾ºã‚’çµ„ã¿åˆã‚ã›ã‚‹ã¨ã€æå¤±é–¢æ•°ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚‹ï¼š

$$
L = - E_{P(R_t)}[log P(R_g|R_t)] + KL[P(R_f)||P(R_t)]
\\
- E_{P(R_t)}[log P(R_f|R_t)] + KL[P(R_g)||P(R_t)]
\tag{11}
$$

We can see that the loss function aims to maximize the likelihood of data observation (i.e., ğ‘ƒ (Rğ‘” | Rğ‘¡) and ğ‘ƒ (Rğ‘“ | Rğ‘¡)) and minimize the KL-divergence between distributions learned from different user behavior data.
æå¤±é–¢æ•°ã¯ã€ãƒ‡ãƒ¼ã‚¿è¦³æ¸¬ã®å°¤åº¦(ã™ãªã‚ã¡ã€$P(R_g|R_t)$ ã¨ $P(R_f|R_t)$)ã‚’æœ€å¤§åŒ–ã—ã€ç•°ãªã‚‹ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã•ã‚ŒãŸåˆ†å¸ƒé–“ã®KL-divã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
(ã“ã“ã§å°¤åº¦ã®æ„å‘³ã£ã¦ã€ãƒ¦ãƒ¼ã‚¶ã®çœŸã®å—œå¥½ãŒ1ã®æ™‚ã«clickãŒè¦³æ¸¬ã•ã‚ŒãŸã‚Šã€conversionãŒè¦³æ¸¬ã•ã‚ŒãŸã‚Šã™ã‚‹æ¡ä»¶ä»˜ãç¢ºç‡ã‚’ã€åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å´ã‚’å¤‰æ•°ã¨ã—ã¦è¦‹ãŸã‚„ã¤??)

The learning process of MBA serves as a filter to simultaneously denoise multiple types of user behavior and conduct beneficial knowledge transfers to infer the true user preferences to enhance the prediction of the target behavior.
MBAã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã‚’åŒæ™‚ã«ãƒã‚¤ã‚ºé™¤å»ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã¨ã—ã¦æ©Ÿèƒ½ã—ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¡Œå‹•ã®äºˆæ¸¬ã‚’å¼·åŒ–ã™ã‚‹ãŸã‚ã«ã€çœŸã®ãƒ¦ãƒ¼ã‚¶å—œå¥½ã‚’æ¨è«–ã™ã‚‹ãŸã‚ã«æœ‰ç›ŠãªçŸ¥è­˜ç§»è»¢ã‚’è¡Œã†ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã  -->

## 3.3. Training details ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®è©³ç´°

As described in Section 3.1, we learn the user preference distributions $P(R_f)$ and $P(R_g)$ from $R_f$ and $R_g$, respectively.
ã‚»ã‚¯ã‚·ãƒ§ãƒ³3.1ã§èª¬æ˜ã—ãŸã‚ˆã†ã«ã€$R_f$ ã¨ $R_g$ ã‹ã‚‰ã€ãã‚Œãã‚Œãƒ¦ãƒ¼ã‚¶ã®å—œå¥½åˆ†å¸ƒ $P(R_f)$ ã¨ $P(R_g)$ ã‚’å­¦ç¿’ã™ã‚‹ã€‚
In order to enhance the learning stability, we pre-train $P(R_f)$ and $P(R_g)$ in Rğ‘“ and Rğ‘”, respectively.
å­¦ç¿’ã®å®‰å®šæ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã«ã€$P(R_f)$ ã¨ $P(R_g)$ ã‚’ãã‚Œãã‚Œ $R_f$ ã¨ $R_g$ ã§äº‹å‰å­¦ç¿’ã™ã‚‹ã€‚(ã‚¯ãƒªãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¯ãƒªãƒƒã‚¯ã®ãƒ­ã‚°ã ã‘ã‚’ä½¿ã£ã¦äº‹å‰å­¦ç¿’ã™ã‚‹ã€ã¿ãŸã„ãª...?)
We use the same model structures of our target recommender ğ‘¡ğœƒ as the pre-training model.
äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ»ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ $t_{\theta}$ ã¨åŒã˜ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
As the training converges, the KL-divergence will gradually approach 0.
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒåæŸã™ã‚‹ã«ã¤ã‚Œã¦ã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯å¾ã€…ã«0ã«è¿‘ã¥ã„ã¦ã„ãã€‚
In order to enhance the role of the KL-divergence in conveying information, we set a hyperparameter ğ›¼ to enhance the effectiveness of the KL-divergence.
æƒ…å ±ä¼é”ã«ãŠã‘ã‚‹KL-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®å½¹å‰²ã‚’å¼·åŒ–ã™ã‚‹ãŸã‚ã«ã€KL-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®æœ‰åŠ¹æ€§ã‚’é«˜ã‚ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\alpha$ ã‚’è¨­å®šã™ã‚‹ã€‚
Then we obtain the following training loss function:
ã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ãªå­¦ç¿’æå¤±é–¢æ•°ãŒå¾—ã‚‰ã‚Œã‚‹:

$$
L_{MBA} = - E_{P(R_t)}[log P(R_g|R_t)] + \alpha KL[P(R_f)||P(R_t)]
\\
- E_{P(R_t)}[log P(R_f|R_t)] + \alpha KL[P(R_g)||P(R_t)]
\tag{12}
$$

### 3.3.1. Expectation derivation. æœŸå¾…å€¤ã®å°å‡ºã€‚

As described in Section 3.1, both Rğ‘“ and Rğ‘” contain various kinds of noise and bias.
ã‚»ã‚¯ã‚·ãƒ§ãƒ³3.1ã§èª¬æ˜ã—ãŸã‚ˆã†ã«ã€$R_f$ ã¨ $R_g$ ã«ã¯æ§˜ã€…ãªç¨®é¡ã®ãƒã‚¤ã‚ºã¨ãƒã‚¤ã‚¢ã‚¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã€‚
In order to infer the latent true user preferences from the corrupted multi-behavior data, we use â„ ğ‘“ ğœ™ (ğ‘¢,ğ‘–) and â„ ğ‘“ ğœ‘ (ğ‘¢,ğ‘–) to capture the correlations between the true user preferences and the observed purchase data.
ç ´æã—ãŸè¤‡æ•°è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ½œåœ¨çš„ãªçœŸã®ãƒ¦ãƒ¼ã‚¶å—œå¥½ã‚’æ¨è«–ã™ã‚‹ãŸã‚ã«ã€$h^{f}_{\phi}(u,i)$ ã¨ $h^{f}_{\varphi}(u,i)$ ã‚’ç”¨ã„ã¦ã€çœŸã®ãƒ¦ãƒ¼ã‚¶å—œå¥½ã¨è¦³æ¸¬ã•ã‚ŒãŸè³¼è²·(target behavior)ãƒ‡ãƒ¼ã‚¿é–“ã®ç›¸é–¢ã‚’æ‰ãˆã‚‹ã€‚
Similarly, â„ ğ‘” ğœ™â€² (ğ‘¢,ğ‘–) and â„ ğ‘” ğœ‘â€² (ğ‘¢,ğ‘–) are used to capture the correlations between the true user preferences and the observed click data, as shown in Eq.2.
åŒæ§˜ã«ã€$h^{g}_{\phi}(u,i)$ ã¨ $h^{g}_{\varphi}(u,i)$ ã‚’ç”¨ã„ã¦ã€å¼.2ã«ç¤ºã™ã‚ˆã†ã«ã€çœŸã®ãƒ¦ãƒ¼ã‚¶å—œå¥½ã¨è¦³æ¸¬ã•ã‚ŒãŸã‚¯ãƒªãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿(support behavior)ã¨ã®ç›¸é–¢ã‚’æ‰ãˆã‚‹ã“ã¨ã‚’è©¦ã¿ã‚‹ã€‚
Specifically, we expand ğ¸ğ‘ƒ (Rğ‘¡ ) [log ğ‘ƒ (Rğ‘” | Rğ‘¡)] as:
å…·ä½“çš„ã«ã¯ã€$E_{P(R_t)}[log P(R_g|R_t)]$ ã‚’æ¬¡ã®ã‚ˆã†ã«å±•é–‹ã™ã‚‹:

$$
E_{P(R_t)}[log P(R_g|R_t)]
= \sum_{u,i} E_{r^{t}_{u,i} \sim P(R_t)} [log P(r^{g}_{u,i}|r^{t}_{u,i})]
\tag{13}
= ...
$$

Similarly, the term ğ¸ğ‘ƒ (Rğ‘¡ ) [log ğ‘ƒ (Rğ‘“ | Rğ‘¡)] can be expanded as:
åŒæ§˜ã«ã€$E_{P(R_t)}[log P(R_f|R_t)]$ ã®é …ã¯æ¬¡ã®ã‚ˆã†ã«å±•é–‹ã§ãã‚‹ï¼š

$$
E_{P(R_t)}[log P(R_f|R_t)] = ...
\tag{14}
$$

By aligning and denoising the observed target behavior and auxiliary behavior data simultaneously, the target recommender ğ‘¡ğœƒ is trained to learn the universal true user preference distribution.
è¦³æ¸¬ã•ã‚ŒãŸtargetè¡Œå‹•ã¨ auxiliaryè¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‚’åŒæ™‚ã«æ•´åˆ—ãƒ»ãƒã‚¤ã‚ºé™¤å»ã™ã‚‹ã“ã¨ã§ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ»ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ $t_{\theta}$ ã¯æ™®éçš„ãªçœŸã®ãƒ¦ãƒ¼ã‚¶å—œå¥½åˆ†å¸ƒã‚’å­¦ç¿’ã™ã‚‹ã‚ˆã†ã«è¨“ç·´ã•ã‚Œã‚‹ã€‚

### 3.3.2. Alternative model training. ä»£æ›¿ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€‚

In the learning stage, we find that directly training ğ‘¡ğœƒ with Eq.12â€“Eq.14 does not yield satisfactory results, which is caused by the simultaneous update of five models (i.e., â„ ğ‘” ğœ™â€² , â„ ğ‘” ğœ‘â€² , â„ ğ‘“ ğœ™ , â„ ğ‘“ ğœ‘ and ğ‘¡ğœƒ ) in such an optimization process.
å­¦ç¿’æ®µéšã«ãŠã„ã¦ã€å¼.12-å¼.14ã‚’ç”¨ã„ã¦ $t_{\theta}$ ã‚’ç›´æ¥å­¦ç¿’ã—ã¦ã‚‚æº€è¶³ã®ã„ãçµæœãŒå¾—ã‚‰ã‚Œãªã„ã“ã¨ãŒã‚ã‹ã‚‹ã€‚(ãã†ãªã®??) ã“ã‚Œã¯ã€5ã¤ã®ãƒ¢ãƒ‡ãƒ«é”ãŒåŒæ™‚ã«æ›´æ–°ã•ã‚Œã‚‹ãŸã‚ã§ã‚ã‚‹ã€‚(i.e. $h^{g}_{\phi'}, h^{g}_{\varphi'}, h^{f}_{\phi}, h^{f}_{\varphi}, t_{\theta}$)
These five models may interfere with each other and prevent ğ‘¡ğœƒ from learning well.
ã“ã‚Œã‚‰5ã¤ã®ãƒ¢ãƒ‡ãƒ«ã¯äº’ã„ã«å¹²æ¸‰ã—åˆã„ã€$t_{\theta}$ (=çœŸã«å¾—ãŸã„ãƒ¢ãƒ‡ãƒ«) ã®å­¦ç¿’ã‚’å¦¨ã’ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
To address this problem, we set two alternative training steps to train the involved models iteratively.
ã“ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€2ã¤ã®ä»£æ›¿å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¨­å®šã—ã€é–¢ä¿‚ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’åå¾©çš„ã«å­¦ç¿’ã™ã‚‹ã€‚

In the first training step, we assume that a user tends to not click or purchase items that the user dislikes.
æœ€åˆã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€**ãƒ¦ãƒ¼ã‚¶ãŒå«Œã„ãªå•†å“ã¯ã‚¯ãƒªãƒƒã‚¯ã—ãªã„ã€è³¼å…¥ã—ãªã„å‚¾å‘ãŒã‚ã‚‹ã¨ä»®å®š**ã™ã‚‹ã€‚
That is to say, given $r^{t}_{u,i}= 0$ we have $r^{f}_{u,i} \approx 0$ and $r^{g}_{u,i} \approx 0$, so we have â„ ğ‘“ ğœ™ â‰ˆ 0 and â„ ğ‘” ğœ™â€² â‰ˆ 0 according to Eq.2.
ã“ã‚Œã¯ã¤ã¾ã‚Šã€çœŸã®å—œå¥½ $r^{t}_{u,i}= 0$ ã®å ´åˆã€$r^{f}_{u,i} \approx 0$ ã¨ $r^{g}_{u,i} \approx 0$ ãŒæˆç«‹ã™ã‚‹ã€ã¤ã¾ã‚Š $h^{f}_{\varphi} \approx 0$ ã¨ $h^{g}_{\varphi'} \approx 0$ ãŒæˆç«‹ã™ã‚‹ã¨ã„ã†ã“ã¨ã§ã‚ã‚‹ã€‚
Thus in this step, only the models â„ ğ‘“ ğœ‘ , â„ ğ‘” ğœ‘â€² and ğ‘¡ğœƒ are trained.
ãªã®ã§ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€3ã¤ã®ãƒ¢ãƒ‡ãƒ« $h^{f}_{\varphi}, h^{g}_{\varphi'}, t_{\theta}$ ã®ã¿ã‚’å­¦ç¿’ã•ã›ã‚‹ã€‚
Then Eq.13 can be reformulated as:
ãã†ã™ã‚‹ã¨ã€å¼.13ã¯æ¬¡ã®ã‚ˆã†ã«å®šå¼åŒ–ã§ãã‚‹:

$$
\tag{15}
$$

where
ã“ã“ã§ã€

$$
\tag{}
$$

Meanwhile, Eq.14 can be reformulated as:
ä¸€æ–¹ã€å¼.14ã¯æ¬¡ã®ã‚ˆã†ã«å®šå¼åŒ–ã§ãã‚‹:

$$
\tag{16}
$$

where
ã“ã“ã§ã€

$$
\tag{}
$$

Here, we denote ğ¶1 as a large positive hyperparameter to replace âˆ’ logâ„ ğ‘” ğœ™â€² (ğ‘¢,ğ‘–) and âˆ’ logâ„ ğ‘“ ğœ™ (ğ‘¢,ğ‘–).
ã“ã“ã§ã¯ã€-logâ„ á‘” â†ªLl_1D719â€² (ğ‘¢,ğ‘–)ã¨-logâ„ ğ‘“ã‚’ç½®ãæ›ãˆã‚‹ãŸã‚ã«ã€$C_1$ ã‚’å¤§ããªæ­£ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã™ã‚‹ã€‚

In the second training step, we assume that a user tends to click and purchase the items that the user likes.
2ã¤ç›®ã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€**ãƒ¦ãƒ¼ã‚¶ãŒæ°—ã«å…¥ã£ãŸå•†å“ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦è³¼å…¥ã™ã‚‹å‚¾å‘ãŒã‚ã‚‹ã¨ä»®å®š**ã™ã‚‹ã€‚
That is to say, given ğ‘Ÿ ğ‘¡ ğ‘¢,ğ‘– = 1 we have ğ‘Ÿ ğ‘“ ğ‘¢,ğ‘– â‰ˆ 1 and ğ‘Ÿ ğ‘” ğ‘¢,ğ‘– â‰ˆ 1, so we have â„ ğ‘“ ğœ‘ â‰ˆ 1 and â„ ğ‘” ğœ‘â€² â‰ˆ 1 according to Eq.2.
ã“ã‚Œã¯ã¤ã¾ã‚Šã€çœŸã®å—œå¥½ $r^{t}_{u,i}= 1$ ã®å ´åˆã€$r^{f}_{u,i} \approx 1$ ã¨ $r^{g}_{u,i} \approx 1$ ãŒæˆç«‹ã™ã‚‹ã€ã¤ã¾ã‚Š $h^{f}_{\phi} \approx 1$ ã¨ $h^{g}_{\phi'} \approx 1$ ãŒæˆç«‹ã™ã‚‹ã¨ã„ã†ã“ã¨ã§ã‚ã‚‹ã€‚
Thus in this step, only the models â„ ğ‘“ ğœ™ , â„ ğ‘” ğœ™â€² and ğ‘¡ğœƒ will be updated.
ãªã®ã§ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€3ã¤ã®ãƒ¢ãƒ‡ãƒ« $h^{f}_{\phi}, h^{g}_{\phi'}, t_{\theta}$ ã®ã¿ã‚’å­¦ç¿’ã•ã›ã‚‹ã€‚

Then Eq.13 can be reformulated as:
ãã†ã™ã‚‹ã¨ã€å¼.13ã¯æ¬¡ã®ã‚ˆã†ã«å®šå¼åŒ–ã§ãã‚‹:

$$
\tag{17}
$$

where
ã“ã“ã§

$$
\tag{}
$$

Eq.14 can be reformulated as:
(åŒæ§˜ã«)å¼.14ã¯æ¬¡ã®ã‚ˆã†ã«å®šå¼åŒ–ã§ãã‚‹:

$$
\tag{18}
$$

where
ã“ã“ã§ã€

$$
\tag{}
$$

ğ¶2 is a large positive hyperparameter to replace âˆ’ log(1âˆ’â„ ğ‘” ğœ‘â€² (ğ‘¢,ğ‘–)) and âˆ’ log(1 âˆ’ â„ ğ‘“ ğœ‘ (ğ‘¢,ğ‘–)).
$C_2$ ã¯ã€-log(1-È (â†ªLl_1D46,ğ‘–)) ã¨ - log(1 - ğœ‘ (â†ªLl_1D462,ğ‘–)) ã‚’ç½®ãæ›ãˆã‚‹å¤§ããªæ­£ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚‹ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã (ä¸€å¿œ) -->

### 3.3.3. Training procedure. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®æ‰‹é †

In order to facilitate the description of sampling and training process, we divide ğ¸ğ‘ƒ (Rğ‘¡ ) [log ğ‘ƒ (Rğ‘” | Rğ‘¡)] and ğ¸ğ‘ƒ (Rğ‘¡ ) [log ğ‘ƒ (Rğ‘“ | Rğ‘¡)] into four parts (see Eq.15 to Eq.18), namely click positive loss (ğ¿ğ¶ğ‘ƒ and ğ¿ â€² ğ¶ğ‘ƒ ), click negative loss (ğ¿ğ¶ğ‘ and ğ¿ â€² ğ¶ğ‘ ), purchase positive loss (ğ¿ğ‘ƒğ‘ƒ and ğ¿ â€² ğ‘ƒğ‘ƒ ), and purchase negative loss (ğ¿ğ‘ƒğ‘ and ğ¿ â€² ğ‘ƒğ‘ ).
ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ—ãƒ­ã‚»ã‚¹ã®èª¬æ˜ã‚’å®¹æ˜“ã«ã™ã‚‹ãŸã‚ã«ã€ğ¸ğ‘ƒ (Rğ‘¡ ) [log ğ‘ƒ (Rğ‘” | Rğ‘¡)] ã¨ ğ¸ğ‘ƒ (Rğ‘¡ ) [log ğ‘ƒ (Rğ‘“ | Rğ‘¡)] ã‚’ 4ã¤ã®éƒ¨åˆ†ã«åˆ†å‰²ã™ã‚‹(å¼ 15 ~ å¼18)ã€‚
hogehoge
Each sample in the training set can be categorized into one of three situations: (i) clicked and purchased, (ii) clicked but not purchased, and (iii) not clicked and not purchased.
**ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã®å„ã‚µãƒ³ãƒ—ãƒ«ã¯ã€3ã¤ã®çŠ¶æ³ã®ã„ãšã‚Œã‹ã«åˆ†é¡ã•ã‚Œã‚‹**(ã†ã‚“ã†ã‚“:thinking:) : (i) ã‚¯ãƒªãƒƒã‚¯ã•ã‚Œè³¼å…¥ã•ã‚ŒãŸã€(ii) ã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸãŒè³¼å…¥ã•ã‚Œãªã‹ã£ãŸã€(iii) ã‚¯ãƒªãƒƒã‚¯ã•ã‚Œãšè³¼å…¥ã•ã‚Œãªã‹ã£ãŸã€‚
The three situations involve different terms in ğ¸ğ‘ƒ (Rğ‘¡ ) [log ğ‘ƒ (Rğ‘” | Rğ‘¡)] and ğ¸ğ‘ƒ (Rğ‘¡ ) [log ğ‘ƒ (Rğ‘“ | Rğ‘¡)].
hogehoge
In situation (i), each sample involves the ğ¿ğ¶ğ‘ƒ and ğ¿ğ‘ƒğ‘ƒ (or ğ¿ â€² ğ¶ğ‘ƒ and ğ¿ â€² ğ‘ƒğ‘ƒ in the alternative training step).
çŠ¶æ³(i)ã§ã¯ã€å„ã‚µãƒ³ãƒ—ãƒ«ã¯ğ¿ğ‘ƒã¨ğ‘ƒï¼ˆã¾ãŸã¯ä»£æ›¿å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ğ‘ƒã¨ğ¿ï¼‰ã‚’å«ã‚€ã€‚
In situation (ii), each sample involves the ğ¿ğ¶ğ‘ƒ and ğ¿ğ‘ƒğ‘ (or ğ¿ â€² ğ¶ğ‘ƒ and ğ¿ â€² ğ‘ƒğ‘ in the alternative training step).
çŠ¶æ³(ii)ã§ã¯ã€å„ã‚µãƒ³ãƒ—ãƒ«ã¯ğ¿ğ¶ğ‘ƒï¼ˆã¾ãŸã¯ä»£æ›¿å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ ğ¿ ğ‘ƒ ğ¿ï¼‰ã‚’å«ã‚€ã€‚
In situation (iii), each sample involves the ğ¿ğ¶ğ‘ and ğ¿ğ‘ƒğ‘ (or ğ¿ â€² ğ¶ğ‘ and ğ¿ â€² ğ‘ƒğ‘ in the alternative training step).
çŠ¶æ³(iii)ã§ã¯ã€å„ã‚µãƒ³ãƒ—ãƒ«ã¯ğ¿ğ‘ï¼ˆã¾ãŸã¯ä»£æ›¿å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ğ‘ƒğ¿ï¼‰ã¨ğ‘ï¼ˆã¾ãŸã¯ğ‘ƒğ‘ï¼‰ã‚’å«ã‚€ã€‚
We then train MBA according to the observed multiple types of user behavior data in situations (i) and (ii), and use the samples in situation (iii) as our negative samples.
ãã—ã¦ã€çŠ¶æ³(i)ã¨(ii)ã§è¦³æ¸¬ã•ã‚ŒãŸè¤‡æ•°ç¨®é¡ã®ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã«å¾“ã£ã¦MBAã‚’è¨“ç·´ã—ã€çŠ¶æ³(iii)ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è² ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã€‚
Details of the training process for MBA are provided in Algorithm 1.
MBAã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã®è©³ç´°ã¯ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 1ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã€‚

# 4. Experimental settings å®Ÿé¨“çš„è¨­å®š

## 4.1. Experimental questions å®Ÿé¨“å•é¡Œ

Our experiments are conducted to answer the following research questions: (RQ1) How do the proposed methods perform compared with state-of-the-art recommendation baselines on different datasets? (RQ2) How do the proposed methods perform compared with other denoising frameworks? (RQ3) Can MBA help to learn universal user preferences from usersâ€™ multiple types of behavior? (RQ4) How do the components and the hyperparameter settings affect the recommendation performance of MBA?
æˆ‘ã€…ã®å®Ÿé¨“ã¯ä»¥ä¸‹ã®ç ”ç©¶èª²é¡Œã«ç­”ãˆã‚‹ãŸã‚ã«è¡Œã‚ã‚ŒãŸï¼š (RQ1)ææ¡ˆæ‰‹æ³•ã¯ã€ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã„ã¦ã€æœ€æ–°ã®æ¨è–¦ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒã—ã¦ã©ã®ã‚ˆã†ã«å‹•ä½œã™ã‚‹ã‹ï¼Ÿ(RQ2) ææ¡ˆæ‰‹æ³•ã¯ä»–ã®ãƒã‚¤ã‚ºé™¤å»ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨æ¯”è¼ƒã—ã¦ã©ã®ã‚ˆã†ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™ã‹ï¼Ÿ(RQ3) MBAã¯ãƒ¦ãƒ¼ã‚¶ã®è¤‡æ•°ç¨®é¡ã®è¡Œå‹•ã‹ã‚‰æ™®éçš„ãªãƒ¦ãƒ¼ã‚¶å—œå¥½ã‚’å­¦ç¿’ã™ã‚‹ã®ã«å½¹ç«‹ã¤ã‹ï¼Ÿ(RQ4) æ§‹æˆè¦ç´ ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®šã¯MBAã®æ¨è–¦æ€§èƒ½ã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ï¼Ÿ

## 4.2. Datasets ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

To evaluate the effectiveness of our method, we conduct a series of experiments on three real-world benchmark datasets, including Beibei1 [12], Taobao2 [47], and MBD (multi-behavior dataset), a dataset we collected from an operational e-commerce platform.
æœ¬æ‰‹æ³•ã®æœ‰åŠ¹æ€§ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã€Beibei1 [12]ã€Taobao2 [47]ã€MBD (multi-behavior dataset)ã®3ã¤ã®å®Ÿä¸–ç•Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ä¸€é€£ã®å®Ÿé¨“ã‚’è¡Œã£ãŸã€‚
The details are as follows: (i) The Beibei dataset is an open dataset collected from Beibei, the largest infant product e-commerce platform in China, which includes three types of behavior, click, add-to-cart and purchase.
è©³ç´°ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã‚ã‚‹ï¼š (i)Beibeiãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ä¸­å›½æœ€å¤§ã®å¹¼å…å‘ã‘å•†å“ECãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã‚ã‚‹Beibeiã‹ã‚‰åé›†ã•ã‚ŒãŸã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚Šã€ã‚¯ãƒªãƒƒã‚¯ã€ã‚«ãƒ¼ãƒˆã«å…¥ã‚Œã‚‹ã€è³¼å…¥ã®3ç¨®é¡ã®è¡Œå‹•ãŒå«ã¾ã‚Œã‚‹ã€‚
This work uses two kinds of behavioral data, clicks and purchases.
ã“ã®ä½œå“ã§ã¯ã€ã‚¯ãƒªãƒƒã‚¯ã¨è³¼å…¥ã¨ã„ã†2ç¨®é¡ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã€‚
(ii) The Taobao dataset is an open dataset collected from Taobao, the largest e-commerce platform in China, which includes three types of behavior, click, add to cart and purchase.
(ii)ã‚¿ã‚ªãƒã‚ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ä¸­å›½æœ€å¤§ã®é›»å­å•†å–å¼•ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã‚ã‚‹ã‚¿ã‚ªãƒã‚ªã‹ã‚‰åé›†ã•ã‚ŒãŸã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚Šã€ã‚¯ãƒªãƒƒã‚¯ã€ã‚«ãƒ¼ãƒˆã«å…¥ã‚Œã‚‹ã€è³¼å…¥ã®3ç¨®é¡ã®è¡Œå‹•ã‚’å«ã‚€ã€‚
In this work, we use clicks and purchases of this dataset.
æœ¬ç ”ç©¶ã§ã¯ã€ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¯ãƒªãƒƒã‚¯æ•°ã¨è³¼å…¥æ•°ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
(iii) The MBD dataset is collected from an operational e-commerce platform, and includes two types of behavior, click and purchase.
(iii) MBDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€é‹ç”¨ä¸­ã®eã‚³ãƒãƒ¼ã‚¹ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‹ã‚‰åé›†ã•ã‚Œã€ã‚¯ãƒªãƒƒã‚¯ã¨è³¼å…¥ã®2ç¨®é¡ã®è¡Œå‹•ã‚’å«ã‚€ã€‚
For each dataset, we ensure that users have interactions on both types of behavior, and we set click data as auxiliary behavior data and purchase data as target behavior data.
å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä¸¡æ–¹ã®è¡Œå‹•ã«é–¢ã™ã‚‹ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã€ã‚¯ãƒªãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è£œåŠ©è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã€è³¼å…¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã¨ã™ã‚‹ã€‚
Table 1 shows the statistics of our datasets
è¡¨1ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆã‚’ç¤ºã™ã€‚

## 4.3. Evaluation protocols è©•ä¾¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«

We divide the datasets into training and test sets with a ratio of 4:1.
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¨“ç·´ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«4:1ã®æ¯”ç‡ã§åˆ†å‰²ã™ã‚‹ã€‚
We adopt two widely used metrics Recall@ğ‘˜ and NDCG@ğ‘˜.
æˆ‘ã€…ã¯ã€åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹2ã¤ã®æŒ‡æ¨™Recall@\_1D458ã¨NDCG@\_1D458ã‚’æ¡ç”¨ã™ã‚‹ã€‚
Recall@ğ‘˜ represents the coverage of true positive items that appear in the final top-ğ‘˜ ranked list.
Recall@â†ªLl458â†©ã¯ã€æœ€çµ‚çš„ãªä¸Šä½â†ªLl458ä½ãƒªã‚¹ãƒˆã«è¡¨ç¤ºã•ã‚Œã‚‹çœŸæ­£é …ç›®ã®ã‚«ãƒãƒ¼ç‡ã‚’è¡¨ã™ã€‚
NDCG@ğ‘˜ measures the ranking quality of the final recommended items.
NDCG@\_1D458 ã¯ã€æœ€çµ‚çš„ãªæ¨å¥¨é …ç›®ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°å“è³ªã‚’æ¸¬å®šã™ã‚‹ã€‚
In our experiments, we use the setting of ğ‘˜ = 10, 20.
å®Ÿé¨“ã§ã¯ã€ğ‘˜ = 10, 20ã®è¨­å®šã‚’ä½¿ç”¨ã—ãŸã€‚
For our method and the baselines, the reported results are the average values over all users.
æˆ‘ã€…ã®æ–¹æ³•ã¨ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«ã¤ã„ã¦ã€å ±å‘Šã•ã‚ŒãŸçµæœã¯å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¹³å‡å€¤ã§ã‚ã‚‹ã€‚
For every result, we conduct the experiments three times and report the average values.
ã™ã¹ã¦ã®çµæœã«ã¤ã„ã¦ã€å®Ÿé¨“ã‚’3å›è¡Œã„ã€ãã®å¹³å‡å€¤ã‚’å ±å‘Šã™ã‚‹ã€‚

## 4.4. Baselines ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

To demonstrate the effectiveness of our method, we compare MBA with several state-of-the-art methods.
æœ¬æ‰‹æ³•ã®æœ‰åŠ¹æ€§ã‚’å®Ÿè¨¼ã™ã‚‹ãŸã‚ã€MBAã‚’ã„ãã¤ã‹ã®æœ€æ–°æ‰‹æ³•ã¨æ¯”è¼ƒã™ã‚‹ã€‚
The methods used for comparison include single-behavior models, multi-behavior models, and recommendation denoising methods.
æ¯”è¼ƒã«ä½¿ç”¨ã—ãŸæ‰‹æ³•ã«ã¯ã€å˜ä¸€è¡Œå‹•ãƒ¢ãƒ‡ãƒ«ã€è¤‡æ•°è¡Œå‹•ãƒ¢ãƒ‡ãƒ«ã€æ¨è–¦ãƒã‚¤ã‚ºé™¤å»æ³•ãªã©ãŒã‚ã‚‹ã€‚
The single-behavior models that we consider are: (i) MF-BPR [28], which uses bayesian personalized ranking (BPR) loss to optimize matrix factorization.
æˆ‘ã€…ãŒè€ƒæ…®ã™ã‚‹å˜ä¸€è¡Œå‹•ãƒ¢ãƒ‡ãƒ«ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã‚ã‚‹ï¼š (i) MF-BPR [28]ã€ã“ã‚Œã¯è¡Œåˆ—åˆ†è§£ã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã«ãƒ™ã‚¤ã‚¸ã‚¢ãƒ³ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºãƒ‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°(BPR)æå¤±ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
(ii) NGCF [34], which encodes collaborative signals into the embedding process through multiple graph convolutional layers and models higher-order connectivity in user-item graphs.
(ii)NGCF[34]ã¯ã€è¤‡æ•°ã®ã‚°ãƒ©ãƒ•ç•³ã¿è¾¼ã¿å±¤ã‚’é€šã—ã¦åŸ‹ã‚è¾¼ã¿ãƒ—ãƒ­ã‚»ã‚¹ã«å”èª¿ä¿¡å·ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼é …ç›®ã‚°ãƒ©ãƒ•ã®é«˜æ¬¡ã®æ¥ç¶šæ€§ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ã€‚
(iii) LightGCN [15], which simplifies graph convolution by removing the matrix transformation and non-linear activation.
(iii) LightGCN [15]ã¯ã€è¡Œåˆ—å¤‰æ›ã¨éç·šå½¢æ´»æ€§åŒ–ã‚’é™¤å»ã™ã‚‹ã“ã¨ã§ã€ã‚°ãƒ©ãƒ•ç•³ã¿è¾¼ã¿ã‚’å˜ç´”åŒ–ã™ã‚‹ã€‚
We use the BPR loss to optimize LightGCN.
BPRãƒ­ã‚¹ã‚’åˆ©ç”¨ã—ã¦LightGCNã‚’æœ€é©åŒ–ã™ã‚‹ã€‚
The multi-behavior models that we consider are: (i) MB-GCN [18], which constructs a multi-behavior heterogeneous graph and uses GCN to perform behavior-aware embedding propagation.
æˆ‘ã€…ãŒæ¤œè¨ã™ã‚‹è¤‡æ•°è¡Œå‹•ãƒ¢ãƒ‡ãƒ«ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã‚ã‚‹ï¼š (i)MB-GCN[18]ã¯ã€ãƒãƒ«ãƒè¡Œå‹•ç•°ç¨®ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ã—ã€GCNã‚’ä½¿ç”¨ã—ã¦è¡Œå‹•ã‚’æ„è­˜ã—ãŸåŸ‹ã‚è¾¼ã¿ä¼æ¬ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
(ii) MB- GMN [39], which incorporates multi-behavior pattern modeling with the meta-learning paradigm.
(ii)MB-GMN[39]ã¯ã€ãƒ¡ã‚¿å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã«ã‚ˆã‚‹è¤‡æ•°è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’çµ„ã¿è¾¼ã‚“ã ã‚‚ã®ã§ã‚ã‚‹ã€‚
(iii) CML [37], which uses a new multi-behavior contrastive learning paradigm to capture the transferable user-item relationships from multi-behavior data.
(iii)CML[37]ã¯ã€æ–°ã—ã„è¤‡æ•°è¡Œå‹•å¯¾ç…§å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’ç”¨ã„ã€è¤‡æ•°è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¼é”å¯èƒ½ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®é–¢ä¿‚ã‚’æ‰ãˆã‚‹ã€‚
To verify that the proposed method improves performance by denoising implicit feedback, we also introduce the following denoising frameworks: (i) WBPR [11], which is a re-sampling-based method which considers popular, but un-interacted items are highly likely to be negative.
ææ¡ˆæ‰‹æ³•ãŒæš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ãƒã‚¤ã‚ºé™¤å»ã™ã‚‹ã“ã¨ã§æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã«ã€ä»¥ä¸‹ã®ãƒã‚¤ã‚ºé™¤å»ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚‚ç´¹ä»‹ã™ã‚‹ï¼š (i)WBPR[11]ã¯ã€å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«åŸºã¥ãæ‰‹æ³•ã§ã‚ã‚Šã€äººæ°—ã®ã‚ã‚‹ã€ã—ã‹ã—å¯¾è©±ã•ã‚Œã¦ã„ãªã„é …ç›®ã¯å¦å®šçš„ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã¨è€ƒãˆã‚‹ã€‚
(ii) T-CE [32], which is a re-weighting based method which discards the large-loss samples with a dynamic threshold in each iteration.
(ii)T-CE[32]ã¯ã€å„åå¾©ã«ãŠã„ã¦å‹•çš„ãªé–¾å€¤ã§æå¤±ã®å¤§ãã„ã‚µãƒ³ãƒ—ãƒ«ã‚’ç ´æ£„ã™ã‚‹ã€å†é‡ã¿ä»˜ã‘ã«åŸºã¥ãæ‰‹æ³•ã§ã‚ã‚‹ã€‚
(iii) DeCA [35], which is a newly proposed denoising method that utilizes the agreement predictions on clean examples across different models and minimizes the KL-divergence between the real user preference parameterized by two recommendation models.
(iii) DeCA [35]ã¯ã€æ–°ãŸã«ææ¡ˆã•ã‚ŒãŸãƒã‚¤ã‚ºé™¤å»æ‰‹æ³•ã§ã‚ã‚Šã€ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«é–“ã®ã‚¯ãƒªãƒ¼ãƒ³ãªä¾‹ã«ãŠã‘ã‚‹ä¸€è‡´äºˆæ¸¬ã‚’åˆ©ç”¨ã—ã€2ã¤ã®æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚ŒãŸå®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½é–“ã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’æœ€å°åŒ–ã™ã‚‹ã€‚
(iv) SGDL [13], which is a new denoising paradigm that utilizes self-labeled memorized data as denoising signals to improve the robustness of recommendation models.
(iv)SGDL[13]ã¯ã€æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®é ‘å¥æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€è‡ªå·±ãƒ©ãƒ™ãƒ«åŒ–ã•ã‚ŒãŸè¨˜æ†¶ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚¤ã‚ºé™¤å»ä¿¡å·ã¨ã—ã¦åˆ©ç”¨ã™ã‚‹æ–°ã—ã„ãƒã‚¤ã‚ºé™¤å»ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§ã‚ã‚‹ã€‚

## 4.5. Implementation details å®Ÿè£…ã®è©³ç´°

We implement our method with PyTorch.3 Without special mention, we set MF as our base model ğ‘¡ğœƒ since MF is still one of the best models for capturing user preferences for recommendations [29].
ç‰¹ã«è¨€åŠã™ã‚‹ã“ã¨ãªãã€MFã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ğ‘¡ã¨ã—ã¦è¨­å®šã—ã¾ã™ã€‚MFã¯ã€ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã‚’æ‰ãˆã‚‹ãŸã‚ã®æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã®1ã¤ã§ã™[29]ã€‚
The model is optimized by Adam [20] optimizer with a learning rate of 0.001, where the batch size is set as 2048.
ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€Adam [20]ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã«ã‚ˆã£ã¦ã€å­¦ç¿’ç‡0.001ã€ãƒãƒƒãƒã‚µã‚¤ã‚º2048ã§æœ€é©åŒ–ã•ã‚Œã‚‹ã€‚
The embedding size is set to 32.
åŸ‹ã‚è¾¼ã¿ã‚µã‚¤ã‚ºã¯32ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã€‚
The hyperparameters ğ›¼, ğ¶1 and ğ¶2 are search from { 1, 10, 100, 1000 }.
ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Ç–, ğ¶1, ğ¶2ã¯{ 1, 10, 100, 1000 }ã‹ã‚‰æ¢ç´¢ã•ã‚Œã‚‹ã€‚
ğ›½ is search from { 0.7, 0.8, 1 }.
â†ªL_1FDâ†©ã¯{ 0.7, 0.8, 1 }ã®ä¸­ã‹ã‚‰æ¢ã™ã€‚
To avoid over-fitting, ğ¿2 normalization is searched in { 10âˆ’6 , 10âˆ’5 , .
ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒˆã‚’é¿ã‘ã‚‹ãŸã‚ã€ â†ªLu_1D43F ã®æ­£è¦åŒ–ã¯ { 10-6 , 10-5 , .
..
..
, 1 }.
, 1 }.
Each training step is formed by one interacted example, and one randomly sampled negative example for efficient computation.
å„è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—ã¯ã€åŠ¹ç‡çš„ãªè¨ˆç®—ã®ãŸã‚ã«ã€1ã¤ã®ç›¸äº’ä½œç”¨ã®ã‚ã‚‹ä¾‹ã¨1ã¤ã®ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸè² ã®ä¾‹ã§å½¢æˆã•ã‚Œã‚‹ã€‚
We use Recall@20 on the test set for early stopping if the value does not increase after 20 epochs.
20ã‚¨ãƒãƒƒã‚¯å¾Œã«å€¤ãŒå¢—åŠ ã—ãªã„å ´åˆã¯ã€ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®Recall@20ã‚’ä½¿ç”¨ã—ã¦æ—©æœŸåœæ­¢ã‚’è¡Œã†ã€‚
For the hyperparameters of all recommendation baselines, we use the values suggested by the original papers with carefully finetuning on the three datasets.
ã™ã¹ã¦ã®æ¨è–¦ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã¯ã€3ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æ…é‡ã«å¾®èª¿æ•´ã‚’è¡Œã„ãªãŒã‚‰ã€å…ƒã®è«–æ–‡ã§ç¤ºå”†ã•ã‚ŒãŸå€¤ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã€‚
For all graph-based methods, the number of graph-based message propagation layers is fixed at 3.
ã™ã¹ã¦ã®ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã«ãŠã„ã¦ã€ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¼æ¬ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ•°ã¯3ã«å›ºå®šã•ã‚Œã¦ã„ã‚‹ã€‚

# 5. Experimental Results å®Ÿé¨“çµæœ

## 5.1. Performance comparison (RQ1) ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒï¼ˆRQ1ï¼‰

To answer RQ1, we conduct experiments on the Beibei, Taobao and MBD datasets.
RQ1ã«ç­”ãˆã‚‹ãŸã‚ã€Beibeiã€Taobaoã€MBDã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å®Ÿé¨“ã‚’è¡Œã£ãŸã€‚
The performance comparisons are reported in Table 2.
æ€§èƒ½æ¯”è¼ƒã‚’è¡¨2ã«ç¤ºã™ã€‚
From the table, we have the following observations.
è¡¨ã‹ã‚‰ã€æˆ‘ã€…ã¯æ¬¡ã®ã‚ˆã†ãªè¦‹è§£ã‚’å¾—ãŸã€‚
First, the proposed MBA method achieves the best performance and consistently outperforms all baselines across all datasets.
ã¾ãšã€ææ¡ˆã•ã‚ŒãŸMBAæ³•ã¯æœ€é«˜ã®æ€§èƒ½ã‚’é”æˆã—ã€ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã„ã¦ã™ã¹ã¦ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸€è²«ã—ã¦ä¸Šå›ã‚‹ã€‚
For instance, the average improvement of MBA over the strongest baseline is approximately 6.3% on the Beibei dataset, 6.6% on the Taobao dataset and 1.5% on the MBD dataset.
ä¾‹ãˆã°ã€æœ€å¼·ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«å¯¾ã™ã‚‹MBAã®å¹³å‡æ”¹å–„ç‡ã¯ã€Beibeiãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ç´„6.3%ã€Taobaoãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§6.6%ã€MBDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§1.5%ã§ã‚ã‚‹ã€‚
These improvements demonstrate the effectiveness of MBA.
ã“ã‚Œã‚‰ã®æ”¹å–„ã¯ã€MBAã®æœ‰åŠ¹æ€§ã‚’è¨¼æ˜ã—ã¦ã„ã‚‹ã€‚
We contribute the significant performance improvement to the following two reasons: (i) we align the user preferences based on two types of two behavior, transferring useful information from the auxiliary behavior data to enhance the performance of the target behavior predictions; (ii) noisy interactions are reduced through preference alignment, which helps to improve the learning of the latent universal true user preferences.
æ€§èƒ½ã®å¤§å¹…ãªå‘ä¸Šã¯ã€ä»¥ä¸‹ã®2ã¤ã®ç†ç”±ã«ã‚ˆã‚‹ï¼š (i)2ç¨®é¡ã®2ã¤ã®è¡Œå‹•ã«åŸºã¥ããƒ¦ãƒ¼ã‚¶å—œå¥½ã‚’æ•´åˆ—ã•ã›ã€è£œåŠ©è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ‰ç”¨ãªæƒ…å ±ã‚’è»¢é€ã—ã€ç›®æ¨™è¡Œå‹•äºˆæ¸¬ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚(ii)å—œå¥½ã®æ•´åˆ—ã«ã‚ˆã‚Šãƒã‚¤ã‚ºã®å¤šã„ç›¸äº’ä½œç”¨ãŒæ¸›å°‘ã—ã€æ½œåœ¨çš„ã§æ™®éçš„ãªçœŸã®ãƒ¦ãƒ¼ã‚¶å—œå¥½ã®å­¦ç¿’ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚
Second, except CML the multi-behavior models outperform the single-behavior models by a large margin.
ç¬¬äºŒã«ã€CMLã‚’é™¤ã„ã¦ã€è¤‡æ•°è¡Œå‹•ãƒ¢ãƒ‡ãƒ«ã¯å˜ä¸€è¡Œå‹•ãƒ¢ãƒ‡ãƒ«ã‚’å¤§ããä¸Šå›ã£ã¦ã„ã‚‹ã€‚
This reflects the fact that adding auxiliary behavior information can improve the recommendation performance of the target behavior.
ã“ã‚Œã¯ã€è£œåŠ©çš„ãªè¡Œå‹•æƒ…å ±ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€å¯¾è±¡è¡Œå‹•ã®æ¨è–¦æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’åæ˜ ã—ã¦ã„ã‚‹ã€‚
We conjecture that CML cannot achieve satisfactory performance because it incorporates the knowledge contained in auxiliary behavior through contrastive meta-learning, which introduces more noisy signals.
ç§ãŸã¡ã¯ã€CMLãŒæº€è¶³ã®ã„ãæ€§èƒ½ã‚’é”æˆã§ããªã„ã®ã¯ã€å¯¾ç…§çš„ãªãƒ¡ã‚¿å­¦ç¿’ã«ã‚ˆã£ã¦è£œåŠ©å‹•ä½œã«å«ã¾ã‚Œã‚‹çŸ¥è­˜ã‚’å–ã‚Šè¾¼ã‚€ãŸã‚ã€ã‚ˆã‚Šãƒã‚¤ã‚ºã®å¤šã„ä¿¡å·ãŒå°å…¥ã•ã‚Œã‚‹ã‹ã‚‰ã ã¨æ¨æ¸¬ã—ã¦ã„ã‚‹ã€‚
Furthermore, we compare MBA with the best single-behavior model (NGCF on the Beibei and MBD datasets, LightGCN on the Taobao dataset), and see that MBA achieves an average improvement of 12.4% on the Beibei dataset, 26.8% on the Taobao dataset and 15.3% on the MBD dataset.
ã•ã‚‰ã«ã€MBAã‚’æœ€è‰¯ã®å˜ä¸€è¡Œå‹•ãƒ¢ãƒ‡ãƒ«ï¼ˆBeibeiãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨MBDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯NGCFã€Taobaoãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯LightGCNï¼‰ã¨æ¯”è¼ƒã™ã‚‹ã¨ã€MBAã¯Beibeiãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å¹³å‡12.4%ã€Taobaoãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§26.8%ã€MBDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§15.3%ã®æ”¹å–„ã‚’é”æˆã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
To conclude, the proposed MBA approach consistently and significantly outperforms related single-behavior and multi-behavior recommendation baselines on the purchase prediction task.
çµè«–ã¨ã—ã¦ã€ææ¡ˆã™ã‚‹MBAã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€è³¼è²·äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€é–¢é€£ã™ã‚‹å˜ä¸€è¡Œå‹•ãŠã‚ˆã³è¤‡æ•°è¡Œå‹•æ¨è–¦ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸€è²«ã—ã¦æœ‰æ„ã«å‡Œé§•ã™ã‚‹ã€‚

## 5.2. Denoising (RQ2)

Table 3 reports on a performance comparison with existing denoising frameworks on the Beibei, Taobao and MBD datasets.
è¡¨3ã¯ã€Beibeiã€Taobaoã€MBDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã‘ã‚‹æ—¢å­˜ã®ãƒã‚¤ã‚ºé™¤å»ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨ã®æ€§èƒ½æ¯”è¼ƒã§ã‚ã‚‹ã€‚
The results demonstrate that MBA can provide more robust recommendations and improve overall performance than competing approaches.
ãã®çµæœã€MBAã¯ç«¶åˆã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚ˆã‚Šã‚‚å¼·å›ºãªæ¨å¥¨ã‚’æä¾›ã—ã€å…¨ä½“çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã“ã¨ãŒå®Ÿè¨¼ã•ã‚ŒãŸã€‚
Most of the denoising baselines do not obtain satisfactory results, even after carefully tuning their hyperparameters.
ã»ã¨ã‚“ã©ã®ãƒã‚¤ã‚ºé™¤å»ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¯ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ³¨æ„æ·±ãèª¿æ•´ã—ã¦ã‚‚ã€æº€è¶³ã®ã„ãçµæœãŒå¾—ã‚‰ã‚Œãªã„ã€‚
Only WBPR can outperform normal training in some cases.
WBPRã ã‘ãŒã€å ´åˆã«ã‚ˆã£ã¦ã¯é€šå¸¸ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ä¸Šå›ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
However, MBA consistently outperforms normal training and other denoising frameworks.
ã—ã‹ã—ã€MBAã¯é€šå¸¸ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„ä»–ã®ãƒã‚¤ã‚ºé™¤å»ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚ˆã‚Šã‚‚å¸¸ã«å„ªã‚Œã¦ã„ã‚‹ã€‚
We think the reasons for this are as follows: (i) In MBA, we use the alignment between multi-behavior data as the denoising signal and then transmit information from the multi-behavior distribution to the latent universal true user preference distribution.
ãã®ç†ç”±ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è€ƒãˆã‚‰ã‚Œã‚‹ï¼š (i)MBAã§ã¯ã€ãƒãƒ«ãƒè¡Œå‹•ãƒ‡ãƒ¼ã‚¿é–“ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’ãƒã‚¤ã‚ºé™¤å»ä¿¡å·ã¨ã—ã¦ä½¿ç”¨ã—ã€ãƒãƒ«ãƒè¡Œå‹•åˆ†å¸ƒã‹ã‚‰æ½œåœ¨çš„ãªæ™®éçš„ãªçœŸã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å—œå¥½åˆ†å¸ƒã«æƒ…å ±ã‚’ä¼é”ã™ã‚‹ã€‚
This learning process facilitates knowledge transfer across multiple types of user behavior and filters out noisy signals.
ã“ã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ã«ã‚ãŸã‚‹çŸ¥è­˜ã®ä¼é”ã‚’å®¹æ˜“ã«ã—ã€ãƒã‚¤ã‚ºã®å¤šã„ä¿¡å·ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã€‚
(ii) In the original papers of the compared denoising baselines, testing is conducted based on explicit user-item ratings.
(ii)æ¯”è¼ƒã•ã‚ŒãŸãƒã‚¤ã‚ºé™¤å»ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ã‚ªãƒªã‚¸ãƒŠãƒ«è«–æ–‡ã§ã¯ã€ãƒ†ã‚¹ãƒˆã¯æ˜ç¤ºçš„ãªãƒ¦ãƒ¼ã‚¶ãƒ¼é …ç›®ã®è©•ä¾¡ã«åŸºã¥ã„ã¦å®Ÿæ–½ã•ã‚Œã¦ã„ã‚‹ã€‚
However, our method does not use any explicit information like ratings, only implicit interaction data is considered.
ã—ã‹ã—ã€æˆ‘ã€…ã®æ–¹æ³•ã§ã¯ã€è¦–è´ç‡ã®ã‚ˆã†ãªæ˜ç¤ºçš„ãªæƒ…å ±ã¯ä¸€åˆ‡ä½¿ç”¨ã›ãšã€æš—é»™çš„ãªç›¸äº’ä½œç”¨ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’è€ƒæ…®ã™ã‚‹ã€‚
To further explore the generalization capability of MBA, we also adopt LightGCN as our base model (i.e., using LightGCN asğ‘¡ğœƒ ).
MBAã®æ±åŒ–èƒ½åŠ›ã‚’ã•ã‚‰ã«æ¢æ±‚ã™ã‚‹ãŸã‚ã«ã€LightGCNã‚‚ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æ¡ç”¨ã—ã¾ã™ï¼ˆã¤ã¾ã‚Šã€LightGCNã‚’ğ‘¡ğœƒã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ï¼‰ã€‚
The results are also shown in Table 3.
çµæœã¯è¡¨3ã«ã‚‚ç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚
We see that MBA is still more effective than the baseline methods.
MBAã¯ä¾ç„¶ã¨ã—ã¦ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ–¹å¼ã‚ˆã‚Šã‚‚åŠ¹æœçš„ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
We find that LightGCN-based MBA does not perform as well as MF-based MBA on the Beibei and Taobao datasets.
LightGCNãƒ™ãƒ¼ã‚¹ã®MBAã¯ã€Beibeiã¨Taobaoã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ã€MFãƒ™ãƒ¼ã‚¹ã®MBAã»ã©ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã•ãªã„ã“ã¨ãŒã‚ã‹ã£ãŸã€‚
We think the possible reasons are as follows: (i) LightGCN is more complex than MF, making MBA more difficult to train; (ii) LightGCN may be more sensitive to noisy signals due to the aggregation of neighbourhoods, resulting in a decline in the MBA performance compared to using MF as the base model.
è€ƒãˆã‚‰ã‚Œã‚‹ç†ç”±ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã‚ã‚‹ï¼š (i)LightGCNã¯MFã‚ˆã‚Šã‚‚è¤‡é›‘ã§ã‚ã‚‹ãŸã‚ã€MBAã®è¨“ç·´ãŒã‚ˆã‚Šå›°é›£ã§ã‚ã‚‹ã€‚(ii)LightGCNã¯è¿‘éš£ã®é›†åˆä½“ã§ã‚ã‚‹ãŸã‚ã€ãƒã‚¤ã‚ºã®å¤šã„ä¿¡å·ã«å¯¾ã—ã¦ã‚ˆã‚Šæ•æ„Ÿã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€ãã®çµæœã€MFã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹å ´åˆã‚ˆã‚Šã‚‚MBAã®æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹ã€‚
To conclude, the proposed MBA can generate more accurate recommendation compared with existing denoising frameworks.
çµè«–ã¨ã—ã¦ã€ææ¡ˆã•ã‚ŒãŸMBAã¯ã€æ—¢å­˜ã®ãƒã‚¤ã‚ºé™¤å»ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨æ¯”è¼ƒã—ã¦ã€ã‚ˆã‚Šæ­£ç¢ºãªæ¨è–¦ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

## 5.3. User preferences visualization (RQ3) ãƒ¦ãƒ¼ã‚¶ãƒ¼å—œå¥½ã®å¯è¦–åŒ–ï¼ˆRQ3ï¼‰

To answer RQ3, we visualize the distribution of usersâ€™ interacted items.
RQ3ã«ç­”ãˆã‚‹ãŸã‚ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¯¾è©±ã‚¢ã‚¤ãƒ†ãƒ ã®åˆ†å¸ƒã‚’å¯è¦–åŒ–ã™ã‚‹ã€‚
We select two users in the Beibei, Taobao and MBD datasets and draw their behavior distributions using the parameters obtained from an MF model trained on the purchase behavior data and the parameters obtained from MBA, respectively.
Beibeiã€Taobaoã€MBDã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãã‚Œãã‚Œ2äººã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’é¸ã³ã€è³¼è²·è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ãŸMFãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨MBAã‹ã‚‰å¾—ã‚‰ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦è¡Œå‹•åˆ†å¸ƒã‚’æãã€‚
Figure 2 visualizes the results.
å›³2ã¯ãã®çµæœã‚’è¦–è¦šåŒ–ã—ãŸã‚‚ã®ã§ã‚ã‚‹ã€‚
From the figure, we observe that for one user, the clicked items and purchased items distributions of MBA stay much closer than that of MF.
å›³ã‹ã‚‰ã€ã‚ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¤ã„ã¦ã€MBAã®ã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã¨è³¼å…¥ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã®åˆ†å¸ƒã¯ã€MFã®åˆ†å¸ƒã‚ˆã‚Šã‚‚ãšã£ã¨è¿‘ã„ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
The observation indicates that MBA can successfully align multiple types of user behavior and infer universal and accurate user preferences.
ã“ã®è¦³å¯Ÿçµæœã¯ã€MBAãŒè¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ã‚’ã†ã¾ãèª¿æ•´ã—ã€æ™®éçš„ã§æ­£ç¢ºãªãƒ¦ãƒ¼ã‚¶ãƒ¼å—œå¥½ã‚’æ¨è«–ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
Besides, we see that different users in MBA have more obvious separations than users in MF, which implies that MBA provides more personalized user-specific recommendation than MF.
ã•ã‚‰ã«ã€MBAã®ç•°ãªã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯MFã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚ˆã‚Šã‚‚æ˜ã‚‰ã‹ã«åˆ†é›¢ã—ã¦ãŠã‚Šã€ã“ã‚Œã¯MBAãŒMFã‚ˆã‚Šã‚‚ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥æ¨è–¦ã‚’æä¾›ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚

## 5.4. Model investigation (RQ4) ãƒ¢ãƒ‡ãƒ«èª¿æŸ»ï¼ˆRQ4ï¼‰

5.4.1 Ablation study.
5.4.1 ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ã€‚
Regarding RQ4, we conduct an ablation study (see Table 4) on the following two settings: (i) MBA-KL: we remove KL-divergence when training MBA; and (ii) MBA-PT: we co-train the ğ‘ƒ (Rğ‘“ ) and ğ‘ƒ (Rğ‘”) in MBA instead of pre-training.
RQ4ã«é–¢ã—ã¦ã¯ã€ä»¥ä¸‹ã®2ã¤ã®è¨­å®šã§ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¿ãƒ‡ã‚£ã‚’è¡Œã£ãŸï¼ˆè¡¨4å‚ç…§ï¼‰ï¼š (i) MBA-KLï¼š (i)MBA-KLï¼šMBAã‚’è¨“ç·´ã™ã‚‹éš›ã«KL-ç™ºæ•£ã‚’é™¤å»ã™ã‚‹ï¼š MBA-PT: ğ‘ƒ (R_1D454) ã¨ ğ‘ƒ (R_1D454)ã‚’äº‹å‰å­¦ç¿’ã®ä»£ã‚ã‚Šã«MBAã§å”èª¿å­¦ç¿’ã•ã›ã‚‹ã€‚
The results show that both parts (KL-divergence and pre-trained models) are essential to MBA because removing either will lead to a performance decrease.
ãã®çµæœã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨äº‹å‰è¨“ç·´ãƒ¢ãƒ‡ãƒ«ã®ä¸¡æ–¹ãŒMBAã«ã¨ã£ã¦ä¸å¯æ¬ ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸã€‚
Without KL-divergence, we see the performance drops substantially in terms of all metrics.
KL-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ãŒãªã„ã¨ã€ã™ã¹ã¦ã®æ¸¬å®šåŸºæº–ã«ãŠã„ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå¤§å¹…ã«ä½ä¸‹ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
Hence, the KL-divergence helps align the user preferences learned from different behaviors, thus improving the recommendation performance.
ã—ãŸãŒã£ã¦ã€KL-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯ã€ç•°ãªã‚‹è¡Œå‹•ã‹ã‚‰å­¦ç¿’ã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å—œå¥½ã‚’èª¿æ•´ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã€æ¨è–¦æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚
Without pre-trained models, the results drop dramatically, especially in the Taobao dataset, which indicates that it is hard to cotrain ğ‘ƒ (Rğ‘“ ) and ğ‘ƒ (Rğ‘”) with MBA.
ã“ã‚Œã¯ã€MBAã§ğ‘ƒ (R_1D443) ã¨ğ‘ƒ (R_1D444) ã‚’å…± è¨“ç·´ã™ã‚‹ã®ãŒé›£ã—ã„ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
Using a pre-trained model can reduce MBAâ€™s complexity and provide prior knowledge so that it can more effectively extract the userâ€™s real preferences from the different types of behavior distributions.5.4.2 Hyperparameter study.
äº‹å‰ã«è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€MBAã®è¤‡é›‘ã•ã‚’è»½æ¸›ã—ã€äº‹å‰çŸ¥è­˜ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ãŸã‚ã€ã•ã¾ã–ã¾ãªã‚¿ã‚¤ãƒ—ã®è¡Œå‹•åˆ†å¸ƒã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®çœŸã®å—œå¥½ã‚’ã‚ˆã‚ŠåŠ¹æœçš„ã«æŠ½å‡ºã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
Next, we conduct experiments to examine the effect of different parameter settings on MBA.
æ¬¡ã«ã€ã•ã¾ã–ã¾ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šãŒMBAã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¿ã¹ã‚‹å®Ÿé¨“ã‚’è¡Œã†ã€‚
Figure 3 shows the effect of ğ›¼, which is used to control the weight of the KL-divergence in conveying information.
å›³3ã¯ã€æƒ…å ±ä¼é”ã«ãŠã‘ã‚‹KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®é‡ã¿ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹â†ªLl_1FC ã®åŠ¹æœã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
On the Beibei dataset, the performance of MBA is affected when the ğ›¼ is greater than or equal to 100.
Beibeiãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ã€ â†ªLl_1D6FC ãŒ100ä»¥ä¸Šã®å ´åˆã€MBAã®æ€§èƒ½ã«å½±éŸ¿ãŒå‡ºã‚‹ã€‚
Thus, when dominated by KL-divergence, MBAâ€™s performance will be close to that of the pre-trained models.
ã—ãŸãŒã£ã¦ã€KL-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã«æ”¯é…ã•ã‚Œã‚‹å ´åˆã€MBAã®æ€§èƒ½ã¯äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã«è¿‘ããªã‚‹ã€‚
On the Taobao and MBD datasets, when ğ›¼ is greater than or equal to 100, MBA will gradually converge, with a relatively balanced state between the KL-divergence and the expectation term.
ã‚¿ã‚ªãƒã‚ªã¨MBDã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ã€Ç¼ãŒ100ä»¥ä¸Šã®å ´åˆã€MBAã¯å¾ã€…ã«åæŸã—ã€KL-ç™ºæ•£ã¨æœŸå¾…é …ã®é–“ã®æ¯”è¼ƒçš„ãƒãƒ©ãƒ³ã‚¹ã®ã¨ã‚ŒãŸçŠ¶æ…‹ã«ãªã‚‹ã€‚
Under this setting, MBA achieves the best performance.
ã“ã®è¨­å®šã§ã¯ã€MBAãŒæœ€é«˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã™ã‚‹ã€‚

# 6. Conclusion çµè«–

In this work, we have focused on the task of multi-behavior recommendation.
æœ¬ç ”ç©¶ã§ã¯ã€è¤‡æ•°ã®è¡Œå‹•ã‚’æ¨è–¦ã™ã‚‹ã‚¿ã‚¹ã‚¯ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸã€‚
We conjectured that multiple types of behavior from the same user reflect similar underlying user preferences.
ç§ãŸã¡ã¯ã€åŒã˜ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®è¡Œå‹•ã¯ã€æ ¹åº•ã«ã‚ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã‚’åæ˜ ã—ã¦ã„ã‚‹ã¨æ¨æ¸¬ã—ãŸã€‚
To tackle the challenges of the gap between data distributions of different types of behavior and the challenge of behavioral data being noisy and biased, we proposed a learning framework, namely multi-behavior alignment (MBA), which can infer universal user preferences from multiple types of observed behavioral data, while performing data denoising to achieve beneficial knowledge transfer.
ç•°ãªã‚‹è¡Œå‹•ã‚¿ã‚¤ãƒ—ã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®ã‚®ãƒ£ãƒƒãƒ—ã‚„ã€è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã«ãƒã‚¤ã‚ºã‚„åã‚ŠãŒã‚ã‚‹ã¨ã„ã†èª²é¡Œã«å¯¾ã—ã¦ã€æˆ‘ã€…ã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ã‚ºé™¤å»ã‚’è¡Œã„ãªãŒã‚‰ã€è¦³æ¸¬ã•ã‚ŒãŸè¤‡æ•°ç¨®é¡ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ™®éçš„ãªãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã‚’æ¨è«–ã—ã€æœ‰ç›ŠãªçŸ¥è­˜ä¼é”ã‚’å®Ÿç¾ã™ã‚‹å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ãƒãƒ«ãƒè¡Œå‹•ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ(MBA)ã‚’ææ¡ˆã—ãŸã€‚
Extensive experiments conducted on three real-world datasets showed the effectiveness of the proposed method.
3ã¤ã®å®Ÿä¸–ç•Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¡Œã£ãŸåºƒç¯„ãªå®Ÿé¨“ã«ã‚ˆã‚Šã€ææ¡ˆæ‰‹æ³•ã®æœ‰åŠ¹æ€§ãŒç¤ºã•ã‚ŒãŸã€‚
Our method proves the value of mining the universal user preferences from multi-behavior data for the implicit feedback-based recommendation.
æœ¬æ‰‹æ³•ã¯ã€æš—é»™ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ãæ¨è–¦ã®ãŸã‚ã«ã€è¤‡æ•°ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ™®éçš„ãªãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã‚’ãƒã‚¤ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã®ä¾¡å€¤ã‚’è¨¼æ˜ã™ã‚‹ã€‚
However, a limitation of MBA is that it can only align between two types of behavioral data.
ã—ã‹ã—ã€MBAã®é™ç•Œã¯ã€2ç¨®é¡ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿é–“ã§ã—ã‹æ•´åˆãŒå–ã‚Œãªã„ã“ã¨ã§ã‚ã‚‹ã€‚
As to our future work, we aim to perform alignment on more types of user behavior.
ä»Šå¾Œã®èª²é¡Œã¨ã—ã¦ã¯ã€ã‚ˆã‚Šå¤šãã®ç¨®é¡ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ã«å¯¾ã—ã¦ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’è¡Œã†ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã‚‹ã€‚
In addition, we plan to develop ways of conducting more effective and efficient model training.
ã•ã‚‰ã«ã€ã‚ˆã‚ŠåŠ¹æœçš„ã§åŠ¹ç‡çš„ãªãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®æ–¹æ³•ã‚’é–‹ç™ºã™ã‚‹äºˆå®šã ã€‚
