## link ãƒªãƒ³ã‚¯

- https://dl.acm.org/doi/pdf/10.1145/3604915.3608811 https://dl.acm.org/doi/pdf/10.1145/3604915.3608811

## title ã‚¿ã‚¤ãƒˆãƒ«

Augmented Negative Sampling for Collaborative Filtering
å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®ãŸã‚ã®æ‹¡å¼µãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

## abstract æŠ„éŒ²

Negative sampling is essential for implicit-feedback-based collaborative filtering, which is used to constitute negative signals from massive unlabeled data to guide supervised learning.
ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯ã€æ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚’å°ããŸã‚ã«ã€å¤§é‡ã®ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚·ã‚°ãƒŠãƒ«ã‚’æ§‹æˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ã€æš—é»™ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ™ãƒ¼ã‚¹ã®å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ä¸å¯æ¬ ã§ã‚ã‚‹ã€‚
The stateof-the-art idea is to utilize hard negative samples that carry more useful information to form a better decision boundary.
æœ€å…ˆç«¯ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã€ã‚ˆã‚Šè‰¯ã„æ±ºå®šå¢ƒç•Œã‚’å½¢æˆã™ã‚‹ãŸã‚ã«ã€ã‚ˆã‚Šæœ‰ç”¨ãªæƒ…å ±ã‚’æŒã¤ãƒãƒ¼ãƒ‰ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
To balance efficiency and effectiveness, the vast majority of existing methods follow the two-pass approach, in which the first pass samples a fixed number of unobserved items by a simple static distribution and then the second pass selects the final negative items using a more sophisticated negative sampling strategy.
åŠ¹ç‡ã¨æœ‰åŠ¹æ€§ã®ãƒãƒ©ãƒ³ã‚¹ã‚’ã¨ã‚‹ãŸã‚ã€æ—¢å­˜ã®æ‰‹æ³•ã®å¤§éƒ¨åˆ†ã¯2ãƒ‘ã‚¹ãƒ»ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«å¾“ã£ã¦ãŠã‚Šã€1ãƒ‘ã‚¹ç›®ã¯å˜ç´”ãªé™çš„åˆ†å¸ƒã«ã‚ˆã£ã¦å›ºå®šæ•°ã®æœªè¦³æ¸¬é …ç›®ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€2ãƒ‘ã‚¹ç›®ã¯ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã‚’ç”¨ã„ã¦æœ€çµ‚çš„ãªãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã‚’é¸æŠã™ã‚‹ã€‚
However, selecting negative samples from the original items in a dataset is inherently restricted due to the limited available choices, and thus may not be able to contrast positive samples well.
ã—ã‹ã—ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚ªãƒªã‚¸ãƒŠãƒ«é …ç›®ã‹ã‚‰ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠã™ã‚‹ã“ã¨ã¯ã€åˆ©ç”¨å¯èƒ½ãªé¸æŠè‚¢ãŒé™ã‚‰ã‚Œã¦ã„ã‚‹ãŸã‚ã€æœ¬è³ªçš„ã«åˆ¶ç´„ãŒã‚ã‚Šã€ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã‚’ã†ã¾ãå¯¾æ¯”ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
In this paper, we confirm this observation via carefully designed experiments and introduce two major limitations of existing solutions: ambiguous trap and information discrimination.
æœ¬ç¨¿ã§ã¯ã€å…¥å¿µã«è¨­è¨ˆã•ã‚ŒãŸå®Ÿé¨“ã«ã‚ˆã£ã¦ã“ã®è¦³å¯Ÿã‚’ç¢ºèªã—ã€æ—¢å­˜ã®è§£æ±ºç­–ã®2ã¤ã®å¤§ããªé™ç•Œã‚’ç´¹ä»‹ã™ã‚‹ï¼š 
æ›–æ˜§ãªç½ ã€ã¨ã€Œæƒ…å ±ã®è­˜åˆ¥ã€ã§ã‚ã‚‹ã€‚
Our response to such limitations is to introduce â€œaugmentedâ€ negative samples that may not exist in the original dataset.
ã“ã®ã‚ˆã†ãªåˆ¶é™ã«å¯¾ã™ã‚‹æˆ‘ã€…ã®å¯¾å¿œã¯ã€å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯å­˜åœ¨ã—ãªã„ã‹ã‚‚ã—ã‚Œãªã„ã€Œå¢—å¼·ã•ã‚ŒãŸã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
This direction renders a substantial technical challenge because constructing unconstrained negative samples may introduce excessive noise that eventually distorts the decision boundary.
ãªãœãªã‚‰ã€åˆ¶ç´„ã®ãªã„è² ã‚µãƒ³ãƒ—ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã¯ã€éå‰°ãªãƒã‚¤ã‚ºã‚’å°å…¥ã—ã€æœ€çµ‚çš„ã«æ±ºå®šå¢ƒç•Œã‚’æ­ªã‚ã¦ã—ã¾ã†å¯èƒ½æ€§ãŒã‚ã‚‹ã‹ã‚‰ã§ã‚ã‚‹ã€‚
To this end, we introduce a novel generic augmented negative sampling (ANS) paradigm and provide a concrete instantiation.
ã“ã®ç›®çš„ã®ãŸã‚ã«ã€æˆ‘ã€…ã¯æ–°ã—ã„ä¸€èˆ¬çš„ãªæ‹¡å¼µãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆANSï¼‰ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’å°å…¥ã—ã€å…·ä½“çš„ãªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã‚’æä¾›ã™ã‚‹ã€‚
First, we disentangle hard and easy factors of negative items.
ã¾ãšã€ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã®ãƒãƒ¼ãƒ‰è¦å› ã¨ã‚¤ãƒ¼ã‚¸ãƒ¼è¦å› ã‚’åˆ†é›¢ã™ã‚‹ã€‚
Next, we generate new candidate negative samples by augmenting only the easy factors in a regulated manner: the direction and magnitude of the augmentation are carefully calibrated.
æ¬¡ã«ã€èª¿æ•´ã•ã‚ŒãŸæ–¹æ³•ã§ç°¡å˜ãªå› å­ã®ã¿ã‚’å¢—å¼·ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€æ–°ã—ã„è² ã‚µãƒ³ãƒ—ãƒ«å€™è£œã‚’ç”Ÿæˆã™ã‚‹ï¼š 
å¢—å¼·ã®æ–¹å‘ã¨å¤§ãã•ã¯æ³¨æ„æ·±ãèª¿æ•´ã•ã‚Œã‚‹ã€‚
Finally, we design an advanced negative sampling strategy to identify the final augmented negative samples, which considers not only the score function used in existing methods but also a new metric called augmentation gain.
æœ€å¾Œã«ã€æ—¢å­˜ã®æ‰‹æ³•ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚¹ã‚³ã‚¢é–¢æ•°ã ã‘ã§ãªãã€ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚²ã‚¤ãƒ³ã¨å‘¼ã°ã‚Œã‚‹æ–°ã—ã„æŒ‡æ¨™ã‚‚è€ƒæ…®ã—ãŸã€æœ€çµ‚çš„ãªã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚ŒãŸãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã‚’ç‰¹å®šã™ã‚‹ãŸã‚ã®é«˜åº¦ãªãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã‚’è¨­è¨ˆã™ã‚‹ã€‚
Extensive experiments on real-world datasets demonstrate that our method significantly outperforms state-of-the-art baselines.
å®Ÿä¸–ç•Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãŸåºƒç¯„ãªå®Ÿé¨“ã«ã‚ˆã‚Šã€æˆ‘ã€…ã®æ‰‹æ³•ãŒæœ€å…ˆç«¯ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹ã“ã¨ãŒå®Ÿè¨¼ã•ã‚ŒãŸã€‚
Our code is publicly available at https://github.com/Asa9aoTK/ANS-Recbole.
æˆ‘ã€…ã®ã‚³ãƒ¼ãƒ‰ã¯ https://github.com/Asa9aoTK/ANS-Recbole ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚

# Introduction ã¯ã˜ã‚ã«

Collaborative filtering (CF), as an important paradigm of recommender systems, leverages observed user-item interactions to model usersâ€™ potential preferences [13, 15, 35].
å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆCFï¼‰ã¯ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®é‡è¦ãªãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ã®æ½œåœ¨çš„ãªå—œå¥½ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ãŸã‚ã«ã€è¦³å¯Ÿã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®ç›¸äº’ä½œç”¨ã‚’æ´»ç”¨ã™ã‚‹[13, 15, 35]ã€‚
In real-world scenarios, such interactions are normally in the form of implicit feedback (e.g., clicks or purchases), instead of explicit ratings [34].
å®Ÿä¸–ç•Œã®ã‚·ãƒŠãƒªã‚ªã§ã¯ã€ã“ã®ã‚ˆã†ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã¯é€šå¸¸ã€æ˜ç¤ºçš„ãªè©•ä¾¡ã§ã¯ãªãã€æš—é»™çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼ˆã‚¯ãƒªãƒƒã‚¯ã‚„è³¼å…¥ãªã©ï¼‰ã®å½¢ã§è¡Œã‚ã‚Œã‚‹[34]ã€‚
Each observed interaction is normally considered a positive sample.
è¦³å¯Ÿã•ã‚ŒãŸå„äº¤äº’ä½œç”¨ã¯é€šå¸¸ã€é™½æ€§ã‚µãƒ³ãƒ—ãƒ«ã¨ã¿ãªã•ã‚Œã‚‹ã€‚
As for negative samples, existing methods usually randomly select some uninteracted items.
ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã«é–¢ã—ã¦ã¯ã€æ—¢å­˜ã®æ–¹æ³•ã¯é€šå¸¸ã€ã„ãã¤ã‹ã®ç›¸äº’ä½œç”¨ã—ã¦ã„ãªã„é …ç›®ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠã™ã‚‹ã€‚
Then a CF model is optimized to give positive samples higher scores than negative ones via, for example, the Bayesian personalized ranking (BPR) loss function [26], where a score function (e.g., inner product) is used to measure the similarity between a user and an item.
ãã—ã¦ã€CFãƒ¢ãƒ‡ãƒ«ã¯ã€ä¾‹ãˆã°ã€ã‚¹ã‚³ã‚¢é–¢æ•°ï¼ˆä¾‹ãˆã°ã€å†…ç©ï¼‰ãŒãƒ¦ãƒ¼ã‚¶ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®é–“ã®é¡ä¼¼æ€§ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ãƒ™ã‚¤ã‚ºãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºãƒ‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆBPRï¼‰æå¤±é–¢æ•°[26]ã‚’ä»‹ã—ã¦ã€è‚¯å®šçš„ãªã‚µãƒ³ãƒ—ãƒ«ã«å¦å®šçš„ãªã‚µãƒ³ãƒ—ãƒ«ã‚ˆã‚Šã‚‚é«˜ã„ã‚¹ã‚³ã‚¢ã‚’ä¸ãˆã‚‹ã‚ˆã†ã«æœ€é©åŒ–ã•ã‚Œã‚‹ã€‚
Recent studies have shown that negative samples have a great impact on model performance [22, 41, 45].
æœ€è¿‘ã®ç ”ç©¶ã§ã¯ã€è² ã®ã‚µãƒ³ãƒ—ãƒ«ãŒãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã«å¤§ããªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹[22, 41, 45]ã€‚
As the state of the art, hard negative sampling strategies whose general idea is to oversample high-score negative items have exhibited promising performance [4, 8, 9, 41, 44].
ä¸€èˆ¬çš„ãªè€ƒãˆæ–¹ã¨ã—ã¦ã€é«˜å¾—ç‚¹ã®ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã‚’ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ãƒãƒ¼ãƒ‰ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ãŒã€æœ‰æœ›ãªæ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã‚‹[4, 8, 9, 41, 44]ã€‚
While selecting a negative item with a high score makes it harder for a model to classify a user, it has the potential to bring more useful information and greater gradients, which are beneficial to model training [4, 25].
é«˜ã„ã‚¹ã‚³ã‚¢ã§å¦å®šçš„ãªé …ç›®ã‚’é¸æŠã™ã‚‹ã“ã¨ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’åˆ†é¡ã™ã‚‹ã“ã¨ã‚’é›£ã—ãã™ã‚‹ä¸€æ–¹ã§ã€ã‚ˆã‚Šæœ‰ç”¨ãªæƒ…å ±ã‚„ã‚ˆã‚Šå¤§ããªå‹¾é…ã‚’ã‚‚ãŸã‚‰ã™å¯èƒ½æ€§ãŒã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«æœ‰ç›Šã§ã‚ã‚‹[4, 25]ã€‚
Ideally, one would calculate the scores of all uninteracted items to identify the best negative samples.
ç†æƒ³çš„ãªã®ã¯ã€æœ€é«˜ã®ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã‚’ç‰¹å®šã™ã‚‹ãŸã‚ã«ã€ç›¸äº’ä½œç”¨ã—ã¦ã„ãªã„ã™ã¹ã¦ã®é …ç›®ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
However, its time complexity is prohibitive.
ã—ã‹ã—ã€æ™‚é–“ã®è¤‡é›‘ã•ã¯æ³•å¤–ã§ã‚ã‚‹ã€‚
To balance efficiency and effectiveness, the two-pass approach has been widely adopted [1, 9, 16, 41, 44].
åŠ¹ç‡ã¨æœ‰åŠ¹æ€§ã®ãƒãƒ©ãƒ³ã‚¹ã‚’ã¨ã‚‹ãŸã‚ã«ã€2ãƒ‘ã‚¹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒåºƒãæ¡ç”¨ã•ã‚Œã¦ã„ã‚‹[1, 9, 16, 41, 44]ã€‚
The first pass samples a fixed number of unobserved items by a static distribution, and the second pass then selects the final negative items with a more sophisticated negative sampling method.
æœ€åˆã®ãƒ‘ã‚¹ã§ã¯ã€å›ºå®šæ•°ã®æœªè¦³æ¸¬é …ç›®ã‚’é™çš„åˆ†å¸ƒã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€2ç•ªç›®ã®ãƒ‘ã‚¹ã§ã¯ã€ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ³•ã§æœ€çµ‚çš„ãªãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã‚’é¸æŠã™ã‚‹ã€‚

Despite the significant progress made by hard negative sampling, selecting negative samples from the original items in a dataset is inherently restricted due to the limited available choices.
ãƒãƒ¼ãƒ‰ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã£ã¦å¤§ããªé€²æ­©ãŒã‚ã£ãŸã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®å…ƒã®é …ç›®ã‹ã‚‰ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠã™ã‚‹ã“ã¨ã¯ã€åˆ©ç”¨å¯èƒ½ãªé¸æŠè‚¢ãŒé™ã‚‰ã‚Œã¦ã„ã‚‹ãŸã‚ã€æœ¬è³ªçš„ã«åˆ¶é™ã•ã‚Œã¦ã„ã‚‹ã€‚
Such original items may not be able to contrast positive samples well.
ã“ã®ã‚ˆã†ãªã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¢ã‚¤ãƒ†ãƒ ã¯ã€é™½æ€§ã‚µãƒ³ãƒ—ãƒ«ã‚’ã†ã¾ãå¯¾æ¯”ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
Indeed, we design a set of intuitive experiments to show that existing works suffer from two major drawbacks.(1) Ambiguous trap.
å®Ÿéš›ã€æˆ‘ã€…ã¯ä¸€é€£ã®ç›´æ„Ÿçš„ãªå®Ÿé¨“ã‚’ãƒ‡ã‚¶ã‚¤ãƒ³ã—ã€æ—¢å­˜ã®ä½œå“ãŒ2ã¤ã®å¤§ããªæ¬ ç‚¹ã«è‹¦ã—ã‚“ã§ã„ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚
Since the vast majority of unobserved items have very low scores (i.e., they are easy negative samples), randomly sampling a small number of candidate negative items in the first pass is difficult to include useful hard negative samples, which, in turn, substantially limits the efficacy of the second pass.(2) Information discrimination.
æœªè¦³æ¸¬é …ç›®ã®å¤§éƒ¨åˆ†ã¯éå¸¸ã«ä½ã„ã‚¹ã‚³ã‚¢ï¼ˆã™ãªã‚ã¡ã€å®¹æ˜“ãªé™°æ€§ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã§ã‚ã‚‹ãŸã‚ã€ç¬¬1ãƒ‘ã‚¹ã§å°‘æ•°ã®é™°æ€§é …ç›®å€™è£œã‚’ç„¡ä½œç‚ºã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ã‚‚ã€æœ‰ç”¨ãªãƒãƒ¼ãƒ‰é™°æ€§ã‚µãƒ³ãƒ—ãƒ«ã‚’å«ã‚ã‚‹ã“ã¨ã¯å›°é›£ã§ã‚ã‚Šã€ã²ã„ã¦ã¯ç¬¬2ãƒ‘ã‚¹ã®æœ‰åŠ¹æ€§ã‚’å¤§å¹…ã«åˆ¶é™ã™ã‚‹ã“ã¨ã«ãªã‚‹(2)ã€‚
In the second pass, most existing studies overly focus on high-score negative items and largely neglect low-score negative items.
2ã¤ç›®ã®ãƒ‘ã‚¹ã§ã¯ã€æ—¢å­˜ã®ç ”ç©¶ã®ã»ã¨ã‚“ã©ãŒã€é«˜å¾—ç‚¹ã®ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã«éåº¦ã«ç„¦ç‚¹ã‚’å½“ã¦ã€ä½å¾—ç‚¹ã®ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã‚’ã»ã¨ã‚“ã©ç„¡è¦–ã—ã¦ã„ã‚‹ã€‚
We empirically show that such low-score negative items also contain critical, unique information that leads to better model performance.
æˆ‘ã€…ã¯ã€ã“ã®ã‚ˆã†ãªä½å¾—ç‚¹ã®ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã«ã‚‚ã€ã‚ˆã‚Šè‰¯ã„ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’ã‚‚ãŸã‚‰ã™é‡è¦ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å®Ÿè¨¼çš„ã«ç¤ºã™ã€‚
Our response to such drawbacks is to introduce â€œaugmentedâ€ negative samples (i.e., synthetic items) that are more similar to positive items while still being negative.
ã“ã®ã‚ˆã†ãªæ¬ ç‚¹ã«å¯¾ã™ã‚‹æˆ‘ã€…ã®å¯¾å¿œç­–ã¯ã€å¦å®šçš„ã§ã‚ã‚ŠãªãŒã‚‰è‚¯å®šçš„ãªé …ç›®ã«ã‚ˆã‚Šä¼¼ã¦ã„ã‚‹ã€Œå¢—å¼·ã•ã‚ŒãŸã€å¦å®šçš„ãªã‚µãƒ³ãƒ—ãƒ«ï¼ˆã™ãªã‚ã¡åˆæˆé …ç›®ï¼‰ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
While data augmentation techniques have been proposed in other domains [17, 37, 38, 40], it is technically challenging to apply similar ideas to negative sampling for collaborative filtering.
ãƒ‡ãƒ¼ã‚¿å¢—å¼·æŠ€è¡“ã¯ä»–ã®é ˜åŸŸã§ã‚‚ææ¡ˆã•ã‚Œã¦ã„ã‚‹ãŒ[17, 37, 38, 40]ã€å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«åŒæ§˜ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’é©ç”¨ã™ã‚‹ã®ã¯æŠ€è¡“çš„ã«å›°é›£ã§ã‚ã‚‹ã€‚
This is because all of them fail to carefully regulate and quantify the augmentation needed to approximate positive items while not introducing excessive noise or still being negative.
ãªãœãªã‚‰ã€ãã®ã©ã‚Œã‚‚ãŒã€éå‰°ãªãƒã‚¤ã‚ºã‚’ç™ºç”Ÿã•ã›ãŸã‚Šã€ãƒã‚¬ãƒ†ã‚£ãƒ–ãªã¾ã¾ã§ã‚ã‚‹ã“ã¨ã‚’é¿ã‘ãªãŒã‚‰ã€ãƒã‚¸ãƒ†ã‚£ãƒ–ãªé …ç›®ã‚’è¿‘ä¼¼ã•ã›ã‚‹ãŸã‚ã«å¿…è¦ãªè£œå¼·ã‚’æ³¨æ„æ·±ãèª¿æ•´ã—ã€å®šé‡åŒ–ã§ãã¦ã„ãªã„ã‹ã‚‰ã§ã‚ã‚‹ã€‚
To this end, we present a novel generic augmented negative sampling (ANS) paradigm and then provide a concrete instantiation.
ã“ã®ç›®çš„ã®ãŸã‚ã«ã€æˆ‘ã€…ã¯æ–°ã—ã„ä¸€èˆ¬çš„ãªæ‹¡å¼µãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆANSï¼‰ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’æç¤ºã—ã€å…·ä½“çš„ãªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã‚’æä¾›ã™ã‚‹ã€‚
Our insight is that it is imperative to understand a negative itemâ€™s hardness from a more fine-granular perspective.
ç§ãŸã¡ã®æ´å¯Ÿã¯ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ãªã‚¢ã‚¤ãƒ†ãƒ ã®ç¡¬ã•ã‚’ã‚ˆã‚Šç´°ã‹ã„ç²’çŠ¶ã®è¦³ç‚¹ã‹ã‚‰ç†è§£ã™ã‚‹ã“ã¨ãŒä¸å¯æ¬ ã§ã‚ã‚‹ã¨ã„ã†ã“ã¨ã§ã‚ã‚‹ã€‚
We propose to disentangle an itemâ€™s embedding into hard and easy factors (i.e., a set of dimensions of the embedding vector), where hardness is defined by whether a negative item has similar values to the corresponding user in the given factor.
æˆ‘ã€…ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ ã®åŸ‹ã‚è¾¼ã¿ã‚’ã€ãƒãƒ¼ãƒ‰ãªå› å­ã¨ã‚¤ãƒ¼ã‚¸ãƒ¼ãªå› å­ï¼ˆåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒã®é›†åˆï¼‰ã«åˆ†é›¢ã™ã‚‹ã“ã¨ã‚’ææ¡ˆã™ã‚‹ã€‚ãƒãƒ¼ãƒ‰ã•ã¯ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚¢ã‚¤ãƒ†ãƒ ãŒã€ä¸ãˆã‚‰ã‚ŒãŸå› å­ã«ãŠã„ã¦ã€å¯¾å¿œã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨åŒæ§˜ã®å€¤ã‚’æŒã¤ã‹ã©ã†ã‹ã«ã‚ˆã£ã¦å®šç¾©ã•ã‚Œã‚‹ã€‚
This definition is in line with the definition of hardness in hard negative sampling.
ã“ã®å®šç¾©ã¯ã€ãƒãƒ¼ãƒ‰ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ãŠã‘ã‚‹ç¡¬åº¦ã®å®šç¾©ã¨ä¸€è‡´ã—ã¦ã„ã‚‹ã€‚
Here the key technical challenge originates from the lack of supervision signals.
ã“ã“ã§é‡è¦ãªæŠ€è¡“çš„èª²é¡Œã¯ã€ç›£ç£ä¿¡å·ã®æ¬ å¦‚ã«èµ·å› ã™ã‚‹ã€‚
Consequently, we propose two learning tasks that combine contrastive learning (CL) [43] and disentanglement methods [12] to guarantee the credibility of the disentanglement.
ãã®çµæœã€æˆ‘ã€…ã¯ã€åˆ†é›¢ã®ä¿¡é ¼æ€§ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã«ã€å¯¾ç…§å­¦ç¿’ï¼ˆCLï¼‰[43]ã¨åˆ†é›¢æ‰‹æ³•[12]ã‚’çµ„ã¿åˆã‚ã›ãŸ2ã¤ã®å­¦ç¿’èª²é¡Œã‚’ææ¡ˆã™ã‚‹ã€‚
Since our goal is to create synthetic negative items similar to positive items, we keep the hard factor of a negative item unchanged and focus on augmenting the easy factor by controlling the direction and magnitude of added noise.
ã‚ã‚Œã‚ã‚Œã®ç›®æ¨™ã¯ã€ãƒã‚¸ãƒ†ã‚£ãƒ–é …ç›®ã¨é¡ä¼¼ã—ãŸåˆæˆãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã‚’ä½œæˆã™ã‚‹ã“ã¨ã§ã‚ã‚‹ãŸã‚ã€ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã®ãƒãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã¯å¤‰æ›´ã›ãšã€ä»˜åŠ ã•ã‚Œã‚‹ãƒã‚¤ã‚ºã®æ–¹å‘ã¨å¤§ãã•ã‚’åˆ¶å¾¡ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€ã‚¤ãƒ¼ã‚¸ãƒ¼ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’å¢—å¼·ã™ã‚‹ã“ã¨ã«é›†ä¸­ã™ã‚‹ã€‚
The augmentation mechanism needs to be carefully designed so that the augmented item will become more similar to the corresponding positive item, but will not cross the decision boundary.
è£œå¼·ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯ã€è£œå¼·ã•ã‚ŒãŸé …ç›®ãŒå¯¾å¿œã™ã‚‹æ­£é …ç›®ã«ä¼¼ã¦ãã‚‹ãŒã€åˆ¤å®šå¢ƒç•Œã‚’è¶Šãˆãªã„ã‚ˆã†ã«æ³¨æ„æ·±ãè¨­è¨ˆã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
Furthermore, we introduce a new metric called augmentation gain to measure the difference between the scores before and after the augmentation.
ã•ã‚‰ã«ã€ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ã‚²ã‚¤ãƒ³ã¨å‘¼ã°ã‚Œã‚‹æ–°ã—ã„æŒ‡æ¨™ã‚’å°å…¥ã—ã€ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å‰å¾Œã®ã‚¹ã‚³ã‚¢ã®å·®ã‚’æ¸¬å®šã™ã‚‹ã€‚
Our sampling strategy is guided by augmentation gain, which gives low-score items with higher augmentation gain a larger probability of being sampled.
ç§ãŸã¡ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã¯ã€ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚²ã‚¤ãƒ³ã«ã‚ˆã£ã¦å°ã‹ã‚Œã€ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚²ã‚¤ãƒ³ã®é«˜ã„ä½ã‚¹ã‚³ã‚¢é …ç›®ãŒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ç¢ºç‡ãŒé«˜ããªã‚‹ã€‚
In this way, we can effectively mitigate information discrimination, leading to better performance.
ã“ã†ã™ã‚‹ã“ã¨ã§ã€æƒ…å ±ã«ã‚ˆã‚‹å·®åˆ¥ã‚’åŠ¹æœçš„ã«ç·©å’Œã—ã€ã‚ˆã‚Šè‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«ã¤ãªã’ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
We summarize our main contributions as follows:
æˆ‘ã€…ã®ä¸»ãªè²¢çŒ®ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã‚ã‚‹ï¼š

- We design a set of intuitive experiments to reveal two notable limitations of existing hard negative sampling methods, namely ambiguous trap and information discrimination. æˆ‘ã€…ã¯ã€æ—¢å­˜ã®ãƒãƒ¼ãƒ‰ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ³•ã®2ã¤ã®é¡•è‘—ãªé™ç•Œã€ã™ãªã‚ã¡æ›–æ˜§ãªç½ ã¨æƒ…å ±ã®è­˜åˆ¥ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ãŸã‚ã«ã€ä¸€é€£ã®ç›´æ„Ÿçš„ãªå®Ÿé¨“ã‚’ãƒ‡ã‚¶ã‚¤ãƒ³ã™ã‚‹ã€‚

- To the best of our knowledge, we are the first to propose to generate negative samples from a fine-granular perspective to improve ç§ãŸã¡ã®çŸ¥ã‚‹é™ã‚Šã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚’ææ¡ˆã—ãŸã®ã¯ç§ãŸã¡ãŒåˆã‚ã¦ã§ã‚ã‚‹ã€‚

- implicit CF. In particular, we propose a new direction of generating regulated augmentation to address the unique challenges of CF. æš—é»™ã®CFã€‚ ç‰¹ã«ã€CFç‰¹æœ‰ã®èª²é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€èª¿ç¯€ã•ã‚ŒãŸè£œå¼·ã‚’ç”Ÿã¿å‡ºã™ã¨ã„ã†æ–°ãŸãªæ–¹å‘æ€§ã‚’ææ¡ˆã™ã‚‹ã€‚

- We propose a general paradigm called augmented negative sampling (ANS) that consists of three steps, including disentanglement, augmentation, and sampling. We also present a concrete implementation of ANS, which is not only performant but also efficient. æˆ‘ã€…ã¯ã€æ‹¡å¼µãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°(ANS)ã¨å‘¼ã°ã‚Œã‚‹ä¸€èˆ¬çš„ãªãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’ææ¡ˆã™ã‚‹ã€‚ANSã¯ã€åˆ†é›¢ã€æ‹¡å¼µã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®3ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ã€‚ ã¾ãŸã€ANSã®å…·ä½“çš„ãªå®Ÿè£…ã«ã¤ã„ã¦ã‚‚ç´¹ä»‹ã™ã‚‹ã€‚

- We conduct extensive experiments on five real-world datasets to demonstrate that ANS can achieve significant improvements over representative state-of-the-art negative sampling methods. æˆ‘ã€…ã¯5ã¤ã®å®Ÿä¸–ç•Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å¤§è¦æ¨¡ãªå®Ÿé¨“ã‚’è¡Œã„ã€ANSãŒä»£è¡¨çš„ãªæœ€å…ˆç«¯ã®ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•ã‚ˆã‚Šã‚‚å¤§å¹…ãªæ”¹å–„ã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã™ã‚‹ã€‚

# Related Work é–¢é€£ä½œå“

## Model-Agnostic Negative Sampling ãƒ¢ãƒ‡ãƒ«ç„¡è¦–ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

A common type of negative sampling strategy selects negative samples based on a pre-determined static distribution [6, 36, 39].
ä¸€èˆ¬çš„ãªã‚¿ã‚¤ãƒ—ã®ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã¯ã€äº‹å‰ã«æ±ºå®šã•ã‚ŒãŸé™çš„åˆ†å¸ƒã«åŸºã¥ã„ã¦ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠã™ã‚‹[6, 36, 39]ã€‚
Such a strategy is normally efficient since it does not need to be adjusted in the model training process.
ã“ã®ã‚ˆã†ãªæˆ¦ç•¥ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°éç¨‹ã§èª¿æ•´ã™ã‚‹å¿…è¦ãŒãªã„ãŸã‚ã€é€šå¸¸ã¯åŠ¹ç‡çš„ã§ã‚ã‚‹ã€‚
Random negative sampling (RNS) [5, 26, 35, 40] is a simple and representative model-agnostic sampling strategy, which selects negative samples from unobserved items according to a uniform distribution.
ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆRNSï¼‰[5, 26, 35, 40]ã¯ã€ä¸€æ§˜åˆ†å¸ƒã«å¾“ã£ã¦æœªè¦³æ¸¬é …ç›®ã‹ã‚‰ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠã™ã‚‹ã€ã‚·ãƒ³ãƒ—ãƒ«ã§ä»£è¡¨çš„ãªãƒ¢ãƒ‡ãƒ«ä¸å¯çŸ¥ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã§ã‚ã‚‹ã€‚
However, the uniform distribution is difficult to guarantee the quality of negative samples.
ã—ã‹ã—ã€ä¸€æ§˜åˆ†å¸ƒã§ã¯é™°æ€§ã‚µãƒ³ãƒ—ãƒ«ã®å“è³ªã‚’ä¿è¨¼ã™ã‚‹ã“ã¨ã¯é›£ã—ã„ã€‚
Inspired by the word-frequency-based distribution [11] and node-degree-based distribution [23] in other domains, an itempopularity-based distribution [2, 6] has been introduced.
ä»–ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã«ãŠã‘ã‚‹å˜èªé »åº¦ãƒ™ãƒ¼ã‚¹ã®åˆ†å¸ƒ[11]ã‚„ãƒãƒ¼ãƒ‰åº¦ãƒ™ãƒ¼ã‚¹ã®åˆ†å¸ƒ[23]ã«è§¦ç™ºã•ã‚Œã€ã‚¢ã‚¤ãƒ†ãƒ ãƒãƒ”ãƒ¥ãƒ©ãƒªãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ã®åˆ†å¸ƒ[2, 6]ãŒå°å…¥ã•ã‚ŒãŸã€‚
Under this distribution, popular items are more likely to be sampled as negative items, which helps to mitigate the widespread popularity bias issue in recommender systems [3].
ã“ã®åˆ†å¸ƒã®ä¸‹ã§ã¯ã€äººæ°—ã‚¢ã‚¤ãƒ†ãƒ ã¯ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚¢ã‚¤ãƒ†ãƒ ã¨ã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒé«˜ããªã‚Šã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã§åºƒãè¦‹ã‚‰ã‚Œã‚‹äººæ°—ãƒã‚¤ã‚¢ã‚¹å•é¡Œã‚’ç·©å’Œã™ã‚‹ã®ã«å½¹ç«‹ã¤[3]ã€‚
Although this kind of strategy is generally efficient, the pre-determined distributions are not customized for the underlying models and not adaptively adjusted during the training process.
ã“ã®ç¨®ã®æˆ¦ç•¥ã¯ä¸€èˆ¬çš„ã«åŠ¹ç‡çš„ã§ã‚ã‚‹ãŒã€äº‹å‰ã«æ±ºå®šã•ã‚ŒãŸåˆ†å¸ƒã¯åŸºç¤ã¨ãªã‚‹ãƒ¢ãƒ‡ãƒ«ç”¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã•ã‚Œã¦ãŠã‚‰ãšã€å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ä¸­ã«é©å¿œçš„ã«èª¿æ•´ã•ã‚Œã‚‹ã“ã¨ã‚‚ãªã„ã€‚
As a result, their performance is often sub-optimal.
ãã®çµæœã€å½¼ã‚‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ã—ã°ã—ã°æœ€é©ã¨ã¯è¨€ãˆãªã„ã€‚

## Model-Aware Negative Sampling ãƒ¢ãƒ‡ãƒ«ã‚’æ„è­˜ã—ãŸãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

These strategies take into consideration some information of the underlying model, denoted by ğ‘“ , to guide the sampling process.
ã“ã‚Œã‚‰ã®æˆ¦ç•¥ã¯ã€â†ªLl_1453 ã§ç¤ºã•ã‚Œã‚‹åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’è€ƒæ…®ã—ã¦ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã‚’å°ãã€‚
Given ğ‘“ , the probability of sampling an item ğ‘– is defined as ğ‘(ğ‘– | ğ‘“ ) âˆ ğ‘”(ğ‘“ , eğ‘–), where ğ‘”(Â·, Â·) is a sampling function, and eğ‘– denotes the embedding of ğ‘–.
ã“ã“ã§ã€á‘”(-, -)ã¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–¢æ•°ã§ã‚ã‚Šã€eğ‘–ã¯ğ‘–ã®åŸ‹ã‚è¾¼ã¿ã‚’è¡¨ã™ã€‚
Existing studies focus on choosing different ğ‘“ and/or designing a proper ğ‘”(Â·, Â·) to achieve better performance.
æ—¢å­˜ã®ç ”ç©¶ã¯ã€ã‚ˆã‚Šè‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã™ã‚‹ãŸã‚ã«ã€ç•°ãªã‚‹ á‘“ ã‚’é¸æŠã™ã‚‹ã“ã¨ã€ãŠã‚ˆã³/ã¾ãŸã¯é©åˆ‡ãª á‘”(-, -) ã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã‚‹ã€‚
The most representative work is hard negative sampling, which defines ğ‘”(Â·, Â·) as a score function.
æœ€ã‚‚ä»£è¡¨çš„ãªç ”ç©¶ã¯ã€ğ‘”(-, -)ã‚’ã‚¹ã‚³ã‚¢é–¢æ•°ã¨ã—ã¦å®šç¾©ã™ã‚‹ãƒãƒ¼ãƒ‰ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ã‚ã‚‹ã€‚
It assigns higher sampling probabilities to the negative items with larger prediction scores [8, 9, 16, 25, 41, 44].
ã“ã‚Œã¯ã€äºˆæ¸¬ã‚¹ã‚³ã‚¢ã®å¤§ãã„å¦å®šçš„ãªé …ç›®ã«ã‚ˆã‚Šé«˜ã„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç¢ºç‡ã‚’å‰²ã‚Šå½“ã¦ã‚‹[8, 9, 16, 25, 41, 44]ã€‚
For example, DNS [41] assumes that the high-score items should be more likely to be selected, and thus chooses ğ‘”(Â·, Â·) to be the inner product and ğ‘“ to be user representations.
ä¾‹ãˆã°ã€DNS [41]ã¯ã€é«˜å¾—ç‚¹ã®é …ç›®ãŒé¸æŠã•ã‚Œã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã¨ä»®å®šã—ã€Ç”(-, -)ã‚’å†…ç©ã€áµ…ã‚’ãƒ¦ãƒ¼ã‚¶è¡¨ç¾ã¨ã™ã‚‹ã€‚
With the goal of mitigating false negative samples, SRNS [9] further incorporates the information about the last few epochs into ğ‘“ and designs ğ‘”(Â·, Â·) to give false negative samples lower scores.
SRNS[9]ã¯ã€å½é™°æ€§ã‚µãƒ³ãƒ—ãƒ«ã‚’è»½æ¸›ã™ã‚‹ç›®çš„ã§ã€ã•ã‚‰ã«ç›´è¿‘ã®æ•°ã‚¨ãƒãƒƒã‚¯ã«é–¢ã™ã‚‹æƒ…å ±ã‚’á‘“ã«çµ„ã¿è¾¼ã¿ã€å½é™°æ€§ã‚µãƒ³ãƒ—ãƒ«ã«ã‚ˆã‚Šä½ã„ã‚¹ã‚³ã‚¢ã‚’ä¸ãˆã‚‹ã‚ˆã†ã«á‘”(-, -)ã‚’è¨­è¨ˆã™ã‚‹ã€‚
IRGAN [33] integrates a generative adversarial network into ğ‘”(Â·, Â·) to determine the probabilities of negative samples through the min-max game.
IRGAN [33]ã¯ã€ç”Ÿæˆçš„ãªæ•µå¯¾çš„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’á‘”(-, -)ã«çµ±åˆã—ã€æœ€å°-æœ€å¤§ã‚²ãƒ¼ãƒ ã‚’é€šã—ã¦è² ã‚µãƒ³ãƒ—ãƒ«ã®ç¢ºç‡ã‚’æ±ºå®šã™ã‚‹ã€‚
ReinforcedNS [8] use reinforcement learning into ğ‘”(Â·, Â·).
ReinforcedNS [8]ã¯ã€å¼·åŒ–å­¦ç¿’ã‚’á‘”(-, -)ã«ç”¨ã„ã‚‹ã€‚
With well-designed ğ‘“ and ğ‘”(Â·, Â·), we can generally achieve better performance.
ã†ã¾ãè¨­è¨ˆã•ã‚ŒãŸ ğ‘” ã¨ ğ‘“(-, -) ãŒã‚ã‚Œã°ã€ä¸€èˆ¬ã«ã€ã‚ˆã‚Šè‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã§ãã‚‹ã€‚
However, selecting suitable negative items need to compute ğ‘”(Â·, Â·) for all unobserved items, which is extremely timeconsuming and prohibitively expensive.
ã—ã‹ã—ã€é©åˆ‡ãªå¦å®šé …ç›®ã‚’é¸æŠã™ã‚‹ãŸã‚ã«ã¯ã€ã™ã¹ã¦ã®æœªè¦³æ¸¬é …ç›®ã«ã¤ã„ã¦á‘”(-, -)ã‚’è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€ã“ã‚Œã¯éå¸¸ã«æ™‚é–“ãŒã‹ã‹ã‚Šã€æ³•å¤–ã«é«˜ä¾¡ã§ã‚ã‚‹ã€‚
Take DNS as an example.
DNSã‚’ä¾‹ã«ã¨ã£ã¦ã¿ã‚ˆã†ã€‚
Calculating the probability of sampling an item is equivalent to performing softmax on all unobserved samples, which is unacceptable in real-world applications [4, 24, 33].
ã‚¢ã‚¤ãƒ†ãƒ ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ç¢ºç‡ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã¯ã€ã™ã¹ã¦ã®æœªè¦³æ¸¬ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã—ã¦ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã¨ç­‰ä¾¡ã§ã‚ã‚Šã€å®Ÿéš›ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯å—ã‘å…¥ã‚Œã‚‰ã‚Œãªã„[4, 24, 33]ã€‚
As a result, most model-aware sampling strategies adopt the two-pass approach or its variants.
ãã®çµæœã€ãƒ¢ãƒ‡ãƒ«ã‚’æ„è­˜ã—ãŸã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã®ã»ã¨ã‚“ã©ã¯ã€2ãƒ‘ã‚¹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¾ãŸã¯ãã®å¤‰å½¢ã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã€‚
In this case, ğ‘”(Â·, Â·) is only applied to a small number of candidates sampled in the first pass.
ã“ã®å ´åˆã€ğ‘”(-, -)ã¯æœ€åˆã®ãƒ‘ã‚¹ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸå°‘æ•°ã®å€™è£œã«ã®ã¿é©ç”¨ã•ã‚Œã‚‹ã€‚
While such two-pass-based negative sampling strategies have been the mainstream methods, they exhibit two notable limitations, namely ambiguous trap and information discrimination, which motivates us to propose an augmented negative sampling paradigm.
ã“ã®ã‚ˆã†ãª2ãƒ‘ã‚¹ãƒ™ãƒ¼ã‚¹ã®ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã¯ä¸»æµã®æ‰‹æ³•ã§ã‚ã‚‹ãŒã€æ›–æ˜§ãƒˆãƒ©ãƒƒãƒ—ã¨æƒ…å ±è­˜åˆ¥ã¨ã„ã†2ã¤ã®é¡•è‘—ãªé™ç•ŒãŒã‚ã‚‹ã€‚
In the next section, we will explain these limitations via a set of intuitive experiments.
æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ä¸€é€£ã®ç›´æ„Ÿçš„ãªå®Ÿé¨“ã‚’é€šã—ã¦ã“ã‚Œã‚‰ã®é™ç•Œã‚’èª¬æ˜ã™ã‚‹ã€‚

# Limitations of the Two-pass approach 2ãƒ‘ã‚¹ãƒ»ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®é™ç•Œ

In this section, we first formulate the problem of implicit CF and then explain ambiguous trap and information discrimination via intuitive experiments.
æœ¬ç¯€ã§ã¯ã€ã¾ãšæš—é»™ã®CFã®å•é¡Œã‚’å®šå¼åŒ–ã—ã€æ¬¡ã«æ›–æ˜§ãªç½ ã¨æƒ…å ±ã®è­˜åˆ¥ã«ã¤ã„ã¦ç›´æ„Ÿçš„ãªå®Ÿé¨“ã‚’é€šã—ã¦èª¬æ˜ã™ã‚‹ã€‚
We consider the Last.fm and Amazon-Baby datasets in experiments.
å®Ÿé¨“ã§ã¯ã€Last.fmã¨Amazon-Babyã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ãŸã€‚
A comprehensive description of the data is provided in Section 5.
ãƒ‡ãƒ¼ã‚¿ã®åŒ…æ‹¬çš„ãªèª¬æ˜ã¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³5ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã€‚

## Implicit CF æš—é»™ã®CF

We denote the set of historical interactions by O + = {(ğ‘¢,ğ‘–+ ) | ğ‘¢ âˆˆ U,ğ‘–+ âˆˆ I}, where U and I are the set of users and the set of items, respectively.
ğ‘¢ âˆˆ U,ğ‘–+ âˆˆ I}, where U and I are the set of users and the set of items, respectively.
The most common implicit CF paradigm is to learn user and item representations (eğ‘¢ and eğ‘– ) from the historical interactions and then predict the scores of unobserved items to recommend the top-K items.
æœ€ã‚‚ä¸€èˆ¬çš„ãªæš—é»™çš„CFã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¯ã€éå»ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®è¡¨ç¾ï¼ˆeğ‘¢ã¨eğ‘–ï¼‰ã‚’å­¦ç¿’ã—ã€ãƒˆãƒƒãƒ—Kã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ¨è–¦ã™ã‚‹ãŸã‚ã«æœªè¦³æ¸¬ã®ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚¹ã‚³ã‚¢ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
The BPR loss function is widely used to optimize the model:
ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–ã«ã¯BPRæå¤±é–¢æ•°ãŒåºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹ï¼š

$$
\tag{1}
$$

where ğœ(Â·) is the sigmoid function, and ğ‘ (Â·, Â·) is a score function (e.g., the inner product) that measures the similarity between the user and item representations.
ã“ã“ã§ã€â†ªL_1D70E ã¯ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã§ã‚ã‚Šã€â†ªL_1D460â†©(-, -) ã¯ã‚¹ã‚³ã‚¢é–¢æ•°ï¼ˆå†…ç©ãªã©ï¼‰ã§ã‚ã‚‹ã€‚
Here ğ‘– âˆ’ is a negative sample selected by a sampling strategy.
ã“ã“ã§ã€ğ‘– - ã¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã«ã‚ˆã£ã¦é¸æŠã•ã‚ŒãŸè² ã®ã‚µãƒ³ãƒ—ãƒ«ã§ã‚ã‚‹ã€‚
Our goal is to design a negative sampling strategy that is generic to different CF models.
æˆ‘ã€…ã®ç›®æ¨™ã¯ã€ç•°ãªã‚‹CFãƒ¢ãƒ‡ãƒ«ã«æ±ç”¨çš„ãªãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
Following previous studies [4, 26, 33], without loss of generality, we consider matrix factorization with Bayesian personalized ranking (MF-BPR) [26] as the basic CF model to illustrate ANS.
å…ˆè¡Œç ”ç©¶[4,26,33]ã«å¾“ã„ï¼Œä¸€èˆ¬æ€§ã‚’æãªã‚ãªã„ç¯„å›²ã§ï¼ŒANS ã‚’èª¬æ˜ã™ã‚‹åŸºæœ¬ CF ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ï¼Œãƒ™ ã‚¤ã‚¸ã‚¢ãƒ³ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºãƒ‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«ã‚ˆã‚‹è¡Œåˆ—åˆ†è§£ï¼ˆMF-BPRï¼‰[26]ã‚’è€ƒãˆã‚‹ï¼

## Ambiguous Trap â™ªã‚ã„ã¾ã„ãªç½ 

We choose DNS [41], which is the most representative hard negative sampling method, to train an MF-BPR model on the Last.fm dataset and calculate the scores of unobserved user-item pairs in different training periods.
Last.fmãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§MF-BPRãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹ãŸã‚ã«ã€æœ€ã‚‚ä»£è¡¨çš„ãªãƒãƒ¼ãƒ‰ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ³•ã§ã‚ã‚‹DNS [41]ã‚’é¸æŠã—ã€ç•°ãªã‚‹è¨“ç·´æœŸé–“ã«ãŠã‘ã‚‹æœªè¦³æ¸¬ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒšã‚¢ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹ã€‚
Figure 1(a), 1(b), 1(c) demonstrates the frequency distributions of the scores at different epochs.
å›³1(a)ã€(b)ã€(c)ã¯ã€ç•°ãªã‚‹ã‚¨ãƒãƒƒã‚¯ã«ãŠã‘ã‚‹ã‚¹ã‚³ã‚¢ã®åº¦æ•°åˆ†å¸ƒã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
The lowest 80% of the scores are emphasized by the pink shade.
å¾—ç‚¹ã®ä¸‹ä½80ï¼…ã¯ãƒ”ãƒ³ã‚¯ã®å½±ã§å¼·èª¿ã•ã‚Œã¦ã„ã‚‹ã€‚
We can observe that as training progresses, more and more scores are concentrated in the low-score region, meaning that the vast majority of unobserved items are easy negative samples.
è¨“ç·´ãŒé€²ã‚€ã«ã¤ã‚Œã¦ã€ã‚ˆã‚Šå¤šãã®ã‚¹ã‚³ã‚¢ãŒä½ã‚¹ã‚³ã‚¢é ˜åŸŸã«é›†ä¸­ã—ã¦ã„ã‚‹ã“ã¨ãŒè¦³å¯Ÿã§ãã‚‹ã€‚ã“ã‚Œã¯ã€æœªè¦³æ¸¬é …ç›®ã®å¤§éƒ¨åˆ†ãŒãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã«ãªã‚Šã‚„ã™ã„ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚
Recall that the first pass samples a fixed number of negative items by a uniform distribution.
æœ€åˆã®ãƒ‘ã‚¹ã§ã¯ã€ä¸€æ§˜åˆ†å¸ƒã«ã‚ˆã£ã¦ä¸€å®šæ•°ã®ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ»ã‚¢ã‚¤ãƒ†ãƒ ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚’æ€ã„å‡ºã—ã¦ã»ã—ã„ã€‚
Randomly sampling a small number of negative items in the first pass is difficult to include useful hard negative samples, which, in turn, substantially limits the efficacy of the second pass.
ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆãƒ‘ã‚¹ã§å°‘æ•°ã®ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã¯ã€æœ‰ç”¨ãªãƒãƒ¼ãƒ‰ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã‚’å«ã‚ã‚‹ã“ã¨ãŒå›°é›£ã§ã‚ã‚Šã€ãã®çµæœã€ã‚»ã‚«ãƒ³ãƒ‰ãƒ‘ã‚¹ã®æœ‰åŠ¹æ€§ãŒå¤§å¹…ã«åˆ¶é™ã•ã‚Œã‚‹ã€‚
We call this phenomenon ambiguous trap.
ç§ãŸã¡ã¯ã“ã®ç¾è±¡ã‚’ã€Œæ›–æ˜§ãªç½ ã€ã¨å‘¼ã‚“ã§ã„ã‚‹ã€‚

To further demonstrate the existence of ambiguous trap, in Figure 1(d), we plot the min-max normalizing maximum and minimum scores of the sampled negative items in the first pass on the Last.fm dataset.
æ›–æ˜§ãªãƒˆãƒ©ãƒƒãƒ—ã®å­˜åœ¨ã‚’ã•ã‚‰ã«ç¤ºã™ãŸã‚ã«ã€å›³1(d)ã§ã¯ã€Last.fmãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æœ€åˆã®ãƒ‘ã‚¹ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã®æœ€å¤§ã‚¹ã‚³ã‚¢ã¨æœ€å°ã‚¹ã‚³ã‚¢ã‚’æœ€å°-æœ€å¤§æ­£è¦åŒ–ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã„ã‚‹ã€‚
It can be seen that the difference between the maximum score and the minimum score is consistently small, suggesting that randomly sampling a small number of negative items makes the hardness of the negative samples obtained from DNS far from ideal.
æœ€å¤§ã‚¹ã‚³ã‚¢ã¨æœ€å°ã‚¹ã‚³ã‚¢ã®å·®ãŒä¸€è²«ã—ã¦å°ã•ã„ã“ã¨ã‹ã‚‰ã€å°‘æ•°ã®ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã‚’ç„¡ä½œç‚ºã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€DNSã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã®ç¡¬åº¦ãŒç†æƒ³ã‹ã‚‰ã‹ã‘é›¢ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
Note that a straightforward attempt to mitigate ambiguous trap is to substantially increase the sample size in the first pass.
æ›–æ˜§ãªãƒˆãƒ©ãƒƒãƒ—ã‚’è»½æ¸›ã™ã‚‹ç°¡å˜ãªè©¦ã¿ã¯ã€æœ€åˆã®ãƒ‘ã‚¹ã§ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’å¤§å¹…ã«å¢—ã‚„ã™ã“ã¨ã§ã‚ã‚‹ã€‚
However, it is inevitably at the cost of substantial time and space overhead [4].
ã—ã‹ã—ã€ãã®ä»£å„Ÿã¨ã—ã¦ã€ã‹ãªã‚Šã®æ™‚é–“ã¨ã‚¹ãƒšãƒ¼ã‚¹ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒé¿ã‘ã‚‰ã‚Œãªã„[4]ã€‚
Inspired by contrastive learning [17, 37, 40], we propose to augment the sampled negative items to increase their hardness.
å¯¾ç…§å­¦ç¿’[17, 37, 40]ã«ãƒ’ãƒ³ãƒˆã‚’å¾—ã¦ã€ç§ãŸã¡ã¯ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã®ç¡¬åº¦ã‚’ä¸Šã’ã‚‹ãŸã‚ã«ã€ãã®é …ç›®ã‚’è£œå¼·ã™ã‚‹ã“ã¨ã‚’ææ¡ˆã™ã‚‹ã€‚

## Information Discrimination æƒ…å ±å·®åˆ¥

In the second pass, most existing studies overly focus on high-score negative items and largely neglect low-score negative items, which also contain critical, unique information to improve model performance.
2ã¤ç›®ã®ãƒ‘ã‚¹ã§ã¯ã€æ—¢å­˜ã®ç ”ç©¶ã®ã»ã¨ã‚“ã©ãŒã€é«˜å¾—ç‚¹ã®ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã«éåº¦ã«æ³¨ç›®ã—ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«é‡è¦ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªæƒ…å ±ã‚’å«ã‚€ä½å¾—ç‚¹ã®ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã‚’ã»ã¨ã‚“ã©ç„¡è¦–ã—ã¦ã„ã‚‹ã€‚
Overemphasizing high-score items as negative samples may result in worse model performance.
é«˜å¾—ç‚¹ã®é …ç›®ã‚’ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦å¼·èª¿ã—ã™ãã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
Several studies [4, 27] have made efforts to assign lower sampling probabilities to low-score items using algorithms like softmax and its derivatives.
ã„ãã¤ã‹ã®ç ”ç©¶[4, 27]ã§ã¯ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚„ãã®æ´¾ç”Ÿå‹ã®ã‚ˆã†ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç”¨ã„ã¦ã€ä½ã‚¹ã‚³ã‚¢é …ç›®ã«ä½ã„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç¢ºç‡ã‚’å‰²ã‚Šå½“ã¦ã‚‹åŠªåŠ›ã‚’ã—ã¦ããŸã€‚
However, selecting those significantly lower-score items in comparison to others, remains a challenge.
ã—ã‹ã—ã€ä»–ã®é …ç›®ã¨æ¯”è¼ƒã—ã¦è‘—ã—ãã‚¹ã‚³ã‚¢ã®ä½ã„é …ç›®ã‚’é¸æŠã™ã‚‹ã“ã¨ã¯ã€ä¾ç„¶ã¨ã—ã¦èª²é¡Œã§ã‚ã‚‹ã€‚
We call this behavior information discrimination.
ç§ãŸã¡ã¯ã“ã®è¡Œå‹•ã‚’æƒ…å ±å·®åˆ¥ã¨å‘¼ã‚“ã§ã„ã‚‹ã€‚
To verify the existence of information discrimination, we introduce a new measure named Pairwise Exclusive-hit Ratio (PER) [18] to CF, which is used to compare the difference between the information learned by two different methods.
æƒ…å ±å¼åˆ¥ã®å­˜åœ¨ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã€CFã«PERï¼ˆPairwise Exclusive-hit Ratioï¼‰[18]ã¨ã„ã†æ–°ã—ã„å°ºåº¦ã‚’å°å…¥ã—ã€2ã¤ã®ç•°ãªã‚‹æ–¹æ³•ã§å­¦ç¿’ã—ãŸæƒ…å ±ã®å·®ã‚’æ¯”è¼ƒã™ã‚‹ã€‚
More specifically, PER(ğ‘¥, ğ‘¦) quantifies the information captured by the method ğ‘¥ but not by ğ‘¦ via
ã‚ˆã‚Šå…·ä½“çš„ã«ã¯ã€PER(ğ‘¥, ğ‘¦)ã¯ã€â†ªLl_1D466 ã§ã¯ãªãâ†ªLl_1D465 ã«ã‚ˆ ã£ã¦æ•æ‰ã•ã‚ŒãŸæƒ…å ±ã‚’å®šé‡åŒ–ã™ã‚‹ã€‚

$$
\tag{2}
$$

where Hğ‘¥ denotes the set of test interactions correctly predicted by the method ğ‘¥.
ã“ã“ã§ H_1D465 ã¯ã€æ‰‹æ³•ğ‘¥ã«ã‚ˆã£ã¦æ­£ã—ãäºˆæ¸¬ã•ã‚ŒãŸãƒ†ã‚¹ãƒˆç›¸äº’ä½œç”¨ã®é›†åˆã‚’è¡¨ã™ã€‚
Next, we choose two representative negative sampling methods, RNS [8] and DNS [41] to train MF-BPR models and calculate the PER between them.
æ¬¡ã«ã€ä»£è¡¨çš„ãªãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•ã§ã‚ã‚‹RNS[8]ã¨DNS[41]ã‚’é¸æŠã—ã€MF-BPRãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’è¡Œã„ã€ä¸¡è€…ã®PERã‚’è¨ˆç®—ã™ã‚‹ã€‚
Recall that DNS is more likely to select high-score negative items while RNS uniformly randomly selects negative items irrespective of their scores.
DNSã¯é«˜å¾—ç‚¹ã®ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã‚’é¸æŠã—ã‚„ã™ãã€RNSã¯å¾—ç‚¹ã«é–¢ä¿‚ãªãä¸€æ§˜ã«ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠã™ã‚‹ã“ã¨ã‚’æ€ã„å‡ºã—ã¦ã»ã—ã„ã€‚
The obtained results are depicted in Figure 2 (excluding HNS data for the current analysis).
å¾—ã‚‰ã‚ŒãŸçµæœã‚’å›³2ã«ç¤ºã™ï¼ˆä»Šå›ã®åˆ†æã§ã¯HNSã®ãƒ‡ãƒ¼ã‚¿ã‚’é™¤ãï¼‰ã€‚
We can observe that: (1) the DNS strategy can indeed learn more information, confirming the benefits of leveraging hard negative samples to form a tighter decision boundary.(2) The values of PER(RNS, DNS) on two datasets (0.2 and 0.33) indicate that even the simple RNS strategy can still learn rich information that is not learned by DNS.
ãã®çµæœ 
(2)2ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã‘ã‚‹PER(RNS, DNS)ã®å€¤(0.2ã¨0.33)ã¯ã€å˜ç´”ãªRNSæˆ¦ç•¥ã§ã‚‚ã€DNSã§ã¯å­¦ç¿’ã•ã‚Œãªã„è±Šå¯Œãªæƒ…å ±ã‚’å­¦ç¿’ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
In other words, the easy negative items overlooked by DNS are still valuable for CF.
ã¤ã¾ã‚Šã€DNSãŒè¦‹éã”ã—ãŒã¡ãªãƒã‚¬ãƒ†ã‚£ãƒ–ãªé …ç›®ã‚‚ã€CFã«ã¨ã£ã¦ã¯è²´é‡ãªã®ã ã€‚
Such an information discrimination problem inspires us to understand a negative itemâ€™s hardness from a more fine-granular perspective in order to extract more useful information.
ã“ã®ã‚ˆã†ãªæƒ…å ±è­˜åˆ¥ã®å•é¡Œã¯ã€ã‚ˆã‚Šæœ‰ç”¨ãªæƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã«ã€ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã®ç¡¬ã•ã‚’ã‚ˆã‚Šç´°ã‹ãªè¦–ç‚¹ã‹ã‚‰ç†è§£ã™ã‚‹ã“ã¨ã‚’ä¿ƒã™ã€‚

# Methodology æ–¹æ³•è«–

Driven by the aforementioned limitations, we propose a novel generic augmented negative sampling (ANS) paradigm, which consists of three major steps: disentanglement, augmentation, and sampling.
å‰è¿°ã—ãŸã‚ˆã†ãªåˆ¶ç´„ã®ä¸­ã§ã€æˆ‘ã€…ã¯3ã¤ã®ä¸»è¦ãªã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰ãªã‚‹ã€æ–°ã—ã„ä¸€èˆ¬çš„ãªæ‹¡å¼µãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆANSï¼‰ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’ææ¡ˆã™ã‚‹ï¼š 
ã“ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¯3ã¤ã®ä¸»è¦ãªã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ã€‚
The disentanglement step learns an itemâ€™s hard and easy factors; the augmentation step adds regulated noise to the easy factor so as to increase the itemâ€™s hardness; the sampling strategy selects the final negative samples based on a new metric we propose.
è£œå¼·ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ ã®ç¡¬ã•ã‚’å¢—åŠ ã•ã›ã‚‹ãŸã‚ã«ã€ã‚¤ãƒ¼ã‚¸ãƒ¼ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã«èª¿æ•´ãƒã‚¤ã‚ºã‚’åŠ ãˆã‚‹ã€‚
The workflow of ANS is illustrated in Figure 3.
ANSã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å›³3ã«ç¤ºã™ã€‚
Note that these steps can be implemented by different methods and thus the overall paradigm is generic.
ã“ã‚Œã‚‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ç•°ãªã‚‹æ–¹æ³•ã§å®Ÿè£…ã§ãã‚‹ãŸã‚ã€å…¨ä½“çš„ãªãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¯æ±ç”¨çš„ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã€‚
We present a possible instantiation in the following sections.
å¯èƒ½ãªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã‚’ä»¥ä¸‹ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ç´¹ä»‹ã™ã‚‹ã€‚

## Disentanglement #ãƒ‡ã‚£ã‚»ãƒ³ã‚·ãƒ§ãƒ³

To understand a negative itemâ€™s hardness from a more fine-granular perspective, we propose to disentangle its embedding into hard and easy factors (i.e., a set of dimensions of the embedding vector), where hardness is defined by whether a negative item has similar values to the corresponding user in the given factor.
ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚¢ã‚¤ãƒ†ãƒ ã®ç¡¬ã•ã‚’ã‚ˆã‚Šç´°ã‹ãç†è§£ã™ã‚‹ãŸã‚ã«ã€ãã®åŸ‹ã‚è¾¼ã¿ã‚’ç¡¬ã„å› å­ã¨ç°¡å˜ãªå› å­ï¼ˆåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒã®é›†åˆï¼‰ã«åˆ†é›¢ã™ã‚‹ã“ã¨ã‚’ææ¡ˆã™ã‚‹ã€‚
Similarly, we follow the two-pass approach to first randomly sample ğ‘€ (ğ‘€ â‰ª |I|) items from the unobserved items to form a candidate negative set E.
I|) items from the unobserved items to form a candidate negative set E.
We design a gating module to identify which dimensions of a negative item eğ‘› âˆˆ R ğ‘‘ in E are hard with respect to user eğ‘¢ âˆˆ R ğ‘‘ via
Eã«ãŠã‘ã‚‹å¦å®šçš„ãªé …ç›®eğ‘› âˆˆ R â†ªLl_1D451 ã®ã©ã®æ¬¡å…ƒãŒã€ãƒ¦ãƒ¼ã‚¶ãƒ¼e ğ‘¢ âˆˆ R ğ‘‘ ã«é–¢ã—ã¦é›£ã—ã„ã‹ã‚’ç‰¹å®šã™ã‚‹ãŸã‚ã«ã€ã‚²ãƒ¼ãƒ† ã‚£ãƒ³ã‚°ãƒ»ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¨­è¨ˆã™ã‚‹ã€‚

$$
\tag{3}
$$

where ğ‘”ğ‘ğ‘¡ğ‘’â„ğ‘ğ‘Ÿğ‘‘ âˆˆ R ğ‘‘ gives the weights of different dimensions.
ã“ã“ã§ã€ğ‘”ğ‘ğ‘‘âˆˆ Rğ‘‘ã¯ç•°ãªã‚‹æ¬¡å…ƒã®é‡ã¿ã‚’ä¸ãˆã‚‹ã€‚
The sigmoid function ğœ(Â·) maps the values to (0, 1).
ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ğœ(-)ã¯ã€å€¤ã‚’(0, 1)ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ã€‚
Wğ‘–ğ‘¡ğ‘’ğ‘š âˆˆ R ğ‘‘Ã—ğ‘‘ and Wğ‘¢ğ‘ ğ‘’ğ‘Ÿ âˆˆ R ğ‘‘Ã—ğ‘‘ are linear transformations used to ensure that the user and item embeddings are in the common latent space [22].
Wğ‘–ğ‘¡ğ‘’ã¨Wğ‘‘âˆˆ Rğ‘‘Ã—ğ‘‘ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®åŸ‹ã‚è¾¼ã¿ãŒå…±é€šã®æ½œåœ¨ç©ºé–“ã«ã‚ã‚‹ã“ã¨ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã®ç·šå½¢å¤‰æ›ã§ã‚ã‚‹[22]ã€‚
âŠ™ is the element-wise product, which measures the similarity between eğ‘– and eğ‘¢ in each dimension [35].
âŠ™ã¯è¦ç´ ã”ã¨ã®ç©ã§ã€å„æ¬¡å…ƒã«ãŠã‘ã‚‹eğ‘–ã¨e_46ã®é¡ä¼¼åº¦ã‚’æ¸¬ã‚‹[35]ã€‚
After obtaining the weights ğ‘”ğ‘ğ‘¡ğ‘’â„ğ‘ğ‘Ÿğ‘‘ , we adopt the element-wise product to extract the hard factor e â„ğ‘ğ‘Ÿğ‘‘ ğ‘› .
é‡ã¿ğ‘”ğ‘ğ‘¡ğ‘’ğ‘ğ‘‘ã‚’å¾—ãŸå¾Œã€è¦ç´ ã”ã¨ã®ç©ã‚’æ¡ç”¨ã—ã¦ãƒãƒ¼ãƒ‰ãƒ»ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼eğ‘ğ‘‘ã‚’æŠ½å‡ºã™ã‚‹ã€‚
The easy factor e ğ‘’ğ‘ğ‘ ğ‘¦ ğ‘› is then calculated via element-wise subtraction [12].
æ¬¡ã«ã€e ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ãŒè¦ç´ ã”ã¨ã®æ¸›ç®—ã«ã‚ˆã£ã¦è¨ˆç®—ã•ã‚Œã¾ã™[12]ã€‚

$$
\tag{4}
$$

Due to the lack of ground truth for hard and easy factors, it is inherently difficult to guarantee the credibility of the disentanglement.
ãƒãƒ¼ãƒ‰ãƒ»ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã¨ã‚¤ãƒ¼ã‚¸ãƒ¼ãƒ»ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã®ã‚°ãƒ©ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ãŒãªã„ãŸã‚ã€ãƒ‡ã‚£ã‚»ãƒ³ã‚·ãƒ§ãƒ³ã®ä¿¡é ¼æ€§ã‚’ä¿è¨¼ã™ã‚‹ã“ã¨ã¯æœ¬è³ªçš„ã«é›£ã—ã„ã€‚
Inspired by its superiority in unsupervised scenarios [19, 43], we propose to adopt contrastive learning to guide the disentanglement.
æ•™å¸«ãªã—ã‚·ãƒŠãƒªã‚ª[19, 43]ã§ã®å„ªä½æ€§ã«è§¦ç™ºã•ã‚Œã€æˆ‘ã€…ã¯ã€åˆ†é›¢ã‚’å°ããŸã‚ã«å¯¾ç…§å­¦ç¿’ã‚’æ¡ç”¨ã™ã‚‹ã“ã¨ã‚’ææ¡ˆã™ã‚‹ã€‚
By definition, the hard factor of a negative item should be more similar to the user, while the easy factor should be the opposite.
å®šç¾©ã«ã‚ˆã‚Œã°ã€ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã®ãƒãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã¯ã‚ˆã‚Šãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è¿‘ã„ã‚‚ã®ã§ã‚ã‚‹ã¹ãã§ã€ã‚¤ãƒ¼ã‚¸ãƒ¼ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã¯ãã®é€†ã§ã‚ã‚‹ã¹ãã§ã‚ã‚‹ã€‚
Therefore, given a score function ğ‘ (Â·, Â·) to calculate the similarity between a pair, we design a contrastive loss Lğ‘ as
ã—ãŸãŒã£ã¦ã€ãƒšã‚¢é–“ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹ã‚¹ã‚³ã‚¢é–¢æ•°ğ‘ (-, -)ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€å¯¾æ¯”çš„æå¤±Lğ‘ã‚’æ¬¡ã®ã‚ˆã†ã«è¨­è¨ˆã™ã‚‹ã€‚

$$
\tag{5}
$$

However, optimizing only Lğ‘ may lead to a trivial solution: including all dimensions as the hard factor.
ã—ã‹ã—ã€L_1Dã®ã¿ã‚’æœ€é©åŒ–ã™ã‚‹ã¨ã€ã¤ã¾ã‚‰ãªã„è§£ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ï¼š 
ã™ã¹ã¦ã®æ¬¡å…ƒã‚’ãƒãƒ¼ãƒ‰ãƒ»ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã«å«ã‚ã‚‹ã€‚
Therefore, we introduce another loss with the auxiliary information from positive items.
ãã“ã§ã€æ­£é …ç›®ã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹è£œåŠ©æƒ…å ±ã‚’ä½¿ã£ã¦ã€åˆ¥ã®æå¤±ã‚’å°å…¥ã™ã‚‹ã€‚
We adopt a similar operation with the same weights ğ‘”ğ‘ğ‘¡ğ‘’â„ğ‘ğ‘Ÿğ‘‘ to obtain the corresponding positive item factor e â€² ğ‘ and e â€²â€² ğ‘ :
åŒã˜é‡ã¿ğ‘”ğ‘ğ‘¡ğ‘’â„ğ‘ğ‘‘ã§åŒæ§˜ã®æ“ä½œã‚’æ¡ç”¨ã—ã€å¯¾å¿œã™ã‚‹æ­£é …ç›®å› å­e â€²ğ‘ã¨e â€²ğ‘ã‚’æ±‚ã‚ã‚‹ï¼š

$$
\tag{6}
$$

This is particularly important as e ğ‘’ğ‘ğ‘ ğ‘¦ ğ‘› may emphasize the first 48 dimensions while e â„ğ‘ğ‘Ÿğ‘‘ ğ‘› may emphasize the last 14.
ã“ã‚Œã¯ç‰¹ã«é‡è¦ã§ã‚ã‚‹ã€‚
In order to ensure coherence in subsequent operations, it is imperative to maintain the correspondence of dimensions.
ãã®å¾Œã®ä½œæˆ¦ã®ä¸€è²«æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã«ã¯ã€å¯¸æ³•ã®å¯¾å¿œé–¢ä¿‚ã‚’ç¶­æŒã™ã‚‹ã“ã¨ãŒä¸å¯æ¬ ã§ã‚ã‚‹ã€‚
For ease of understanding, reader can directly regard them as positive samples.
ç†è§£ã—ã‚„ã™ã„ã‚ˆã†ã«ã€èª­è€…ã¯ã“ã‚Œã‚‰ã‚’ç›´æ¥ãƒã‚¸ãƒ†ã‚£ãƒ–ãªã‚µãƒ³ãƒ—ãƒ«ã¨ã¿ãªã™ã“ã¨ãŒã§ãã‚‹ã€‚
Intuitively, e â„ğ‘ğ‘Ÿğ‘‘ ğ‘› should be more similar to positive since it is difficult for users to discern it as a negative sample (both have a high similarity to the user).
ç›´æ„Ÿçš„ã«ã¯ã€e â„ğ‘ğ‘‘ğ‘‘ğ‘‘ã¯ã€ãƒ¦ãƒ¼ã‚¶ãŒãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ»ã‚µãƒ³ãƒ—ãƒ«ã¨åˆ¤åˆ¥ã—ã«ãã„ãŸã‚ã€ãƒã‚¸ãƒ†ã‚£ãƒ–ã«è¿‘ã„ã¯ãšã§ã™ï¼ˆã©ã¡ã‚‰ã‚‚ãƒ¦ãƒ¼ã‚¶ã¨ã®é¡ä¼¼åº¦ãŒé«˜ã„ï¼‰ã€‚
However, this signal is not entirely reliable, as it may not accurately reflect a userâ€™s level of interest.
ã—ã‹ã—ã€ã“ã®ä¿¡å·ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é–¢å¿ƒåº¦ã‚’æ­£ç¢ºã«åæ˜ ã—ã¦ã„ã‚‹ã¨ã¯é™ã‚‰ãªã„ãŸã‚ã€å®Œå…¨ã«ä¿¡é ¼ã§ãã‚‹ã‚‚ã®ã§ã¯ãªã„ã€‚
Therefore, instead of relying on a stringent constraint like Equation 5, we introduce another disentanglement loss Lğ‘‘ as
ã—ãŸãŒã£ã¦ã€å¼5ã®ã‚ˆã†ãªå³ã—ã„åˆ¶ç´„ã«é ¼ã‚‹ä»£ã‚ã‚Šã«ã€åˆ¥ã®åˆ†é›¢æå¤±L_1451ã‚’æ¬¡ã®ã‚ˆã†ã«å°å…¥ã™ã‚‹ã€‚

$$
\tag{7}
$$

where the Euclidean distance is used to measure the similarity between e â€² ğ‘ and e â„ğ‘ğ‘Ÿğ‘‘ ğ‘› while the score function is used to measure the similarity between e ğ‘’ğ‘ğ‘ ğ‘¦ ğ‘ and e â€²â€² ğ‘› .
ã“ã“ã§ã€ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã¯ e â€² ğ‘ ã¨ e â„ğ‘ğ‘Ÿğ‘ ğ‘› ã®é¡ä¼¼æ€§ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã«ç”¨ã„ã‚‰ã‚Œã€ã‚¹ã‚³ã‚¢é–¢æ•°ã¯ e ğ‘’ğ‘¦ ã¨ e â€² ğ‘› ã®é¡ä¼¼æ€§ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã«ç”¨ã„ã‚‰ã‚Œã‚‹ã€‚
This is because we want to leverage only reliable hardness while we can be more lenient with the easy part.
ã“ã‚Œã¯ã€ä¿¡é ¼ã§ãã‚‹ç¡¬ã•ã ã‘ã‚’æ´»ç”¨ã—ã€ç°¡å˜ãªéƒ¨åˆ†ã«ã¯ã‚‚ã£ã¨ç”˜ãã—ã¦ã‚‚ã„ã„ã‹ã‚‰ã ã€‚

## Augmentation 

Next, we propose an augmentation module to create synthetic negative items which are more similar to the corresponding positive items.
æ¬¡ã«ã€å¯¾å¿œã™ã‚‹ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚¢ã‚¤ãƒ†ãƒ ã«ã‚ˆã‚Šé¡ä¼¼ã—ãŸåˆæˆãƒã‚¬ãƒ†ã‚£ãƒ–ã‚¢ã‚¤ãƒ†ãƒ ã‚’ä½œæˆã™ã‚‹ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ææ¡ˆã™ã‚‹ã€‚
After the disentanglement step, we have obtained the hard and easy factors of a negative item, where the hard factor contains more useful information for model training.
é›¢æ•£åŒ–ã‚¹ãƒ†ãƒƒãƒ—ã®å¾Œã€ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã®ãƒãƒ¼ãƒ‰å› å­ã¨ã‚¤ãƒ¼ã‚¸ãƒ¼å› å­ãŒå¾—ã‚‰ã‚Œã€ãƒãƒ¼ãƒ‰å› å­ã«ã¯ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã«ã‚ˆã‚Šæœ‰ç”¨ãªæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã€‚
Therefore, our goal is to augment the easy factor to improve model performance.
ã—ãŸãŒã£ã¦ã€æˆ‘ã€…ã®ç›®æ¨™ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€ã‚¤ãƒ¼ã‚¸ãƒ¼ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’å¢—å¼·ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
However, existing augmentation techniques fail to carefully regulate and quantify the augmentation needed to approximate positive items while still being negative.
ã—ã‹ã—ã€æ—¢å­˜ã®ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æŠ€è¡“ã¯ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã§ã‚ã‚ŠãªãŒã‚‰ãƒã‚¸ãƒ†ã‚£ãƒ–ãªé …ç›®ã‚’è¿‘ä¼¼ã™ã‚‹ãŸã‚ã«å¿…è¦ãªã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ³¨æ„æ·±ãèª¿æ•´ã—ã€å®šé‡åŒ–ã™ã‚‹ã“ã¨ã«å¤±æ•—ã—ã¦ã„ã‚‹ã€‚
To this end, we propose to regulate the augmentation from two different aspects: Direction: Intuitively, the direction of the augmentation on a negative item should be towards the corresponding positive item.
ã“ã®ç›®çš„ã®ãŸã‚ã«ã€æˆ‘ã€…ã¯2ã¤ã®ç•°ãªã‚‹å´é¢ã‹ã‚‰è£œå¼·ã‚’è¦åˆ¶ã™ã‚‹ã“ã¨ã‚’ææ¡ˆã™ã‚‹ï¼š 
æ–¹å‘ï¼š 
ç›´æ„Ÿçš„ã«ã¯ã€å¦å®šçš„ãªé …ç›®ã«å¯¾ã™ã‚‹è£œå¼·ã®æ–¹å‘ã¯ã€å¯¾å¿œã™ã‚‹è‚¯å®šçš„ãªé …ç›®ã«å‘ã‹ã†ã¹ãã§ã‚ã‚‹ã€‚
Therefore, we first calculate the difference eğ‘‘ğ‘– ğ‘“ between the factor of the positive item e â€²â€² ğ‘ and the easy factor of the negative item e ğ‘’ğ‘ğ‘ ğ‘¦ ğ‘› :
ãã“ã§ã€ã¾ãšæ­£é …ç›®ã®å› å­e ğ‘“ã¨è² é …ç›®ã®æ˜“å› å­e ğ‘’ğ‘¦ã®å·®eğ‘‘ğ‘–ã‚’è¨ˆç®—ã™ã‚‹ï¼š

$$
\tag{8}
$$

A first attempt is to directly make eğ‘‘ğ‘– ğ‘“ as the direction of the augmentation.
æœ€åˆã®è©¦ã¿ã¯ã€eğ‘‘ğ‘–ğ‘“ã‚’å¢—å¼·ã®æ–¹å‘ã¨ã—ã¦ç›´æ¥ä½œã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
This design is less desirable because (1) it introduces too much positive information, which may turn the augmented negative item into positive.(2) It contains too much prior information (i.e., the easy factor is identical to that of the positive item), which can lead to the model collapse problem [28, 29].
ã“ã®ãƒ‡ã‚¶ã‚¤ãƒ³ã¯ã€(1)ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæƒ…å ±ãŒå¤šã™ãã‚‹ãŸã‚ã€æ‹¡å¼µã•ã‚ŒãŸãƒã‚¬ãƒ†ã‚£ãƒ–ãªé …ç›®ãŒãƒã‚¸ãƒ†ã‚£ãƒ–ã«ãªã£ã¦ã—ã¾ã†å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚(2)äº‹å‰æƒ…å ±ãŒå¤šã™ãã‚‹ï¼ˆã¤ã¾ã‚Šã€ç°¡å˜ãªè¦å› ãŒãƒã‚¸ãƒ†ã‚£ãƒ–ãªé …ç›®ã®ãã‚Œã¨åŒã˜ã§ã‚ã‚‹ï¼‰ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«å´©å£Šã®å•é¡Œã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹[28, 29]ã€‚
Inspired by [40], we carefully smooth the direction of augmentation by extracting the quadrant information eğ‘‘ğ‘–ğ‘Ÿ of eğ‘‘ğ‘– ğ‘“ with the sign function ğ‘ ğ‘”ğ‘›(Â·):
40]ã«è§¦ç™ºã•ã‚Œã¦ã€eá‘‘ğ‘–á‘Ÿã®è±¡é™æƒ…å ±eá‘“ã‚’ç¬¦å·é–¢æ•°á‘‘ğ‘–á‘”ğ‘›(-)ã§æŠ½å‡ºã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€å¢—å¤§æ–¹å‘ã‚’æ³¨æ„æ·±ãå¹³æ»‘åŒ–ã™ã‚‹ï¼š

$$
\tag{9}
$$

The direction eğ‘‘ğ‘–ğ‘Ÿ âˆˆ R ğ‘‘ effectively compresses the embedding augmentation space into a quadrant space, which provides essential direction information without having the aforementioned issues.
æ–¹å‘eğ‘‘ğ‘– ğ‘Ÿâˆˆ Rğ‘‘ã¯ã€åŸ‹ã‚è¾¼ã¿è£œå¼·ç©ºé–“ã‚’å››åˆ†å††ç©ºé–“ã«åŠ¹æœçš„ã«åœ§ç¸®ã—ã€å‰è¿°ã®å•é¡Œã‚’æŠ±ãˆã‚‹ã“ã¨ãªãæœ¬è³ªçš„ãªæ–¹å‘æƒ…å ±ã‚’æä¾›ã™ã‚‹ã€‚
Magnitude: Magnitude determines the strength of augmentation.
ãƒã‚°ãƒ‹ãƒãƒ¥ãƒ¼ãƒ‰ï¼š 
ãƒã‚°ãƒ‹ãƒãƒ¥ãƒ¼ãƒ‰ã¯ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®å¼·ã•ã‚’æ±ºå®šã™ã‚‹ã€‚
Several studies [14] have shown that when the perturbation to the embedding is overly large, it will dramatically change its original semantics.
ã„ãã¤ã‹ã®ç ”ç©¶[14]ã¯ã€åŸ‹ã‚è¾¼ã¿ã«å¯¾ã™ã‚‹æ‘‚å‹•ãŒéåº¦ã«å¤§ãããªã‚‹ã¨ã€å…ƒã®ã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹ãŒåŠ‡çš„ã«å¤‰åŒ–ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
Therefore, we need to carefully calibrate the magnitude of the augmentation.
ã—ãŸãŒã£ã¦ã€è£œå¼·ã®å¤§ãã•ã‚’æ…é‡ã«èª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
We design a two-step approach to generate a regulated magnitude Î” âˆˆ R ğ‘‘ .
è¦åˆ¶ã•ã‚ŒãŸå¤§ãã•Î” âˆˆ R ğ‘‘ ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ã€2æ®µéšã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è¨­è¨ˆã™ã‚‹ã€‚
We first consider the distribution of Î”.
ã¾ãšÎ”ã®åˆ†å¸ƒã‚’è€ƒãˆã‚‹ã€‚
As Î” is a noise embedding, we adopt a uniform distribution on the interval [0, 0.1].
Î”ã¯ãƒã‚¤ã‚ºã®åŸ‹ã‚è¾¼ã¿ã§ã‚ã‚‹ãŸã‚ã€åŒºé–“[0, 0.1]ä¸Šã®ä¸€æ§˜åˆ†å¸ƒã‚’æ¡ç”¨ã™ã‚‹ã€‚
The uniform distribution introduces a certain amount of randomness, which is beneficial to improve the robustness of the model.
ä¸€æ§˜åˆ†å¸ƒã¯ã‚ã‚‹ç¨‹åº¦ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’å°å…¥ã—ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒã‚¹ãƒˆæ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã®ã«æœ‰åŠ¹ã§ã‚ã‚‹ã€‚
Second, we restrict Î” to be smaller than a margin.
æ¬¡ã«ã€Î”ãŒãƒãƒ¼ã‚¸ãƒ³ã‚ˆã‚Šå°ã•ããªã‚‹ã‚ˆã†ã«åˆ¶é™ã™ã‚‹ã€‚
Instead of using a static scalar [22], we dynamically set the margin by calculating the similarity between the hard factors of the negative and corresponding positive items.
é™çš„ãªã‚¹ã‚«ãƒ©ãƒ¼[22]ã‚’ä½¿ç”¨ã™ã‚‹ä»£ã‚ã‚Šã«ã€ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã¨å¯¾å¿œã™ã‚‹ãƒã‚¸ãƒ†ã‚£ãƒ–é …ç›®ã®ãƒãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼é–“ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€ãƒãƒ¼ã‚¸ãƒ³ã‚’å‹•çš„ã«è¨­å®šã™ã‚‹ã€‚
The intuition is that a higher similarity between the hard factors suggests that the negative item already contains much useful information, and thus we should augment it with a smaller magnitude.
ç›´æ„Ÿçš„ã«ã¯ã€ãƒãƒ¼ãƒ‰ãƒ»ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼é–“ã®é¡ä¼¼åº¦ãŒé«˜ã‘ã‚Œã°é«˜ã„ã»ã©ã€ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã«ã¯ã™ã§ã«å¤šãã®æœ‰ç”¨ãªæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã€ã—ãŸãŒã£ã¦ã€ã‚ˆã‚Šå°ã•ãªå¤§ãã•ã§ãã‚Œã‚’è£œå¼·ã™ã¹ãã§ã‚ã‚‹ã€‚
Finally, the regulated magnitude Î” is calculated via
æœ€å¾Œã«ã€è¦åˆ¶ã•ã‚ŒãŸå¤§ãã•Î”ã¯æ¬¡ã®ã‚ˆã†ã«ã—ã¦è¨ˆç®—ã•ã‚Œã‚‹ã€‚

$$
\tag{10}
$$

where the element-wise product âŠ™ is used to calculate the similarity between e â„ğ‘ğ‘Ÿğ‘‘ ğ‘› and e â€² ğ‘ .
ã“ã“ã§ã€e â„â†ªLl44Eâ†© ğ‘› ã¨ e â€² ğ‘ ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã«è¦ç´ ã”ã¨ã®ç© âŠ™ ãŒä½¿ã‚ã‚Œã‚‹ã€‚
The transformation matrix W âˆˆ R 1Ã—ğ‘‘ is used to map the similarity vector to a scalar.
å¤‰æ›è¡Œåˆ—WâˆˆR 1Ã—_1D45ã¯ã€é¡ä¼¼æ€§ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã‚¹ã‚«ãƒ©ãƒ¼ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ã€‚
The sigmoid function ğœ(Â·) helps remove the sign information to avoid interfering with the learned direction eğ‘‘ğ‘–ğ‘Ÿ.
ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ğœ(-)ã¯ã€å­¦ç¿’ã•ã‚ŒãŸæ–¹å‘eğ‘‘ğ‘–ğ‘Ÿã«å¹²æ¸‰ã—ãªã„ã‚ˆã†ã«ç¬¦å·æƒ…å ±ã‚’é™¤å»ã™ã‚‹ã®ã«å½¹ç«‹ã¤ã€‚
After carefully determining the direction and magnitude, we can generate the augmented version e ğ‘ğ‘¢ğ‘” ğ‘› of the negative item eğ‘› as follows
æ–¹å‘ã¨å¤§ãã•ã‚’æ³¨æ„æ·±ãæ±ºå®šã—ãŸå¾Œã€è² ã®é …ç›®eğ‘›ã®å¢—å¤§ç‰ˆe ğ‘ğ‘¢ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

$$
\tag{11}
$$

With our design, the augmented negative item becomes more similar to the corresponding positive item without causing huge changes of the semantics, and can remain negative.
ç§ãŸã¡ã®ãƒ‡ã‚¶ã‚¤ãƒ³ã§ã¯ã€æ‹¡å¼µã•ã‚ŒãŸãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã¯ã€æ„å‘³è«–ã«å¤§ããªå¤‰åŒ–ã‚’ã‚‚ãŸã‚‰ã™ã“ã¨ãªãã€å¯¾å¿œã™ã‚‹ãƒã‚¸ãƒ†ã‚£ãƒ–é …ç›®ã«ã‚ˆã‚Šè¿‘ããªã‚Šã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã®ã¾ã¾ã§ã‚ã‚Šç¶šã‘ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
We apply the above operation to every item in the candidate negative set E and obtain the augmented negative set E ğ‘ğ‘¢ğ‘” .
å€™è£œå¦å®šé›†åˆEã®ã™ã¹ã¦ã®é …ç›®ã«ä¸Šè¨˜ã®æ“ä½œã‚’é©ç”¨ã—ã€æ‹¡å¼µå¦å®šé›†åˆE ğ‘ğ‘„ã‚’å¾—ã‚‹ã€‚

## Sampling ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

After obtaining the augmented candidate negative set E ğ‘ğ‘¢ğ‘”, we need to devise a sampling strategy to select the best negative item from E ğ‘ğ‘¢ğ‘” to facilitate model training.
æ‹¡å¼µã•ã‚ŒãŸå¦å®šå€™è£œé›†åˆEğ‘ãŒå¾—ã‚‰ã‚ŒãŸã‚‰ã€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’å®¹æ˜“ã«ã™ã‚‹ãŸã‚ã«ã€Eğ‘ğ‘”ã‹ã‚‰æœ€é©ãªå¦å®šé …ç›®ã‚’é¸æŠã™ã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã‚’è€ƒæ¡ˆã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
Existing hard negative sampling methods select the negative item with the highest score, where the score is calculated by the score function ğ‘ (Â·, Â·) with the user and negative item embeddings as input.
æ—¢å­˜ã®ãƒãƒ¼ãƒ‰ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ³•ã¯ã€æœ€ã‚‚é«˜ã„ã‚¹ã‚³ã‚¢ã‚’æŒã¤ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã‚’é¸æŠã™ã‚‹ã€‚ã‚¹ã‚³ã‚¢ã¯ã€ãƒ¦ãƒ¼ã‚¶ã¨ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã®åŸ‹ã‚è¾¼ã¿ã‚’å…¥åŠ›ã¨ã—ã¦ã€ã‚¹ã‚³ã‚¢é–¢æ•°ğ‘ (-, -)ã«ã‚ˆã£ã¦è¨ˆç®—ã•ã‚Œã‚‹ã€‚
However, as explained before, negative items with relatively low scores will be seldom selected, which leads to the information discrimination issue.
ã—ã‹ã—ã€å…ˆã«èª¬æ˜ã—ãŸã‚ˆã†ã«ã€ç›¸å¯¾çš„ã«å¾—ç‚¹ã®ä½ã„å¦å®šçš„ãªé …ç›®ã¯ã»ã¨ã‚“ã©é¸æŠã•ã‚Œãªã„ã€‚
Although the augmentation can already alleviate information discrimination to a certain extent, we further design a more flexible and effective sampling strategy by introducing a new metric named augmentation gain.
ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã™ã§ã«æƒ…å ±è­˜åˆ¥ã‚’ã‚ã‚‹ç¨‹åº¦ç·©å’Œã™ã‚‹ã“ã¨ãŒã§ãã‚‹ãŒã€æˆ‘ã€…ã¯ã•ã‚‰ã«ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚²ã‚¤ãƒ³ã¨ã„ã†æ–°ã—ã„æŒ‡æ¨™ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚ŠæŸ”è»Ÿã§åŠ¹æœçš„ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã‚’è¨­è¨ˆã™ã‚‹ã€‚
Augmentation gain measures the score difference before and after the augmentation.
ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ã‚²ã‚¤ãƒ³ã¯ã€ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å‰å¾Œã®ã‚¹ã‚³ã‚¢å·®ã‚’æ¸¬å®šã™ã‚‹ã€‚
Formally, the score ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ and the augmentation gain ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘” are calculated as:
æ­£å¼ã«ã¯ã€ã‚¹ã‚³ã‚¢ğ‘ ğ‘ğ‘œğ‘…ğ‘’ã¨å¢—å¼·ã‚²ã‚¤ãƒ³ğ‘ ğ‘ğ‘ğ‘¢ğ‘”ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã‚‹ï¼š

$$
\tag{12}
$$

The sampling module we propose considers both ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ and ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘” to select the suitable negative item e ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™ ğ‘› from the augmented candidate negative set E ğ‘ğ‘¢ğ‘”.
æˆ‘ã€…ãŒææ¡ˆã™ã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ»ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ğ‘ ğ‘ğ‘…ã¨ğ‘’ğ‘ğ‘…ã®ä¸¡æ–¹ã‚’è€ƒæ…®ã—ã€æ‹¡å¼µã•ã‚ŒãŸå¦å®šå€™è£œé›†åˆEáµ„Eáµ„ã‹ã‚‰é©åˆ‡ãªå¦å®šé …ç›®eáµ„Eğ‘”ã‚’é¸æŠã™ã‚‹ã€‚
To explicitly balance the contributions between ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ and ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘”, we introduce a trade-off parameter ğœ–.
...
The final augmented negative item e ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™ ğ‘› is chosen via
æœ€çµ‚çš„ãªå¦å®šé …ç›®e ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ğ‘›ã¯æ¬¡ã®ã‚ˆã†ã«ã—ã¦é¸ã°ã‚Œã‚‹ã€‚

$$
\tag{13}
$$

## Discussion 

It is worth noting that most existing negative sampling methods can be considered as a special case of ANS.
æ—¢å­˜ã®ã»ã¨ã‚“ã©ã®ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ³•ã¯ã€ANSã®ç‰¹æ®Šãªã‚±ãƒ¼ã‚¹ã¨è€ƒãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã“ã¨ã¯æ³¨ç›®ã«å€¤ã™ã‚‹ã€‚
For example, DNS [41] can be obtained by removing the disentanglement and augmentation steps.
ä¾‹ãˆã°ã€DNS [41]ã¯ã€åˆ†é›¢ã¨ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ã§å¾—ã‚‰ã‚Œã‚‹ã€‚
MixGCF [16] can be obtained by removing the disentanglement step and replacing the regulated augmentation with unconstrained augmentation based on graph structure information and positive item information.
MixGCF[16]ã¯ã€é›¢æ•£åŒ–ã‚¹ãƒ†ãƒƒãƒ—ã‚’å‰Šé™¤ã—ã€ã‚°ãƒ©ãƒ•æ§‹é€ æƒ…å ±ã¨æ­£é …ç›®æƒ…å ±ã«åŸºã¥ãç„¡åˆ¶ç´„ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã§è¦åˆ¶ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç½®ãæ›ãˆã‚‹ã“ã¨ã§å¾—ã‚‰ã‚Œã‚‹ã€‚
SRNS [9] can be obtained by removing the augmentation step and considering variance in the sampling strategy step.
SRNS[9]ã¯ã€ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’å‰Šé™¤ã—ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã®ã‚¹ãƒ†ãƒƒãƒ—ã§åˆ†æ•£ã‚’è€ƒæ…®ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

## Model Optimization ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–

Finally, we adopt the proposed ANS method as the negative sampling strategy and take into consideration also the recommendation loss L to optimize the parameters Î˜ of an implicit CF model (e.g., MF-BPR).
æœ€å¾Œã«ã€æš—é»™çš„CFãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹ãˆã°MF-BPRï¼‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Î˜ã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã«ã€è² ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã¨ã—ã¦ææ¡ˆã•ã‚ŒãŸANSæ³•ã‚’æ¡ç”¨ã—ã€æ¨è–¦æå¤±Lã‚‚è€ƒæ…®ã™ã‚‹ã€‚

$$
\tag{14}
$$

where ğœ† is a hyper-parameter controlling the strength of ğ¿2 regularization, and ğ›¾ is another hyper-parameter used to adjust the impact of the contrastive loss and the disentanglement loss.
ã“ã“ã§ğœ†ã¯â†ªLu_1D43F æ­£å‰‡åŒ–ã®å¼·ã•ã‚’åˆ¶å¾¡ã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã€Ç¾ã¯ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆæå¤±ã¨ä¸é€£ç¶šæ€§æå¤±ã®å½±éŸ¿ã‚’èª¿æ•´ã™ã‚‹ãŸã‚ã®ã‚‚ã†1ã¤ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚‹ã€‚
Observably, our negative sampling strategy paradigm can be seamlessly incorporated into mainstream models without the need for substantial modifications.
è¦³å¯Ÿã«ã‚ˆã‚Œã°ã€æˆ‘ã€…ã®ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¯ã€å¤§å¹…ãªä¿®æ­£ã‚’å¿…è¦ã¨ã™ã‚‹ã“ã¨ãªãã€ä¸»æµã®ãƒ¢ãƒ‡ãƒ«ã«ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«çµ„ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã‚‹ã€‚

# Experiment å®Ÿé¨“

In this section, we conduct comprehensive experiments to answer the following key research questions: â€¢ RQ1: How does ANS perform compared to the baselines and integrating ANS into different mainstream CF models perform compared with the original ones? â€¢ RQ2: How accurate is the disentanglement step in the absence of ground truth? â€¢ RQ3: Can ANS alleviate ambiguous trap and information discrimination? â€¢ RQ4: How do different steps affect ANSâ€™s performance? â€¢ RQ5: How do different hyper-parameter settings (i.e., ğ›¾, ğœ–, and ğ‘€) affect ANSâ€™s performance? â€¢ RQ6: How does ANS perform in efficiency?
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ä»¥ä¸‹ã®ä¸»è¦ãªç ”ç©¶èª²é¡Œã«ç­”ãˆã‚‹ãŸã‚ã«åŒ…æ‹¬çš„ãªå®Ÿé¨“ã‚’è¡Œã†ï¼š 
- RQ1ï¼š 
ANSã¯ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒã—ã¦ã€ã¾ãŸANSã‚’ç•°ãªã‚‹ä¸»æµCFãƒ¢ãƒ‡ãƒ«ã«çµ±åˆã—ãŸå ´åˆã€å…ƒã®ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦ã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã®ã‹ï¼Ÿ- RQ2ï¼š 
ã‚°ãƒ©ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ãŒãªã„å ´åˆã€ãƒ‡ã‚£ã‚¹ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã‚¹ãƒ†ãƒƒãƒ—ã¯ã©ã®ç¨‹åº¦æ­£ç¢ºã‹ï¼Ÿ- RQ3ï¼š 
ANSã¯æ›–æ˜§ãƒˆãƒ©ãƒƒãƒ—ã¨æƒ…å ±è­˜åˆ¥ã‚’ç·©å’Œã§ãã‚‹ã‹ï¼Ÿ- RQ4ï¼š 
ç•°ãªã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã¯ANSã®æ€§èƒ½ã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ï¼Ÿ- RQ5ï¼š 
ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®è¨­å®šï¼ˆã™ãªã‚ã¡ã€â†ªLl_1FE, â†ªLl_1FE, â†ªLl_1FE, ğ‘€ï¼‰ã®é•ã„ã¯ANSã®æ€§èƒ½ã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ï¼Ÿ- RQ6ï¼š 
ANSã®åŠ¹ç‡æ€§ã¯ã©ã®ç¨‹åº¦ã‹ï¼Ÿ

## Experimental Setup å®Ÿé¨“ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### Datasets. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

We consider five public benchmark datasets in the experiments: Amazon-Baby, Amazon-Beauty, Yelp2018, Gowalla, and Last.fm.
å®Ÿé¨“ã§ã¯ã€5ã¤ã®ãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ¤œè¨ã™ã‚‹ï¼š 
Amazon-Babyã€Amazon-Beautyã€Yelp2018ã€Gowallaã€Last.fmã§ã‚ã‚‹ã€‚
In order to comprehensively showcase the efficacy of the proposed methodology, we have partitioned the dataset into two distinct categories for processing.
ææ¡ˆæ‰‹æ³•ã®æœ‰åŠ¹æ€§ã‚’åŒ…æ‹¬çš„ã«ç¤ºã™ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’2ã¤ã®ç•°ãªã‚‹ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«åˆ†å‰²ã—ã¦å‡¦ç†ã—ãŸã€‚
For the datasets Amazonbaby, Amazon-Beauty, and Last.fm, the training set is constructed by including only the interactions that occurred on or before a specified timestamp, similar to the approach used in the state-of-theart method DENS [21].
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆAmazonbabyã€Amazon-Beautyã€Last.fmã«ã¤ã„ã¦ã¯ã€æœ€æ–°ã®æ‰‹æ³•DENS[21]ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨åŒæ§˜ã«ã€æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»¥å‰ã«ç™ºç”Ÿã—ãŸã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿ã‚’å«ã‚€ã“ã¨ã«ã‚ˆã£ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆãŒæ§‹ç¯‰ã•ã‚Œã‚‹ã€‚
After reserving the remaining interactions for the test set, a validation set is created by randomly sampling 10% of the interactions from the training set.
ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆç”¨ã«æ®‹ã‚Šã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç¢ºä¿ã—ãŸå¾Œã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã‹ã‚‰10%ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦æ¤œè¨¼ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹ã€‚
The adoption of this strategy, as suggested by previous works [4, 22, 32], provides the benefit of preventing data leakage.
å…ˆè¡Œç ”ç©¶[4, 22, 32]ã§ç¤ºå”†ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«ã€ã“ã®æˆ¦ç•¥ã‚’æ¡ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ‡ãƒ¼ã‚¿æ¼æ´©ã‚’é˜²ãã“ã¨ãŒã§ãã‚‹ã¨ã„ã†åˆ©ç‚¹ãŒã‚ã‚‹ã€‚
For Yelp and Gowalla, we have followed the conventional practice of utilizing an 80% training set, 10% test set, and 10% validation set.
Yelpã¨Gowallaã«ã¤ã„ã¦ã¯ã€80ï¼…ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã€10ï¼…ã®ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã€10ï¼…ã®æ¤œè¨¼ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã™ã‚‹ã¨ã„ã†å¾“æ¥ã®ã‚„ã‚Šæ–¹ã«å¾“ã£ã¦ã„ã‚‹ã€‚
These datasets have different statistical properties, which can reliably validate the performance of a model [7].
ã“ã‚Œã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ç•°ãªã‚‹çµ±è¨ˆçš„ç‰¹æ€§ãŒã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ç¢ºå®Ÿã«æ¤œè¨¼ã™ã‚‹ã“ã¨ãŒã§ãã‚‹[7]ã€‚
Table 1 summarizes the statistics of the datasets.
è¡¨1ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆã‚’ã¾ã¨ã‚ãŸã€‚

### 2 Baseline Algorithms. 2 ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ»ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

To demonstrate the effectiveness of the proposed ANS method, we compare it with several representative state-of-the-art negative sampling methods.
ææ¡ˆã™ã‚‹ANSæ³•ã®æœ‰åŠ¹æ€§ã‚’å®Ÿè¨¼ã™ã‚‹ãŸã‚ã«ã€ä»£è¡¨çš„ãªæœ€æ–°ã®ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ³•ã¨æ¯”è¼ƒã™ã‚‹ã€‚
â€¢ RNS [26]: Random negative sampling (RNS) adopts a uniform distribution to sample unobserved items.
- RNS [26]ï¼š 
ãƒ©ãƒ³ãƒ€ãƒ ãƒ»ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°(RNS)ã¯ã€æœªè¦³æ¸¬é …ç›®ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ä¸€æ§˜åˆ†å¸ƒã‚’æ¡ç”¨ã™ã‚‹ã€‚
â€¢ DNS [41]: Dynamic negative sampling (DNS) adaptively selects items with the highest score as the negative samples.
- DNS [41]ï¼š 
Dynamic negative sampling (DNS)ã¯é©å¿œçš„ã«æœ€ã‚‚é«˜ã„ã‚¹ã‚³ã‚¢ã‚’æŒã¤ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦é¸æŠã™ã‚‹ã€‚
â€¢ SRNS [9]: SRNS introduces variance to avoid the false negative item problem based on DNS.
- SRNS [9]ï¼š 
SRNSã¯ã€DNSã«åŸºã¥ãå½é™°æ€§é …ç›®ã®å•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã«åˆ†æ•£ã‚’å°å…¥ã™ã‚‹ã€‚
â€¢ MixGCF [16]: MixGCF injects information from positive and graph to synthesizes harder negative samples.
- MixGCF [16]ï¼š 
MixGCFã¯ãƒã‚¸ãƒ†ã‚£ãƒ–ã¨ã‚°ãƒ©ãƒ•ã‹ã‚‰æƒ…å ±ã‚’æ³¨å…¥ã—ã€ã‚ˆã‚Šé›£ã—ã„ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã‚’åˆæˆã™ã‚‹ã€‚
â€¢ DENS [21]: DENS disentangles relevant and irrelevant factors of items and designs a factor-aware sampling strategy.
- DENS [21]ï¼š 
DENSã¯é …ç›®ã®é–¢é€£å› å­ã¨éé–¢é€£å› å­ã‚’åˆ†é›¢ã—ã€å› å­ã‚’æ„è­˜ã—ãŸã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã‚’è¨­è¨ˆã™ã‚‹ã€‚
To further validate the effectiveness of our proposed methodology, we have integrated it with a diverse set of representative models.
ææ¡ˆã—ãŸæ‰‹æ³•ã®æœ‰åŠ¹æ€§ã‚’ã•ã‚‰ã«æ¤œè¨¼ã™ã‚‹ãŸã‚ã€å¤šæ§˜ãªä»£è¡¨çš„ãƒ¢ãƒ‡ãƒ«ç¾¤ã¨ã®çµ±åˆã‚’è¡Œã£ãŸã€‚
â€¢ NGCF [35]: NGCF employs a message-passing scheme to effectively leverage the high-order information.
- NGCF [35]ï¼š 
NGCFã¯ã€é«˜æ¬¡æƒ…å ±ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã™ã‚‹ãŸã‚ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°æ–¹å¼ã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã€‚
â€¢ LightGCN [13]: LightGCN adopts a simplified approach by eliminating the non-linear transformation and instead utilizing a sum-based pooling module to enhance its performance.
- LightGCN [13]ï¼š 
LightGCNã¯ã€éç·šå½¢å¤‰æ›ã‚’æ’é™¤ã—ã€ãã®ä»£ã‚ã‚Šã«å’Œãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ¼ãƒªãƒ³ã‚°ãƒ»ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’åˆ©ç”¨ã—ã¦æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã¨ã„ã†ã€ç°¡ç´ åŒ–ã•ã‚ŒãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã€‚
â€¢ SGL [37]: SGL incorporates contrastive learning.
- SGLï¼»37ï¼½ï¼š 
SGLã¯å¯¾ç…§å­¦ç¿’ã‚’å–ã‚Šå…¥ã‚ŒãŸã‚‚ã®ã§ã‚ã‚‹ã€‚
The objective is to enhance the agreement between various views of the same node, while minimizing the agreement between views of different nodes.
ç›®çš„ã¯ã€åŒã˜ãƒãƒ¼ãƒ‰ã®æ§˜ã€…ãªãƒ“ãƒ¥ãƒ¼é–“ã®ä¸€è‡´ã‚’é«˜ã‚ã€ç•°ãªã‚‹ãƒãƒ¼ãƒ‰ã®ãƒ“ãƒ¥ãƒ¼é–“ã®ä¸€è‡´ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚

### Implementation Details. å®Ÿæ–½å†…å®¹

Similar to previous studies [4, 26, 33], we consider MF-BPR [26] as the basic CF model.
å…ˆè¡Œç ”ç©¶[4, 26, 33]ã¨åŒæ§˜ã«ã€MF-BPR[26]ã‚’åŸºæœ¬çš„ãªCFãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦è€ƒãˆã‚‹ã€‚
For a fair comparison, the size of embeddings is fixed to 64, and the embeddings are initialized with Xavier [10] for all methods.
å…¬å¹³ãªæ¯”è¼ƒã®ãŸã‚ã«ã€åŸ‹ã‚è¾¼ã¿ã‚µã‚¤ã‚ºã¯64ã«å›ºå®šã•ã‚Œã€åŸ‹ã‚è¾¼ã¿ã¯ã™ã¹ã¦ã®æ‰‹æ³•ã§Xavier [10]ã§åˆæœŸåŒ–ã•ã‚Œã‚‹ã€‚
We use Adam [20] to optimize the parameters with a default learning rate of 0.001 and a default mini-batch size of 2048.
Adam[20]ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå­¦ç¿’ç‡0.001ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚º2048ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã™ã‚‹ã€‚
The ğ¿2 regularization coefficient ğœ† is set to 10âˆ’4 .
æ­£å‰‡åŒ–ä¿‚æ•° â†ªLu_1D43F ã¯ 10-4 ã«è¨­å®šã•ã‚Œã‚‹ã€‚
The size of the candidate negative item set ğ‘€ is tested in the range of {2, 4, 8, 16, 32}.
å¦å®šé …ç›®å€™è£œé›†åˆã®ã‚µã‚¤ã‚ºğ‘€ã¯{2, 4, 8, 16, 32}ã®ç¯„å›²ã§ãƒ†ã‚¹ãƒˆã•ã‚Œã‚‹ã€‚
The weight ğ›¾ of the contrastive and disentanglement losses and ğœ– of the augmentation gain are both searched in the range of [0, 1].
ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆæå¤±ã¨ãƒ‡ã‚£ã‚»ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆæå¤±ã®é‡ã¿Ç¾ã¨ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚²ã‚¤ãƒ³ã®é‡ã¿ğœ–ã¯ã¨ã‚‚ã«[0, 1]ã®ç¯„å›²ã§æ¢ç´¢ã•ã‚Œã‚‹ã€‚
In order to guarantee the replicability, our approach is implemented by the RecBole v1.1.1 framework [42].
å†ç¾æ€§ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã«ã€æˆ‘ã€…ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯RecBole v1.1.1ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯[42]ã«ã‚ˆã£ã¦å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ã€‚
We conducted statistical tests to evaluate the significance of our experimental results.
å®Ÿé¨“çµæœã®æœ‰æ„æ€§ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«çµ±è¨ˆçš„æ¤œå®šã‚’è¡Œã£ãŸã€‚

## RQ1: Overall Performance Comparison RQ1ï¼š 
ç·åˆçš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

Table 2 shows the results of training MF-BPR with different negative sampling methods.
è¡¨2ã¯ã€ç•°ãªã‚‹ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ–¹æ³•ã§MF-BPRã‚’è¨“ç·´ã—ãŸçµæœã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
Additionally, the performance of ANS under different models is presented in Table 3.
ã•ã‚‰ã«ã€ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹ANSã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è¡¨3ã«ç¤ºã™ã€‚
Due to space limitations, we are unable to present the results of other models using various negative sampling strategies.
ç´™é¢ã®éƒ½åˆä¸Šã€æ§˜ã€…ãªãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã‚’ç”¨ã„ãŸä»–ã®ãƒ¢ãƒ‡ãƒ«ã®çµæœã‚’ç´¹ä»‹ã™ã‚‹ã“ã¨ã¯ã§ããªã„ã€‚
Nonetheless, it is worth noting that the experimental results obtained were similar with Table 2.
ãã‚Œã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€å¾—ã‚‰ã‚ŒãŸå®Ÿé¨“çµæœãŒè¡¨2ã¨åŒæ§˜ã§ã‚ã£ãŸã“ã¨ã¯æ³¨ç›®ã«å€¤ã™ã‚‹ã€‚
We can make the following key observations:
ç§ãŸã¡ã¯æ¬¡ã®ã‚ˆã†ãªé‡è¦ãªè¦‹è§£ã‚’ç¤ºã™ã“ã¨ãŒã§ãã‚‹ï¼š

â€¢ ANS yields the best performance on almost all datasets.
- ANSã¯ã€ã»ã¨ã‚“ã©ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æœ€é«˜ã®æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã€‚
In particular, its highest improvements over the strongest baselines are 29.74%, 23.76%, and 31.06% in terms of ğ»ğ‘–ğ‘¡ ğ‘…ğ‘ğ‘¡ğ‘–ğ‘œ@15, ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@15, and ğ‘ğ·ğ¶ğº@15 in Beauty, respectively.
ç‰¹ã«ã€æœ€å¼·ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«å¯¾ã™ã‚‹æœ€é«˜ã®æ”¹å–„ã¯ã€ğ»ğ‘–ğ‘¡ áµáµ… áµ@15, áµáµ…@15, áµáµ@15, áµáµ@15 ã§ã€ãã‚Œãã‚Œ29.74%ã€23.76%ã€31.06%ã§ã‚ã‚‹ã€‚
This demonstrates that ANS is capable of generating more informative negative samples.
ã“ã‚Œã¯ã€ANSãŒã‚ˆã‚Šæœ‰ç›Šãªé™°æ€§ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
â€¢ The ANSâ€™s remarkable adaptability is a noteworthy feature that allows for its seamless integration into various models.
- ANSã®å“è¶Šã—ãŸé©å¿œæ€§ã¯ã€ã•ã¾ã–ã¾ãªãƒ¢ãƒ‡ãƒ«ã¸ã®ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ãªçµ±åˆã‚’å¯èƒ½ã«ã™ã‚‹ç‰¹ç­†ã™ã¹ãç‰¹å¾´ã§ã‚ã‚‹ã€‚
The results presented in Table 3 demonstrate that the incorporation of PAN into the base models leads to improvements across all datasets.
è¡¨3ã«ç¤ºã™çµæœã¯ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«PANã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æ”¹å–„ãŒè¦‹ã‚‰ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
â€¢ The model-aware methods always outperform the model-agnostic methods (RNS).
- ãƒ¢ãƒ‡ãƒ«ã‚’è€ƒæ…®ã—ãŸæ–¹æ³•ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’ç„¡è¦–ã—ãŸæ–¹æ³•ï¼ˆRNSï¼‰ã‚ˆã‚Šã‚‚å¸¸ã«å„ªã‚Œã¦ã„ã‚‹ã€‚
In general, model-agnostic methods are difficult to guarantee the quality of negative samples.
ä¸€èˆ¬çš„ã«ã€ãƒ¢ãƒ‡ãƒ«è¨ºæ–­çš„æ‰‹æ³•ã¯é™°æ€§ã‚µãƒ³ãƒ—ãƒ«ã®è³ªã‚’ä¿è¨¼ã™ã‚‹ã“ã¨ãŒé›£ã—ã„ã€‚
Leveraging various information from the underlying model is indeed a promising research direction.
åŸºç¤ã¨ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ§˜ã€…ãªæƒ…å ±ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã¯ã€å®Ÿã«æœ‰æœ›ãªç ”ç©¶ã®æ–¹å‘æ€§ã§ã‚ã‚‹ã€‚
â€¢ Despite its simplicity, DNS is a strong baseline.
- ãã®ã‚·ãƒ³ãƒ—ãƒ«ã•ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€DNSã¯å¼·åŠ›ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã§ã‚ã‚‹ã€‚
This fact justifies our motivation of studying the hardness from a more fine granularity.
ã“ã®äº‹å®Ÿã¯ã€ã‚ˆã‚Šç´°ã‹ã„ç²’åº¦ã‹ã‚‰ç¡¬ã•ã‚’ç ”ç©¶ã™ã‚‹ã¨ã„ã†æˆ‘ã€…ã®å‹•æ©Ÿã‚’æ­£å½“åŒ–ã™ã‚‹ã€‚

## RQ2: Disentanglement Performance RQ2ï¼š 
åˆ‡æ–­ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

To verify the disentanglement performance, we spot-check a user and use the T-SNE algorithm [30] to map the disentangled factors into a two-dimensional space.
é€†æ¥ã®æ€§èƒ½ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ã‚¹ãƒãƒƒãƒˆãƒã‚§ãƒƒã‚¯ã—ã€T-SNEã‚¢ãƒ«ã‚´ ãƒªã‚ºãƒ [30]ã‚’ä½¿ç”¨ã—ã¦ã€é€†æ¥ã•ã‚ŒãŸå› å­ã‚’2æ¬¡å…ƒç©ºé–“ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ã€‚
The results are shown in Figure 4(a).
çµæœã‚’å›³4(a)ã«ç¤ºã™ã€‚
We can observe that e â„ğ‘ğ‘Ÿğ‘‘ ğ‘› and e â€² ğ‘ are clustered together, confirming that they are indeed similar.
e â„ğ‘ğ‘‘ã¨e â€²ğ‘ãŒä¸€ç·’ã«ã‚¯ãƒ©ã‚¹ã‚¿åŒ–ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒè¦³å¯Ÿã•ã‚Œã€ä¸¡è€…ãŒç¢ºã‹ã«é¡ä¼¼ã—ã¦ã„ã‚‹ã“ã¨ãŒç¢ºèªã§ãã‚‹ã€‚
In contrast, e ğ‘’ğ‘ğ‘ ğ‘¦ ğ‘› and e â€²â€² ğ‘ are more scattered, indicating that they do not carry similar information, which is consistent with our previous analysis.
å¯¾ç…§çš„ã«ã€e ğ‘’ ğ‘›ã¨e â€² ğ‘ ğ‘ ã¯ã‚ˆã‚Šæ•£ã‚‰ã°ã£ã¦ãŠã‚Šã€åŒæ§˜ã®æƒ…å ±ã‚’æŒã£ã¦ã„ãªã„ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
The hard factors should contain most of the useful information of negative items, and thus if we use only hard factors, instead of the entire original item, to train a model, we should achieve similar performance.
ãƒãƒ¼ãƒ‰ãƒ»ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã«ã¯ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã®æœ‰ç”¨ãªæƒ…å ±ãŒã»ã¨ã‚“ã©å«ã¾ã‚Œã¦ã„ã‚‹ã¯ãšãªã®ã§ã€å…ƒã®é …ç›®å…¨ä½“ã§ã¯ãªãã€ãƒãƒ¼ãƒ‰ãƒ»ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã ã‘ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦ã‚‚ã€åŒã˜ã‚ˆã†ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå¾—ã‚‰ã‚Œã‚‹ã¯ãšã§ã‚ã‚‹ã€‚
We validate it by two experiments.
æˆ‘ã€…ã¯2ã¤ã®å®Ÿé¨“ã«ã‚ˆã£ã¦ãã‚Œã‚’æ¤œè¨¼ã™ã‚‹ã€‚
First, we plot the curves of ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@20 in Figure 4(b).
ã¾ãšã€å›³4(b)ã«ğ‘…áµ…@20 ã®æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ã€‚
HNS is a variant of RNS, which uses our disentanglement step to extract the hard factors of items and then uses only hard factors to train the model.
HNSã¯RNSã®å¤‰å½¢ã§ã‚ã‚Šã€æˆ‘ã€…ãŒé–‹ç™ºã—ãŸé›¢æ•£åŒ–ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä½¿ç”¨ã—ã¦ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’æŠ½å‡ºã—ã€æ¬¡ã«ãƒãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹ã€‚
It can be observed that the performance of HNS is comparable to that of RNS.
HNSã®æ€§èƒ½ã¯RNSã¨åŒç­‰ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
The two curves are similar, confirming that the hard factors indeed capture the most useful information of negative items.
ã“ã®2ã¤ã®æ›²ç·šã¯é¡ä¼¼ã—ã¦ãŠã‚Šã€ãƒãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãŒç¢ºã‹ã«ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã®æœ€ã‚‚æœ‰ç”¨ãªæƒ…å ±ã‚’æ‰ãˆã¦ã„ã‚‹ã“ã¨ã‚’è£ä»˜ã‘ã¦ã„ã‚‹ã€‚
Second, we revisit Figure 2 in Section 3.3.It can be seen that PER(RNS, HNS) is small, which means that HNS captures most of information learned in RNS.
PER(RNSã€HNS)ã¯å°ã•ãã€ã“ã‚Œã¯HNSãŒRNSã§å­¦ç¿’ã—ãŸæƒ…å ±ã®ã»ã¨ã‚“ã©ã‚’æ‰ãˆã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚
In summary, our disentanglement step can effectively disentangle a negative sample into hard and easy factors, which lays a solid foundation for the subsequent steps in ANS.
ã¾ã¨ã‚ã‚‹ã¨ã€æˆ‘ã€…ã®é€†æ¥ã‚¹ãƒ†ãƒƒãƒ—ã¯ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã¨ã‚¤ãƒ¼ã‚¸ãƒ¼ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã«åŠ¹æœçš„ã«é€†æ¥ã§ãã‚‹ã€‚

## RQ3: Ambiguous Trap and Information Discrimination RQ3ï¼š 
æ›–æ˜§ãªç½ ã¨æƒ…å ±å¼åˆ¥

### Ambiguous Trap. æ›–æ˜§ãªç½ ã€‚

Demonstrating how ANS can mitigate the ambiguous trap is a challenging task.
ANSãŒæ›–æ˜§ãªç½ ã‚’ã©ã®ã‚ˆã†ã«ç·©å’Œã§ãã‚‹ã‹ã‚’å®Ÿè¨¼ã™ã‚‹ã“ã¨ã¯ã€æŒ‘æˆ¦çš„ãªèª²é¡Œã§ã‚ã‚‹ã€‚
A first attempt is to show the augmentation gain of the augmented negative samples.
æœ€åˆã®è©¦ã¿ã¯ã€ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚ŒãŸãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã®ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚²ã‚¤ãƒ³ã‚’ç¤ºã™ã“ã¨ã§ã‚ã‚‹ã€‚
However, this idea is flawed because larger augmentation gain cannot always guarantee better performance (e.g., larger augmentation gain can be achieved by introducing a large number of false positive items).
ã—ã‹ã—ã€ã“ã®è€ƒãˆæ–¹ã«ã¯æ¬ é™¥ãŒã‚ã‚‹ã€‚ã¨ã„ã†ã®ã‚‚ã€ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆ©å¾—ãŒå¤§ãã‘ã‚Œã°å¤§ãã„ã»ã©ã€å¸¸ã«æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã¨ã¯é™ã‚‰ãªã„ã‹ã‚‰ã ï¼ˆãŸã¨ãˆã°ã€ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆ©å¾—ã‚’å¤§ããã™ã‚‹ã¨ã€å¤šæ•°ã®å½é™½æ€§é …ç›®ã‚’å°å…¥ã™ã‚‹ã“ã¨ã«ãªã‚‹ï¼‰ã€‚
Therefore, we choose to analyze the curves of ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@20 to show how ANS can mitigate the ambiguous trap, which is shown in Figure 5.
ãã“ã§ã€å›³5ã«ç¤ºã™ã‚ˆã†ã«ã€ANSãŒã©ã®ã‚ˆã†ã«æ›–æ˜§ãªãƒˆãƒ©ãƒƒãƒ—ã‚’è»½æ¸›ã§ãã‚‹ã‹ã‚’ç¤ºã™ãŸã‚ã«ã€ğ‘…á‘’á‘™@20ã®æ›²ç·šã‚’è§£æã™ã‚‹ã“ã¨ã«ã—ãŸã€‚
Generally, the steeper the curve is, the more information the model can learn in this epoch from negative samples.
ä¸€èˆ¬çš„ã«ã€æ›²ç·šãŒæ€¥ã§ã‚ã‚Œã°ã‚ã‚‹ã»ã©ã€ãƒ¢ãƒ‡ãƒ«ã¯ã“ã®ã‚¨ãƒãƒƒã‚¯ã§è² ã®ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰ã‚ˆã‚Šå¤šãã®æƒ…å ±ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
We can observe that DNS outperforms RNS because its ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@20 is always higher than that of RNS and the curve is steeper than that of RNS.
DNS ã® ğ‘…á‘’@20 ã¯å¸¸ã« RNS ã‚ˆã‚Šã‚‚é«˜ãã€RNS ã® á‘™á‘™@20 ã‚ˆã‚Šã‚‚æ›²ç·šãŒæ€¥ã§ã‚ã‚‹ãŸã‚ã€DNS ãŒ RNS ã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
It again confirms that hard negative sampling is an effective sampling strategy.
ãƒãƒ¼ãƒ‰ãƒ»ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒåŠ¹æœçš„ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã§ã‚ã‚‹ã“ã¨ãŒæ”¹ã‚ã¦ç¢ºèªã•ã‚ŒãŸã€‚
In contrast, the curve of ANS exhibits distinct patterns.
å¯¾ç…§çš„ã«ã€ANSã®æ›²ç·šã¯æ˜ç¢ºãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¤ºã™ã€‚
At the beginning (from epoch 0 to epoch 30), ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@20 is low because ANS needs extra efforts to learn how to disentangle and augment negative samples.
åˆæœŸï¼ˆã‚¨ãƒãƒƒã‚¯0ã‹ã‚‰ã‚¨ãƒãƒƒã‚¯30ã¾ã§ï¼‰ã«ã¯ã€ ğ‘…áµ…@20 ãŒä½ã„ã€‚ã“ã‚Œã¯ã€ANSãŒè² ã®ã‚µãƒ³ãƒ—ãƒ«ã®åˆ†é›¢ã¨è£œå¼·ã®æ–¹æ³•ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ä½™åˆ†ãªåŠªåŠ›ãŒå¿…è¦ã ã‹ã‚‰ã§ã‚ã‚‹ã€‚
As the training process progresses (from epoch 30 to epoch 95), ANS demonstrates a greater average gradient.
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé€²ã‚€ã«ã¤ã‚Œã¦ï¼ˆã‚¨ãƒãƒƒã‚¯30ã‹ã‚‰ã‚¨ãƒãƒƒã‚¯95ã¾ã§ï¼‰ã€ANSã¯ã‚ˆã‚Šå¤§ããªå¹³å‡å‹¾é…ã‚’ç¤ºã™ã€‚
This proves that ANS can generate harder synthetic negative samples, which can largely mitigate the ambiguous trap issue.
ã“ã‚Œã¯ã€ANSãŒã‚ˆã‚Šå›°é›£ãªåˆæˆãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã§ãã‚‹ã“ã¨ã‚’è¨¼æ˜ã—ã¦ãŠã‚Šã€æ›–æ˜§ãªãƒˆãƒ©ãƒƒãƒ—ã®å•é¡Œã‚’å¤§ããè»½æ¸›ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

### Information Discrimination. æƒ…å ±å·®åˆ¥ã€‚

As for the information discrimination problem, we have shown that the disentanglement step can effectively extract the useful information from low-score items.
æƒ…å ±è­˜åˆ¥å•é¡Œã«é–¢ã—ã¦ã¯ã€åˆ†é›¢ã‚¹ãƒ†ãƒƒãƒ—ãŒä½å¾—ç‚¹é …ç›®ã‹ã‚‰æœ‰ç”¨ãªæƒ…å ±ã‚’åŠ¹æœçš„ã«æŠ½å‡ºã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚
However, we have not shown that ANS can select more low-score negative items in the training process.
ã—ã‹ã—ã€ANSãŒè¨“ç·´éç¨‹ã«ãŠã„ã¦ã€ã‚ˆã‚Šä½å¾—ç‚¹ã®ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã‚’é¸æŠã§ãã‚‹ã“ã¨ã¯ç¤ºã—ã¦ã„ãªã„ã€‚
To this end, we analyze the percentages of overlapping negative samples between ANS and DNS.
ãã®ãŸã‚ã€ANSã¨DNSã®é™°æ€§ã‚µãƒ³ãƒ—ãƒ«ã®é‡è¤‡ç‡ã‚’åˆ†æã—ãŸã€‚
The results are presented in Table 4.
çµæœã‚’è¡¨4ã«ç¤ºã™ã€‚
Recall that DNS always chooses the negative items with the highest scores as the hard negative samples.
DNSã¯å¸¸ã«æœ€ã‚‚ã‚¹ã‚³ã‚¢ã®é«˜ã„ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã‚’ãƒãƒ¼ãƒ‰ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦é¸æŠã™ã‚‹ã“ã¨ã‚’æ€ã„å‡ºã—ã¦ãã ã•ã„ã€‚
A less than 50% overlapping indicates that ANS does sample more low-score negative items before the augmentation and effectively alleviates the information discrimination problem.
ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ãŒ50ï¼…ä»¥ä¸‹ã§ã‚ã‚‹ã“ã¨ã¯ã€ANSãŒã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®å‰ã«ä½ã‚¹ã‚³ã‚¢ã®ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã‚’ã‚ˆã‚Šå¤šãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€æƒ…å ±è­˜åˆ¥ã®å•é¡Œã‚’åŠ¹æœçš„ã«è»½æ¸›ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

## RQ4: Ablation Study RQ4ï¼š 
ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶

We analyze the effectiveness of different components in our model, and evaluate the performance of the following variants of our model: (1) ANS without disentanglement (ANS w/o dis).(2) ANS without augmentation gain (ANS w/o gain).(3) ANS without regulated direction (ANS w/o dir).(4) ANS without regulated magnitude (ANS w/o mag).
æˆ‘ã€…ã¯ã€æˆ‘ã€…ã®ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹æ§˜ã€…ãªæ§‹æˆè¦ç´ ã®æœ‰åŠ¹æ€§ã‚’åˆ†æã—ã€æˆ‘ã€…ã®ãƒ¢ãƒ‡ãƒ«ã®ä»¥ä¸‹ã®å¤‰ç¨®ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ï¼š 
(1) ANS without disentanglement (ANS w/o dis) (2) ANS without augmentation gain (ANS w/o gain) (3) ANS without regulated direction (ANS w/o dir) (4) ANS without regulated magnitude (ANS w/o mag).
It is noteworthy to state that the complete elimination of the augmentation step is not taken into consideration due to its equivalence to DNS.
DNSã¨åŒç­‰ã§ã‚ã‚‹ãŸã‚ã€è£œå¼·ã‚¹ãƒ†ãƒƒãƒ—ã®å®Œå…¨ãªæ’é™¤ã¯è€ƒæ…®ã•ã‚Œã¦ã„ãªã„ã“ã¨ã¯æ³¨ç›®ã«å€¤ã™ã‚‹ã€‚
The results are presented in Table 5.
çµæœã‚’è¡¨5ã«ç¤ºã™ã€‚
We can observe that all components we propose can positively contribute to model performance.
æˆ‘ã€…ã¯ã€æˆ‘ã€…ãŒææ¡ˆã™ã‚‹ã™ã¹ã¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«ãƒ—ãƒ©ã‚¹ã«å¯„ä¸ã™ã‚‹ã“ã¨ã‚’è¦³å¯Ÿã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
In particular, the results show that unconstrained augmentation (e.g., ANS w/o mag) cannot achieve meaningful performance.
ç‰¹ã«ã€åˆ¶ç´„ã®ãªã„ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆä¾‹ãˆã°ã€ãƒã‚°ã‚’ä½¿ã‚ãªã„ANSï¼‰ã§ã¯ã€æ„å‘³ã®ã‚ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã§ããªã„ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚
This fact confirms that the existing unconstrained augmentation techniques cannot be directly applied to CF.
ã“ã®äº‹å®Ÿã¯ã€æ—¢å­˜ã®åˆ¶ç´„ãªã—ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æŠ€è¡“ãŒCFã«ç›´æ¥é©ç”¨ã§ããªã„ã“ã¨ã‚’è£ä»˜ã‘ã¦ã„ã‚‹ã€‚

## RQ5: Hyper-Parameter Sensitivity RQ5ï¼š 
ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®æ„Ÿåº¦

### Impact of ğ›¾. ã®å½±éŸ¿ã€‚

We present the effect of the weight of the contrastive loss and disentanglement loss, ğ›¾, in Figure 6.
å›³6ã«ã€ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆæå¤±ã¨ãƒ‡ã‚£ã‚»ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆæå¤±ã®é‡ã¿â†ªL_1FEâ†©ã®åŠ¹æœã‚’ç¤ºã™ã€‚
As the value of ğ›¾ increases, we can first observe notable performance improvements, which proves that both loss functions are beneficial for the CF model.
ã“ã®ã“ã¨ã¯ã€ã©ã¡ã‚‰ã®æå¤±é–¢æ•°ã‚‚CFãƒ¢ãƒ‡ãƒ«ã«ã¨ã£ã¦æœ‰ç›Šã§ã‚ã‚‹ã“ã¨ã‚’è¨¼æ˜ã—ã¦ã„ã‚‹ã€‚
It is interesting to observe that once ğ›¾ becomes larger than a threshold, the performance drops sharply.
Ç¾ãŒé–¾å€¤ã‚ˆã‚Šå¤§ãããªã‚‹ã¨ã€æ€§èƒ½ãŒæ€¥æ¿€ã«ä½ä¸‹ã™ã‚‹ã“ã¨ã¯èˆˆå‘³æ·±ã„ã€‚
This observation is expected because in this case the CF model considers the disentanglement as the primary task and ignores the recommendation task.
ã“ã®å ´åˆã€CFãƒ¢ãƒ‡ãƒ«ã¯ãƒ‡ã‚£ã‚»ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆã‚’ä¸»è¦ãªã‚¿ã‚¹ã‚¯ã¨ã¿ãªã—ã€æ¨è–¦ã‚¿ã‚¹ã‚¯ã‚’ç„¡è¦–ã™ã‚‹ãŸã‚ã€ã“ã®ã‚ˆã†ãªçµæœãŒå‡ºã‚‹ã“ã¨ãŒäºˆæƒ³ã•ã‚Œã‚‹ã€‚
Nevertheless, ANS can achieve reasonable performance under a relatively wide range of ğ›¾ values.
ã¨ã¯ã„ãˆã€ANSã¯æ¯”è¼ƒçš„åºƒã„ç¯„å›²ã®â†ªLl_1FE å€¤ã®ä¸‹ã§å¦¥å½“ãªæ€§èƒ½ã‚’é”æˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

### Impact of ğœ–. ğœ– ã®å½±éŸ¿ã€‚

Recall that ğœ– is the parameter to balance the importance between the score and the augmentation gain in the sampling step.
â†ªLl_1D716 ã¯ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—ã«ãŠã‘ã‚‹ã‚¹ã‚³ã‚¢ã¨å¢—å¤§ã‚²ã‚¤ãƒ³ã®é‡è¦åº¦ã®ãƒãƒ©ãƒ³ã‚¹ã‚’ã¨ã‚‹ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚‹ã€‚
Figure 7(a) presents the results of different ğœ– values.
å›³7(a)ã¯ã€â†ªLl_1D716 ã®å€¤ã‚’å¤‰ãˆãŸå ´åˆã®çµæœã§ã‚ã‚‹ã€‚
A small ğœ– value overlooks the importance of the augmentation gain and only achieves sub-optimal performance; a large ğœ– value may favor an item with a lower score (but a larger score difference), which will reduce the gradient and hurt model performance.
ğœ–å€¤ã‚’å°ã•ãã™ã‚‹ã¨ã€è£œå¼·åˆ©å¾—ã®é‡è¦æ€§ãŒè¦‹è½ã¨ã•ã‚Œã€æœ€é©ä»¥ä¸‹ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã—ã‹å¾—ã‚‰ã‚Œã¾ã›ã‚“ã€‚ğœ–å€¤ã‚’å¤§ããã™ã‚‹ã¨ã€ã‚¹ã‚³ã‚¢ãŒä½ã„ï¼ˆã—ã‹ã—ã‚¹ã‚³ã‚¢å·®ãŒå¤§ãã„ï¼‰é …ç›®ãŒæœ‰åˆ©ã«ãªã‚Šã€å‹¾é…ãŒæ¸›å°‘ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒä½ä¸‹ã—ã¾ã™ã€‚
But still, ANS can obtain good performance under a relatively wide range of ğœ– values.
ã—ã‹ã—ãã‚Œã§ã‚‚ã€ANSã¯æ¯”è¼ƒçš„åºƒã„ç¯„å›²ã®ğœ–å€¤ã®ä¸‹ã§è‰¯å¥½ãªæ€§èƒ½ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

### Impact of ğ‘€. ğ‘€ ã®å½±éŸ¿ã€‚

ğ‘€ denotes the size of candidate negative set E.
ğ‘€ ã¯å¦å®šå€™è£œé›†åˆEã®ã‚µã‚¤ã‚ºã‚’è¡¨ã™ã€‚
The present study illustrates the impact of ğ‘€ on performance, as depicted in Figure 8.
æœ¬ç ”ç©¶ã§ã¯ã€å›³8ã«ç¤ºã™ã‚ˆã†ã«ã€â†ªLu_1D440 ãŒãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
It is evident that an increase in ğ‘€ leads to more negative samples, we can get harder negative samples by augmentation, thereby resulting in a performance improvement.
â†ªLu_1D440 ã‚’å¢—åŠ ã•ã›ã‚‹ã¨ã€ã‚ˆã‚Šå¤šãã®è² ã‚µãƒ³ãƒ—ãƒ«ãŒå¾—ã‚‰ã‚Œã‚‹ã“ã¨ã¯æ˜ã‚‰ã‹ã§ã‚ã‚Šã€ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã£ã¦ã‚ˆã‚Šé›£ã—ã„è² ã‚µãƒ³ãƒ—ãƒ«ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
Nevertheless, it is noteworthy that excessively large values of ğ‘€ can considerably impede the efficiency of the models, and degrade performance due to noise (e.g., false negative samples).
ã¨ã¯ã„ãˆã€ â†ªLu_1D440 ã®å€¤ãŒå¤§ãã™ãã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡æ€§ãŒè‘—ã—ãæãªã‚ã‚Œã€ãƒã‚¤ã‚ºï¼ˆå½é™°æ€§ã‚µãƒ³ãƒ—ãƒ«ãªã©ï¼‰ã«ã‚ˆã‚Šæ€§èƒ½ãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã¯æ³¨ç›®ã«å€¤ã™ã‚‹ã€‚

## RQ6: Efficiency Analysis RQ6ï¼š 
åŠ¹ç‡æ€§åˆ†æ

Following the previous work [22, 31], we also examine the efficiency of different negative sampling methods in Table 6.
å…ˆè¡Œç ”ç©¶[22, 31]ã«å¾“ã„ã€è¡¨6ã§ã¯ç•°ãªã‚‹ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ³•ã®åŠ¹ç‡ã‚‚æ¤œè¨¼ã—ã¦ã„ã‚‹ã€‚
We report the average running time (in seconds) per epoch and the total number of epochs needed before reaching convergence.
ã‚¨ãƒãƒƒã‚¯ã‚ãŸã‚Šã®å¹³å‡å®Ÿè¡Œæ™‚é–“ï¼ˆç§’ï¼‰ã¨ã€åæŸã«é”ã™ã‚‹ã¾ã§ã«å¿…è¦ãªã‚¨ãƒãƒƒã‚¯æ•°ã®åˆè¨ˆã‚’å ±å‘Šã™ã‚‹ã€‚
All three methods are relatively efficient.
ã“ã®3ã¤ã®æ–¹æ³•ã¯ã„ãšã‚Œã‚‚æ¯”è¼ƒçš„åŠ¹ç‡çš„ã ã€‚
There is no wonder that RNS is the most efficient method as it is a model-agnostic strategy.
ãƒ¢ãƒ‡ãƒ«ã«ã¨ã‚‰ã‚ã‚Œãªã„æˆ¦ç•¥ã§ã‚ã‚‹RNSãŒæœ€ã‚‚åŠ¹ç‡çš„ãªæ–¹æ³•ã§ã‚ã‚‹ã“ã¨ã¯ä¸æ€è­°ã§ã¯ãªã„ã€‚
Compared to DNS, our proposed ANS method requires more running time.
DNSã¨æ¯”è¼ƒã™ã‚‹ã¨ã€æˆ‘ã€…ã®ææ¡ˆã™ã‚‹ANSæ³•ã¯ã‚ˆã‚Šå¤šãã®å®Ÿè¡Œæ™‚é–“ã‚’å¿…è¦ã¨ã™ã‚‹ã€‚
However, considering the huge performance improvement ANS brings, the additional running time is well justified.
ã—ã‹ã—ã€ANSãŒã‚‚ãŸã‚‰ã™ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å¤§å¹…ãªå‘ä¸Šã‚’è€ƒãˆã‚Œã°ã€è¿½åŠ ã•ã‚Œã‚‹å®Ÿè¡Œæ™‚é–“ã¯ååˆ†ã«æ­£å½“åŒ–ã§ãã‚‹ã€‚

# Conclusion çµè«–

Motivated by ambiguous trap and information discrimination, from which the state-of-the-art negative sampling methods suffer, for the first time, we proposed to introduce synthetic negative samples from a fine-granular perspective to improve implicit CF.
æœ€æ–°ã®ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ³•ãŒè‹¦ã—ã‚“ã§ã„ã‚‹æ›–æ˜§ãªãƒˆãƒ©ãƒƒãƒ—ã¨æƒ…å ±è­˜åˆ¥ã«å‹•æ©Ÿã¥ã‘ã‚‰ã‚Œã€æˆ‘ã€…ã¯åˆã‚ã¦ã€æš—é»™çš„CFã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«ã€ç´°ã‹ã„ç²’åº¦ã®è¦³ç‚¹ã‹ã‚‰åˆæˆãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã‚’å°å…¥ã™ã‚‹ã“ã¨ã‚’ææ¡ˆã—ãŸã€‚
We put forward a novel generic augmented negative sampling (ANS) paradigm, along with a concrete implementation.
æˆ‘ã€…ã¯ã€æ–°ã—ã„ä¸€èˆ¬çš„ãªæ‹¡å¼µãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆANSï¼‰ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¨ã€ãã®å…·ä½“çš„ãªå®Ÿè£…ã‚’ææ¡ˆã™ã‚‹ã€‚
The paradigm consists of three major steps.
ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¯å¤§ãã3ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰ãªã‚‹ã€‚
The disentanglement step disentangles negative items into hard and easy factors in the absence of supervision signals.
åˆ†é›¢ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€ç›£ç£ä¿¡å·ãŒãªã„å ´åˆã€ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã‚’ãƒãƒ¼ãƒ‰è¦å› ã¨ã‚¤ãƒ¼ã‚¸ãƒ¼è¦å› ã«åˆ†é›¢ã™ã‚‹ã€‚
The augmentation step generates synthetic negative items using carefully calibrated noise.
å¢—å¼·ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€æ…é‡ã«èª¿æ•´ã•ã‚ŒãŸãƒã‚¤ã‚ºã‚’ä½¿ç”¨ã—ã¦åˆæˆãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã‚’ç”Ÿæˆã™ã‚‹ã€‚
The sampling step makes use of a new metric called augmentation gain to effectively alleviate information discrimination.
ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ»ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€æƒ…å ±ã®è­˜åˆ¥ã‚’åŠ¹æœçš„ã«ç·©å’Œã™ã‚‹ãŸã‚ã«ã€ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ã‚²ã‚¤ãƒ³ã¨å‘¼ã°ã‚Œã‚‹æ–°ã—ã„ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
Comprehensive experiments demonstrate that ANS can significantly improve model performance and represents an exciting new research direction.
åŒ…æ‹¬çš„ãªå®Ÿé¨“ã«ã‚ˆã‚Šã€ANSãŒãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å¤§å¹…ã«æ”¹å–„ã§ãã‚‹ã“ã¨ãŒå®Ÿè¨¼ã•ã‚Œã€ã‚¨ã‚­ã‚µã‚¤ãƒ†ã‚£ãƒ³ã‚°ãªæ–°ã—ã„ç ”ç©¶ã®æ–¹å‘æ€§ãŒç¤ºã•ã‚ŒãŸã€‚
In our future work, we intend to explore the efficacy of augmented negative samples in tackling various issues such as fairness and popularity bias.
ä»Šå¾Œã®ç ”ç©¶ã§ã¯ã€å…¬å¹³æ€§ã‚„äººæ°—ãƒã‚¤ã‚¢ã‚¹ã¨ã„ã£ãŸæ§˜ã€…ãªå•é¡Œã«å–ã‚Šçµ„ã‚€ä¸Šã§ã€å¢—å¼·ã•ã‚ŒãŸãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã®æœ‰åŠ¹æ€§ã‚’æ¢ã‚‹ã¤ã‚‚ã‚Šã§ã‚ã‚‹ã€‚
Additionally, we will actively investigate the effectiveness of employing augmented negative sampling in online experiments.
ã•ã‚‰ã«ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å®Ÿé¨“ã«ãŠã‘ã‚‹å¢—å¼·ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æœ‰åŠ¹æ€§ã‚’ç©æ¥µçš„ã«èª¿æŸ»ã™ã‚‹ã€‚