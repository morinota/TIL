## link ãƒªãƒ³ã‚¯

- https://arxiv.org/pdf/2202.06317.pdf https://arxiv.org/pdf/2202.06317.pdf

## title ã‚¿ã‚¤ãƒˆãƒ«

Off-Policy Evaluation for Large Action Spaces via Embeddings
åŸ‹ã‚è¾¼ã¿ã«ã‚ˆã‚‹å¤§è¦æ¨¡è¡Œå‹•ç©ºé–“ã®ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è©•ä¾¡

## Abstruct ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆ

Off-policy evaluation (OPE) in contextual bandits has seen rapid adoption in real-world systems, since it enables offline evaluation of new policies using only historic log data.
ã‚³ãƒ³ãƒ†ã‚¯ã‚¹ãƒãƒ¥ã‚¢ãƒ«ãƒ»ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆ(i.e. æ±ºå®šè«–çš„ã§ã¯ãªãç¢ºç‡çš„ãªæ¨è–¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ )ã«ãŠã‘ã‚‹ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è©•ä¾¡ï¼ˆOPEï¼‰ã¯ã€éå»ã®ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã¦æ–°ã—ã„ãƒãƒªã‚·ãƒ¼ã‚’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§è©•ä¾¡ã§ãã‚‹ãŸã‚ã€å®Ÿä¸–ç•Œã®ã‚·ã‚¹ãƒ†ãƒ ã§æ€¥é€Ÿã«æ¡ç”¨ãŒé€²ã‚“ã§ã„ã‚‹ã€‚
Unfortunately, when the number of actions is large, existing OPE estimators â€“ most of which are based on inverse propensity score weighting â€“ degrade severely and can suffer from extreme bias and variance.
**æ®‹å¿µãªã“ã¨ã«ã€è¡Œå‹•æ•°ãŒå¤šã„å ´åˆã€æ—¢å­˜ã®OPEæ¨å®šé‡-ãã®ã»ã¨ã‚“ã©ã¯é€†å‚¾å‘ã‚¹ã‚³ã‚¢é‡ã¿ä»˜ã‘ã«åŸºã¥ã-ã¯è‘—ã—ãåŠ£åŒ–**ã—ã€æ¥µç«¯ãªãƒã‚¤ã‚¢ã‚¹ã¨åˆ†æ•£ã«æ‚©ã¾ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚
This foils the use of OPE in many applications from recommender systems to language models.
ã“ã®ãŸã‚ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰è¨€èªãƒ¢ãƒ‡ãƒ«ã¾ã§ã€**å¤šãã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§OPEã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ããªã„**.
To overcome this issue, we propose a new OPE estimator that leverages marginalized importance weights when action embeddings provide structure in the action space.
ã“ã®å•é¡Œã‚’å…‹æœã™ã‚‹ãŸã‚ã«ã€æˆ‘ã€…ã¯ã€**action embeddings**ãŒã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“ã®æ§‹é€ ã‚’æä¾›ã™ã‚‹å ´åˆã«ã€**å‘¨è¾ºåŒ–ã•ã‚ŒãŸé‡è¦åº¦é‡ã¿**ã‚’æ´»ç”¨ã™ã‚‹æ–°ã—ã„OPEæ¨å®šå™¨ã‚’ææ¡ˆã™ã‚‹ã€‚
We characterize the bias, variance, and mean squared error of the proposed estimator and analyze the conditions under which the action embedding provides statistical benefits over conventional estimators.
ææ¡ˆã™ã‚‹æ¨å®šé‡ã®ãƒã‚¤ã‚¢ã‚¹ã€åˆ†æ•£ã€å¹³å‡äºŒä¹—èª¤å·®ã‚’ç‰¹å¾´ä»˜ã‘ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿ãŒå¾“æ¥ã®æ¨å®šé‡ã«å¯¾ã—ã¦çµ±è¨ˆçš„ãªåˆ©ç‚¹ã‚’ã‚‚ãŸã‚‰ã™æ¡ä»¶ã‚’åˆ†æã™ã‚‹ã€‚
In addition to the theoretical analysis, we find that the empirical performance improvement can be substantial, enabling reliable OPE even when existing estimators collapse due to a large number of actions.
ç†è«–çš„ãªåˆ†æã«åŠ ãˆã€çµŒé¨“çš„ãªæ€§èƒ½ã®å‘ä¸Šã¯ã‹ãªã‚Šã®ã‚‚ã®ã§ã‚ã‚Šã€å¤šæ•°ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«ã‚ˆã£ã¦æ—¢å­˜ã®æ¨å®šå™¨ãŒå´©å£Šã—ãŸå ´åˆã§ã‚‚ã€ä¿¡é ¼æ€§ã®é«˜ã„OPEã‚’å¯èƒ½ã«ã™ã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸã€‚

# Introduction ã¯ã˜ã‚ã«

Many intelligent systems (e.g., recommender systems, voice assistants, search engines) interact with the environment through a contextual bandit process where a policy observes a context, takes an action, and obtains a reward.
**å¤šãã®çŸ¥çš„ã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã€éŸ³å£°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ãªã©ï¼‰ã¯ã€ãƒãƒªã‚·ãƒ¼ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦³å¯Ÿã—ã€è¡Œå‹•ã‚’èµ·ã“ã—ã€å ±é…¬ã‚’å¾—ã‚‹ã¨ã„ã†ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ»ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆéç¨‹ã‚’é€šã˜ã¦ç’°å¢ƒã¨ç›¸äº’ä½œç”¨ã™ã‚‹ã€‚**
Logs of these interactions provide valuable data for off-policy evaluation (OPE), which aims to accurately evaluate the performance of new policies without ever deploying them in the field.
ã“ã®ã‚ˆã†ãªã‚„ã‚Šã¨ã‚Šã®ãƒ­ã‚°ã¯ã€ã‚ªãƒ•ãƒ»ãƒãƒªã‚·ãƒ¼è©•ä¾¡ï¼ˆOPEï¼‰ã®ãŸã‚ã®è²´é‡ãªãƒ‡ãƒ¼ã‚¿ã¨ãªã‚‹ã€‚OPEã¯ã€æ–°ã—ã„ãƒãƒªã‚·ãƒ¼ã‚’ç¾å ´ã«å°å…¥ã™ã‚‹ã“ã¨ãªãã€ãã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ­£ç¢ºã«è©•ä¾¡ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã‚‹ã€‚
OPE is of great practical relevance, as it helps avoid costly online A/B tests and can also act as subroutines for batch policy learning (DudÂ´Ä±k et al., 2014; Su et al., 2020a).
OPEã¯ã€ã‚³ã‚¹ãƒˆã®ã‹ã‹ã‚‹ã‚ªãƒ³ãƒ©ã‚¤ãƒ³A/Bãƒ†ã‚¹ãƒˆã‚’å›é¿ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã€ãƒãƒƒãƒãƒãƒªã‚·ãƒ¼å­¦ç¿’ã®ã‚µãƒ–ãƒ«ãƒ¼ãƒãƒ³ã¨ã—ã¦ã‚‚æ©Ÿèƒ½ã™ã‚‹ãŸã‚ã€å®Ÿç”¨çš„ãªæ„ç¾©ãŒå¤§ãã„ï¼ˆDudÂ´Ä±k et al, 2014; Su et al, 2020aï¼‰ã€‚
However, OPE is challenging, since the logs contain only partial-information feedback â€“ specifically the reward of the chosen action, but not the counterfactual rewards of all the other actions a different policy might choose.
ã—ã‹ã—ã€OPEã¯å›°é›£ã§ã‚ã‚‹ã€‚ãªãœãªã‚‰ã€ãƒ­ã‚°ã«ã¯éƒ¨åˆ†çš„ãªæƒ…å ±ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã—ã‹å«ã¾ã‚Œã¦ã„ãªã„ã‹ã‚‰ã§ã‚ã‚‹ã€‚å…·ä½“çš„ã«ã¯ã€é¸æŠã•ã‚ŒãŸè¡Œå‹•ã®å ±é…¬ã§ã‚ã‚‹ãŒã€åˆ¥ã®æ”¿ç­–ãŒé¸æŠã™ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ä»–ã®ã™ã¹ã¦ã®è¡Œå‹•ã®åäº‹å®Ÿçš„å ±é…¬ã§ã¯ãªã„ã€‚

When the action space is small, recent advances in the design of OPE estimators have led to a number of reliable methods with good theoretical guarantees (DudÂ´Ä±k et al., 2014; Swaminathan & Joachims, 2015a; Wang et al., 2017; Farajtabar et al., 2018; Su et al., 2019; 2020a; Metelli et al., 2021).
è¡Œå‹•ç©ºé–“ãŒå°ã•ã„å ´åˆã€OPEæ¨å®šé‡ã®è¨­è¨ˆã«ãŠã‘ã‚‹æœ€è¿‘ã®é€²æ­©ã«ã‚ˆã‚Šã€ç†è«–çš„ã«ä¿è¨¼ã•ã‚ŒãŸä¿¡é ¼æ€§ã®é«˜ã„æ‰‹æ³•ãŒæ•°å¤šãç™»å ´ã—ã¦ã„ã‚‹ï¼ˆDudÂ´Ä±k et al., 2014; Swaminathan & Joachims, 2015a; Wang et al., 2017; Farajtabar et al., 2018; Su et al., 2019; 2020a; Metelli et al., 2021ï¼‰ã€‚
Unfortunately, these estimators can degrade severely when the number of available actions is large.
æ®‹å¿µãªãŒã‚‰ã€**åˆ©ç”¨å¯èƒ½ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æ•°ãŒå¤šã„å ´åˆã€ã“ã‚Œã‚‰ã®æ¨å®šé‡ã¯è‘—ã—ãä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹**ã€‚
Large action spaces are prevalent in many potential applications of OPE, such as recommender systems where policies have to handle thousands or millions of items (e.g., movies, songs, products).
å¤§ããªè¡Œå‹•ç©ºé–“ã¯ã€ä½•åƒã€ä½•ç™¾ä¸‡ã‚‚ã®ã‚¢ã‚¤ãƒ†ãƒ ï¼ˆæ˜ ç”»ã€æ›²ã€å•†å“ãªã©ï¼‰ã‚’ãƒãƒªã‚·ãƒ¼ã¨ã—ã¦æ‰±ã‚ãªã‘ã‚Œã°ãªã‚‰ãªã„ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ãªã©ã€OPEã®å¤šãã®æ½œåœ¨çš„ãªå¿œç”¨ä¾‹ã§åºƒã¾ã£ã¦ã„ã‚‹ã€‚
In such a situation, the existing estimators based on inverse propensity score (IPS) weighting (Horvitz & Thompson, 1952) can incur high bias and variance, and as a result, be impractical for OPE.
ã“ã®ã‚ˆã†ãªçŠ¶æ³ã§ã¯ã€é€†å‚¾å‘ã‚¹ã‚³ã‚¢(IPS)é‡ã¿ä»˜ã‘(Horvitz & Thompson, 1952)ã«åŸºã¥ãæ—¢å­˜ã®æ¨å®šé‡ã§ã¯ã€ãƒã‚¤ã‚¢ã‚¹ã¨åˆ†æ•£ãŒå¤§ãããªã‚Šã€çµæœã¨ã—ã¦OPEã«ã¯å®Ÿç”¨çš„ã§ã¯ãªã„ã€‚
First, a large action space makes it challenging for the logging policy to have common support with the target policies, and IPS is biased under support deficiency (Sachdeva et al., 2020).
ç¬¬ä¸€ã«ã€**è¡Œå‹•ç©ºé–“ãŒå¤§ãã„ã¨ã€ãƒ­ã‚®ãƒ³ã‚°ãƒ»ãƒãƒªã‚·ãƒ¼ãŒã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ»ãƒãƒªã‚·ãƒ¼ã¨å…±é€šã®æ”¯æŒã‚’æŒã¤ã“ã¨ãŒé›£ã—ããªã‚Š**ã€IPSã¯æ”¯æŒä¸è¶³ã®ä¸‹ã§ãƒã‚¤ã‚¢ã‚¹ãŒã‹ã‹ã‚‹ï¼ˆSachdeva et al, 2020ï¼‰ã€‚
Second, a large number of actions typically leads to high variance of IPS due to large importance weights.
ç¬¬äºŒã«ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æ•°ãŒå¤šã„ã¨ã€ä¸€èˆ¬çš„ã«é‡è¦åº¦ã®é‡ã¿ãŒå¤§ãããªã‚‹ãŸã‚ã€IPSã®åˆ†æ•£ãŒå¤§ãããªã‚‹ã€‚
To illustrate, we find in our experiments that the variance and mean squared error of IPS inflate by a factor of over 300 when the number of actions increases from 10 to 5000 given a fixed sample size.
ä¾‹ã‚’æŒ™ã’ã‚‹ã¨ã€ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå›ºå®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°ãŒ10ã‹ã‚‰5000ã«å¢—åŠ ã™ã‚‹ã¨ã€IPSã®åˆ†æ•£ã¨å¹³å‡äºŒä¹—èª¤å·®ãŒ300å€ä»¥ä¸Šã«è†¨ã‚Œä¸ŠãŒã‚‹ã“ã¨ãŒå®Ÿé¨“ã§ã‚ã‹ã£ãŸã€‚
While doubly robust (DR) estimators can somewhat reduce the variance by introducing a reward estimator as a control variate (DudÂ´Ä±k et al., 2014), they do not address the fundamental issues that come with large action spaces.
äºŒé‡ãƒ­ãƒã‚¹ãƒˆï¼ˆDRï¼‰æ¨å®šé‡ã¯ã€å ±é…¬æ¨å®šé‡ã‚’åˆ¶å¾¡å¤‰é‡ã¨ã—ã¦å°å…¥ã™ã‚‹ã“ã¨ã§ã€åˆ†æ•£ã‚’ã„ãã‚‰ã‹æ¸›ã‚‰ã™ã“ã¨ãŒã§ãã‚‹ãŒï¼ˆDudÂ´Ä±k et al.

To overcome the limitations of the existing estimators when the action space is large, we leverage additional information about the actions in the form of action embeddings.
**ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“ãŒå¤§ãã„å ´åˆã®æ—¢å­˜ã®æ¨å®šå™¨ã®é™ç•Œã‚’å…‹æœã™ã‚‹ãŸã‚ã«ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿ã¨ã„ã†å½¢ã§ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«é–¢ã™ã‚‹è¿½åŠ æƒ…å ±ã‚’æ´»ç”¨ã™ã‚‹ã€‚**
There are many cases where we have access to such prior information.
ãã†ã—ãŸäº‹å‰æƒ…å ±ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚±ãƒ¼ã‚¹ã¯å¤šã„ã€‚
For example, movies are characterized by auxiliary information such as genres (e.g., adventure, science fiction, documentary), director, or actors.
ä¾‹ãˆã°ã€æ˜ ç”»ã¯ã‚¸ãƒ£ãƒ³ãƒ«ï¼ˆä¾‹ï¼šã‚¢ãƒ‰ãƒ™ãƒ³ãƒãƒ£ãƒ¼ã€SFã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ã‚¿ãƒªãƒ¼ï¼‰ã€ç›£ç£ã€ä¿³å„ªãªã©ã®è£œåŠ©æƒ…å ±ã«ã‚ˆã£ã¦ç‰¹å¾´ã¥ã‘ã‚‰ã‚Œã‚‹ã€‚
We should then be able to utilize these supplemental data to infer the value of actions under-explored by the logging policy, potentially achieving much more accurate policy evaluation than the existing estimators.
ãã—ã¦ã€ã“ã‚Œã‚‰ã®è£œè¶³ãƒ‡ãƒ¼ã‚¿ã‚’åˆ©ç”¨ã—ã¦ã€ä¼æ¡æ”¿ç­–ã«ã‚ˆã£ã¦ååˆ†ã«èª¿æŸ»ã•ã‚Œã¦ã„ãªã„è¡Œå‹•ã®ä¾¡å€¤ã‚’æ¨æ¸¬ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã¯ãšã§ã‚ã‚Šã€æ—¢å­˜ã®æ¨å®šå€¤ã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«æ­£ç¢ºãªæ”¿ç­–è©•ä¾¡ã‚’é”æˆã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
We first provide the conditions under which action embeddings provide another path for unbiased OPE, even with support deficient actions.
æˆ‘ã€…ã¯ã¾ãšã€**ã‚µãƒãƒ¼ãƒˆä¸è¶³ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã§ã‚ã£ã¦ã‚‚ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿ãŒä¸åOPEã®ãŸã‚ã®åˆ¥ã®é“ã‚’æä¾›ã™ã‚‹æ¡ä»¶**ã‚’æä¾›ã™ã‚‹ã€‚
We then propose the Marginalized IPS (MIPS) estimator, which uses the marginal distribution of action embeddings, rather than actual actions, to define a new type of importance weights.
ã“ã‚Œã¯ã€å®Ÿéš›ã®è¡Œå‹•ã§ã¯ãªãã€è¡Œå‹•ã®åŸ‹ã‚è¾¼ã¿ã®ãƒãƒ¼ã‚¸ãƒŠãƒ«åˆ†å¸ƒã‚’åˆ©ç”¨ã—ã¦ã€æ–°ã—ã„ã‚¿ã‚¤ãƒ—ã®é‡è¦åº¦é‡ã¿ã‚’å®šç¾©ã™ã‚‹ã€‚
We show that MIPS is unbiased under an alternative condition, which states that the action embeddings should mediate every causal effect of the action on the reward.
MIPSã¯ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®åŸ‹ã‚è¾¼ã¿ãŒå ±é…¬ã«å¯¾ã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å› æœåŠ¹æœã‚’ã™ã¹ã¦åª’ä»‹ã™ã‚‹ã€ã¨ã„ã†åˆ¥ã®æ¡ä»¶ä¸‹ã§ã¯ä¸åã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚
Moreover, we show that MIPS has a lower variance than IPS, especially when there is a large number of actions, and thus the vanilla importance weights have a high variance.
ã•ã‚‰ã«ã€MIPSã¯IPSã‚ˆã‚Šã‚‚åˆ†æ•£ãŒå°ã•ãã€ç‰¹ã«ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°ãŒå¤šã„å ´åˆã€ãƒãƒ‹ãƒ©é‡è¦åº¦é‡ã¿ãŒé«˜ã„åˆ†æ•£ã‚’æŒã¤ã“ã¨ã‚’ç¤ºã™ã€‚
We also characterize the gain in MSE provided by MIPS, which implies an interesting bias-variance trade-off with respect to the quality of the action embeddings.
ã¾ãŸã€MIPSã«ã‚ˆã£ã¦å¾—ã‚‰ã‚Œã‚‹MSEã®åˆ©å¾—ã®ç‰¹å¾´ã‚‚æ˜ã‚‰ã‹ã«ã™ã‚‹ã€‚ã“ã‚Œã¯ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã®è³ªã«é–¢ã—ã¦ã€èˆˆå‘³æ·±ã„ãƒã‚¤ã‚¢ã‚¹ã¨åˆ†æ•£ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æ„å‘³ã™ã‚‹ã€‚
Including many embedding dimensions captures the causal effect better, leading to a smaller bias of MIPS.
å¤šãã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã‚’å«ã‚ã‚‹ã“ã¨ã§ã€å› æœåŠ¹æœã‚’ã‚ˆã‚Šã‚ˆãæ‰ãˆã€MIPSã®ãƒã‚¤ã‚¢ã‚¹ã‚’ã‚ˆã‚Šå°ã•ãã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
In contrast, using only a subset of the embedding dimensions reduces the variance more.
å¯¾ç…§çš„ã«ã€åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã®ã‚µãƒ–ã‚»ãƒƒãƒˆã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€åˆ†æ•£ã¯ã‚ˆã‚Šå°ã•ããªã‚‹ã€‚
We thus propose a strategy to intentionally violate the assumption about the action embeddings by discarding less relevant embedding dimensions for achieving a better MSE at the cost of introducing some bias.
ãã“ã§æˆ‘ã€…ã¯ã€å¤šå°‘ã®ãƒã‚¤ã‚¢ã‚¹ã‚’å°å…¥ã™ã‚‹ä»£å„Ÿã¨ã—ã¦ã€ã‚ˆã‚Šè‰¯ã„MSEã‚’é”æˆã™ã‚‹ãŸã‚ã«ã€é–¢é€£æ€§ã®ä½ã„åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã‚’ç ´æ£„ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿ã«é–¢ã™ã‚‹ä»®å®šã‚’æ„å›³çš„ã«ç ´ã‚‹æˆ¦ç•¥ã‚’ææ¡ˆã™ã‚‹ã€‚
Comprehensive experiments on synthetic and real-world bandit data verify the theoretical findings, indicating that MIPS can provide an effective bias-variance trade-off in the presence of many actions.
åˆæˆãƒ‡ãƒ¼ã‚¿ãŠã‚ˆã³å®Ÿä¸–ç•Œã®ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸåŒ…æ‹¬çš„ãªå®Ÿé¨“ã«ã‚ˆã‚Šã€ç†è«–çš„ãªçŸ¥è¦‹ãŒæ¤œè¨¼ã•ã‚Œã€MIPSãŒå¤šãã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã™ã‚‹å ´åˆã«åŠ¹æœçš„ãªãƒã‚¤ã‚¢ã‚¹ã¨åˆ†æ•£ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æä¾›ã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚

# Off-Policy Evaluation

We follow the general contextual bandit setup, and an extensive discussion of related work is given in Appendix A.
æˆ‘ã€…ã¯ä¸€èˆ¬çš„ãªã‚³ãƒ³ãƒ†ã‚¯ã‚¹ãƒˆãƒ»ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®è¨­å®šã«å¾“ã†ã€‚é–¢é€£ã™ã‚‹ç ”ç©¶ã®åºƒç¯„ãªè­°è«–ã¯ä»˜éŒ²Aã«ç¤ºã™ã€‚
Let x âˆˆ X âŠ† R dx be a dx-dimensional context vector drawn i.i.d.
xâˆˆXâŠ†Rã®dxã‚’i.i.d.æç”»ã•ã‚ŒãŸdxæ¬¡å…ƒã®æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã¨ã™ã‚‹ã€‚
from an unknown distribution p(x).
æœªçŸ¥ã®åˆ†å¸ƒp(x)ã‹ã‚‰
Given context x, a possibly stochastic policy Ï€(a|x) chooses action a from a finite action space denoted as A.
ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆxãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€ç¢ºç‡çš„ãªæ”¿ç­–Ï€(a|x)ã¯ã€Aã¨ã—ã¦ç¤ºã•ã‚Œã‚‹æœ‰é™ã®è¡Œå‹•ç©ºé–“ã‹ã‚‰è¡Œå‹•aã‚’é¸æŠã™ã‚‹ã€‚
The reward r âˆˆ [0, rmax] is then sampled from an unknown conditional distribution p(r|x, a).
å ±é…¬râˆˆ[0, rmax]ã¯ã€æœªçŸ¥ã®æ¡ä»¶ä»˜ãåˆ†å¸ƒp(r|x, a)ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ã€‚
We measure the effectiveness of a policy Ï€ through its value
æˆ‘ã€…ã¯ã€æ”¿ç­–Ï€ã®æœ‰åŠ¹æ€§ã‚’ãã®å€¤ã«ã‚ˆã£ã¦æ¸¬å®šã™ã‚‹ã€‚

$$
\tag{1}
$$

where q(x, a) := E[r|x, a] denotes the expected reward given context x and action a.
x, a] denotes the expected reward given context x and action a.

In OPE, we are given logged bandit data collected by a logging policy Ï€0.
OPEã§ã¯ã€ãƒ­ã‚®ãƒ³ã‚°ãƒãƒªã‚·ãƒ¼Ï€0ã«ã‚ˆã£ã¦åé›†ã•ã‚ŒãŸãƒ­ã‚®ãƒ³ã‚°ã•ã‚ŒãŸãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ãŒä¸ãˆã‚‰ã‚Œã‚‹ã€‚
Specifically, let D := {(xi , ai , ri)} n i=1 be a sample of logged bandit data containing n independent observations drawn from the logging policy as (x, a, r) âˆ¼ p(x)Ï€0(a|x)p(r|x, a).
x)p(r|x, a).
We aim to develop an estimator VË† for the value of a target policy Ï€ (which is different from Ï€0) using only the logged data in D.
Dã«è¨˜éŒ²ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ç”¨ã„ã¦ã€ï¼ˆÏ€0ã¨ã¯ç•°ãªã‚‹ï¼‰ç›®æ¨™ãƒãƒªã‚·ãƒ¼ã®å€¤Ï€ã®æ¨å®šé‡VË†ã‚’é–‹ç™ºã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚
The accuracy of VË† is quantified by its mean squared error (MSE)
VË†ã®ç²¾åº¦ã¯ã€å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMSEï¼‰ã«ã‚ˆã£ã¦å®šé‡åŒ–ã•ã‚Œã‚‹ã€‚

$$
\tag{}
$$

where ED[Â·] takes the expectation over the logged data and
ã“ã“ã§ã€ED[-]ã¯è¨˜éŒ²ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æœŸå¾…å€¤ã‚’ã¨ã‚Š

$$
\tag{}
$$

In the following theoretical analysis, we focus on the IPS estimator, since most advanced OPE estimators are based on IPS weighting (DudÂ´Ä±k et al., 2014; Wang et al., 2017; Su et al., 2019; 2020a; Metelli et al., 2021).
ä»¥ä¸‹ã®ç†è«–çš„åˆ†æã§ã¯ã€å…ˆé€²çš„ãªOPEæ¨å®šå™¨ã®ã»ã¨ã‚“ã©ãŒIPSé‡ã¿ä»˜ã‘ã«åŸºã¥ã„ã¦ã„ã‚‹ãŸã‚ã€IPSæ¨å®šå™¨ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ï¼ˆDudÂ´Ä±k et al., 2014; Wang et al., 2017; Su et al., 2019; 2020a; Metelli et al., 2021ï¼‰ã€‚
IPS estimates the value of Ï€ by re-weighting the observed rewards as follow
IPSã¯ã€è¦³æ¸¬ã•ã‚ŒãŸå ±é…¬ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«å†é‡ã¿ä»˜ã‘ã™ã‚‹ã“ã¨ã§Ï€ã®å€¤ã‚’æ¨å®šã™ã‚‹ã€‚

$$
\tag{}
$$

where w(x, a) := Ï€(a|x)/Ï€0(a|x) is called the (vanilla) importance weight.
x)/Ï€0(a|x) is called the (vanilla) importance weight.
This estimator is unbiased (i.e., ED[VË† IPS(Ï€; D)] = V (Ï€)) under the following common support assumption.
ã“ã®æ¨å®šé‡ã¯ã€ä»¥ä¸‹ã®å…±é€šæ”¯æŒã®ä»®å®šã®ä¸‹ã§ã¯ä¸åã§ã‚ã‚‹ï¼ˆã™ãªã‚ã¡ã€ED[VË† IPS(Ï€; D)] = V (Ï€)ï¼‰ã€‚

Assumption 2.1.(Common Support) The logging policy Ï€0 is said to have common support for policy Ï€ if Ï€(a|x) > 0 â†’ Ï€0(a|x) > 0 for all a âˆˆ A and x âˆˆ X .
x) > 0 â†’ Ï€0(a|x) > 0 for all a âˆˆ A and x âˆˆ X .

The unbiasedness of IPS is desirable, making this simple re-weighting technique so popular.
IPSã®ä¸åæ€§ã¯æœ›ã¾ã—ã„ã‚‚ã®ã§ã‚ã‚Šã€ã“ã®å˜ç´”ãªå†é‡ã¿ä»˜ã‘æŠ€è¡“ã‚’éå¸¸ã«äººæ°—ã®ã‚ã‚‹ã‚‚ã®ã«ã—ã¦ã„ã‚‹ã€‚
However, IPS can still be highly biased, particularly when the action space is large.
ã—ã‹ã—ã€IPSã¯ã€ç‰¹ã«ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“ãŒå¤§ãã„å ´åˆã€éå¸¸ã«åã£ãŸã‚‚ã®ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
Sachdeva et al.(2020) indicate that IPS has the following bias when Assumption 2.1 is not true.
Sachdevaã‚‰(2020)ã¯ã€ä»®å®š2.1ãŒçœŸã§ãªã„å ´åˆã€IPSã¯ä»¥ä¸‹ã®ã‚ˆã†ãªãƒã‚¤ã‚¢ã‚¹ã‚’æŒã¤ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

$$
\tag{}
$$

where U0(x, Ï€0) := {a âˆˆ A | Ï€0(a|x) = 0} is the set of unsupported or deficient actions for context x under Ï€0.
Ï€0(a|x) = 0} is the set of unsupported or deficient actions for context x under Ï€0.
Note that U0(x, Ï€0) can be large especially when A is large.
U0(x,Ï€0)ã¯ã€ç‰¹ã«AãŒå¤§ãã„ã¨ãã«å¤§ãããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã€‚
This bias is due to the fact that the logged dataset D does not contain any information about the unsupported actions.
ã“ã®ãƒã‚¤ã‚¢ã‚¹ã¯ã€ãƒ­ã‚°ã«è¨˜éŒ²ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆDã«ã€ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«é–¢ã™ã‚‹æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ãªã„ã“ã¨ã«èµ·å› ã™ã‚‹ã€‚
Another critical issue of IPS is that its variance can be large, which is given as follows (DudÂ´Ä±k et al., 2014)
IPSã®ã‚‚ã†ä¸€ã¤ã®é‡å¤§ãªå•é¡Œã¯ã€ãã®åˆ†æ•£ãŒå¤§ãããªã‚Šã†ã‚‹ã“ã¨ã§ã€ãã‚Œã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ä¸ãˆã‚‰ã‚Œã‚‹ï¼ˆDudÂ´Ä±k et al, 2014ï¼‰ã€‚

$$
\tag{2}
$$

where Ïƒ 2 (x, a) := V[r|x, a].
x, a].
The variance consists of three terms.
åˆ†æ•£ã¯3ã¤ã®é …ã‹ã‚‰æˆã‚Šç«‹ã£ã¦ã„ã‚‹ã€‚
The first term reflects the randomness in the rewards.
ç¬¬1é …ã¯å ±é…¬ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’åæ˜ ã—ã¦ã„ã‚‹ã€‚
The second term represents the variance due to the randomness over the contexts.
ç¬¬2é …ã¯ã‚³ãƒ³ãƒ†ã‚¯ã‚¹ãƒˆã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ã«èµ·å› ã™ã‚‹åˆ†æ•£ã‚’è¡¨ã™ã€‚
The final term is the penalty arising from the use of IPS weighting, and it is proportional to the weights and the true expected reward.
æœ€å¾Œã®é …ã¯ã€IPSé‡ã¿ä»˜ã‘ã®ä½¿ç”¨ã‹ã‚‰ç”Ÿã˜ã‚‹ãƒšãƒŠãƒ«ãƒ†ã‚£ã§ã‚ã‚Šã€é‡ã¿ã¨çœŸã®æœŸå¾…å ±é…¬ã«æ¯”ä¾‹ã™ã‚‹ã€‚
The variance contributed by the first and third terms can be extremely large when the weights w(x, a) have a wide range, which occurs when Ï€ assigns large probabilities to actions that have low probability under Ï€0.
ç¬¬1é …ã¨ç¬¬3é …ãŒå¯„ä¸ã™ã‚‹åˆ†æ•£ã¯ã€é‡ã¿w(x, a)ã®ç¯„å›²ãŒåºƒã„å ´åˆã«éå¸¸ã«å¤§ãããªã‚‹ã€‚
The latter can be expected when the action space A is large and the logging policy Ï€0 aims to have universal support (i.e., Ï€0(a|x) > 0 for all a and x).
å¾Œè€…ã¯ã€è¡Œå‹•ç©ºé–“AãŒå¤§ããã€ãƒ­ã‚®ãƒ³ã‚°æ–¹é‡Ï€0ãŒæ™®éçš„ãªæ”¯æŒï¼ˆã™ãªã‚ã¡ã€ã™ã¹ã¦ã®aã¨xã«å¯¾ã—ã¦Ï€0(a|x)>0ï¼‰ã‚’æŒã¤ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã‚‹å ´åˆã«äºˆæƒ³ã•ã‚Œã‚‹ã€‚
Swaminathan et al.(2017) also point out that the variance of IPS grows linearly with w(x, a), which can be â„¦(|A|).
A|).

This variance issue can be lessened by incorporating a reward estimator qË†(x, a) â‰ˆ q(x, a) as a control variate, resulting in the DR estimator (DudÂ´Ä±k et al., 2014).
ã“ã®åˆ†æ•£ã®å•é¡Œã¯ï¼Œå ±é…¬æ¨å®šé‡qË†(x, a) â‰ˆ q(x, a)ã‚’åˆ¶å¾¡å¤‰é‡ã¨ã—ã¦çµ„ã¿è¾¼ã‚€ã“ã¨ã§è»½æ¸›ã§ãï¼ŒDRæ¨å®šé‡ã¨ãªã‚‹(DudÂ´Ä±k et al., 2014)ã€‚
DR often improves the MSE of IPS due to its variance reduction property.
DRã¯ã€ãã®åˆ†æ•£å‰Šæ¸›ç‰¹æ€§ã«ã‚ˆã‚Šã€IPSã®MSEã‚’æ”¹å–„ã™ã‚‹ã“ã¨ãŒå¤šã„ã€‚
However, DR still suffers when the number of actions is large, and it can experience substantial performance deterioration as we demonstrate in our experiments.
ã—ã‹ã—ã€DRã¯ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æ•°ãŒå¤šã„å ´åˆã€ä¾ç„¶ã¨ã—ã¦å•é¡Œã‚’æŠ±ãˆã¦ãŠã‚Šã€æˆ‘ã€…ã®å®Ÿé¨“ã§å®Ÿè¨¼ã—ãŸã‚ˆã†ã«ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå¤§å¹…ã«ä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

# The Marginalized IPS Estimator é™ç•ŒåŒ–IPSæ¨å®šé‡

The following proposes a new estimator that circumvents the challenges of IPS for large action spaces.
ä»¥ä¸‹ã§ã¯ã€å¤§ããªè¡Œå‹•ç©ºé–“ã«å¯¾ã™ã‚‹IPSã®èª²é¡Œã‚’å›é¿ã™ã‚‹æ–°ã—ã„æ¨å®šå™¨ã‚’ææ¡ˆã™ã‚‹ã€‚
Our approach is to bring additional structure into the estimation problem, providing a path forward despite the minimax optimality of IPS and DR.
æˆ‘ã€…ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€IPSã¨DRã®ãƒŸãƒ‹ãƒãƒƒã‚¯ã‚¹æœ€é©æ€§ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€æ¨å®šå•é¡Œã«ã•ã‚‰ãªã‚‹æ§‹é€ ã‚’æŒã¡è¾¼ã¿ã€å‰é€²ã™ã‚‹é“ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
In particular, IPS and DR achieve the minimax optimal MSE of at most O(n âˆ’1 (EÏ€0 [w(x, a) 2Ïƒ 2 (x, a) + w(x, a) 2 r 2 max])), which means that they are impossible to improve upon in the worst case beyond constant factors (Wang et al., 2017; Swaminathan et al., 2017), unless we bring in additional structure.
ç‰¹ã«ã€IPSã¨DRã¯ã€æœ€å¤§ã§ã‚‚O(n -1 (EÏ€0 [w(x, a) 2Ïƒ 2 (x, a) + w(x, a) 2 r 2 max])ã®æœ€å°æœ€é©MSEã‚’é”æˆã—ã€ã“ã‚Œã¯ã€è¿½åŠ ã®æ§‹é€ ã‚’æŒã¡è¾¼ã¾ãªã„é™ã‚Šã€æœ€æ‚ªã®å ´åˆã€å®šæ•°å› å­ã‚’è¶…ãˆã¦æ”¹å–„ã™ã‚‹ã“ã¨ãŒä¸å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹(Wang et al., 2017; Swaminathan et al., 2017)ã€‚
Our key idea for overcoming the limits of IPS and DR is to assume the existence of action embeddings as prior information.
IPSã¨DRã®é™ç•Œã‚’å…‹æœã™ã‚‹ãŸã‚ã®æˆ‘ã€…ã®é‡è¦ãªã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã€äº‹å‰æƒ…å ±ã¨ã—ã¦ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã®å­˜åœ¨ã‚’ä»®å®šã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
The intuition is that this can help the estimator transfer information between similar actions.
ç›´æ„Ÿçš„ã«ã¯ã€ã“ã‚Œã¯æ¨å®šè€…ãŒé¡ä¼¼ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é–“ã§æƒ…å ±ã‚’ä¼é”ã™ã‚‹ã®ã«å½¹ç«‹ã¤ã€‚
More formally, suppose we are given a de-dimensional action embedding e âˆˆ E âŠ† R de for each action a, where we merely assume that the embedding is drawn i.i.d.
ã‚ˆã‚Šæ­£å¼ã«ã¯ã€å„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³aã«ã¤ã„ã¦ã€å˜ã«åŸ‹ã‚è¾¼ã¿ãŒi.i.d.æç”»ã•ã‚Œã‚‹ã¨ä»®å®šã™ã‚‹ã€éæ¬¡å…ƒã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿eï¼ˆEâŠ†Rãƒ‡ï¼‰ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ã™ã‚‹ã€‚
from some unknown distribution p(e|x, a).
ã‚ã‚‹æœªçŸ¥ã®åˆ†å¸ƒp(e|x, a)ã‹ã‚‰ã€‚
The simplest example is to construct action embeddings using predefined category information (e.g., product category).
æœ€ã‚‚å˜ç´”ãªä¾‹ã¯ã€ã‚ã‚‰ã‹ã˜ã‚å®šç¾©ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªæƒ…å ±ï¼ˆä¾‹ãˆã°ã€å•†å“ã‚«ãƒ†ã‚´ãƒªï¼‰ã‚’ä½¿ã£ã¦ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
Then, the embedding distribution is independent of the context and it is deterministic given the action.
ãã®å ´åˆã€åŸ‹ã‚è¾¼ã¿åˆ†å¸ƒã¯æ–‡è„ˆã«ä¾å­˜ã›ãšã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒä¸ãˆã‚‰ã‚Œã‚Œã°æ±ºå®šè«–çš„ã§ã‚ã‚‹ã€‚
Our framework is also applicable to the most general case of continuous, stochastic, and context-dependent action embeddings.
æˆ‘ã€…ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€é€£ç¶šçš„ã€ç¢ºç‡çš„ã€æ–‡è„ˆä¾å­˜çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã¨ã„ã†æœ€ã‚‚ä¸€èˆ¬çš„ãªå ´åˆã«ã‚‚é©ç”¨å¯èƒ½ã§ã‚ã‚‹ã€‚
For example, product prices may be generated by a personalized pricing algorithm running behind the system.
ä¾‹ãˆã°ã€å•†å“ä¾¡æ ¼ã¯ã€ã‚·ã‚¹ãƒ†ãƒ ã®èƒŒå¾Œã§å®Ÿè¡Œã•ã‚Œã‚‹ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸä¾¡æ ¼è¨­å®šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚Œã‚‹ã€‚
In this case, the embedding is continuous, depends on the user context, and can be stochastic if there is some randomness in the pricing algorithm.
ã“ã®å ´åˆã€ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã¯é€£ç¶šçš„ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ä¾å­˜ã—ã€ãƒ—ãƒ©ã‚¤ã‚·ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ãƒ©ãƒ³ãƒ€ãƒ æ€§ãŒã‚ã‚Œã°ç¢ºç‡çš„ã§ã‚ã‚‹ã€‚
Using the action embeddings, we now refine the definition of the policy value as:
ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã‚’ç”¨ã„ã¦ã€æ”¿ç­–å€¤ã®å®šç¾©ã‚’æ¬¡ã®ã‚ˆã†ã«æ´—ç·´ã™ã‚‹ï¼š

$$


$$

Note here that q(x, a) = Ep(e|x,a) [q(x, a, e)] where q(x, a, e) := E[r|x, a, e], so the above refinement does not contradict the original definition given in Eq.(1).
x,a) [q(x, a, e)] where q(x, a, e) := E[r|x, a, e], so the above refinement does not contradict the original definition given in Eq.(1).
A logged bandit dataset now contains action embeddings for each observation in D = {(xi , ai , ei , ri)} n i=1, where each tuple is generated by the logging policy as (x, a, e, r) âˆ¼ p(x)Ï€0(a|x)p(e|x, a)p(r|x, a, e).
x)p(e
Our strategy is to leverage this additional information for achieving a more accurate OPE for large action spaces.
ç§ãŸã¡ã®æˆ¦ç•¥ã¯ã€ã“ã®è¿½åŠ æƒ…å ±ã‚’æ´»ç”¨ã—ã¦ã€å¤§ããªä½œç”¨ç©ºé–“ã«å¯¾ã—ã¦ã‚ˆã‚Šæ­£ç¢ºãªOPEã‚’é”æˆã™ã‚‹ã“ã¨ã§ã™ã€‚
To motivate our approach, we introduce two properties characterizing an action embedding.
æˆ‘ã€…ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å‹•æ©Ÿã¥ã‘ã‚‹ãŸã‚ã«ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã‚’ç‰¹å¾´ã¥ã‘ã‚‹2ã¤ã®æ€§è³ªã‚’ç´¹ä»‹ã™ã‚‹ã€‚

Assumption 3.1.(Common Embedding Support) The logging policy Ï€0 is said to have common embedding support for policy Ï€ if p(e|x, Ï€) > 0 â†’ p(e|x, Ï€0) > 0 for all e âˆˆ E and x âˆˆ X , where p(e|x, Ï€) := P aâˆˆA p(e|x, a)Ï€(a|x) is the marginal distribution over the action embedding space given context x and policy Ï€.
x, Ï€) > 0 â†’ p(e

Assumption 3.1 is analogous to Assumption 2.1, but requires only the common support with respect to the action embedding space, which can be substantially more compact than the action space itself.
ä»®å®š3.1ã¯ä»®å®š2.1ã«é¡ä¼¼ã—ã¦ã„ã‚‹ãŒã€ä½œç”¨åŸ‹ã‚è¾¼ã¿ç©ºé–“ã«é–¢ã™ã‚‹å…±é€šã‚µãƒãƒ¼ãƒˆã®ã¿ã‚’å¿…è¦ã¨ã—ã€ä½œç”¨ç©ºé–“ãã®ã‚‚ã®ã‚ˆã‚Šã‚‚å®Ÿè³ªçš„ã«ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã«ãªã‚Šã†ã‚‹ã€‚
Indeed, Assumption 3.1 is weaker than common support of IPS (Assumption 2.1).1 Next, we characterize the expressiveness of the embedding in the ideal case, but we will relax this assumption later.
å®Ÿéš›ã€ä»®å®š3.1ã¯IPSã®å…±é€šã‚µãƒãƒ¼ãƒˆï¼ˆä»®å®š2.1ï¼‰ã‚ˆã‚Šã‚‚å¼±ã„1ã€‚æ¬¡ã«ã€ç†æƒ³çš„ãªå ´åˆã®åŸ‹ã‚è¾¼ã¿ã®è¡¨ç¾åŠ›ã‚’ç‰¹å¾´ä»˜ã‘ã‚‹ãŒã€ã“ã®ä»®å®šã¯å¾Œã§ç·©å’Œã™ã‚‹ã€‚

Assumption 3.2.(No Direct Effect) Action a has no direct effect on the reward r, i.e., a âŠ¥ r | x, e.
x, e.
As illustrated in Figure 1, Assumption 3.2 requires that every possible effect of a on r be fully mediated by the observed embedding e.
å›³1ã«ç¤ºã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«ã€ä»®å®š3.2ã¯ã€rã«å¯¾ã™ã‚‹aã®ã™ã¹ã¦ã®å¯èƒ½ãªåŠ¹æœãŒã€è¦³æ¸¬ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿eã«ã‚ˆã£ã¦å®Œå…¨ã«åª’ä»‹ã•ã‚Œã‚‹ã“ã¨ã‚’è¦æ±‚ã—ã¦ã„ã‚‹ã€‚
For now, we rely on the validity of Assumption 3.2, as it is convenient for introducing the proposed estimator.
ä»Šã®ã¨ã“ã‚ï¼Œä»®å®š3.2ã¯ææ¡ˆã™ã‚‹æ¨å®šé‡ã‚’å°å…¥ã™ã‚‹ã®ã«ä¾¿åˆ©ãªã®ã§ï¼Œã“ã®ä»®å®š3.2ã®å¦¥å½“æ€§ã«ä¾æ‹ ã™ã‚‹ã€‚
However, we later show that it is often beneficial to strategically discard some embedding dimensions and violate the assumption to achieve a better MSE.
ã—ã‹ã—ã€ã‚ˆã‚Šè‰¯ã„MSEã‚’é”æˆã™ã‚‹ãŸã‚ã«ã¯ã€æˆ¦ç•¥çš„ã«ã„ãã¤ã‹ã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã‚’æ¨ã¦ã€ä»®å®šã«é•åã™ã‚‹ã“ã¨ãŒæœ‰ç›Šã§ã‚ã‚‹å ´åˆãŒå¤šã„ã“ã¨ã‚’å¾Œã§ç¤ºã™ã€‚
We start the derivation of our new estimator with the observation that Assumption 3.2 gives us another path to unbiased estimation of the policy value without Assumption 2.1.
ä»®å®š3.2ãŒã€ä»®å®š2.1ã«ã‚ˆã‚‰ãªã„æ”¿ç­–ä¾¡å€¤ã®ä¸åæ¨å®šã¸ã®åˆ¥ã®é“ã‚’ä¸ãˆã¦ãã‚Œã‚‹ã¨ã„ã†è¦³å¯Ÿã‹ã‚‰ã€æ–°ã—ã„æ¨å®šé‡ã®å°å‡ºã‚’å§‹ã‚ã‚‹ã€‚

Proposition 3.3.Under Assumption 3.2, we have
å‘½é¡Œ3.3.å‰æ3.2ã®ä¸‹ã§ã¯ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚‹ã€‚

$$
\tag{}
$$

See Appendix B.1 for the proof.
è¨¼æ˜ã¯ä»˜éŒ²B.1ã‚’å‚ç…§ã€‚
Proposition 3.3 provides another expression of the policy value without explicitly relying on the action variable a.
å‘½é¡Œ3.3ã¯ã€è¡Œå‹•å¤‰æ•°aã«æ˜ç¤ºçš„ã«ä¾å­˜ã™ã‚‹ã“ã¨ãªãã€æ”¿ç­–å€¤ã‚’è¡¨ã™åˆ¥ã®è¡¨ç¾ã‚’æä¾›ã™ã‚‹ã€‚
This new expression naturally leads to the following marginalized inverse propensity score (MIPS) estimator, which is our main proposal.
ã“ã®æ–°ã—ã„å¼ã¯ã€è‡ªç„¶ã«æ¬¡ã®ã‚ˆã†ãªMIPSï¼ˆmarginalized inverse propensity scoreï¼‰æ¨å®šé‡ã«ã¤ãªãŒã‚Šã¾ã™ã€‚

$$
\tag{}
$$

where w(x, e) := p(e|x, Ï€)/p(e|x, Ï€0) is the marginal importance weight defined with respect to the marginal distribution over the action embedding space.
x, Ï€)/p(e|x, Ï€0) is the marginal importance weight defined with respect to the marginal distribution over the action embedding space.
To obtain an intuition for the benefits of MIPS, we provide a toy example in Table 1 with X = {x1}, A = {a1, a2, a3}, and E = {e1, e2, e3} (a special case of our formulation with a discrete embedding space).
MIPSã®åˆ©ç‚¹ã‚’ç›´æ„Ÿçš„ã«ç†è§£ã™ã‚‹ãŸã‚ã«ã€Xï¼ï½›x1ï½ã€Aï¼ï½›a1ã€a2ã€a3ï½ã€Eï¼ï½›e1ã€e2ã€e3ï½ï¼ˆé›¢æ•£åŸ‹ã‚è¾¼ã¿ç©ºé–“ã‚’æŒã¤æˆ‘ã€…ã®å®šå¼åŒ–ã®ç‰¹åˆ¥ãªå ´åˆï¼‰ã®ãŠã‚‚ã¡ã‚ƒã®ä¾‹ã‚’è¡¨1ã«ç¤ºã™ã€‚
The left table describes the logging and target policies with respect to A and implies that Assumption 2.1 is violated (Ï€0(a1|x1) = 0.0).
å·¦ã®è¡¨ã¯ã€Aã«é–¢ã™ã‚‹ãƒ­ã‚®ãƒ³ã‚°ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ»ãƒãƒªã‚·ãƒ¼ã‚’è¨˜è¿°ã—ã€ä»®å®š2.1ãŒç ´ã‚‰ã‚Œã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ï¼ˆÏ€0(a1|x1) = 0.0ï¼‰ã€‚
The middle table describes the conditional distribution of the action embedding e given action a (e.g., probability of a movie a belonging to a genre e).
çœŸã‚“ä¸­ã®è¡¨ã¯ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³aãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿eã®æ¡ä»¶ä»˜ãåˆ†å¸ƒï¼ˆä¾‹ãˆã°ã€æ˜ ç”»aãŒã‚¸ãƒ£ãƒ³ãƒ«eã«å±ã™ã‚‹ç¢ºç‡ï¼‰ã‚’è¨˜è¿°ã—ã¦ã„ã‚‹ã€‚
The right table describes the marginal distributions over E, which are calculable from the other two tables.
å³ã®è¡¨ã¯ã€Eä¸Šã®å‘¨è¾ºåˆ†å¸ƒã‚’è¨˜è¿°ã—ãŸã‚‚ã®ã§ã€ä»–ã®2ã¤ã®è¡¨ã‹ã‚‰è¨ˆç®—ã§ãã‚‹ã€‚
By considering the marginal distribution, Assumption 3.1 is ensured in the right table, even if Assumption 2.1 is not true in the left table.
é™ç•Œåˆ†å¸ƒã‚’è€ƒæ…®ã™ã‚Œã°ã€å·¦ã®è¡¨ã§ä»®å®š2.1ãŒçœŸã§ãªãã¦ã‚‚ã€å³ã®è¡¨ã§ä»®å®š3.1ãŒä¿è¨¼ã•ã‚Œã‚‹ã€‚
Moreover, the maximum importance weight is smaller for the right table (maxeâˆˆE w(x1, e) < maxaâˆˆA w(x1, a)), which may contribute to a variance reduction of the resulting estimator.
ã•ã‚‰ã«ã€æœ€å¤§é‡è¦åº¦ã‚¦ã‚§ã‚¤ãƒˆã¯å³ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ–¹ãŒå°ã•ãï¼ˆmaxeâˆˆE w(x1, e) < maxaâˆˆA w(x1, a)ï¼‰ã€ã“ã‚Œã¯çµæœã¨ã—ã¦å¾—ã‚‰ã‚Œã‚‹æ¨å®šé‡ã®åˆ†æ•£ä½æ¸›ã«å¯„ä¸ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
Below, we formally analyze the key statistical properties of MIPS and compare them with those of IPS, including the realistic case where Assumption 3.2 is violated.
ä»¥ä¸‹ã§ã¯ã€MIPSã®ä¸»è¦ãªçµ±è¨ˆçš„æ€§è³ªã‚’æ­£å¼ã«åˆ†æã—ã€ä»®å®š3.2ã«é•åã™ã‚‹ç¾å®Ÿçš„ãªå ´åˆã‚’å«ã‚ã€IPSã®ãã‚Œã¨æ¯”è¼ƒã™ã‚‹ã€‚

## Theoretical Analysis ç†è«–çš„åˆ†æ

First, the following proposition shows that MIPS is unbiased under assumptions different from those of IPS.
ã¾ãšã€ä»¥ä¸‹ã®å‘½é¡Œã¯ã€MIPSãŒIPSã¨ã¯ç•°ãªã‚‹ä»®å®šã®ã‚‚ã¨ã§ã¯ä¸åã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

Proposition 3.4.Under Assumptions 3.1 and 3.2, MIPS is unbiased, i.e., ED[VË†MIPS(Ï€; D)] = V (Ï€) for any Ï€.
ã™ãªã‚ã¡ã€ED[VË†MIPS(Ï€; D)] = V (Ï€)ã§ã‚ã‚‹ã€‚
See Appendix B.2 for the proof.
è¨¼æ˜ã¯ä»˜éŒ²B.2ã‚’å‚ç…§ã€‚
Proposition 3.4 states that, even when Ï€0 fails to provide common support over A such that IPS is biased, MIPS can still be unbiased if Ï€0 provides common support over E (Assumption 3.1) and e fully captures the causal effect of a on r (Assumption 3.2).
å‘½é¡Œ3.4ã¯ã€IPSãŒåã‚‹ã‚ˆã†ãªAä¸Šã®å…±é€šæ”¯æŒã‚’Ï€0ãŒæä¾›ã§ããªã„å ´åˆã§ã‚‚ã€Ï€0ãŒEä¸Šã®å…±é€šæ”¯æŒã‚’æä¾›ã—ï¼ˆä»®å®š3.1ï¼‰ã€eãŒrä¸Šã®aã®å› æœåŠ¹æœã‚’å®Œå…¨ã«æ•ã‚‰ãˆã‚‹ãªã‚‰ã°ï¼ˆä»®å®š3.2ï¼‰ã€MIPSã¯ä¾ç„¶ã¨ã—ã¦ä¸åã§ã‚ã‚Šå¾—ã‚‹ã“ã¨ã‚’è¿°ã¹ã¦ã„ã‚‹ã€‚

Having multiple estimators that enable unbiased OPE under different assumptions is in itself desirable, as we can choose the appropriate estimator depending on the data generating process.
ç•°ãªã‚‹ä»®å®šã®ä¸‹ã§ä¸åOPEã‚’å¯èƒ½ã«ã™ã‚‹è¤‡æ•°ã®æ¨å®šé‡ã‚’æŒã¤ã“ã¨ã¯ã€ãã‚Œè‡ªä½“æœ›ã¾ã—ã„ã“ã¨ã§ã‚ã‚Šã€ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã«å¿œã˜ã¦é©åˆ‡ãªæ¨å®šé‡ã‚’é¸æŠã§ãã‚‹ã‹ã‚‰ã§ã‚ã‚‹ã€‚
However, it is also helpful to understand how violations of the assumptions influence the bias of the resulting estimator.
ã—ã‹ã—ï¼Œä»®å®šã®é•åãŒçµæœã®æ¨å®šé‡ã®ãƒã‚¤ã‚¢ã‚¹ã«ã©ã®ã‚ˆã†ã«å½±éŸ¿ã™ã‚‹ã‹ã‚’ç†è§£ã™ã‚‹ã“ã¨ã‚‚æœ‰ç›Šã§ã‚ã‚‹ã€‚
In particular, for MIPS, it is difficult to verify whether Assumption 3.2 is true in practice.
ç‰¹ã«MIPSã®å ´åˆã€ä»®å®š3.2ãŒå®Ÿéš›ã«æ­£ã—ã„ã‹ã©ã†ã‹ã‚’æ¤œè¨¼ã™ã‚‹ã®ã¯é›£ã—ã„ã€‚
The following theorem characterizes the bias of MIPS.
æ¬¡ã®å®šç†ã¯ã€MIPSã®ãƒã‚¤ã‚¢ã‚¹ã‚’ç‰¹å¾´ã¥ã‘ã‚‹ã‚‚ã®ã§ã‚ã‚‹ã€‚
Theorem 3.5.(Bias of MIPS) If Assumption 3.1 is true, but Assumption 3.2 is violated, MIPS has the following bias.
å®šç†3.5.(MIPSã®ãƒã‚¤ã‚¢ã‚¹) ä»®å®š3.1ãŒçœŸã§ã€ä»®å®š3.2ãŒç ´ã‚Œã¦ã„ã‚‹å ´åˆã€MIPSã¯æ¬¡ã®ã‚ˆã†ãªãƒã‚¤ã‚¢ã‚¹ã‚’æŒã¤ã€‚

$$
\tag{}
$$

where a, b âˆˆ A.
ã“ã“ã§aã€bã¯Aã§ã‚ã‚‹ã€‚
See Appendix B.3 for the proof.
è¨¼æ˜ã¯ä»˜éŒ²B.3ã‚’å‚ç…§ã®ã“ã¨ã€‚
Theorem 3.5 suggests that three factors contribute to the bias of MIPS when Assumption 3.2 is violated.
å®šç†3.5ã¯ã€ä»®å®š3.2ã«é•åã—ãŸå ´åˆã€3ã¤ã®è¦å› ãŒMIPSã®åã‚Šã«å¯„ä¸ã™ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚
The first factor is the predictivity of the action embeddings with respect to the actual actions.
ç¬¬ä¸€ã®è¦å› ã¯ã€å®Ÿéš›ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«å¯¾ã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã®äºˆæ¸¬æ€§ã§ã‚ã‚‹ã€‚
When action a is predictable given context x and embedding e, Ï€0(a|x, e) is close to zero or one (deterministic), meaning that Ï€0(a|x, e)Ï€0(b|x, e) is close to zero.
x, e) is close to zero or one (deterministic), meaning that Ï€0(a
This suggests that even if Assumption 3.2 is violated, action embeddings that identify the actions well still enable a nearly unbiased estimation of MIPS.
ã“ã®ã“ã¨ã¯ã€ä»®ã«ä»®å®š3.2ã«é•åã—ãŸã¨ã—ã¦ã‚‚ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã†ã¾ãè­˜åˆ¥ã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿ã«ã‚ˆã£ã¦ã€MIPSã‚’ã»ã¼ä¸åã«æ¨å®šã§ãã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚
The second factor is the amount of direct effect of the action on the reward, which is quantified by q(x, a, e) âˆ’ q(x, b, e).
2ã¤ç›®ã®è¦å› ã¯ã€å ±é…¬ã«å¯¾ã™ã‚‹è¡Œå‹•ã®ç›´æ¥çš„åŠ¹æœã®å¤§ãã•ã§ã€q(x, a, e) - q(x, b, e)ã§å®šé‡åŒ–ã•ã‚Œã‚‹ã€‚
When the direct effect of a on r is small, q(x, a, e) âˆ’ q(x, b, e) also becomes small and so is the bias of MIPS.
rã«å¯¾ã™ã‚‹aã®ç›´æ¥åŠ¹æœãŒå°ã•ã„å ´åˆã€q(x, a, e) - q(x, b, e)ã‚‚å°ã•ããªã‚Šã€MIPSã®ãƒã‚¤ã‚¢ã‚¹ã‚‚å°ã•ããªã‚‹ã€‚
In an ideal situation where Assumption 3.2 is satisfied, we have q(x, a, e) = q(x, b, e) = q(x, e), thus MIPS is unbiased, which is consistent with Proposition 3.4.Note that the first two factors suggest that, to reduce the bias, the action embeddings should be informative so that they are either predictive of the actions or mediate a large amount of the causal effect.
ä»®å®š3.2ãŒæº€ãŸã•ã‚Œã‚‹ç†æƒ³çš„ãªçŠ¶æ³ã§ã¯ã€q(x, a, e) = q(x, b, e) = q(x, e)ã¨ãªã‚Šã€MIPSã¯ä¸åã§ã‚ã‚‹ã€‚
The final factor is the similarity between logging and target policies quantified by w(x, a) âˆ’ w(x, b).
æœ€å¾Œã®è¦å› ã¯ã€w(x, a) - w(x, b)ã§å®šé‡åŒ–ã•ã‚Œã‚‹ãƒ­ã‚®ãƒ³ã‚°ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒªã‚·ãƒ¼ã®é¡ä¼¼æ€§ã§ã‚ã‚‹ã€‚
When Assumption 3.2 is satisfied, MIPS is unbiased for any target policy, however, Theorem 3.5 suggests that if the assumption is not true, MIPS produces a larger bias for target policies dissimilar from the logging policy.2
ä»®å®š3.2ãŒæº€ãŸã•ã‚Œã‚‹å ´åˆã€MIPSã¯ã©ã®ã‚ˆã†ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒªã‚·ãƒ¼ã«å¯¾ã—ã¦ã‚‚ä¸åã§ã‚ã‚‹ãŒã€å®šç†3.5ã¯ä»®å®šãŒçœŸã§ãªã„å ´åˆã€MIPSã¯ãƒ­ã‚®ãƒ³ã‚°ãƒãƒªã‚·ãƒ¼ã¨ç•°ãªã‚‹ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒªã‚·ãƒ¼ã«å¯¾ã—ã¦ã‚ˆã‚Šå¤§ããªãƒã‚¤ã‚¢ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹2ã€‚

Next, we analyze the variance of MIPS, which we show is never worse than that of IPS and can be substantially lower.
æ¬¡ã«ã€MIPSã®åˆ†æ•£ã‚’åˆ†æã™ã‚‹ã€‚MIPSã®åˆ†æ•£ã¯IPSã®åˆ†æ•£ã‚ˆã‚Šã‚‚æ±ºã—ã¦æ‚ªããªãã€å¤§å¹…ã«å°ã•ããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚
Theorem 3.6.(Variance Reduction of MIPS) Under Assumptions 2.1, 3.1, and 3.2, we have
å®šç†3.6(MIPSã®åˆ†æ•£å‰Šæ¸›) ä»®å®š2.1ã€3.1ã€3.2ã®ä¸‹ã§ã€æ¬¡ãŒæˆã‚Šç«‹ã¤ã€‚

$$
\tag{}
$$

which is non-negative.
ã“ã‚Œã¯éè² ã§ã‚ã‚‹ã€‚
Note that the variance reduction is also lower bounded by zero even when Assumption 3.2 is not true.
ä»®å®š3.2ãŒçœŸã§ãªã„å ´åˆã§ã‚‚ã€åˆ†æ•£ã®æ¸›å°‘ã¯ã‚¼ãƒ­ã§ä¸‹ç•Œã•ã‚Œã‚‹ã“ã¨ã«æ³¨æ„ã€‚
See Appendix B.4 for the proof.
è¨¼æ˜ã¯ä»˜éŒ²B.4ã‚’å‚ç…§ã®ã“ã¨ã€‚
There are two factors that affect the amount of variance reduction.
åˆ†æ•£ã®æ¸›å°‘é‡ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹è¦å› ã¯2ã¤ã‚ã‚‹ã€‚
The first factor is the second moment of the reward with respect to p(r|x, e).
æœ€åˆã®è¦å› ã¯ã€p(r|x, e)ã«å¯¾ã™ã‚‹å ±é…¬ã®äºŒæ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã§ã‚ã‚‹ã€‚
This term becomes large when, for example, the reward is noisy even after conditioning on the action embedding e.
ã“ã®é …ãŒå¤§ãããªã‚‹ã®ã¯ã€ä¾‹ãˆã°ã€è¡Œå‹•åŸ‹ã‚è¾¼ã¿eã‚’æ¡ä»¶ä»˜ã‘ã—ãŸå¾Œã§ã‚‚å ±é…¬ã«ãƒã‚¤ã‚ºãŒã‚ã‚‹å ´åˆã§ã‚ã‚‹ã€‚
The second factor is the variance of w(x, a) with respect to the conditional distribution Ï€0(a|x, e), which becomes large when (i) w(x, a) has a wide range or (ii) there remain large variations in a even after conditioning on action embedding e so that Ï€0(a|x, e) remains stochastic.
x, e), which becomes large when (i) w(x, a) has a wide range or (ii) there remain large variations in a even after conditioning on action embedding e so that Ï€0(a|x, e) remains stochastic.
Therefore, MIPS becomes increasingly favorable compared to IPS for larger action spaces where the variance of w(x, a) becomes larger.
ã—ãŸãŒã£ã¦ã€w(x, a)ã®åˆ†æ•£ãŒå¤§ãããªã‚‹ã‚ˆã†ãªå¤§ããªè¡Œå‹•ç©ºé–“ã§ã¯ã€MIPSã¯IPSã«æ¯”ã¹ã¦ã¾ã™ã¾ã™æœ‰åˆ©ã«ãªã‚‹ã€‚
Moreover, to obtain a large variance reduction, the action embedding should ideally not be unnecessarily predictive of the actions.
ã•ã‚‰ã«ã€å¤§ããªåˆ†æ•£å‰Šæ¸›ã‚’å¾—ã‚‹ãŸã‚ã«ã¯ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã¯ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¸å¿…è¦ã«äºˆæ¸¬ã—ãªã„ã“ã¨ãŒç†æƒ³çš„ã§ã‚ã‚‹ã€‚
Finally, the next theorem describes the gain in MSE we can obtain from MIPS when Assumption 3.2 is violated.
æœ€å¾Œã«ã€æ¬¡ã®å®šç†ã¯ã€ä»®å®š3.2ã«é•åã—ãŸå ´åˆã«MIPSã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹MSEã®åˆ©å¾—ã‚’è¨˜è¿°ã—ã¦ã„ã‚‹ã€‚

Theorem 3.7.(MSE Gain of MIPS) Under Assumptions 2.1 and 3.1, we have
å®šç†3.7.(MIPSã®MSEåˆ©å¾—) ä»®å®š2.1ã¨3.1ã®ä¸‹ã§ã€æ¬¡ãŒæˆã‚Šç«‹ã¤ã€‚

$$
\tag{}
$$

See Appendix B.5 for the proof.
è¨¼æ˜ã¯ä»˜éŒ²B.5ã‚’å‚ç…§ã®ã“ã¨ã€‚
Note that IPS can have some bias when Assumption 2.1 is not true, possibly producing a greater MSE gain for MIPS
ä»®å®š2.1ãŒçœŸã§ãªã„å ´åˆã€IPSã«åã‚ŠãŒç”Ÿã˜ã€MIPSã®MSEåˆ©å¾—ãŒå¤§ãããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

## Data-Driven Embedding Selection ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹åŸ‹ã‚è¾¼ã¿é¸æŠ

The analysis in the previous section implies a clear biasvariance trade-off with respect to the quality of the action embeddings.
å‰ç¯€ã®åˆ†æã¯ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã®è³ªã«é–¢ã—ã¦ã€ãƒã‚¤ã‚¢ã‚¹ã¨åˆ†æ•£ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒæ˜ç¢ºã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚
Specifically, Theorem 3.5 suggests that the action embeddings should be as informative as possible to reduce the bias when Assumption 3.2 is violated.
å…·ä½“çš„ã«ã¯ã€å®šç†3.5ã¯ã€ä»®å®š3.2ã«é•åã—ãŸå ´åˆã®ãƒã‚¤ã‚¢ã‚¹ã‚’æ¸›ã‚‰ã™ãŸã‚ã«ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã¯å¯èƒ½ãªé™ã‚Šæƒ…å ±çš„ã§ã‚ã‚‹ã¹ãã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚
On the other hand, Theorem 3.6 suggests that the action embeddings should be as coarse as possible to gain a greater variance reduction.
ä¸€æ–¹ã€å®šç†3.6ã¯ã€ã‚ˆã‚Šå¤§ããªåˆ†æ•£å‰Šæ¸›ã‚’å¾—ã‚‹ãŸã‚ã«ã¯ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã‚’ã§ãã‚‹ã ã‘ç²—ãã™ã¹ãã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚
Theorem 3.7 summarizes the bias-variance trade-off in terms of MSE.
å®šç†3.7ã¯ã€ãƒã‚¤ã‚¢ã‚¹ã¨åˆ†æ•£ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’MSEã®è¦³ç‚¹ã‹ã‚‰ã¾ã¨ã‚ãŸã‚‚ã®ã§ã‚ã‚‹ã€‚
A possible criticism to MIPS is Assumption 3.2, as it is hard to verify whether this assumption is satisfied using only the observed logged data.
MIPSã«å¯¾ã™ã‚‹æ‰¹åˆ¤ã¨ã—ã¦è€ƒãˆã‚‰ã‚Œã‚‹ã®ã¯ã€ä»®å®š3.2ã§ã‚ã‚‹ã€‚ãªãœãªã‚‰ã€è¦³æ¸¬ã•ã‚ŒãŸãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã ã‘ã§ã¯ã€ã“ã®ä»®å®šãŒæº€ãŸã•ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’æ¤œè¨¼ã™ã‚‹ã®ãŒé›£ã—ã„ã‹ã‚‰ã§ã‚ã‚‹ã€‚
However, the above discussion about the bias-variance trade-off implies that it might be effective to strategically violate Assumption 3.2 by discarding some embedding dimensions.
ã—ã‹ã—ã€ãƒã‚¤ã‚¢ã‚¹ã¨åˆ†æ•£ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã«é–¢ã™ã‚‹ä¸Šè¨˜ã®è­°è«–ã¯ã€ã„ãã¤ã‹ã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã‚’æ¨ã¦ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€æˆ¦ç•¥çš„ã«ä»®å®š3.2ã«é•åã™ã‚‹ã“ã¨ãŒåŠ¹æœçš„ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚
This action embedding selection can lead to a large variance reduction at the cost of introducing some bias, possibly improving the MSE of MIPS.
ã“ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ»ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ãƒ»ã‚»ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã¯ã€å¤šå°‘ã®ãƒã‚¤ã‚¢ã‚¹ã‚’å°å…¥ã™ã‚‹ä»£å„Ÿã¨ã—ã¦ã€å¤§ããªåˆ†æ•£å‰Šæ¸›ã‚’ã‚‚ãŸã‚‰ã—ã€MIPSã®MSEã‚’æ”¹å–„ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
To implement the action embedding selection, we can adapt the estimator selection method called SLOPE proposed in Su et al.(2020b) and Tucker & Lee (2021).
ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿é¸æŠã‚’å®Ÿè£…ã™ã‚‹ã«ã¯ã€Su et al.(2020b)ã‚„Tucker & Lee(2021)ã§ææ¡ˆã•ã‚Œã¦ã„ã‚‹SLOPEã¨å‘¼ã°ã‚Œã‚‹æ¨å®šé‡é¸æŠæ³•ã‚’é©å¿œã™ã‚Œã°ã‚ˆã„ã€‚
SLOPE is based on Lepskiâ€™s principle for bandwidth selection in nonparametric statistics (Lepski & Spokoiny, 1997) and is used to tune the hyperparameters of OPE estimators.
SLOPEã¯ã€ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯çµ±è¨ˆã«ãŠã‘ã‚‹å¸¯åŸŸå¹…é¸æŠã®ãŸã‚ã®Lepskiã®åŸç†(Lepski & Spokoiny, 1997)ã«åŸºã¥ã„ã¦ãŠã‚Šã€OPEæ¨å®šé‡ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ã€‚
A benefit of SLOPE is that it avoids estimating the bias of the estimator, which is as difficult as OPE.
SLOPEã®åˆ©ç‚¹ã¯ã€OPEã¨åŒæ§˜ã«å›°é›£ãªæ¨å®šå€¤ã®ãƒã‚¤ã‚¢ã‚¹ã®æ¨å®šã‚’å›é¿ã§ãã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
Appendix C describes how to apply SLOPE to the action embedding selection in our setup, and Section 4 evaluates its benefit empirically.
ä»˜éŒ²Cã§ã¯ã€æˆ‘ã€…ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«ãŠã‘ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿é¸æŠã«SLOPEã‚’é©ç”¨ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³4ã§ã¯ãã®åˆ©ç‚¹ã‚’å®Ÿè¨¼çš„ã«è©•ä¾¡ã™ã‚‹ã€‚

## Estimating the Marginal Importance Weights é™ç•Œé‡è¦åº¦é‡ã¿ã®æ¨å®š

When using MIPS, we might have to estimate w(x, e) depending on how the embeddings are given.
MIPSã‚’ä½¿ã†å ´åˆã€åŸ‹ã‚è¾¼ã¿ãŒã©ã®ã‚ˆã†ã«ä¸ãˆã‚‰ã‚Œã‚‹ã‹ã«ã‚ˆã£ã¦ã€w(x, e)ã‚’æ¨å®šã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã‹ã‚‚ã—ã‚Œãªã„ã€‚
A simple approach to this is to utilize the following transformation
ã“ã‚Œã«å¯¾ã™ã‚‹ç°¡å˜ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ä»¥ä¸‹ã®å¤‰æ›ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚

$$
\tag{3}
$$

Eq.(3) implies that we need an estimate of Ï€0(a|x, e), which we compute by regressing a on (x, e).
å¼(3)ã¯ã€Ï€0(a|x, e)ã®æ¨å®šå€¤ãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã€ã“ã‚Œã¯aã‚’(x, e)ã«å›å¸°ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦è¨ˆç®—ã•ã‚Œã‚‹ã€‚
We can then estimate w(x, e) as wË†(x, e) = EÏ€Ë†0(a|x,e) [w(x, a)].3 This procedure is easy to implement and tractable, even when the embedding space is high-dimensional and continuous.
ãã—ã¦ã€w(x,e)ã‚’wË†(x,e)=EÏ€Ë†0(a|x,e) [w(x, a)]ã¨ã—ã¦æ¨å®šã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚3 ã“ã®æ‰‹ç¶šãã¯ã€åŸ‹ã‚è¾¼ã¿ç©ºé–“ãŒé«˜æ¬¡å…ƒã§é€£ç¶šçš„ãªå ´åˆã§ã‚‚ã€ç°¡å˜ã«å®Ÿè¡Œã§ãã€æ‰±ã„ã‚„ã™ã„ã€‚
Note that, even if there are some deficient actions, we can directly estimate w(x, e) by solving density ratio estimation as binary classification as done in Sondhi et al.(2020).
ãªãŠã€æ¬ æã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã£ãŸã¨ã—ã¦ã‚‚ã€Sondhiã‚‰(2020)ã®ã‚ˆã†ã«å¯†åº¦æ¯”æ¨å®šã‚’äºŒå€¤åˆ†é¡ã¨ã—ã¦è§£ãã“ã¨ã§ã€w(x, e)ã‚’ç›´æ¥æ¨å®šã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

# Empirical Evaluation å®Ÿè¨¼çš„è©•ä¾¡

We first evaluate MIPS on synthetic data to identify the situations where it enables a more accurate OPE.
ã¾ãšMIPSã‚’åˆæˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ã—ã€MIPSãŒã‚ˆã‚Šæ­£ç¢ºãªOPEã‚’å¯èƒ½ã«ã™ã‚‹çŠ¶æ³ã‚’ç‰¹å®šã™ã‚‹ã€‚
Second, we validate real-world applicability on data from an online fashion store.
ç¬¬äºŒã«ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ»ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ãƒ»ã‚¹ãƒˆã‚¢ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ã€å®Ÿä¸–ç•Œã§ã®é©ç”¨å¯èƒ½æ€§ã‚’æ¤œè¨¼ã™ã‚‹ã€‚
Our experiments are conducted using the OpenBanditPipeline (OBP)4 , an open-source software for OPE provided by Saito et al.(2020).
æˆ‘ã€…ã®å®Ÿé¨“ã¯ã€Saitoã‚‰(2020)ãŒæä¾›ã™ã‚‹OPEç”¨ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã§ã‚ã‚‹OpenBanditPipeline(OBP)4ã‚’ç”¨ã„ã¦è¡Œã‚ã‚ŒãŸã€‚
Our experiment implementation is available at https://github.com/usaito/icml2022-mips
æˆ‘ã€…ã®å®Ÿé¨“ã®å®Ÿè£…ã¯ https://github.com/usaito/icml2022-mips ã«ã‚ã‚‹ã€‚

## Synthetic Data åˆæˆãƒ‡ãƒ¼ã‚¿

For the first set of experiments, we create synthetic data to be able to compare the estimates to the ground-truth value of the target policies.
æœ€åˆã®å®Ÿé¨“ã‚»ãƒƒãƒˆã§ã¯ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ»ãƒãƒªã‚·ãƒ¼ã®ã‚°ãƒ©ãƒ³ãƒ‰ãƒ»ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹å€¤ã¨æ¨å®šå€¤ã‚’æ¯”è¼ƒã§ãã‚‹ã‚ˆã†ã«ã€åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹ã€‚
To create the data, we sample 10- dimensional context vectors x from the standard normal distribution.
ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã€æ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰10æ¬¡å…ƒã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ»ãƒ™ã‚¯ãƒˆãƒ«xã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã€‚
We also sample de-dimensional categorical action embedding e âˆˆ E from the following conditional distribution given action a.
ã¾ãŸã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³aãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã®ä»¥ä¸‹ã®æ¡ä»¶åˆ†å¸ƒã‹ã‚‰ã€éæ¬¡å…ƒã®ã‚«ãƒ†ã‚´ãƒªãƒ¼çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿eâˆˆEã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã€‚

$$
\tag{4}
$$

which is independent of the context x in the synthetic experiment.
ã“ã‚Œã¯ã€åˆæˆå®Ÿé¨“ã«ãŠã‘ã‚‹æ–‡è„ˆxã¨ã¯ç„¡é–¢ä¿‚ã§ã‚ã‚‹ã€‚
{Î±a,ek } is a set of parameters sampled independently from the standard normal distribution.
{Î±a,ek}ã¯æ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ç‹¬ç«‹ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é›†åˆã§ã‚ã‚‹ã€‚
Each dimension of E has a cardinality of 10, i.e., Ek = {1, 2, .
ã™ãªã‚ã¡ã€Ek = {1, 2, .
..
..
, 10}.
, 10}.
We then synthesize the expected reward as
ãã—ã¦ã€æœŸå¾…å ±é…¬ã‚’æ¬¡ã®ã‚ˆã†ã«åˆæˆã™ã‚‹ã€‚

$$
\tag{5}
$$

where M, Î¸x, and Î¸e are parameter matrices or vectors to define the expected reward.
ã“ã“ã§ã€Mã€Î¸xã€Î¸eã¯æœŸå¾…å ±é…¬ã‚’å®šç¾©ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¡Œåˆ—ã¾ãŸã¯ãƒ™ã‚¯ãƒˆãƒ«ã§ã‚ã‚‹ã€‚
These parameters are sampled from a uniform distribution with range [âˆ’1, 1].
ã“ã‚Œã‚‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ç¯„å›²[-1, 1]ã®ä¸€æ§˜åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ã€‚
xek is a context vector corresponding to the k-th dimension of the action embedding, which is unobserved to the estimators.
xekã¯ã€è¡Œå‹•åŸ‹ã‚è¾¼ã¿ã®kç•ªç›®ã®æ¬¡å…ƒã«å¯¾å¿œã™ã‚‹æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã§ã‚ã‚Šã€æ¨å®šé‡ã«ã¨ã£ã¦ã¯æœªè¦³æ¸¬ã§ã‚ã‚‹ã€‚
Î·k specifies the importance of the k-th dimension of the action embedding, which is sampled from Dirichlet distribution so that Pde k=1 Î·k = 1.
Î·kã¯ä½œç”¨åŸ‹è¾¼ã¿ã®kç•ªç›®ã®æ¬¡å…ƒã®é‡è¦åº¦ã‚’æŒ‡å®šã—ã€Pde k=1 Î·k = 1ã¨ãªã‚‹ã‚ˆã†ã«ãƒ‡ã‚£ãƒªã‚¯ãƒ¬åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ã€‚
Note that if we observe all dimensions of E, then q(x, e) = q(x, a, e).
Eã®ã™ã¹ã¦ã®æ¬¡å…ƒã‚’è¦³æ¸¬ã™ã‚‹å ´åˆã€q(x, e) = q(x, a, e)ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã€‚
On the other hand, q(x, e) 6= q(x, a, e), if there are some missing dimensions, which means that Assumption 3.2 is violated.
ä¸€æ–¹ã€q(x, e) 6= q(x,a,e)ã§ã‚ã‚‹ãŒã€ã“ã‚Œã¯æ¬¡å…ƒãŒã„ãã¤ã‹æ¬ ã‘ã¦ã„ã‚‹å ´åˆã§ã‚ã‚Šã€ä»®å®š3.2ã«é•åã™ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚
We synthesize the logging policy Ï€0 by applying the softmax function to q(x, a) = Ep(e|a) [q(x, e)] a
æˆ‘ã€…ã¯ã€qï¼ˆxï¼Œaï¼‰ï¼Epï¼ˆe|aï¼‰ï¼»qï¼ˆxï¼Œeï¼‰ï¼½aã«ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€ãƒ­ã‚®ãƒ³ã‚°ãƒãƒªã‚·ãƒ¼Ï€0ã‚’åˆæˆã™ã‚‹ã€‚

$$
\tag{6}
$$

where Î² is a parameter that controls the optimality and entropy of the logging policy.
ã“ã“ã§ã€Î²ã¯ãƒ­ã‚®ãƒ³ã‚°ãƒãƒªã‚·ãƒ¼ã®æœ€é©æ€§ã¨ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’åˆ¶å¾¡ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚‹ã€‚
A large positive value of Î² leads to a near-deterministic and well-performing logging policy, while lower values make the logging policy increasingly worse.
Î²ã®å¤§ããªæ­£ã®å€¤ã¯ã€ã»ã¼æ±ºå®šè«–çš„ã§æ€§èƒ½ã®è‰¯ã„ãƒ­ã‚®ãƒ³ã‚°ãƒ»ãƒãƒªã‚·ãƒ¼ã«ã¤ãªãŒã‚Šã€ä½ã„å€¤ã¯ãƒ­ã‚®ãƒ³ã‚°ãƒ»ãƒãƒªã‚·ãƒ¼ã‚’ã¾ã™ã¾ã™æ‚ªãã™ã‚‹ã€‚
In the main text, we use Î² = âˆ’1, and additional results for other values of Î² can be found in Appendix D.2.In contrast, the target policy Ï€ is defined a
ãƒ¡ã‚¤ãƒ³ãƒ»ãƒ†ã‚­ã‚¹ãƒˆã§ã¯ã€Î² = -1ã‚’ä½¿ç”¨ã—ã€Î²ã®ä»–ã®å€¤ã«é–¢ã™ã‚‹è¿½åŠ çµæœã¯ä»˜éŒ²D.2ã«ã‚ã‚‹ã€‚

$$
\tag{}
$$

where the noise  âˆˆ [0, 1] controls the quality of Ï€.
ã“ã“ã§ã€ãƒã‚¤ã‚ºâˆˆ[0, 1]ã¯Ï€ã®è³ªã‚’åˆ¶å¾¡ã™ã‚‹ã€‚
In the main text, we set  = 0.05, which produces a near-optimal and near-deterministic target policy.
æœ¬æ–‡ã§ã¯ã€=0.05ã¨ã—ã€ã»ã¼æœ€é©ã§ã»ã¼æ±ºå®šè«–çš„ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ»ãƒãƒªã‚·ãƒ¼ã‚’ç”Ÿæˆã™ã‚‹ã€‚
We share additional results for other values of  in Appendix D.2.To summarize, we first sample context x and define the expected reward q(x, e) as in Eq.(5).
è¦ç´„ã™ã‚‹ã¨ã€ã¾ãšã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆxã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€æœŸå¾…å ±é…¬q(x, e)ã‚’å¼(5)ã®ã‚ˆã†ã«å®šç¾©ã™ã‚‹ã€‚
We then sample discrete action a from Ï€0 based on Eq.(6).
æ¬¡ã«ã€å¼(6)ã«åŸºã¥ã„ã¦Ï€0ã‹ã‚‰é›¢æ•£ã‚¢ã‚¯ã‚·ãƒ§ãƒ³aã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã€‚
Given action a, we sample categorical action embedding e based on Eq.(4).
ã‚¢ã‚¯ã‚·ãƒ§ãƒ³aãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€å¼(4)ã«åŸºã¥ã„ã¦ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿eã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã€‚
Finally, we sample the reward from a normal distribution with mean q(x, e) and standard deviation Ïƒ = 2.5.Iterating this procedure n times generates logged data D with n independent copies of (x, a, e, r).
æœ€å¾Œã«ã€å¹³å‡q(x, e)ã€æ¨™æº–åå·®Ïƒ = 2.5ã‚’æŒã¤æ­£è¦åˆ†å¸ƒã‹ã‚‰å ±é…¬ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã€‚ã“ã®æ‰‹é †ã‚’nå›ç¹°ã‚Šè¿”ã™ã¨ã€(x, a, e, r)ã®nå€‹ã®ç‹¬ç«‹ã—ãŸã‚³ãƒ”ãƒ¼ã‚’æŒã¤ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿DãŒç”Ÿæˆã•ã‚Œã‚‹ã€‚

### BASELINES ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

We compare our estimator with Direct Method (DM), IPS, and DR.5 We use the Random Forest (Breiman, 2001) implemented in scikit-learn (Pedregosa et al., 2011) along with 2-fold cross-fitting (Newey & Robins, 2018) to obtain qË†(x, e) for DR and DM.
æˆ‘ã€…ã¯ï¼ŒDRã¨DMã®qË†(x, e)ã‚’æ±‚ã‚ã‚‹ãŸã‚ã«ï¼Œscikit-learn (Pedregosa et al, 2011)ã«å®Ÿè£…ã•ã‚ŒãŸRandom Forest (Breiman, 2001)ã¨2-fold cross-fitting (Newey & Robins, 2018)ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
We use the Logistic Regression of scikit-learn to estimate Ï€Ë†0(a|x, e) for MIPS.
scikit-learnã®ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚’ç”¨ã„ã¦ã€MIPSã®Ï€Ë†0(a|x, e)ã‚’æ¨å®šã™ã‚‹ã€‚
We also report the results of MIPS with the true importance weights as â€œMIPS (true)â€.
ã¾ãŸã€çœŸã®é‡è¦åº¦é‡ã¿ã‚’ç”¨ã„ãŸMIPSã®çµæœã‚‚ã€ŒMIPSï¼ˆçœŸã®ï¼‰ã€ã¨ã—ã¦å ±å‘Šã™ã‚‹ã€‚
MIPS (true) provides the best performance we could achieve by improving the procedure for estimating the importance weights of MIPS
MIPSï¼ˆtrueï¼‰ã¯ã€MIPSã®é‡è¦åº¦é‡ã¿ã‚’æ¨å®šã™ã‚‹æ‰‹é †ã‚’æ”¹å–„ã™ã‚‹ã“ã¨ã§é”æˆã§ãã‚‹æœ€é«˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æä¾›ã™ã‚‹ã€‚

### RESULTS çµæœ

The following reports and discusses the MSE, squared bias, and variance of the estimators computed over 100 different sets of logged data replicated with different seeds.
ä»¥ä¸‹ã§ã¯ã€ç•°ãªã‚‹ã‚·ãƒ¼ãƒ‰ã§å†ç¾ã•ã‚ŒãŸ100ã‚»ãƒƒãƒˆã®ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦è¨ˆç®—ã•ã‚ŒãŸæ¨å®šé‡ã®MSEã€äºŒä¹—ãƒã‚¤ã‚¢ã‚¹ã€åˆ†æ•£ã‚’å ±å‘Šã—ã€è­°è«–ã™ã‚‹ã€‚

#### How does MIPS perform with varying numbers of actions? MIPSã¯æ§˜ã€…ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°ã§ã©ã®ã‚ˆã†ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã™ã‚‹ã‹ï¼Ÿ

First, we evaluate the estimatorsâ€™ performance when we vary the number of actions from 10 to 5000.
ã¾ãšã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°ã‚’10ã‹ã‚‰5000ã¾ã§å¤‰åŒ–ã•ã›ãŸã¨ãã®æ¨å®šå€¤ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ã€‚
The sample size is fixed at n = 10000.
ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã¯n = 10000ã«å›ºå®šã•ã‚Œã¦ã„ã‚‹ã€‚
Figure 2 shows how the number of actions affects the estimatorsâ€™ MSE (both on linear- and log-scale).
å›³2ã¯ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°ãŒæ¨å®šå€¤ã®MSEï¼ˆç·šå½¢ã‚¹ã‚±ãƒ¼ãƒ«ã¨å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã®ä¸¡æ–¹ï¼‰ã«ã©ã®ã‚ˆã†ã«å½±éŸ¿ã™ã‚‹ã‹ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
We observe that MIPS provides substantial improvements over IPS and DR particularly for larger action sets.
æˆ‘ã€…ã¯ã€MIPSãŒIPSã‚„DRã‚ˆã‚Šã‚‚ã€ç‰¹ã«å¤§è¦æ¨¡ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆã«ãŠã„ã¦å¤§å¹…ã«æ”¹å–„ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ãŸã€‚
More specifically, when |A| = 10, MSE(VË†IPS) MSE(VË†MIPS) = 1.38, while MSE(VË†IPS) MSE(VË†MIPS) = 12.38 for |A| = 5000, indicating a significant performance improvement of MIPS for larger action spaces as suggested in Theorem 3.6.MIPS is also consistently better than DM, which suffers from high bias.
A
The figure also shows that MIPS (true) is even better than MIPS in large action sets, mostly due to the reduced bias when using the true marginal importance weights.
ã¾ãŸã€MIPS(true)ã¯ã€å¤§è¦æ¨¡ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆã«ãŠã„ã¦ã€MIPSã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã‚‹ã€‚
This observation implies that there is room for further improvement in how to estimate the marginal importance weights.
ã“ã®è¦³å¯Ÿçµæœã¯ã€é™ç•Œé‡è¦åº¦ã‚¦ã‚¨ã‚¤ãƒˆã®æ¨å®šæ–¹æ³•ã«ã•ã‚‰ãªã‚‹æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚

#### How does MIPS perform with varying sample sizes? ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’å¤‰åŒ–ã•ã›ãŸå ´åˆã®MIPSã®æ€§èƒ½ã¯ï¼Ÿ

Next, we compare the estimators under varying numbers of samples (n âˆˆ {800, 1600, 3200, 6400, 12800, 25600}).
æ¬¡ã«ã€ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆnâˆˆ{800, 1600, 3200, 6400, 12800, 25600}ï¼‰ã‚’å¤‰åŒ–ã•ã›ã¦æ¨å®šé‡ã‚’æ¯”è¼ƒã™ã‚‹ã€‚
The number of actions is fixed at |A| = 1000.
A
Figure 3 reports how the estimatorsâ€™ MSE changes with the size of logged bandit data.
å›³3ã¯ã€ãƒ­ã‚®ãƒ³ã‚°ã•ã‚ŒãŸãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã«ã‚ˆã£ã¦æ¨å®šå€¤ã®MSEãŒã©ã®ã‚ˆã†ã«å¤‰åŒ–ã™ã‚‹ã‹ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
We can see that MIPS is appealing in particular for small sample sizes where it outperforms IPS and DR by a larger margin than in large sample regimes ( MSE(VË†IPS) MSE(VË†MIPS) = 9.10 when n = 800, while MSE(VË†IPS) MSE(VË†MIPS) = 4.87 when n = 25600).
MIPSã¯ç‰¹ã«ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã„å ´åˆã«é­…åŠ›çš„ã§ã‚ã‚Šã€ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã„å ´åˆã‚ˆã‚Šã‚‚å¤§ããªãƒãƒ¼ã‚¸ãƒ³ã§IPSã¨DRã‚’ä¸Šå›ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ï¼ˆn = 800ã®ã¨ãã®MSE(VË†IPS) MSE(VË†MIPS) = 9.10ã€n = 25600ã®ã¨ãã®MSE(VË†IPS) MSE(VË†MIPS) = 4.87ï¼‰ã€‚
With the growing sample size, MIPS, IPS, and DR improve their MSE as their variance decreases.
ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹ã«ã¤ã‚Œã¦ã€MIPSã€IPSã€DRã¯åˆ†æ•£ãŒå°ã•ããªã‚‹ã«ã¤ã‚Œã¦MSEãŒå‘ä¸Šã™ã‚‹ã€‚
In contrast, the accuracy of DM does not change across different sample sizes, but it performs better than IPS and DR because they converge very slowly in the presence of many actions.
å¯¾ç…§çš„ã«ã€DMã®ç²¾åº¦ã¯ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒç•°ãªã£ã¦ã‚‚å¤‰ã‚ã‚‰ãªã„ãŒã€å¤šãã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã™ã‚‹å ´åˆã«ã¯åæŸãŒéå¸¸ã«é…ããªã‚‹ãŸã‚ã€IPSã‚„DRã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã‚‹ã€‚
In contrast, MIPS is better than DM except for n = 800, as the bias of MIPS is much smaller than that of DM.
ä¸€æ–¹ã€MIPSã®ãƒã‚¤ã‚¢ã‚¹ã¯DMã®ãƒã‚¤ã‚¢ã‚¹ã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«å°ã•ã„ãŸã‚ã€n = 800ã‚’é™¤ã„ã¦MIPSã¯DMã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã‚‹ã€‚
Moreover, MIPS becomes increasingly better than DM with the growing sample size, as the variance of MIPS decreases while DM remains highly biased.
ã•ã‚‰ã«ã€MIPSã®åˆ†æ•£ãŒæ¸›å°‘ã™ã‚‹ä¸€æ–¹ã§ã€DMã¯éå¸¸ã«åã£ãŸã¾ã¾ã§ã‚ã‚‹ãŸã‚ã€ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹ã«ã¤ã‚Œã¦ã€MIPSã¯DMã‚ˆã‚Šã‚‚ã¾ã™ã¾ã™è‰¯ããªã‚‹ã€‚

#### How does MIPS perform with varying numbers of deficient actions? MIPSã¯æ¬ é™¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æ•°ã‚’å¤‰åŒ–ã•ã›ãŸå ´åˆã€ã©ã®ã‚ˆã†ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã™ã‚‹ã®ã ã‚ã†ã‹ã€‚

We also compare the estimators under varying numbers of deficient actions (|U0| âˆˆ {0, 100, 300, 500, 700, 900}) with a fixed action set (|A| = 1000).
U0
Figure 4 shows how the number of deficient actions affects the estimatorsâ€™ MSE, squared bias, and variance.
å›³4ã¯ã€æ¬ é™¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æ•°ãŒæ¨å®šå€¤ã®MSEã€äºŒä¹—ãƒã‚¤ã‚¢ã‚¹ã€åˆ†æ•£ã«ã©ã®ã‚ˆã†ã«å½±éŸ¿ã™ã‚‹ã‹ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
The results suggest that MIPS (true) is robust and not affected by the existence of deficient actions.
ãã®çµæœã€MIPSï¼ˆçœŸï¼‰ã¯ãƒ­ãƒã‚¹ãƒˆã§ã‚ã‚Šã€æ¬ é™¥è¡Œå‹•ã®å­˜åœ¨ã«å½±éŸ¿ã•ã‚Œãªã„ã“ã¨ãŒç¤ºå”†ã•ã‚ŒãŸã€‚
In addition, MIPS is mostly better than DM, IPS, and DR even when there are many deficient actions.
ã¾ãŸã€MIPSã¯DMã€IPSã€DRã«æ¯”ã¹ã€æ¬ é™¥ã®ã‚ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒå¤šã„å ´åˆã§ã‚‚ã€ã»ã¨ã‚“ã©å„ªã‚Œã¦ã„ã‚‹ã€‚
However, we also observe that the gap between MIPS and MIPS (true) increases for large numbers of deficient actions due to the bias in estimating the marginal importance weights.
ã—ã‹ã—ã€MIPSã¨MIPS(true)ã®é–“ã®ã‚®ãƒ£ãƒƒãƒ—ã¯ã€é™ç•Œé‡è¦åº¦é‡ã¿ã‚’æ¨å®šã™ã‚‹éš›ã®ãƒã‚¤ã‚¢ã‚¹ã«ã‚ˆã‚Šã€æ¬ é™¥ã®ã‚ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æ•°ãŒå¤šã„ã»ã©å¤§ãããªã‚‹ã“ã¨ã‚‚è¦³å¯Ÿã•ã‚ŒãŸã€‚
Note that the MSE of IPS and DR decreases with increasing number of deficient actions, because their variance becomes smaller with a smaller number of supported actions, even though their bias increases as suggested by Sachdeva et al.(2020).
Sachdevaã‚‰(2020)ãŒç¤ºå”†ã™ã‚‹ã‚ˆã†ã«ã€IPSã¨DRã®ãƒã‚¤ã‚¢ã‚¹ã¯å¢—åŠ ã™ã‚‹ãŒã€ãã®åˆ†æ•£ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æ•°ãŒå°‘ãªã„ã»ã©å°ã•ããªã‚‹ãŸã‚ã€IPSã¨DRã®MSEã¯æ¬ é™¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æ•°ãŒå¢—ãˆã‚‹ã»ã©æ¸›å°‘ã™ã‚‹ã“ã¨ã«æ³¨æ„ã€‚

#### How does MIPS perform when Assumption 3.2 is violated? ä»®å®š3.2ãŒç ´ã‚‰ã‚ŒãŸã¨ãã€MIPSã¯ã©ã®ã‚ˆã†ãªæ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã‹ï¼Ÿ

Here, we evaluate the accuracy of MIPS when Assumption 3.2 is violated.
ã“ã“ã§ã¯ã€ä»®å®š3.2ã«é•åã—ãŸå ´åˆã®MIPSã®ç²¾åº¦ã‚’è©•ä¾¡ã™ã‚‹ã€‚
To adjust the amount of violation, we modify the action embedding space and reduce the cardinality of each dimension of E to 2 (i.e., Ek = {0, 1}), while we increase the number of dimensions to 20 (de = 20).
é•åé‡ã‚’èª¿æ•´ã™ã‚‹ãŸã‚ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿ç©ºé–“ã‚’å¤‰æ›´ã—ã€Eã®å„æ¬¡å…ƒã®ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã‚’2ï¼ˆã™ãªã‚ã¡ã€Ekï¼{0,1}ï¼‰ã«æ¸›ã‚‰ã™ä¸€æ–¹ã€æ¬¡å…ƒæ•°ã‚’20ï¼ˆdeï¼20ï¼‰ã«å¢—ã‚„ã™ã€‚
This leads to |E| = 220 = 1, 048, 576, and we can now drop some dimensions to increase violation.
E
In particular, when we observe all dimensions of E, Assumption 3.2 is perfectly satisfied.
ç‰¹ã«ã€Eã®ã™ã¹ã¦ã®æ¬¡å…ƒã‚’è¦³æ¸¬ã™ã‚‹å ´åˆã€ä»®å®š3.2ã¯å®Œå…¨ã«æº€ãŸã•ã‚Œã‚‹ã€‚
However, when we withhold {0, 2, 4, .
ã—ã‹ã—ã€{0, 2, 4, .
..
..
, 18} embedding dimensions, the assumption becomes increasingly invalid.
18}ã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã§ã¯ã€ã“ã®ä»®å®šã¯ã¾ã™ã¾ã™ç„¡åŠ¹ã«ãªã£ã¦ã„ãã€‚
When many dimensions are missing, the bias of MIPS is expected to increase as suggested in Theorem 3.5, potentially leading to a worse MSE.
å¤šãã®æ¬¡å…ƒãŒæ¬ è½ã—ã¦ã„ã‚‹å ´åˆã€å®šç†3.5ã§ç¤ºå”†ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«MIPSã®ãƒã‚¤ã‚¢ã‚¹ãŒå¢—åŠ ã—ã€MSEãŒæ‚ªåŒ–ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
Figure 5 shows how the MSE, squared bias, and variance of the estimators change with varying numbers of unobserved embedding dimensions.
å›³5ã¯ã€æ¨å®šé‡ã®MSEã€äºŒä¹—ãƒã‚¤ã‚¢ã‚¹ã€åˆ†æ•£ãŒã€æœªè¦³æ¸¬ã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã®æ•°ã‚’å¤‰ãˆã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã©ã®ã‚ˆã†ã«å¤‰åŒ–ã™ã‚‹ã‹ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
Somewhat surprisingly, we observe that MIPS and MIPS (true) perform better when there are some missing dimensions, even if it leads to the violated assumption.
å°‘ã—æ„å¤–ãªã“ã¨ã«ã€MIPSã¨MIPSï¼ˆtrueï¼‰ã¯ã€ãŸã¨ãˆãã‚ŒãŒä»®å®šã«åã™ã‚‹ã“ã¨ã«ãªã£ãŸã¨ã—ã¦ã‚‚ã€ã„ãã¤ã‹ã®æ¬¡å…ƒãŒæ¬ è½ã—ã¦ã„ã‚‹å ´åˆã«ã€ã‚ˆã‚Šè‰¯ã„æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ãŒè¦³å¯Ÿã•ã‚ŒãŸã€‚
Specifically, the MSE of MIPS and MIPS (true) is minimized when there are 4 and 8 missing dimensions (out of 20), respectively.
å…·ä½“çš„ã«ã¯ã€MIPSã¨MIPS(true)ã®MSEã¯ã€ãã‚Œãã‚Œï¼ˆ20æ¬¡å…ƒä¸­ï¼‰4æ¬¡å…ƒã¨8æ¬¡å…ƒã®æ¬ è½ãŒã‚ã‚‹å ´åˆã«æœ€å°åŒ–ã•ã‚Œã‚‹ã€‚
This phenomenon is due to the reduced variance.
ã“ã®ç¾è±¡ã¯åˆ†æ•£ã®æ¸›å°‘ã«ã‚ˆã‚‹ã‚‚ã®ã§ã‚ã‚‹ã€‚
The third column of Figure 5 implies that the variance of MIPS and MIPS (true) decreases substantially with an increasing number of unobserved dimensions, while the bias increases with the violated assumption as expected.
å›³5ã®ç¬¬3åˆ—ã¯ã€MIPSã¨MIPSï¼ˆtrueï¼‰ã®åˆ†æ•£ãŒã€è¦³å¯Ÿã•ã‚Œãªã„æ¬¡å…ƒã®æ•°ãŒå¢—ãˆã‚‹ã«ã¤ã‚Œã¦å¤§å¹…ã«æ¸›å°‘ã™ã‚‹ä¸€æ–¹ã€ãƒã‚¤ã‚¢ã‚¹ã¯äºˆæƒ³é€šã‚Šä»®å®šã«é•åã™ã‚‹ã«ã¤ã‚Œã¦å¢—åŠ ã™ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¦ã„ã‚‹ã€‚
These observations suggest that MIPS can be highly effective despite the violated assumption.
ã“ã‚Œã‚‰ã®è¦³å¯Ÿçµæœã¯ã€MIPSãŒå‰æã«é•åã—ã¦ã„ã‚‹ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€é«˜ã„åŠ¹æœã‚’ç™ºæ®ã§ãã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚

#### How does data-driven embedding selection perform combined with MIPS? ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°é¸æŠã¯MIPSã¨ã©ã®ã‚ˆã†ã«çµ„ã¿åˆã‚ã•ã‚Œã‚‹ã®ã‹ï¼Ÿ

The previous section showed that there is a potential to improve the accuracy of MIPS by selecting a subset of dimensions for estimating the marginal importance weights.
å‰ç¯€ã§ã¯ã€é™ç•Œé‡è¦åº¦é‡ã¿ã‚’æ¨å®šã™ã‚‹ãŸã‚ã«æ¬¡å…ƒã®ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’é¸æŠã™ã‚‹ã“ã¨ã§ã€MIPSã®ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚
We now evaluate whether we can effectively address this embedding selection problem.
æ¬¡ã«ã€ã“ã®åŸ‹ã‚è¾¼ã¿é¸æŠå•é¡Œã«åŠ¹æœçš„ã«å¯¾å‡¦ã§ãã‚‹ã‹ã©ã†ã‹ã‚’è©•ä¾¡ã™ã‚‹ã€‚
Figure 6 compares the MSE, squared bias, and variance of MIPS and MIPS with SLOPE (MIPS w/ SLOPE) using the same embedding space as in the previous section.
å›³6ã¯ã€å‰ç¯€ã¨åŒã˜åŸ‹ã‚è¾¼ã¿ç©ºé–“ã‚’ç”¨ã„ãŸMIPSã¨MIPS with SLOPEï¼ˆMIPS w/SLOPEï¼‰ã®MSEã€äºŒä¹—ãƒã‚¤ã‚¢ã‚¹ã€åˆ†æ•£ã‚’æ¯”è¼ƒã—ãŸã‚‚ã®ã§ã‚ã‚‹ã€‚
Note that we vary the sample size n and fix |A| = 1000.
A
The results suggest that the data-driven embedding selection provides a substantial improvement in MSE for small sample sizes.
ãã®çµæœã€ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ã®åŸ‹ã‚è¾¼ã¿é¸æŠã¯ã€ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã„å ´åˆã«MSEã®å¤§å¹…ãªæ”¹å–„ã‚’ã‚‚ãŸã‚‰ã™ã“ã¨ãŒç¤ºå”†ã•ã‚ŒãŸã€‚
As shown in the second and third columns in Figure 6, the embedding selection significantly reduces the variance at the cost of introducing some bias by strategically violating the assumption, which results in a better MSE.
å›³6ã®2åˆ—ç›®ã¨3åˆ—ç›®ã«ç¤ºã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«ã€åŸ‹ã‚è¾¼ã¿é¸æŠã¯ã€æˆ¦ç•¥çš„ã«ä»®å®šã«é•åã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦å¤šå°‘ã®ãƒã‚¤ã‚¢ã‚¹ã‚’å°å…¥ã™ã‚‹ä»£å„Ÿã¨ã—ã¦ã€åˆ†æ•£ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã€ãã®çµæœã€MSEãŒæ”¹å–„ã•ã‚Œã‚‹ã€‚

#### Other benefits of MIPS. MIPSã®ãã®ä»–ã®åˆ©ç‚¹

MIPS has additional benefits over the conventional estimators.
MIPSã«ã¯ã€å¾“æ¥ã®æ¨å®šé‡ã‚ˆã‚Šã‚‚ã•ã‚‰ã«å¤§ããªåˆ©ç‚¹ãŒã‚ã‚‹ã€‚
In fact, in addition to the case with many actions, IPS is also vulnerable when logging and target policies differ substantially and the reward is noisy (see Eq.(2)).
å®Ÿéš›ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒå¤šã„å ´åˆã ã‘ã§ãªãã€ãƒ­ã‚°ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒãƒªã‚·ãƒ¼ãŒå¤§ããç•°ãªã‚Šã€å ±é…¬ãŒãƒã‚¤ã‚¸ãƒ¼ãªå ´åˆã«ã‚‚IPSã¯è„†å¼±ã§ã‚ã‚‹ï¼ˆå¼(2)å‚ç…§ï¼‰ã€‚
Appendix D.2 empirically investigates the additional benefits of MIPS with varying logging/target policies and varying noise levels with a fixed action set.
ä»˜éŒ²D.2ã§ã¯ã€ãƒ­ã‚®ãƒ³ã‚°ï¼ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒªã‚·ãƒ¼ã‚’å¤‰åŒ–ã•ã›ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆã‚’å›ºå®šã—ã¦ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã‚’å¤‰åŒ–ã•ã›ãŸå ´åˆã®MIPSã®ä»˜åŠ çš„ãªåˆ©ç‚¹ã‚’å®Ÿè¨¼çš„ã«èª¿æŸ»ã—ã¦ã„ã‚‹ã€‚
We observe that MIPS is substantially more robust to the changes in policies and added noise than IPS or DR, which provides further arguments for the applicability of MIPS.
æˆ‘ã€…ã¯ã€MIPSãŒIPSã‚„DRã‚ˆã‚Šã‚‚ã€ãƒãƒªã‚·ãƒ¼ã®å¤‰æ›´ã‚„ãƒã‚¤ã‚ºã®ä»˜åŠ ã«å¯¾ã—ã¦ãƒ­ãƒã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’è¦³å¯Ÿã—ãŸã€‚

## Real-World Data å®Ÿä¸–ç•Œã®ãƒ‡ãƒ¼ã‚¿

To assess the real-world applicability of MIPS, we now evaluate MIPS on real-world bandit data.
MIPSã®å®Ÿä¸–ç•Œã§ã®é©ç”¨æ€§ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã€æ¬¡ã«MIPSã‚’å®Ÿä¸–ç•Œã®ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ã™ã‚‹ã€‚
In particular, we use the Open Bandit Dataset (OBD)6 (Saito et al., 2020), a publicly available logged bandit dataset collected on a large-scale fashion e-commerce platform.
ç‰¹ã«ã€å¤§è¦æ¨¡ãªãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³Eã‚³ãƒãƒ¼ã‚¹ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§åé›†ã•ã‚ŒãŸã€ä¸€èˆ¬ã«åˆ©ç”¨å¯èƒ½ãªãƒ­ã‚°ãƒ»ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ãƒ»ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆOBDï¼‰6ï¼ˆSaito et al.
We use 100,000 observations that are randomly sub-sampled from the â€œALLâ€ campaign of OBD.
æˆ‘ã€…ã¯ã€OBDã® "ALL "ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‹ã‚‰ç„¡ä½œç‚ºã«ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸ100,000ã®ã‚ªãƒ–ã‚¶ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
The dataset contains user contexts x, fashion items to recommend as action a âˆˆ A where |A| = 240, and resulting clicks as reward r âˆˆ {0, 1}.
A
OBD also includes 4-dimensional action embedding vectors such as hierarchical category information about the fashion items.
OBDã¯ã¾ãŸã€ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã«é–¢ã™ã‚‹éšå±¤çš„ãªã‚«ãƒ†ã‚´ãƒªãƒ¼æƒ…å ±ã®ã‚ˆã†ãª4æ¬¡å…ƒã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å«ã‚€ã€‚
The dataset consists of two sets of logged bandit data collected by two different policies (uniform random and Thompson sampling) during an A/B test of these policies.
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€2ã¤ã®ç•°ãªã‚‹ãƒãƒªã‚·ãƒ¼ï¼ˆä¸€æ§˜ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰ã®A/Bãƒ†ã‚¹ãƒˆã§åé›†ã•ã‚ŒãŸ2ã‚»ãƒƒãƒˆã®ãƒ­ã‚°ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ã€‚
We regard uniform random and Thompson sampling as logging and target policies, respectively, to perform an evaluation of OPE estimators.
ä¸€æ§˜ç„¡ä½œç‚ºã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ãã‚Œãã‚Œãƒ­ã‚®ãƒ³ã‚°æ”¿ç­–ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ”¿ç­–ã¨ã¿ãªã—ã¦ã€OPEæ¨å®šé‡ã®è©•ä¾¡ã‚’è¡Œã†ã€‚
Appendix D.3 describes the detailed experimental procedure to evaluate the accuracy of the estimators on real-world bandit data.
ä»˜éŒ²D.3ã§ã¯ã€å®Ÿéš›ã®ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦æ¨å®šå€¤ã®ç²¾åº¦ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®è©³ç´°ãªå®Ÿé¨“æ‰‹é †ã‚’èª¬æ˜ã™ã‚‹ã€‚

#### Results. çµæœ

We evaluate MIPS (w/o SLOPE) and MIPS (w/ SLOPE) in comparison to DM, IPS, DR, Switch-DR, More Robust DR (Farajtabar et al., 2018), DRos, and DR-Î».
MIPSï¼ˆSLOPEãªã—ï¼‰ã¨MIPSï¼ˆSLOPEã‚ã‚Šï¼‰ã‚’ã€DMã€IPSã€DRã€Switch-DRã€More Robust DRï¼ˆFarajtabar et al, 2018ï¼‰ã€DRosã€DR-Î»ã¨æ¯”è¼ƒã—ã¦è©•ä¾¡ã™ã‚‹ã€‚
We apply SLOPE to tune the built-in hyperparameters of SwitchDR, DRos, and DR-Î».
SLOPEã‚’é©ç”¨ã—ã¦ã€SwitchDRã€DRosã€DR-Î»ã®çµ„ã¿è¾¼ã¿ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã™ã‚‹ã€‚
Figure 7 compares the estimators by drawing the cumulative distribution function (CDF) of their squared errors estimated with 150 different bootstrapped samples of the logged data.
å›³7ã¯ã€ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã®150ã®ç•°ãªã‚‹ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒ«ã§æ¨å®šã•ã‚ŒãŸäºŒä¹—èª¤å·®ã®ç´¯ç©åˆ†å¸ƒé–¢æ•°ï¼ˆCDFï¼‰ã‚’æç”»ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€æ¨å®šé‡ã‚’æ¯”è¼ƒã—ãŸã‚‚ã®ã§ã‚ã‚‹ã€‚
Note that the squared errors are normalized by that of IPS.
ãªãŠã€äºŒä¹—èª¤å·®ã¯IPSã®èª¤å·®ã§æ­£è¦åŒ–ã•ã‚Œã¦ã„ã‚‹ã€‚
We find that MIPS (w/ SLOPE) outperforms IPS in about 80% of the simulation runs, while other estimators, including MIPS (w/o SLOPE), work similarly to IPS.
MIPS(SLOPEä»˜ã)ã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ç´„80%ã§IPSã‚’ä¸Šå›ã‚Šã€MIPS(SLOPEãªã—)ã‚’å«ã‚€ä»–ã®æ¨å®šé‡ã¯IPSã¨åŒæ§˜ã«æ©Ÿèƒ½ã™ã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸã€‚
This result demonstrates the real-world applicability of our estimator as well as the importance of implementing action embedding selection in practice.
ã“ã®çµæœã¯ã€æˆ‘ã€…ã®æ¨å®šå™¨ã®å®Ÿä¸–ç•Œã§ã®é©ç”¨å¯èƒ½æ€§ã‚’ç¤ºã™ã¨ã¨ã‚‚ã«ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿é¸æŠã‚’å®Ÿéš›ã«å®Ÿè£…ã™ã‚‹ã“ã¨ã®é‡è¦æ€§ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
We report qualitatively similar results for other sample sizes (from 10,000 to 500,000) in Appendix D.3.
ä»˜éŒ²D.3ã§ã¯ã€ä»–ã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆ10,000ã‹ã‚‰500,000ã¾ã§ï¼‰ã«ã¤ã„ã¦ã‚‚å®šæ€§çš„ã«ã¯åŒæ§˜ã®çµæœã‚’å ±å‘Šã—ã¦ã„ã‚‹ã€‚

# Conclusion and Future Work çµè«–ã¨ä»Šå¾Œã®èª²é¡Œ

We explored the problem of OPE for large action spaces.
æˆ‘ã€…ã¯ã€å¤§è¦æ¨¡ãªè¡Œå‹•ç©ºé–“ã«å¯¾ã™ã‚‹OPEã®å•é¡Œã‚’æ¢æ±‚ã—ãŸã€‚
In this setting, existing estimators based on IPS suffer from impractical variance, which limits their applicability.
ã“ã®ã‚ˆã†ãªè¨­å®šã«ãŠã„ã¦ã€IPSã«åŸºã¥ãæ—¢å­˜ã®æ¨å®šé‡ã¯å®Ÿç”¨çš„ã§ãªã„åˆ†æ•£ã«æ‚©ã¾ã•ã‚Œã€é©ç”¨ãŒåˆ¶é™ã•ã‚Œã‚‹ã€‚
This problem is highly relevant for practical applications, as many real decision making problems such as recommender systems have to deal with a large number of discrete actions.
æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ã‚ˆã†ãªå®Ÿéš›ã®æ„æ€æ±ºå®šå•é¡Œã®å¤šãã¯ã€å¤šæ•°ã®é›¢æ•£çš„ãªè¡Œå‹•ã‚’æ‰±ã‚ãªã‘ã‚Œã°ãªã‚‰ãªã„ãŸã‚ã€ã“ã®å•é¡Œã¯å®Ÿç”¨çš„ãªå¿œç”¨ã«éå¸¸ã«é©ã—ã¦ã„ã‚‹ã€‚
To achieve an accurate OPE for large action spaces, we propose the MIPS estimator, which builds on the marginal importance weights computed with action embeddings.
å¤§è¦æ¨¡ãªè¡Œå‹•ç©ºé–“ã«å¯¾ã—ã¦æ­£ç¢ºãªOPEã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«ã€æˆ‘ã€…ã¯è¡Œå‹•åŸ‹ã‚è¾¼ã¿ã§è¨ˆç®—ã•ã‚ŒãŸé™ç•Œé‡è¦åº¦é‡ã¿ã‚’åŸºã«ã—ãŸMIPSæ¨å®šå™¨ã‚’ææ¡ˆã™ã‚‹ã€‚
We characterize the important statistical properties of the proposed estimator and discuss when it is superior to the conventional ones.
ææ¡ˆã™ã‚‹æ¨å®šé‡ã®é‡è¦ãªçµ±è¨ˆçš„æ€§è³ªã‚’æ˜ã‚‰ã‹ã«ã—ã€ã©ã®ã‚ˆã†ãªå ´åˆã«å¾“æ¥ã®æ¨å®šé‡ã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ã‹ã‚’è­°è«–ã™ã‚‹ã€‚
Extensive experiments demonstrate that MIPS provides a significant gain in MSE when the vanilla importance weights become large due to large action spaces, substantially outperforming IPS and related estimators.
åºƒç¯„ãªå®Ÿé¨“ã«ã‚ˆã‚Šã€è¡Œå‹•ç©ºé–“ãŒå¤§ãã„ãŸã‚ã«ãƒãƒ‹ãƒ©é‡è¦åº¦é‡ã¿ãŒå¤§ãããªã£ãŸå ´åˆã€MIPSã¯MSEã«ãŠã„ã¦æœ‰æ„ãªåˆ©å¾—ã‚’æä¾›ã—ã€IPSã‚„é–¢é€£æ¨å®šé‡ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹ã“ã¨ãŒå®Ÿè¨¼ã•ã‚ŒãŸã€‚

Our work raises several interesting research questions.
ç§ãŸã¡ã®ç ”ç©¶ã¯ã€ã„ãã¤ã‹ã®èˆˆå‘³æ·±ã„ç ”ç©¶èª²é¡Œã‚’æèµ·ã—ã¦ã„ã‚‹ã€‚
For example, this work assumes the existence of some predefined action embeddings and analyzes the resulting statistical properties of MIPS.
ä¾‹ãˆã°ã€ã“ã®ç ”ç©¶ã§ã¯ã€äº‹å‰ã«å®šç¾©ã•ã‚ŒãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®åŸ‹ã‚è¾¼ã¿ãŒå­˜åœ¨ã™ã‚‹ã¨ä»®å®šã—ã€ãã®çµæœç”Ÿã˜ã‚‹MIPSã®çµ±è¨ˆçš„ç‰¹æ€§ã‚’åˆ†æã—ã¦ã„ã‚‹ã€‚
Even though we discussed how to choose which embedding dimensions to use for OPE (Section 3.2), it would be intriguing to develop a more principled method to optimize or learn (possibly continuous) action embeddings from the logged data for further improving MIPS.
OPEã«ä½¿ç”¨ã™ã‚‹åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã‚’é¸æŠã™ã‚‹æ–¹æ³•ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³3.2ï¼‰ã«ã¤ã„ã¦èª¬æ˜ã—ãŸãŒã€MIPSã‚’ã•ã‚‰ã«å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¡Œå‹•åŸ‹ã‚è¾¼ã¿ã‚’æœ€é©åŒ–ã¾ãŸã¯å­¦ç¿’ã™ã‚‹ï¼ˆãŠãã‚‰ãé€£ç¶šçš„ãªï¼‰ã‚ˆã‚ŠåŸç†çš„ãªæ–¹æ³•ã‚’é–‹ç™ºã™ã‚‹ã“ã¨ã¯èˆˆå‘³æ·±ã„ã€‚
Developing a method for accurately estimating the marginal importance weight would also be crucial to fill the gap between MIPS and MIPS (true) observed in our experiments.
ã¾ãŸã€é™ç•Œé‡è¦åº¦ã‚¦ã‚§ã‚¤ãƒˆã‚’æ­£ç¢ºã«æ¨å®šã™ã‚‹æ–¹æ³•ã‚’é–‹ç™ºã™ã‚‹ã“ã¨ã¯ã€æˆ‘ã€…ã®å®Ÿé¨“ã§è¦³å¯Ÿã•ã‚ŒãŸMIPSã¨MIPSï¼ˆçœŸï¼‰ã®é–“ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’åŸ‹ã‚ã‚‹ãŸã‚ã«æ¥µã‚ã¦é‡è¦ã§ã‚ã‚‹ã€‚
It would also be interesting to explore off-policy learning using action embeddings and possible applications of marginal importance weighting to other estimators that depend on the vanilla importance weight such as DR.
ã¾ãŸã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã‚’ç”¨ã„ãŸã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å­¦ç¿’ã‚„ã€DRã®ã‚ˆã†ãªãƒãƒ‹ãƒ©é‡è¦åº¦ã‚¦ã‚§ã‚¤ãƒˆã«ä¾å­˜ã™ã‚‹ä»–ã®æ¨å®šé‡ã¸ã®é™ç•Œé‡è¦åº¦ã‚¦ã‚§ã‚¤ãƒˆã®å¿œç”¨ã®å¯èƒ½æ€§ã‚’æ¢ã‚‹ã“ã¨ã‚‚èˆˆå‘³æ·±ã„ã€‚
