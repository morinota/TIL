## 0.1. link

- https://paperswithcode.com/paper/recommender-systems-a-primer
- [pdf](https://arxiv.org/pdf/2302.02579v1.pdf)

## 0.2. title

Recommender Systems: A Primer

## 0.3. abstract

Personalized recommendations have become a common feature of modern online services, including most major e-commerce sites, media platforms and social networks. Today, due to their high practical relevance, research in the area of recommender systems is flourishing more than ever. However, with the new application scenarios of recommender systems that we observe today, constantly new challenges arise as well, both in terms of algorithmic requirements and with respect to the evaluation of such systems. In this paper, we first provide an overview of the traditional formulation of the recommendation problem. We then review the classical algorithmic paradigms for item retrieval and ranking and elaborate how such systems can be evaluated. Afterwards, we discuss a number of recent developments in recommender systems research, including research on session-based recommendation, biases in recommender systems, and questions regarding the impact and value of recommender systems in practice.

# 1. Basic Concepts

## 1.1. Introduction

Automated and often personalized recommendations are omnipresent in today’s online world. Nowadays, whenever we go online, there is a good chance that we will very soon receive recommendations about more things to shop, trending apps to download, or new music or artists to discover. These personalized suggestions are provided to us by recommender systems, which are software components that determine—based on statistics and machine learning models—the most suitable items that should be presented to an individual user. Due to their widespread use in practice and their demonstrated capabilities of creating value both for consumers and businesses, recommender systems can be seen as one of the most visible success stories of artificial intelligence. Today, research in this area is flourishing more than ever, both due to the practical relevance of such systems and due to the ongoing rapid developments in machine learning.

Historically, the area of recommender systems has various relationships to the field of information retrieval (IR). The problem of retrieving and ranking a set of items from a larger collection, for example, is central both in recommendation and search tasks. The two areas furthermore oftentimes rely on similar algorithms and machine learning models and they use the same or similar evaluation methodology to assess the performance of these models. Recommendation is however also different from IR in a number of ways. The selection of items in recommendation scenarios, for example, is typically based on individual user profiles and not on interactive queries. From an IR perspective, one way to look at recommendation therefore is to see it as a problem setting where the item retrieval is based on an implicit query—which may also depend on the user’s context—rather than on explicit queries. Moreover, recommendation is more often a push communication, whereas IR systems are typically reactive applications.

The roots of information retrieval can be traced to as far back as the 1950s [Sanderson and Croft 2012]. The foundations of modern-day recommender systems, in contrast, were mainly laid in the 1990s. In 1992, Tapestry, a prototype of a personalized email filtering system, was developed at Xerox PARC [Goldberg et al. 1992]. One main innovative idea of this system was to leverage the opinion of others (ratings) in the filtering process, and the term collaborative filtering (CF) was popularized with this system. Soon later, several research groups worked on automating Tapestry’s rule-based approach, and in 1994, the highly influential GroupLens system was proposed [Resnick et al. 1994]. The main idea in this system was to automatically determine like-minded “neighbors” of a user and to recommend those news items to a user that these neighbors have rated highly. In the context of this early work, the recommendation problem was operationalized as a “matrix filling” problem, where the input to a recommendation algorithm is a user-item rating matrix and the goal is to predict the missing entries in this matrix. This research operationalization and the use of collaborative filtering techniques is still predominant in today’s research and practice. Over the last almost three decades, however, many more elaborate machine learning algorithms were designed or applied for recommendation problems, including various matrix factorization techniques— which were also used first in the 1990s [Billsus and Pazzani 1998]—and, most recently, sophisticated deep learning techniques, e.g., [Liang et al. 2018].

An alternative to using collaborative preference signals is to rank and filter documents in a personalized way by looking at their content. The corresponding content-based filtering techniques were also investigated in the 1990s, often under the terms information filtering or personalized information filtering [Foltz and Dumais 1992]. These systems are often largely based on known IR techniques like TF-IDF encodings1 of the documents. In pure contentbased recommender systems, the main idea is to suggest items to the users that are similar to those that they liked in the past, i.e., without considering the opinions of others. Nowadays, however, we very often observe that hybrid systems are used in practice that combine different recommendation techniques and types of data. An early example of such a system is Fab [Balabanovic and Shoham 1997], a content-based collaborative system designed for web page ´ recommendation at Stanford.

Most of the early systems discussed here were developed as research prototypes and mostly used in academic settings. However, already before the end of the 1990s a number of successful deployments of recommendation techniques were reported for domains such as of e-commerce or music, see, e.g., [Schafer et al. 1999]. Later on, Amazon was probably one of the first organizations that relied on recommendation technology at scale [Linden et al. 2003]. Nowadays, as mentioned above, there are many online services where recommendations are central to the user experience, e.g., at Netflix [Gomez-Uribe and Hunt 2015], Spotify [Semerci et al. 2019], or YouTube [Covington et al. 2016]. Over the years, also a number of success stories can be found in the literature, where the potential business value of recommender systems is documented, see [Jannach and Jugovac 2019] for an overview.

Overall, the field has reached a certain level of maturity and standardization, in particular with respect to the operationalization of the research problem and the evaluation methodology. However, the recommendation problem is far from being solved, as there is a constant stream of new application scenarios that have not been adequately addressed so far in the research community. This paper reflects this situation and consists of two main parts. In the first part, in Sections 1.2 to 1.4, we provide an overview on the basic concepts of recommender systems and how recommender systems are commonly evaluated. Afterwards, from Section 2.1 on, we discuss a number of current and future topics in recommender systems research.

## 1.2. The Recommendation Task

The main computational task2 of any recommender system is to determine which items to show to a user in a given situation. Therefore, in any individual application setting, before an algorithm is implemented and evaluated, the question has to be answered based on which criteria the items should be selected and ranked. At the most general level, any recommender system is designed to create a certain value or utility for one or more of the involved stakeholders [Jannach and Adomavicius 2016]. In practice, there are many ways of how value can be created. From the perspective of a consumer, for example, a recommender system is usually assumed to reduce problems of information overload. From the perspective of the provider, on the other hand, providing personalized recommendations can help to increase business-related key-performance indicators (KPIs) such as sales numbers or customer retention. We will discuss related questions of the impact and the value of recommender systems in more depth later in Section 2.3.

Academic research often aims to abstract from the specifics of individual domains, particular applications, or business models of recommendation providers. The main computational task is therefore often framed in a correspondingly abstract way, usually to compute an estimate of the absolute or relative relevance of individual items for a given user. On a general level, this problem can be formalized as follows, see [Adomavicius and Tuzhilin 2005]. Given

- a set of users U,
- a set of items I and
- a utility function $f :U\times I -> R$, which maps user and items to a utility value taken from a totally ordered set R (e.g., of nonnegative real numbers),

recommend the item $i' \in I$, which maximizes the utility function, more formally:

$$
\tag{1.1}
$$

Typically, we are interested in recommending more than one item. This can be done by returning those N items that have the highest utility values. Using this definition, algorithmic research in recommender systems (see Section 1.3) amounts to defining or learning the utility function f , where f can be based on different types of additional information.

In the collaborative-filtering GroupLens system from 1994 mentioned earlier and in countless subsequent works, for example, the utility function f was designed to return a rating prediction. The additional information that is used within f was the user-item matrix of known ratings. Table 1.1 shows an example of such a rating matrix, where the specific task that is highlighted in the table is to make a recommendation for user u1. This is accomplished by predicting the ratings of user u1 for the so far unrated items i4 and i5.

The specific implementation of f in the GroupLens system was a nearest-neighbor method. In pure content-based recommendation approaches, in contrast, f is not based on the useritem rating matrix, but on the observed past behavior of an individual user and additional information about the items in I. In hybrid approaches, finally, the implementation of f might simultaneously consider the rating matrix, item meta-data, and other sources of information including demographics, various other types of observed user behavior beyond past ratings, or the users’ embedding in a social network, see also Section 1.3.6.

Collaborative filtering is the most frequently addressed problem setting in the literature and a large body of the literature aims at learning f from the usually noisy data in the user-item matrix. Correspondingly, all sorts of machine learning algorithms were applied over the years to learn f . Historically, the user-item rating matrix was considered to contain explicit, userspecified item ratings. Nowadays, research is more focused on situations where only implicit user feedback is available. In such cases, the entries in the rating matrix are not values ranging, e.g., from one to five stars, but unary. A positive value, usually represented as a 1, in the matrix therefore expresses that a user has for example viewed or purchased an item in the past. Most commonly, implicit feedback datasets do not contain negative feedback values, i.e., a value is either 1 or it is missing. Clearly, a past purchase not necessarily means that a user has liked a certain item, and implicit feedback, i.e., observed past user behavior, can be more noisy than explicit item ratings.

In principle, all sorts of information can be incorporated in the utility function f . One piece of information that received special interest in the literature is that of context, which refers to the particular situation in which a recommendation is made. The current context can have a major impact on the usefulness of a particular recommendation. One restaurant might, for example, be a good recommendation during summer but not in winter. Considering the time of the year as context may therefore be crucial. Overall, the difference to other types of side information is that the utility of an item i can be different for a specific user u, depending on the current situation. One proposal therefore is to extend the signature of f accordingly, to make this aspect explicit [Adomavicius et al. 2022]. Correspondingly, we may have

$$
f : U \times I \times C \rightarrow R
$$

as an extended utility function, where C denotes the context in which a recommendation should be made.

Generally, utility functions that make individual relevance predictions—with or without considering context—are widely used in the literature. Nowadays, however, predicting individual ratings on an absolute scale, as was done in the Netflix Prize competition [Bennett et al. 2007], is not considered the most relevant problem in practice anymore. Instead, more focus is put on creating ranked lists of suggestions, leading to the top-N recommendation task. Technically, one can use the same algorithms that were designed for rating prediction problems, and rank the items based on the predicted rating. Alternatively, learning-to-rank algorithms can be used, which do not consider the recommendable items individually, but directly aim to optimize the ranking.

As a result of this changed problem setting—creating a top-N list of items instead of making point-wise predictions—an alternative utility function f of the form

$$
f: U \times L* \rightarrow R
$$

can be defined, where L ∗ is the set of all permutations up to the length of N of the powerset of I, see also [Quadrana et al. 2018]. Given this function, we then recommend the list of items that maximizes the utility value R. Correspondingly, the problem when designing an algorithm is to define or learn a function that predicts the utility of length-restricted ranked lists of items.

Regarding the concept of utility, note that we so far have not made any assumptions regarding how utility should be defined or measured. In the past, starting with the GroupLens system, ratings were considered as proxies for utility. The goal was to predict how a user would rate a yet unseen item and to then recommend the items with the highest predicted ratings, assuming that these items are the most useful ones. Note, however, that our definitions are not generally limited to consumer value, and a utility function might consider the provider profit as well, and thus consider the value perspective of more than one stakeholder, see also [Abdollahpouri et al. 2020a, Mehrotra et al. 2018b].

In traditional settings, mostly utility functions of the form $f : U×I \rightarrow R$ were considered, where each item’s utility is considered independently from other items that might be shown to a user in a single recommendation list. Such an approach however does not allow us to assess the quality of top-N item lists as a whole, e.g., in terms of their diversity. Therefore, utility functions—and corresponding evaluation metrics—are nowadays commonly used that consider more than point-wise utility estimates, see also Section 1.4 on the evaluation of recommender systems.

## 1.3. Recommendation Algorithms

The development of recommendation algorithms has naturally mirrored the evolution of the task definition, hand in hand with the design of evaluation procedures and metrics (which we discuss later in Section 1.4) suited to the task. Recommendation can be addressed, in essence, as a supervised learning problem: given examples of observed user choices, we aim to predict present or future (yet unobserved) user interests. Variations in the task formulation give rise to different algorithmic approaches—and different metrics are appropriate to evaluate for different tasks.

As a machine learning problem, recommendation is quite unique. What makes recommendation singular in this field is, in essence, the human factor at the core of recommendation tasks. In these tasks, both the input signal and the prediction target consist of or involve user behavior at their core. This brings about a specific level of complexity compared to, for instance, recognizing shapes in an image or diagnosing a medical condition from medical tests. Furthermore, recommendation is often not just about predicting people’s actions, but about enhancing (and hence changing) such actions by bringing awareness about potentially better choices. What makes a recommendation good (and therefore the algorithm that produces it) thus involves a great deal of subjectivity and is a challenging question, that we discuss later in Section 1.4.

Recommendation algorithms have been traditionally classified into collaborative and content-based [Adomavicius et al. 2022]. The latter follow the principle that people’s tastes are related to inherent item characteristics and tend to persist over time. The former build, in a myriad different ways, on the presumed existence of regularities and structure in the distribution of user preferences and choices over the user-item space. For instance, some users may have non-random similarities to other users in their personal interests. Again, such structures are assumed to persist over time.

Different algorithmic approaches have different strengths and weaknesses. Robust and effective recommender systems therefore combine elements or entire components from different such approaches. In an aim to make algorithm categorization exhaustive, such compound solutions have been often referred to as hybrid systems. As with many machine learning or information retrieval systems, production recommender systems typically combine a core initial (commonly hybrid) algorithm that produces base item rankings, followed by post-processing algorithms, business rules (e.g. implementing marketing constraints) and fine enhancements to further improve the final recommendation ranking quality and optimizing for additional objectives [Amatriain and Basilico 2015, Covington et al. 2016].

### 1.3.1. Recommendation as a Machine Learning Problem

Modern recommendation algorithms tend to be formulated explicitly as a supervised machine learning task: a utility function f (in some of the aforementioned forms in the previous section) is learnt that minimizes a cost function, which involves the utility function within its definition. Cost can be, for instance, the sum of squared prediction error over available training ratings (the latter playing the part of labels in supervised machine learning terminology), or the binary cross-entropy loss of a sigmoid function, or the amount (measured through some monotonic function) of item pairs incorrectly ranked by the utility function to be learned. In this formulation, recommendation algorithms are distinguished from each other in the form the utility function takes (a parameterized model), the cost function, and the procedure to minimize the latter.

An example utility function is the dot product of user and item embedding vectors3 , where the coordinates of such vectors are the parameters of the model to be learned, as we shall see in Section 1.3.3.2 below. The function therefore involves some arbitrary choice (of a family of functions often referred to as the hypothesis space), and learning the function typically means learning its parameters (a “model”) by fitting them to the training data (through minimization of the cost function on training labels). Some algorithms however do not express an explicit cost function, and the utility function is defined heuristically rather than learned. This is the case, for instance, of the common nearest-neighbors collaborative filtering formulation that we overview later in Section 1.3.3.1. In either case, the utility function has hyperparameters that need to be optimized, usually taking a separate validation subset out of the training data. The ultimate goal of all algorithms is to maximize some final metric(s) of interest, such as precision, recall, or revenue—this is why some algorithms take the final metric, or a more tractable approximation thereof, as the objective function to be optimized.

Next, we provide an overview of the main algorithmic approaches and principles in the recommendation field. We start by briefly discussing different types of input data for recommendation, which determine broad differences in the algorithmic approaches developed thereupon. Each algorithmic family may deserve a paper (or an entire book) by itself. Therefore, we will provide only a broad overview here and focus on some main highlights.

### 1.3.2. Characterizing Approaches Based on their Input Data

Recommendation algorithms can be based on input data of different nature. Probably the most common data source for recommendation—and we might add, the one enabling the most powerful approaches—are records of logged user-item interactions: the rating matrix, as already discussed. When this is the only input of an algorithm we say the algorithm follows a collaborative filtering approach [Adomavicius and Tuzhilin 2005]. Collaborative filtering is based on the abstract principle that people can benefit from the experience and discoveries of other people, and not just their own, in making future choices. The simplicity of this principle and the high potential of collective data as a source for prediction are also the Achilles heel of collaborative filtering as a pure approach: where the interaction matrix is sparse, the algorithm struggles to produce reliable predictions, due to a lack of sufficient input information. Collaborative techniques may fail to deliver proper recommendations in cold start situations where little interaction has yet been recorded, as is the case at the early stages of a new deployed system, or whenever a new user or a new item enter the system. However, when sufficient interaction records are available, collaborative filtering is one of the most effective and powerful approaches.

All other input that a recommendation algorithm can take beyond user-item interactions is often referred to as side information, see Section 1.3.6. Using such information is a good complement to collaborative filtering, particularly in sparse regions of the user-item matrix. A most common type of side information is any available data and metadata directly associated to the items, to which we may refer as item features: taxonomic classifications (e.g., movie genre, music style, online store section), tags, free text associated to the items (e.g., product descriptions and reviews), etc. Algorithms that solely use such data are referred to as being content-based [Pazzani and Billsus 2007]. Instead of turning to other users for hints on what a customer might like, pure content-based approaches just focus on one user at a time, considering the interaction records of the target user4—in particular, the features of the items the user engaged with, and the features of the items to be recommended. Several other information sources can be effectively exploited by a recommender system to enhance the effectiveness and value of recommendation, such as social interactions between users [Guy 2022], data about the user (e.g., demographic information), contextual information (e.g., user and/or item geolocation, session, time, device, etc.).

When substantial interaction data is available, collaborative filtering can be more effective than other approaches as a base algorithmic core, not just in terms of delivering accurate predictions, but also in providing users with rich options beyond their individual prior experience [Vargas and Castells 2011]. These algorithms are moreover fairly general and domainagnostic, as they make no assumption about what the items are.

### 1.3.3. Collaborative Filtering

#### 1.3.3.1. Nearest Neighbors

The earliest collaborative algorithms were inspired by a typical word-of-mouth human behavior where a person takes advice from trusted friends when making a decision—one of the main sources for guidance when people make decisions and choices. Following this metaphor, socalled k-nearest-neighbor (kNN) algorithms suggest choices defined as the weighted average of the advice of the target user’s friends.

Following this heuristic intuition, recommendation may be viewed as a regression problem, where a kNN algorithm predicts the level of preference of a user u for an item i as a linear combination of the observed preferences for i in a subset of selected users: the target user’s neighbors N[u]. Following the notation in the previous section, items are ranked by the following utility function:

$$
\tag{1.2}
$$

where Ci is a normalizing term, and r(v,i) 6= 0/ means that interaction data (a ‘rating’) is available involving v and i. Typical definitions for the weights in the linear combination are based on pairwise user similarity wu,v = sim(u,s), where the similarity function can be, for instance, cosine, or Pearson correlation between the rating vectors of users u and v. The standard definitions of those measures need to be further particularized to deal with missing values in the user vectors. For instance, when using cosine similarity, missing ratings may be assigned a value of zero.

The neighborhood N[u] is usually picked as the top k most similar users to u, where k becomes a hyperparameter of the algorithm. Neighborhood selection is nonetheless a modular operation that has been the object of multiple variations and enhancements. When recommendation was addressed as rating prediction, the kNN score in Equation 1.2 above was normalized by Ci = 1/∑v∈N[u]:r(v,i)6=0/ |sim(u, v)|, in order to produce values in the rating scale. When evaluated as a ranking task, omitting normalization (i.e. Ci = 1) has often been found to produce better results [Cremonesi et al. 2010].

Multiple further variations of the kNN scheme arise from here, such as the widely used item-based version (which roughly swaps the role of users and items), multiple similarity functions, different neighbor selection approaches, and so forth. Though kNN has been developed as a heuristic scheme, Canamares and Castells [Ca ˜ namares and Castells 2017] showed ˜ that kNN can be given a probabilistic formalization where the ranking function is derived from an objective function to be maximized: the expected number of recommended items the user will like. For a more comprehensive review of kNN variants see [Nikolakopoulos et al. 2022].

kNN is an old idea [Linden et al. 2003] yet it remains a competitive approach today [Ferrari Dacrema et al. 2021, Ludewig et al. 2021]. It is very easy to understand, simple to implement, and computationally inexpensive compared to other recent, more complex approaches. As such, it is an advisable reference baseline to include in experimental recommender systems research [Canamares and Castells 2018, Cremonesi et al. 2010].

#### 1.3.3.2. Matrix Factorization

By the mid 2000s matrix factorization became popular in the field and very soon became the preferred collaborative filtering approach, due to its empirical effectiveness, relative simplicity, and flexibility as a framework enabling multiple algorithmic developments and variations. Matrix factorization is based on the assumption that the interests of users can be described in a low-dimensional space of latent factors that synthesize the subjacent properties of items that determine why a person may like them [Koren et al. 2009]. In this perspective, users and items are assumed to be describable as vectors in a common latent factor vector space, in such a way that users with similar tastes would have similar factor vectors, and so would items that please similar users. These low-dimensional vectors have come to be referred to as embeddings in the recent literature, making a connection to similar techniques in fields such as natural language processing and information retrieval [Mikolov et al. 2013, Pennington et al. 2014].

The latent factors do not necessarily correspond to actual properties of items or user traits as we would probably represent them in our human understanding. They are handled as abstract dimensions for which users and items are given values by an algorithmic approach, and are usually not interpretable by eye inspection or a clear natural intuition. Matrix factorization algorithms only require deciding how many factors we wish consider—which becomes a hyperparameter of the approach—but not what these factors are, other than axes of a multidimensional vector space.

Formally, for a k-dimensional factor space, users u ∈ U and items i ∈ I are represented by vectors pu ∈ Rk , qi ∈ Rk , where the coordinates pu,z ∈ R, pi,z ∈ R represent how interested user u is in factor z, and how much of factor z is present in item i, respectively—the higher the z coordinate value, the more the user interests are about this factor, and the more important the factor is in the nature of the item. Based on this representation, an additional assumption is made: that the interest of a user u for an item i can be captured by the dot product of their latent vectors—following our utility function notation, f(u,i) = q t i pu.

The common approach to produce the latent vectors is by minimization of a cost function R (also referred to as risk or expected loss in the empirical risk minimization machine learning framework [Vapnik 1998]), which generally involves the error in predicting the training data with some regularization. For instance, a basic formulation would be:

$$
\tag{1.3}
$$

The term added to the squared difference is a common regularization factor to avoid overfitting, where k · k represents the L2 norm, and λ is a hyperparameter that is tuned by cross validation. The minimization is often solved for by stochastic gradient descent or alternating least squares, resulting in easy implementations [Koren et al. 2009]. Once the latent vectors are learned by the algorithm, the dot product f(u,i) = q t i pu is used as the ranking function for recommendation.

The summation in the cost function can be limited to the user-item pairs for which training data are available, but it is common to extend the sum to all or a sample subset of unobserved user-item preferences [Koren et al. 2009]. In the latter case, a value needs to be imputed to the missing preference observations, which can be done in different ways (such as a constant parameter value or a randomized value). The error term in Equation 1.3 can be weighted differently for every user-item pair, often representing confidence in the corresponding (observed or imputed) preference data point. Most usually, two different weight values are used for pairs having and not having training data [Hu et al. 2008]. The combination of error weighting and missing value imputation reflect different nuances in the recommendation task and result in different algorithmic behavior (see e.g. [Steck 2013]). Besides these configuration options, matrix factorization has been rich in a myriad of further variations, such as summing item, user and global bias terms as additional parameters in the rating representation q t i pu [Koren et al. 2009], temporal awareness [Koren 2009], probabilistic formulations [Hofmann 2004, Salakhutdinov and Mnih 2007], and many other elaborations. In particular, variations in the cost function have given rise to entire new approaches, which we summarize next.

### 1.3.4. Learning to Rank

With the realization that the effectiveness of recommendation in real scenarios relies more on item ranking than on point-wise rating prediction, a shift emerged in the field towards addressing recommendation as a learning to rank (LTR) problem, drawing from a similar earlier trend in the information retrieval field [Joachims 2002]. Broadly speaking, LTR involves the introduction of loss functions that explicitly involve item ordering rather than rating error or interaction prediction error [Rendle et al. 2009]. In different possible ways, the cost functions typically represent the amount of “contradiction” between the rankings produced by the model to be learned, and how items should be ranked according to the training preference values. In other words, the model is trained to maximize order-wise agreement with the available observations. Most methods take a pairwise approach, where the “ranking error” is defined in terms of incorrectly ranked item pairs:

$$
R(\theta) = \sum_{u} \sum_i \sum_j l(u,i,j|\theta)
$$

where θ are the parameters of the recommendation model, and the loss $l(u,i, j|θ)$ quantifies how much in contradiction to the available observations the model θ ranks i and j for u (and $l$ involves the utility function f—the model—to be learned). This is summed over all pairs of items i, j that u has available training data for.

A representative and effective example in this area is Bayesian Personalized Ranking (BPR) [Rendle et al. 2009], which has become a common reference and baseline in the literature. Ranking inconsistency in BPR is expressed and developed in terms of the probability that the scores predicted by the learned model θ rank pairs of items in contradiction to the training rating data: essentially, and omitting details, $l(u,i, j) = 1_{r(u,i)>r(u, j)}P(\text{j is ranked above i for u}|θ)P(θ)$. The probability of ranking precedence is smoothed (for differentiability) as a logistic sigmoid of the ranking score difference.

For instance, for a matrix factorization model where θ consists of the latent vectors $p_u$, $q_i$ for $(u,i) \in U \times I$, and the utility scoring function is $f(u,i) = q^t_i p_u$ , we get $P(\text{j is ranked above i for u}|p,q) := 1/(1 + e^{q^t_j p_u − q^t_i p_u})$, and under normality and factor independence assumptions, the prior $P(p,q)$ results in a typical L2 regularization term. The resulting cost function can then be minimized by stochastic gradient descent. BPR can thus be seen as a layer on top of matrix factorization and a means to train the latent vectors, alternative to point-wise (i.e., user-item-wise) cost minimization.

Many alternatives to such a scheme have been proposed on a similar principle. For instance, also in a pairwise approach, RankALS [Takacs and Tikk 2012] essentially takes $l(u,i, j) = ((r(u,i) − r(u, j)) − (q^t_i p_u − q^t_j p_u))^2$ as the core pairwise ordering error to be minimized, and alternating least squares instead of gradient descent as the minimization procedure. Other approaches introduce a target ranking evaluation metric—such as nDCG, Mean Average Precision (MAP) or Mean Reciprocal Rank (MRR)—in the objective function, i.e., the objective approximates how much is lost in the metric by a suboptimal item ordering. For instance, CLiMF [Shi et al. 2012] defines the objective function as a smooth lowerbound approximation of MRR. These methods are referred to as listwise in the LTR literature [Liu 2009], because even though the loss function is still often evolved into pairwise form, it reflects a ranking goodness function (the target evaluation metric) rather than a metricagnostic item pair classification error in a binary order.

### 1.3.5. Neural Recommendation

While matrix factorization assumes the dot product of latent vectors as the scoring function, one may consider more complex, not necessarily linear5 utility scoring for recommendation. One such possibility is to use neural networks as a particular family of non-linear functions that have well-studied properties and training techniques [He et al. 2017, Zhang et al. 2022]. After revolutionizing machine learning application domains such as computer vision, speech recognition, or natural language processing, deep learning has gained blazing popularity in recent years as a basis for devising a wide variety of recommendation approaches. A comprehensive coverage of this area exceeds the scope of this paper—we provide instead a summarized overview; the reader is referred to e.g., [Zhang et al. 2022] for a wider survey.

Several technical and strategic reasons motivate an optimistic prospect for deep learning as an approach to recommendation: the ability to approximate any prediction function; efficient training (in comparison to other machine learning approaches at comparable complexity); smooth integration of heterogeneous data sources (including unstructured content), data dimensions and predictive signals; feature engineering effort savings; proved empirical effectiveness in other machine learning domains; and a growing availability of software resources and knowledge derived from a thriving activity in deep learning in many domains and fields. Furthermore, as universal function approximators, neural networks can be seen as a smooth generalization of virtually any simpler model (such as dot-product in matrix factorization as described earlier). From this perspective, a neural vs. non-neural dichotomy may become moot, strictly speaking. “Depth” in deep learning suggests however that the opportunity to grow and handle complexity is actually being leveraged by “bigger” layered models.

Neural networks provide arbitrarily high expressive power to capture complex relations in the user-item space, that matrix factor models may fail to grasp. At the top layer (the output layer), the loss function can involve a pointwise (rating or binary prediction error) [He et al. 2017] or pairwise (ranking error) element [Song et al. 2018] similar to other matrix factorization and LTR approaches discussed earlier. The generality of neural architectures further enables the smooth integration of side information in addition to user-item interaction. We discuss the use of side-information in recommendation more generally later in Section 1.3.6.3. The elaborations and combinations that can be developed in this area are as wide as imagination can afford. A huge variety of complex network architectures have been proposed in the literature using network structures such as autoencoders [Liang et al. 2018], convolutional neural networks [Tang and Wang 2018, Yuan et al. 2019], recurrent neural networks [Hidasi and Karatzoglou 2018, Wu et al. 2017], and attention layers [Kang and McAuley 2018], to name just a few. Particular architectures have been developed for specific tasks, such as session-based recommendation discussed later in Section 2.1. Interestingly, the best performance in recommendation is sometimes achieved with shallower neural models compared other domains [Anelli et al. 2022, Steck 2019].

Deep learning has proved to be empirically effective in recommendation, and steps have been taken towards its adoption in industry [Covington et al. 2016, Okura et al. 2017, Wu and Grbovic 2020]. A certain hype in this domain may have also induced a degree of noise and haste in early empirical analyses that sometimes make it difficult to get a precise perception of the actual effectiveness of neural approaches, or any conditions for which they may be best suited [Ferrari Dacrema et al. 2021, Garg et al. 2019, Kouki et al. 2020, Ludewig et al. 2021, Rendle et al. 2020, Steck 2019]. Clear effectiveness improvements have been found in specific tasks such as sequential recommendation [Sun et al. 2019b], and for specific conditions when, for instance, side information or massive data are available.

Deep learning can often also simplify and reduce feature engineering efforts. See [Steck et al. 2021] for a discussion of deep learning for recommendations at Netflix. Deep learning faces similar challenges in recommendation as it does in other domains: a large number of hyperparameters to tune, a heavy training cost tradeoff to reach the effectiveness potential, the need for massive data availability, and a black box nature involving interpretability and explainability challenges [Afchar and Hennequin 2020]. As a result, the cost and complexity of neural approaches may not necessarily always pay off in enhanced effectiveness as a universal solution for all recommendation settings and problems [Ferrari Dacrema et al. 2021, Rendle et al. 2020]. On the other hand, the data richness and volume often required to make the best of deep recommendation architectures is not always within reach of research outside corporate boundaries. Neural recommendation is nonetheless a thriving area where a profuse stream of work is being published in top tier research outlets as we speak, and industry is striving to leverage its potential [Cen et al. 2020, Covington et al. 2016, Feng et al. 2020, Steck et al. 2021]. We suggest the reader to follow through recent literature and surveys [Zhang et al. 2022] and to stay tuned to ongoing developments and future potential breakthroughs.

### 1.3.6. Content-based and Hybrid Recommender Systems

Collaborative filtering (CF) methods, as discussed in the previous section, are highly effective in many application scenarios due to their ability to detect and utilize preference or behavior patterns in a user community. Furthermore, they are particularly good at helping users to discover types of content that were previously unknown to them, e.g., by considering preferences of like-minded users. One intriguing aspect of CF methods is that they are able to accomplish this even without knowing anything about the items themselves, for example, an item’s category in an e-commerce store.

However, in cases where we dispose of such knowledge about the items and a user’s preference towards such individual item properties, it may of course make sense to consider it in the recommendation processes. For example, if we know that a user usually prefers certain movie genres or types of news, it is only intuitive to recommend more content of the same type. Moreover, in case we do not have logged transaction or rating data yet, i.e., if the user has not rated many items yet (user cold-start) or if there are new items in the catalog without purchase history (item cold-start), collaborative filtering methods may not work well.

#### 1.3.6.1. Pure Content-based Systems

In such cases, when there is little or no collaborative information available, one option is to match user-individual past preferences with knowledge about item features. Historically, this process is called content-based filtering6 . Technically, many early content-based filtering methods were based on ideas and techniques from information retrieval. Both in recommendation and retrieval scenarios, the goal is to identify and rank a set of relevant documents. While in the IR case the starting point is a (search) query, the retrieval process in a recommender system is based on a content-based user profile.

On a general level, content-based recommendation can be characterized as follows [Adomavicius and Tuzhilin 2005]:

$$
\tag{1.4}
$$

where the predicted relevance f for a given user u and item i is based on a scoring function score which matches a content-based profile of u with the content features of an item i.

Various ways of implementing the details of such a content-based approach are possible. In case where textual descriptions of an item i are available, a traditional way would be to represent the items using a TF-IDF (Term-Frequency Inverse Document Frequency) encoding. In such an implementation, the function Content(i) would return a vector of real values, where each element of the vector corresponds to a term (word), and the value indicates the importance of the term. The importance values in a TF-IDF encoding are determined by multiplying two relatively simple counting statistics, TF and IDF. The term frequency TF corresponds to the number of times a word appears in the given document i, usually normalized to account for different document lengths. The Inverse Document Frequency IDF in contrast indicates how “informative” a term is in the given document collection. The underlying logic is that if there is a term that appears in almost all documents, it carries little information to distinguish one document from each other.

The next question is how to represent the interest profile of a user u, i.e., how to implement the function ContentBasedProfile. A common goal is to use a representation that can be easily matched with the content representation of individual items. In the case of text documents, one could for example first encode all items which the user liked in the past with TF-IDF. Then, the ContentBasedProfile could be defined as being the average of the vectors of the liked items. Finally, now that the user profile and the items are represented in a compatible way, the function score can for example be implemented through the cosine similarity function.

Various alternative implementations of the different functions are of course possible. The options range from even more simple approaches, e.g., by counting overlapping features such as actors in a movie recommender systems, to more elaborate embeddings, which aim to better capture the semantics of the documents.7 The prediction function can of course be learned in a supervised approach, with input features based on item content information, and any sort of suitable shallow or deep model. Furthermore, a variety of additional pieces of information beyond the document itself were considered in the past decades to better understand the similarity or relations between different items. Besides item meta-data, various forms of exogenous information about the items, e.g., from Wikipedia, have been incorporated into content-based (or: semantics-aware) recommender systems, see [Musto et al. 2022] for an in-depth discussion.

In terms of application areas, pure content-based systems are often used when there is not sufficient collaborative information available. A typical use case is the recommendation of news, where we have to deal with a constant stream of new items, see [Kirshenbaum et al. 2012] for a case study from industry. Content-based systems have however also been applied in practice in other domains such as movies or mobile apps [Bambini et al. 2011, Jannach and Hegelich 2009], where the user preferences can be relatively stable, e.g., in terms of the preferred genres. A special use case of content-based methods are “similar item” recommendations, where the reference point to making a recommendation is not a user profile, but a specific item. Similar item recommendations are common on video streaming platforms, e.g., to recommend movies that are similar to the user just has watched, see [Trattner and Jannach 2019, Yao and Harper 2018].

#### 1.3.6.2. Hybrid Recommender Systems

Pure content-based systems can have certain limitations. An intrinsic feature of these systems is that they recommend “more of the same” by design, thus limiting the support for discovery of new types of content for users. In addition, content-based systems might surface content that is too niche. A movie recommender system that for example only considers the genre or plot descriptions may miss important quality aspects of a movie. It may thus return movies which are content-wise related to those that the user liked in the past, but in the end are not recommendations that the user will like.

A common solution to deal with such problems is to build hybrid systems, i.e., systems that not only rely on one single paradigm, e.g., collaborative filtering, but combine different approaches. This way, the shortcomings of individual methods should be avoided while the advantages are combined. In our example of content-based movie recommendations, one could apply some quality filters before the recommendations are returned, e.g., by removing all movies that have a low average community rating.

In the literature, a variety of ways of building hybrid recommender systems have been identified. In [Burke 2002], Burke identified seven such ways, which were later organized in three larger categories in [Jannach et al. 2010] as follows:

- Monolithic: In such a design, aspects of different recommendation strategies are implemented in one algorithm. A very common realization of such an approach is to design a machine learning model that includes both collaborative signals and content-based features (“side information”).
- Parallelized: In parallelized designs, the outcomes of two or more algorithms are first determined independently and then combined in some way. The combination could for example be done on the user interface level, where users are presented a list that contains personalized recommendations as well as recently trending items. Or, some weighted approach can be applied where each item receives a score from each recommender. An extreme case of such a weighting approach would be a switching hybrid, where the recommendations are always taken from one particular algorithm, depending on the situation.8
- Pipelined: In this design, one algorithm uses the outputs from another one as an input. In practice, one could for example retrieve a number of recommendation candidates based on popularity, and then only rank these items according to the assumed user preferences in a personalized way.9

#### 1.3.6.3. Collaborative Filtering with Side Information

Probably the most common hybrid approach is to enhance the power of collaborative filtering with various forms of side information. Such side information is however not limited to item-related information as in content-based filtering approaches. Wu et al. [Wu et al. 2022] categorize side information into the following categories of information that a recommender system can use in addition to the observed user-item interaction data. Figure 1.1 visualizes the different forms of information.

- User data: These are static or slowly changing features of the user, such as age, gender, or nationality. Historically, systems that leverage such information were called demographic recommender systems. In addition, researchers have also explored the consideration of personality traits in the recommendation process, see [Dhelim et al. 2022].
- Item data: These are aspects that are tied to specific items. Various forms of domain specific item-related features has been considered in the literature, including pre-structured ones like categories, unstructured ones like textual item descriptions, reviews or images, or community-provided semi-structured information in the form of tags.
- Context data: Various forms of comparably frequently changing context information have been explored in the literature, most importantly the geographical location of users, the time of the day, weather conditions or the end user device, see [Adomavicius et al. 2022].

In terms of the user-item interaction data, research has historically focused on ratings as the only form of interaction. In more recent years, recommending based on implicit feedback signals has been become predominant, see [Jannach et al. 2018] for a survey. In most published research works, only one type of implicit feedback is assumed, indicating, e.g., if a user has interacted with an item or not. In reality, multiple types of interactions are available and could be used. In an e-commerce shop, the available signals may included item views, add-to-cart-events, purchases, category navigation events, search actions and even later item returns etc. Research on leveraging combinations of such types of interaction data is somewhat limited today, probably due to the application specific nature of the interaction events. Moreover, in real-world interaction logs, information is often stored about the point in time when an interaction happened. With that information, time-aware recommender systems [Campos et al. 2014] and sequence-aware based approaches can be implemented, see also Section 2.1.

Finally, as mentioned above, various approaches exist in the literature that aim to incorporate exogenous information into the recommendation process. Examples of such information (including “world knowledge”) are Wikipedia articles, publicly accessible domain ontologies and knowledge graphs, or data that can be accessed or queried through Linked Open Data endpoints, see [Musto et al. 2022] for a discussion. Besides such types of information that refer mostly to the items, exogenous information that is tied to individual users has been explored in the literature as well, for instance, the users’ social network, their trust relationships, or reviews written by them, see [Chen et al. 2015, Dong et al. 2022, He and Chu 2010].

Recent surveys on technical approaches to build “information-rich” collaborative filtering systems can be found in [Wu et al. 2022]. and [Sun et al. 2019c]. In particular the latter work provides an overview of how various types of deep learning models incorporate side information of different types, including flat features, hierarchical features, knowledge graphs, image features and so forth.

### 1.3.7. Discussion

We have reviewed a selection of important algorithmic approaches to recommendation in this section. We emphasize that this is only a high-level rundown and the reader can find a myriad of other algorithms and variations in the literature. For instance, Rendle [2010, 2012] developed the factorization machines framework, generalizing several factor models including matrix factorization. Like neural approaches, factorization machines can smoothly leverage user and item side information in the framework for an integrated hybrid recommendation approach. Around that time and in the scope of linear models, Ning and Karypis [2011] proposed SLIM, a generalization of item-item collaborative filtering that, in a way, learns the similarity matrix as a regularized optimization problem akin to Equation 1.3 for matrix factorization in Section 1.3.3.2. This algorithm has become a common baseline in the recommender systems literature for its empirical effectiveness. Later on, Steck [2019] derived a closed form solution for computing a very similar model to SLIM. By removing the need for a costly iterative optimization procedure, the method is considerably more efficient.

Other algorithmic developments are targeted at specific recommendation scenarios such as sequential recommendation [Quadrana et al. 2018], discussed later in Section 2.1, and/or revised perspectives of the recommendation problem such as reinforcement learning [Li et al. 2016, Xin et al. 2022a], which we consider in Section 2.2.

Generally, despite the success of in particular of collaborative filtering approaches in practice, recommendation is far from a solved problem: we suggest the reader to retrospect on their own experience as a recommender system user, and form their own opinion. Many elements are involved in bringing the recommendation experience anywhere near, for instance, the effectiveness of modern search engines. Strictly speaking, this is not exactly possible—such a comparison is unfair, as search systems receive an explicit expression of intent from the user: the query. Competing against search would anyway misrepresent the purpose of recommendation: recommendation brings value in areas that search cannot solve, or for which search should not be needed. Be that as it may, room for improvement of current recommendation technology certainly remains, and different angles need to be addressed to achieve it, as we cover in the rest of this paper. The development of better algorithms is certainly one of them, and a major focus of attention and efforts in the field.

## 1.4. Evaluation of Recommender Systems

As discussed in the previous section, there is a myriad of choices available when one wants to deploy a recommender system, including a multitude of algorithms, their variants and specific configurations. Making an informed choice requires suitable and reliable evaluation methodologies. The design and selection of such reliable evaluation methodologies is therefore a central piece in recommendation technology development and research. At the same time, these evaluation methodologies drive algorithmic evolution in the field—as a fitness function, they shape the state of the art.

Evaluation generally involves a comparison between two or more recommender systems or variants. The most direct approach to compare systems in a production setting is online A/B testing, see for example [Garcin et al. 2014]. Online experiments are a finite resource though, they take time and involve a potential cost in user experience degradation by exposition to suboptimal system variants. Offline evaluation is therefore used as a complementary instrument to filter out which variants and change proposals are brought to more expensive online testing [Gruson et al. 2019, Gunawardana et al. 2022]. Offline evaluation enables, on the other hand, fast and safe hyperparameter exploration, and remains the main empirical resort widely available to academic research.

In the sections that follow we summarize the main lines of approach to recommender system evaluation. We briefly address online approaches (a perspective that will find further elaboration later in Section 2.3) to then focus on offline evaluation. More specific evaluation methodologies in the context of session-based recommendation are also discussed in Section 2.1.3.

### 1.4.1. Online Evaluation

Recommendation technologies as we experience them on Spotify, Netflix, Amazon, YouTube, Booking, Twitter, Facebook—you name it—have undergone a filtering funnel from the early conception of an algorithmic idea (perhaps in an academic research context) to the final production system [Amatriain and Basilico 2015]. This pipeline involves a combination of offline and online experimentation where new ideas are compared to each other, to established baselines, and finally, to the recommendation algorithms currently operating in a live application. By and large, offline experimentation precedes online evaluation, given the cost and bandwidth constraints of live testing [Gruson et al. 2019]. The final test in validating a new idea consists in launching it in the production platform alongside the current version, driving a fraction of user traffic to it, and comparing its performance to that of the present system— whichever wins takes over or remains the working system version.

This is called an A/B test, since two systems A and B are compared, where by A one usually means the current system (often referred to as control, borrowing from clinical trial terminology), and B is the challenger (treatment). In practice, it is common to run so-called multivariate tests comparing more than two systems simultaneously. Popular applications such as the ones mentioned at the beginning of this section are running hundreds of simultaneous A/B tests as we speak [Amatriain and Basilico 2015]. One genuine characteristic of A/B tests is that users do not know which version of the system they are facing, just like subjects in drug research do not know whether they are dispensed the actual treatment under trial (system B) or a placebo (system A). This aims to maximize the objectivity of the comparison by avoiding the introduction of confounding distortions in the experiment.

In addition to testing ideas in the most realistic possible conditions, A/B tests allow measuring the effects of algorithms on ultimate business performance and goals, beyond theoretical, more speculative effectiveness metrics. Effects can be measured on engagement (clicks, playcount, dwelling time), sales (revenue, profit, number of purchases), customer retention, and any other metric or key performance indicator the business relies on. We further elaborate on the business value of recommendation from wider perspectives in Section 2.3, also considering other possible stakeholders—beyond the customer and the system—involved in and impacted by recommendations.

A/B tests are commonly run until a statistically significant difference is found between system A and B, or until an affordable time limit has elapsed without a clear conclusion. A typical duration for an A/B test is in the order of weeks—the time needed to collect sufficient evidence to make data-driven decisions about system changes. This, and the risk involved in exposing customers to new untested changes, limits the number of experiments than can be run simultaneously on a production platform. For this reason, offline evaluation is used to shortlist a small selection of change proposals to be brought to live evaluation—we discuss offline experimentation methodology in some detail in the next section.

One open challenge to this respect is the weak or missing correlation often observed between the outcome of offline and online comparisons [Garcin et al. 2014, Gomez-Uribe and Hunt 2015, Jannach and Jugovac 2019, Kouki et al. 2020]. While this remains a major open research question in the field [Gilotte et al. 2018, Jannach and Bauer 2020, Rossetti et al. 2016], offline evaluation is routinely practiced as a selection yardstick before online testing in industry, and is by far the main resource for empirical observation in academic research.

Besides A/B testing, other forms of online experimentation and user studies can be designed with specific purposes [Knijnenburg and Willemsen 2015], not necessarily involving a production system. We discuss such studies with humans in the loop in Section 2.3.1.

### 1.4.2. Offline Evaluation

An offline experiment can be seen as a simulation of the system interacting with users, where different proxies—i.e., offline metrics—for system effectiveness are measured [Castells and Moffat 2022]. The most informative offline experiment is the one that best simulates and represents the production setting. In industrial developments, very specific production settings may be required. Therefore, in general research, a number of abstractions are usually made.

The difference between online and offline evaluation can be neatly defined as follows: whereas online evaluation acquires user feedback data (for metric computation) after recommendations are delivered to users—from these same users—in offline evaluation the test data is collected before recommendations are produced—and users are in fact never delivered the evaluated recommendations.

Setting up a basic offline experiment for evaluating a recommendation algorithm may seem a straightforward endeavor at first sight, given the wide body of well established methodologies and experimental design principles, in fields such as machine learning and information retrieval, to draw upon. The recommendation task has however peculiarities of its own that result in a fair degree of hidden complexity and pitfalls, that can produce inconsistent evaluation outcomes more easily than one might think [Canamares et al. 2020]. ˜ The experimenter is advised to carefully consider, understand and report the detailed design choices made in an experiment and their implications. We overview some of them in the following sections.

### 1.4.3. Offline Data

Offline evaluation generally distinguishes training data and test data, which should be strictly disjoint [Guy 2022]. Training data is supplied as input to the recommender system being evaluated, whereas test data is hidden from the system and is used to compute metrics on the returned recommendations. Training data in an offline experiment can be similar to the data that a production system may have available (the more similar the better), whereas test data is used in the experiment to simulate user feedback in reaction to the delivered recommendations. The training and test data for recommendation usually reflect user-item interactions as one of their major components—that is, they can be seen as disjoint subsets of the rating matrix. Training data can however also include additional side-information that specific recommendation algorithms may consume.

Training and test data can be acquired in many different ways. A good approach for the training set is to export a subset of the input data that a real recommender system is using at a certain point in time. Over the years, a number of such datasets were made publicly available, most prominently the datasets from the non-commercial MovieLens system10. Various other datasets are nowadays used by researchers containing items rating, purchase information or listening events, see also [Harper and Konstan 2015]11. Most recently, several datasets were also published that do not contain a matrix of user-item interactions, but sequential logs of recorded interactions, which can be used to evaluate session-based recommendation algorithms, see Section 2.1. Unfortunately, for many of the datasets it is not clear under which particular circumstances they were collected. For example, when an e-commerce or music streaming platform already has a recommender in place at the point in time of data collection, what we observe in the logs may be to certain extent be biased by the existing recommendation algorithm. In any case, given such a dataset of logged interactions, a trivial means to obtain test data is to subsample from the training set. This is usually referred to as splitting the data, and can be carried out in different ways, which we discuss in the next subsection.

The data typically collected for recommender system experimentation displays heavy sampling biases, originated by the working system (its algorithms, user interface and business rules) through which it was collected, external biases (marketing, fashion, social influence, etc.) and behavioral biases in user engagement. This raises issues in evaluation that we discuss later in Section 2.2. Test data can however also be obtained from a separate source from the training set. For instance, the Yahoo! R3 dataset [Marlin and Zemel 2009] includes a test set consisting of randomly sampled ratings from users for music—which means users were required to rate music that was uniformly sampled for them—whereas the training set was collected from free user interaction with music. The Coat dataset [Swaminathan et al. 2017] was built in a similar way in the clothing domain. The CM100k dataset [Canamares and Castells 2018] collected user ratings for music entirely at random, and provides a complementary label for user familiarity with the music, which is suggested as a proxy for non-uniformly distributed input training data.

#### 1.4.3.1. Data Splitting

When the data collected for offline evaluation does not include a separate test subset, the latter is usually subsampled from the available dataset. The sampling—referred to as splitting— procedure can be carried out in different ways. A first parameter of the procedure is the split ratio, i.e. the proportion of train-to-test data, typically expressed as a percentage or a ratio in [0,1]. It is common to allocate a larger data share for training (e.g. ≥ 80%) than test, given the usual data sparsity challenges when training recommendation models.

A second dimension of choice in the sampling procedure is random sampling vs. temporal splitting. For a chosen point in time, a temporal split places all the data produced prior to that point in the training subset, and the rest in the test subset [Koren 2009]. The time point is sometimes chosen in terms of meaningful time units (a number of weeks, months, etc., worth of data), or in such a way as to obtain a specific split ratio (a proportion between the amount of training and test data). More elaborate variants involving segmentation into multiple time windows have also been explored [Lathia 2010]. A temporal split generally adds to the soundness of the evaluation methodology, as it better represents a real recommendation task: predicting future (or present) usefulness based on past observed user behavior. Furthermore, it provides a cleaner data separation, since mixing past and future records can be seen as a form of data leakage. To this respect, a global time split point can be cleaner than different points for different users (as per e.g., a leave-last-out approach). Otherwise we might be predicting that a user will like some item in the “future” based on a leakage of foresight information that, for instance, the item would become popular or trendy by then (according to the training records of users with a later split time point), when in fact the item might not have even been created yet during the training period for that user.

Temporal splitting can however not always be possible, or not strictly required. For instance, meaningful data timestamps might not be available, or user needs might be relatively stable, or the experimenter might aim to focus on a specific problem where time is not a priority variable. The common alternative is to sample test and training data randomly, based on the split ratio. Random sampling is more flexible than a temporal split and enables, for instance, n-fold cross-validation, where n = 5 is a typical number (n = 10, for instance, is also usual).

A natural implementation of a random split is by i.i.d. Bernoulli sampling (coin flip) B(1, p) of data records with p = the split ratio. Other implementations have been documented that sample an exact number of data records uniformly at random without replacement, sometimes separately for each user or each item. Other authors have explored sampling an equal amount (rather than an equal ratio) of test data per item or per user, as a way to reduce evaluation biases [Bellog´ın et al. 2017]. Research on specific problems can also deploy orthogonal variations of the split procedure to simulate specific conditions such as cold-start or long-tail by placing the items or users of interest in the test set [Hurley and Zhang 2011].

#### 1.4.3.2. Candidate Item Sampling

A somewhat hidden option in the design of offline experiments is selecting the set of candidate items (sometimes referred to as target items [Canamares and Castells 2020, Sarwar et al. ˜ 2001]) that the evaluated system should rank for each target user. A natural setting for this option might consider selecting all the items in the catalog. This is not the case however when recommendation is viewed as a matrix completion problem: the known matrix cells need not be completed. In other words, an item is not included in the recommendations to users who have the item in their training records when discovery is part of the aim of recommendation, as is the case of most of the literature.

Experimenters may consider restricting candidate items to an even smaller set. Koren [2008] was the first to suggest a fixed number of target items per user. The idea caught up and is still common nowadays in the research literature [Canamares and Castells 2020, Krichene ˜ and Rendle 2020]. When small candidate sets are used, some authors refer to this option as computing sampled metrics [Krichene and Rendle 2020]. An extreme, often called condensed rankings [Buckley and Voorhees. 2004], consists in taking only the items with test ratings in the candidate set—this is the option when the error metrics are used (see Section 1.4.4.1 above), as it is not possible to compute a rating error where no rating is available.

Recent studies show that candidate sampling can have a deep impact on the outcome of offline evaluation and should be better understood [Canamares and Castells 2020, Krichene ˜ and Rendle 2020, Steck 2013]. Authors unanimously report disagreements in system comparisons arising when all vs. no test-unrated items are included in the candidate set. Canamares ˜ and Castells [2020] further show that the extremes in this settings have each their own shortcomings, and suggest that some point in between condensed and full rankings might optimize the informativeness of offline evaluation.

### 1.4.4. Recommendation Task and Metrics

In practice, every recommender system is designed to fulfil a certain purpose in order to create value for users and organizations, see also Section 2.3. Depending on the purpose, different computational tasks have to be implemented in the system [Jannach and Adomavicius 2016]. In their seminal work on recommender systems evaluation, Herlocker et al. [2004] identified tasks such as “find good items”, “annotation in context” (predict ratings), or “recommend sequence”. When designing new algorithms, the main question is to understand how effective this new algorithm is in terms of fulfilling these tasks. In offline experiments, this assessment is done with the help of computational metrics, which serve as proxies for the effectiveness of the system in its context of use.

Traditionally, the most important general aim of recommendation has been understood to involve an accurate grasp of user needs. Metrics devised to assess this have therefore been broadly referred to as accuracy metrics. Recommendation accuracy is nowadays understood as a synonym for ranking quality—metrics and evaluation protocols have therefore been borrowed and adapted from the information retrieval field [Bellog´ın et al. 2017], as we discuss next in Section 1.4.4.2. We nonetheless first discuss earlier perspectives based on rating prediction, for historical interest.

#### 1.4.4.1. Rating prediction

As mentioned in Section 1.2, the recommendation task was initially understood as a rating
prediction problem—that is, a regression task where a function f : U ×I → R is to be learned
from examples. As such, it seemed natural to evaluate recommendation by error metrics
such as Mean Average Error (MAE) and Root Mean Squared Error (RMSE). The error was
measured across the test data in the rating matrix:

$$
\text{MAE}
\\
\text{RMSE}
$$

where T ⊂ U × I denotes the subset of test data records, and the lower the value of these metrics, the better the recommender system is considered.

Such metrics were used for almost two decades in the recommender systems literature, and were the basis for such an important initiative in the growth of the field as the Netflix Prize [Liu et al. 2007]. The community has moved beyond rating prediction nonetheless, and nowadays tends to see and address recommendation as a ranking task, also in line with business models that are prominent in industry [Cremonesi et al. 2010]. Many recommendation algorithms, on the other hand, output scores for user-item pairs that do not have a meaningful interpretation as ratings, but are highly effective to select lists of top-scored items to recommend—error metrics are not meaningful when applied to such scores.

#### 1.4.4.2. Ranking Quality: Recommendation as an IR Task

During the 2000’s the view that delivering recommendations has many commonalities with delivering search results grew stronger in the community [Herlocker et al. 2004]. Both recommendation and search systems assist users in accessing relevant information or products from a large collection. One main difference lies in the absence of an explicit query in the recommendation task—still the problem can be understood to involve a user need to be satisfied, even if it is not explicitly conveyed by the user. In fact, search and recommendation often work together and complement each other in many applications. This view was recognized time before [Belkin and Croft 1992] but did not seem to catch up community-wide as—to some degree—a paradigm change in evaluation until the 2010’s [Bellog´ın et al. 2017, Cremonesi et al. 2010].

In this realization, the key for recommendation effectiveness is in the returned ranking: effective recommendations should return as many relevant items as early as possible in the ranking, and as few non-relevant items as possible in the top positions. The ranking determines what items the user will see, and how soon, when browsing recommendations. The interpretation of the system scores, by which items are ranked, as accurate rating predictions becomes irrelevant. The notion of ranking can be naturally generalized to other, not necessarily linear displays of recommendations (e.g. a “shelve” matrix), where some regions in the screen layout—equivalent to the top rank notion—are more likely than others to be examined by users.

Ranking evaluation naturally motivated researchers to borrow concepts and methodologies from the information retrieval field, where offline evaluation procedures and metrics had been researched and developed for half a century. The adaptation is however not straightforward and involves subtleties that need to be handled with care. We discuss this adaptation through the main elements involved in offline evaluation practice in the information retrieval field [Sanderson 2010].

- Collection. The set of items I can be considered an equivalent of the set of all “documents” in the IR literature. Item collections—often referred to as “item catalogs”—are in fact often the retrieval space for complementary search and recommendation functionalities in most recommender system applications.
- Query and Information Need. Search and recommendation are both motivated by a need on the user side that the system aims to help satisfy. Whereas users actively describe their need with explicit queries to a search engine, recommender systems derive user needs from observed user activity and interactions with the items. The information need representation in a recommender system is therefore considerably vague and incomplete compared to a search engine, and calls for different retrieval techniques. Search queries can have different degrees of vagueness too, and recommendation can be seen as the endpoint in a continuous spectrum of query specificity for a retrieval task, where the query is just empty.
- Relevance. The notion of relevance, central to IR, is just as meaningful in recommendation, and can be taken in quite the same sense: a recommended item is relevant to a user if they like, enjoy, are pleased by, are interested in, etc., the item. Relevance can be considered as a necessary condition for a recommendation to bring any value to the user. If a recommended item is not relevant the user will ignore it and no gain will be derived.

A major difference should be noted though as to how relevance is handled in offline experiments: while relevance is often assumed to be objectifiable—i.e. user-independent—in face of a query as a reasonable simplification, this assumption is not reasonable in recommendation, where relevance is acknowledged to be highly user-dependent, and this is typically an intrinsic characteristic of the recommendation task. This has important consequences in offline evaluation when it comes to eliciting relevance judgments: delegating item labeling as relevant or non-relevant to assessors on behalf of users is too far a stretch to enable any kind of meaningful evaluation of personalized recommendations. Offline evaluation is therefore not separable from actual end users in the way search evaluation can be.

In offline recommendation experiments, test data obtained from target users play the role of relevance judgments in IR evaluation. Compared to judgment pooling in IR [Sanderson 2010], test data can introduce extremely sharp selection biases in evaluation when data are collected in the wild, and/or by mediation of a recommender system, as is typically the case. Test data is very unevenly distributed over items, displaying a so-called popularity bias which can heavily distort evaluation outcomes [Bellog´ın et al. 2017, Canamares and Castells 2018, ˜ Jannach et al. 2015b], as we discuss further in Section 2.2.

Metrics. Once an equivalent of relevance judgments is defined and obtained, any IR metric can be applied to the output of a recommender system: precision, recall, mean average precision (MAP), mean reciprocal rank (MRR), normalized discounted cumulative gain (nDCG), are commonly used to assess the ranking quality of recommendations. Since precision and recall are rank-insensitive metrics, they are usually measured on a subset of top-n recommended items—a ranking cutoff—as P@n and Recall@n. Cutoffs can also be taken in rank-sensitive metrics such as nDCG, MAP and MRR, to further focus measurements on the ranking top.

The Area Under the ROC Curve (AUC) [Bamber 1975] can also be considered a ranking metric inasmuch as it provides a perspective of the relevant vs. non-relevant recommended items tradeoff across the ranking. AUC is commonly used as a metric for classifier evaluation, but then so are precision and recall: they view recommender systems as binary classifiers into the relevant and non-relevant classes. Focusing the measurements on top n rank positions does the trick for such measures, turning them into informative ranking metrics.

Compared to search experiments, bias and user-dependence in relevance judgments for recommendation exacerbate the sparsity of relevance labels in the ranking tops to be evaluated in offline recommendation experiments. Deciding how to handle the missing judgments becomes a key issue when importing IR ranking metrics, as discussed earlier in Section 1.4.3.2. An additional challenge is the non-random nature of the missing data, and the resulting bias in evaluation results. We discuss this further in Section 2.2.

### 1.4.5. Beyond Accuracy

While accuracy was the primary perspective on algorithmic evaluation for a long time, it eventually became apparent that this is an incomplete view of what makes a recommendation useful and profitable for consumers and providers [Ge et al. 2010, McNee et al. 2006]. For example, a fan of the Beatles would very likely love the song ‘Yesterday’—the song is highly relevant in the IR sense—but how useful would this be as a recommendation? The song is a most widely known music piece worldwide, more so for a Beatles fan. The user might as well search for it when they fancy. The song might make sense as a convenience recommendation in some scenarios, e.g., in an automatic playlist while driving on a trip, but the added-value of such a recommendation (e.g., with respect to searching and browsing) is clearly more restricted than might be, for instance, the discovery of new music or a new band or a new taste the user was not aware of and can now enjoy.

Likewise, when a streaming platform lists movies and series the user might enjoy, it may be wise to include movies from a variety of genres and directors, to better represent the diversity of interests people have—and the varying mood. While people’s tastes can be stable, we may feel like watching comedy one day and a documentary the next day, and those changes are very difficult to guess. Rather than risking a double-or-nothing bet on all-comedy or alldocumentary recommendation, a diversified offering of potential favorites would seem a more sensible approach considering this. Moreover, by diversifying recommendations the system takes opportunities to explore unseen user interests and learn about them to keep improving in the future.

Novelty and diversity are just two dimensions in the objectives beyond accuracy that can maximize the value of recommendation, and we focus on them here. It is useful to distinguish between novelty and diversity as highly related but different perspectives, a distinction we reflect in the summary that follows. Broader perspectives on the direct key measures of the value, user satisfaction, business performance and the impact at large that recommendation has on the involved stakeholders, are discussed in Section 2.3. A recommender system finds several motivations to enhance novelty and diversity, and different angles to these general notions, that can be operationalized into metrics and algorithms that enhance and measure them, as we discuss next. For a more extensive review of novelty and diversity in recommender systems, the reader is referred to [Castells et al. 2022].

#### 1.4.5.1. Novelty

At an abstract level, recommendation novelty can defined as the difference between the recommended items and a certain context of reference [Vargas and Castells 2011]. The context can typically be the past experience of the person to whom a recommendation is delivered or, considering the sparse and partial system knowledge about that, the aggregated experience of all users in the system. In the latter case, novelty equates to rarity: a recommended item is novel if it is in the long-tail of the interaction frequency distribution [Celma and Herrera 2008]. Personalized novelty, often referred to as unexpectedness [Adamopoulos and Tuzhilin 2014], would reflect how different a recommendation is from the items the target user was observed interacting with in the past. Long-tail novelty and unexpectedness can be quantified in different ways, which we summarize next.

##### 1.4.5.1.1. Long-tail novelty.

This dimension can be measured, for instance, as the average of a monotonically decreasing function φ on the amount of past engagement Li with each recommended

$$
LT(R) =
$$

where R is the returned recommendation and Li denotes the set of observed interactions involving item i. The φ function has been defined in the literature, for instance, as the complement or the negative log of the ratio of observed interactions involving the item [Vargas and Castells 2011]:

$$
(x) = 1 -\frac{x}{|L|}
$$

where L represents the set of all observed interactions. With the first definition, novelty can be read as the probability that a random user has never interacted in the past with a recommended item in R. When repeated interaction is ignored (i.e., each user-item pair is counted at most once in the above equations), the second definition is equivalent to inverse document frequency as defined in IR models for document retrieval, users being here the equivalent of documents—hence sometimes referred to asinverse user frequency [Breese et al. 1998]. When measured this way, novelty can be seen as a condition of coldness (lack of data) or unpopularity.

##### 1.4.5.1.2. Unexpectedness.

This notion is typically quantified as:

$$
Unex(R)
$$

where d denotes a set dissimilarity operator and $E_u \in I$ is a reference set of items representing “the expected” for each user u ∈U. The expected set can be, for instance, the items that the target user has interacted with, or the recommendations delivered by a supposedly conventional algorithm, or any other proxy for an unsurprising experience [Adamopoulos and Tuzhilin 2014]. Set dissimilarity can be any conventional measure such as Jaccard, Hausdorff, average item-wise distance, or other meaningful elaborations to quantify how different the two sets are. When pairwise item distance is used, feature-based dissimilarity (measured e.g., by the cosine or Jaccard distance between feature vectors/sets) is typically a good option. Other set-based dissimilarity operators can have a probabilistic interpretation, e.g. $d(R,E_u) = |R\ E_u|/|R|$ can be read as the probability that a recommended item is not expected.

##### 1.4.5.1.3. Serendipity.

An additional important notion in the scope of novelty is serendipity. While the definitions slightly vary in the literature, the dominant convention considers a recommendation as serendipitous if it is surprising (i.e. novel) and valuable [Chen et al. 2019]. If we equate value, in this sense, to relevance, a straightforward way to measure serendipity is to restrict the computation of the above novelty metrics to the recommended items that are relevant to the target user (according to the available test data), and ignore the rest.

##### 1.4.5.1.4. Further notions.

The above novelty notions are probably the most common in the literature but they are not meant to be exhaustive [Castells et al. 2022]. Freshness (how recently a recommended item was created), for instance, is often an important property by itself, and typically a signal that correlates with the above notions. Lathia and Amatriain [Lathia et al. 2010] explored finer temporal notions of novelty and diversity, involving the history of past recommendations—capturing, broadly speaking, how much a recommender system is repeating itself.

#### 1.4.5.2. Diversity

Related to but different from novelty notions, diversity is generally defined as the amount of variety covered within recommendations. For instance, ‘Orinoco Flow’ by Enya and ‘Highway to Hell’ by AC/DC can hardly be considered novelties (anyone has listened to this music sometime), but together they would make for a highly diverse music recommendation because they are very different to each other. Likewise, two very similar music rarities would not be diverse while they may be highly novel.

#### 1.4.5.3. Enhancing Novelty and Diversity

Enhancing novelty and diversity can be addressed as the maximization of the measure of interest, taking metrics such as the ones discussed above as the target for maximization. This finds two main problems. The first and perhaps most important is that maximizing for novelty or diversity is not aligned with maximizing relevance, in the IR sense. In fact, both objectives sometimes display a tradeoff—for example, random recommendations typically score extremely high on novelty and diversity, and extremely low on relevance. Rather than a single optimum the problem displays a Pareto front. As a multi-objective problem, diversity and novelty enhancement can be addressed by common strategies: scalarization, evolutionary algorithms, etc. [Veloso et al. 2014]. Since state of the art recommendation algorithms do not necessarily lie on the Pareto front, some works report improvements in both directions [Jannach et al. 2015a, Vargas and Castells 2014].

In doing so, a top-level distinction can be made between diversification strategies: intrinsic or extrinsic. Extrinsic approaches re-rank an initial relevance-oriented recommendation, seeking to improve the relevance-diversity (and/or novelty) tradeoff. A basic, typical approach in this line is re-ranking the initial recommendation by greedy maximization of a linear combination of relevance and diversity. In intrinsic approaches relevance and diversity are addressed internally at the same time as built-in targets in the algorithm [Vargas and Castells 2014]. For instance, novelty and/or diversity can be injected along with accuracy in a multi-objective function to be maximized [Veloso et al. 2014] in the perspective discussed earlier in Section 1.3.

A second difficulty arises when the diversity metric is not defined separately for each user and aggregated or averaged afterwards, but is defined on the set of all recommendations— as is the case of coverage, explained earlier. In those cases the relevance-diversity tradeoff cannot be optimized on each user independently, and the problem space becomes even larger: recommendations become interdependent in their contribution to the global diversity, and should ideally be optimized at the same time. Different levels of greediness in the approach are still possible, though an additional tradeoff needs then to be withstood between cost and optimality [Sanz-Cruzado and Castells 2018].

In most recent years, diversification approaches that operate on the user-individual level have become more popular. In an earlier work, Oh et al. [2011] proposed to adapt the novelty of the recommendations according the past popularity tendencies of individual users. A similar and more generic re-ranking approach was later proposed in [Jugovac et al. 2017], which also supports the consideration of multiple optimization objectives per user in parallel. Today, such approaches are known as calibrated recommendation [Steck 2018]. An important aspect of calibrated recommendations that is currently not covered in depth so far is that a user’s diversity of novelty preferences may change over time and depending on the context, see [Kapoor et al. 2015] for an analysis of such phenomena in the music domain.

#### 1.4.5.4. User Perceptions of Diversity and Novelty

Over the last decade, a multitude of algorithmic approaches were proposed to create more novel and diverse recommendations by optimizing the list of suggestions according to corresponding computational metrics. As with any computational metric, it is however important to ensure that these metrics are actually valid proxies of the users’ perceptions, e.g., of the novelty and diversity of the recommendations. Moreover, it is crucial to understand how these perceptions then impact the users’ satisfaction with the system.

Intra-list dissimilarity, as mentioned above, is widely used in the literature to express the diversity of a recommendation list. In [Ziegler et al. 2005], the impact of topic diversification on user satisfaction was explored with the help of a user study13. Different levels of diversifications were tried and the obtained peak value in satisfaction indicated that the participants preferred a certain degree of diversification in their book recommendations. Similar observations were made in [Castagnos et al. 2013] in the context of movie recommendations. In their study, the authors also presented study participants with recommendation lists that had different degrees of diversity. The authors found that (i) participants notice the different diversity levels and (ii) that diversity may positively influence user satisfaction. However, in case of diversified lists it was observed that providing additional explanations may be advisable so that users can better link the recommendations with their preferences. Positive effects of diversity on user satisfaction in the movie domain was also reported in [Ekstrand et al. 2014]; however, in this study it also turned out that increased levels of novelty—e.g., when too many unfamiliar items are recommended—may negatively impact user satisfaction.

In the mentioned studies in the book and movie domains, the used similarity functions were based on topic categories and movie attributes and proved effective in the experiments. Such a validation of the particular similarity function is however often missing in other works. More work is therefore required to understand which features of an item are determining the similarity perceptions of users, see [Trattner and Jannach 2019, Yao and Harper 2018] for studies in the movies and food domain. Moreover, in the traditional intra-list dissimilarity measure, the position of the diverse elements does not matter when assessing the overall diversity of a given list. In reality, indications exist that the ordering of the items may impact user perceptions [Ge et al. 2012]. In this direction, drawing from earlier work on IR metrics built upon user models (e.g., [Moffat and Zobel 2008]), Vargas and Castells [2011] propose a probabilistic metric framework where a wide variety of novelty and diversity metrics— including the ones mentioned in this section—can be endowed with rank sensitivity.

Instead of varying the positions of the items in one recommendation list, as done in [Ge et al. 2012], Hu and Pu [2011] suggested to organize the items in groups. In a user study, in which also the participants’ eye movements were tracked, they then found that the “organization” interface helped to increase the diversity perception of users. Finally, in a more recent work, Chen et al. [2019] report the outcome of a large-scale user study on the quality perception of recommendations on an e-commerce platform. The study revealed that serendipity plays a decisive role for user satisfaction and is more important than diversity and novelty alone.

# 2. Recent Topics

## 2.1. Sequential and Session-based Recommendation

The traditional problem formulation of recommender systems, as described in Section 1.2, is designed for making non-contextual item suggestions based on stable long-term user preferences. Such an approach is for example meaningful to present landing-page recommendations to a frequent user of a media streaming site. However, there are many other important application scenarios where we cannot assume to have long-term preference information available. Think, for example, of visitors of an e-commerce shop who are not logged in or who are first time users. Moreover, on an e-commerce site, a user’s interests and needs might change from shopping session to shopping session. Therefore, even if we would know the identity of the user it is important to consider the user’s short-term history and current intents when we determine what we should recommend to the user next.

### 2.1.1. Problem Definition and Terminology

These requirements, which cannot be easily addressed with the traditional matrix completion abstraction, led to the development of different types of sequence-aware recommender systems [Quadrana et al. 2018]. Instead of a user-item rating matrix, the main input to this type of recommenders consists of a sequence of logged interactions. The entries of these logs are typically collected through the application, e.g., a web store, and they can have different types. A web shop might for example record item views, add-to-cart events, or purchases. Moreover, each log entry is usually associated with an individual user. This can be a known user, who is logged in to the web shop, or an anonymous one which is only identified through a cookie or IP address. Finally, a last difference to the user-item rating matrix is that the interaction log may contain multiple user-item interaction pairs, for example, when the user viewed an item multiple times before a purchase. In addition, a log entry may have additional types of meta-data attached such as a time stamp. Overall, while the inputs are different from the matrix-completion problem, the output of a sequence-aware recommender is usually a ranked list of items. Figure 2.1 shows an overview of the problem setting.

Within the family of sequence-aware recommender systems, two related terms can be identified in the literature: (i) sequential recommendation and (ii) session-based recommendation. Sequential recommendation refers to the more general problem of making next-item recommendations. A typical example is the problem of recommending the next Point-Of-Interest (POI) to a user in a tourism scenario or to recommend an entire next shopping basket in an ecommerce scenario. In session-based recommendation scenarios, the problem is also to make next-item predictions, but the underlying assumptions are that (a) the past interaction log is organized in usage sessions and that (b) recommendations are made in the context of an ongoing session. Another underlying assumption of session-based approaches typically is that the user’s interests and needs can change from session to session. Furthermore, in many practical problem settings, no long-term preference information is available at all for the current user, which means that the recommendations must be based on a small set of interactions—e.g., a few item view events—that are observed in an ongoing session.

In the literature, two alternative situations are considered when recommendations have to be made in the context of an ongoing session. In one case, nothing is known about the current user but the interactions that are observed in the session. In the other case, also information about past sessions of the same user is available. This second scenario is often referred to as personalized session-based recommendation or session-aware recommendation. Figure 2.2 visualizes the differences between (pure) session-based and session-aware recommendation problems.

### 2.1.2. Algorithms for Sequential and Session-based Recommendation

Various algorithmic approaches were explored for sequence-aware recommendation problems. The following main types of technical approaches were identified in [Quadrana et al. 2018]: (i) Sequence Learning Approaches (ii) Sequence-Aware Matrix Factorization (iii) Hybrid Approaches, and (iv) Nearest Neighbors and Other Methods.

#### 2.1.2.1. Sequence Learning Approaches.

Approaches in this category can be subdivided into several main paradigms.

- Frequent Pattern Mining techniques were historically often applied for the problem of detecting (unexpected) patterns in shopping baskets, e.g., in the form of association rules. Later, these techniques were extended to consider sequential patterns, in which the order of the elements is relevant.
- Sequence Modeling approaches try to go beyond mined patterns and aim to learn models from past time-ordered data to predict future events. A large variety of corresponding machine learning models were proposed over the years. The predominant approaches in the literature are based on Markov Models, Reinforcement Learning, or Deep Learning, in particular in the form of Recurrent Neural Networks (RNNs). Often, modern sequential models are based on ideas from Natural Language Processing to model the sequences in the interaction data, see e.g., [Sun et al. 2019a].
- Distributed Item Representations are another form of encoding sequential information. These representations (embeddings) are projections from sequences of item-related events (e.g., views) into a lower-dimensional representation in which the transition between items is preserved.
- Supervised Learning with Sliding Windows, finally, is a rather specific approach, where the idea is to reframe the next-item prediction problem to a supervised learning problem by moving a sliding window over the sequence of events and to derive the corresponding feature values from this sliding window to predict the next event.

#### 2.1.2.2. Sequence-Aware Matrix Factorization.

A few examples exist in the literature that extend matrix factorization approaches to consider sequential information, usually derived from timestamps that are available for the entries in the user-item rating matrix. These approaches are conceptually related to time-aware recommender systems [Campos et al. 2014], which aim to identify changes in user preferences over larger time spans.

#### 2.1.2.3. Hybrid Approaches.

A more common approach in the literature is to combine sequence modeling with the power of latent factor models. One of the earlier approaches of that type is the Factorized Personalized Markov Chain (FPMC) method [Rendle et al. 2010]. This method jointly factorizes a first-order Markov Chain, which models user-specific item transitions, and a traditional user-item rating matrix to make next-item predictions. Similar approaches were proposed later on, e.g., in [He and McAuley 2016]. More recently, methods that combine neural techniques (e.g., RNNs, Convolutional Neural Networks, or Attention) with matrix factorization layers, have become more popular, e.g., [Kang and McAuley 2018, Tang and Wang 2018].

#### 2.1.2.4. Nearest-Neighbors and Other Methods.

In particular in the context of session-based recommendation, a simple yet effective method is often to apply neighborhood-based techniques. In [Jannach and Ludewig 2017] and subsequent works, the idea was explored to use those past sessions in the data as a basis for predicting the next event, which are most similar to the ongoing one. Despite the conceptual simplicity of the approach, it turned out to be competitive or even superior to much more complex models. Finally, a number of alternative proposals were identified in [Quadrana et al. 2018], including methods that rely on (non-neural) graphbased models or techniques from discrete optimization, where the latter class of problems may however suffer from scalability problems given the usually high number of recommendable items.

### 2.1.3. Evaluation of Sequential and Session-based Recommender Systems

In practical applications, recommender systems are evaluated with respect to the organizational goals and purpose they should fulfill (e.g., increase customer retention) and the specific computational tasks they implement that serve this purpose (e.g., help users find novel content) [Jannach and Adomavicius 2016]. While the general goals of sequence-aware recommender systems are usually not very different from traditional ones, sequential and sessionbased recommenders often implement a number of computational tasks based on the particular nature of their input data, as identified in [Quadrana et al. 2018]:

- Context Adaptation is the fundamental problem of session-based algorithms, i.e., to guess the user’s situation and intents from the interactions observed in an ongoing session. The observed user behavior thus represents a form of interactional context [Adomavicius et al. 2022].
- Trend Detection is a still under-explored topic in academic research and relates both to individual trends (e.g., interest drift) and community trends that might, for example, depend on seasonal patterns or popularity peaks of items.
- Repeated Recommendations, which are usually not considered in matrix-completion problems, can both be valuable in the context of consumable items (e.g., printer ink) and for the sake of reminding users of items they liked in the past (e.g., a past favorite artist).
- Consideration of Order Constraints and Patterns: In a number of domains, there is a strict order in which items should or must be recommended (e.g., learning course recommendation); in other domains, there might be a “natural” order in which things should be recommended (e.g., in the case of movie sequels). Sequence-aware recommenders can learn and/or enforce such patterns as a computational task.

In principle, all evaluation approaches outlined in Section 1.4 can be applied for the evaluation of sequential and session-based recommender systems, including offline experiments, user studies, and field tests (A/B) tests. Offline experiments dominate the research landscape and academic research also for sequence-aware recommenders and, as usual, such experiments aims to abstract both from the value perspective and the particular computational tasks.

#### 2.1.3.1. Offline Evaluation

Remember from above that the output of a sequence-aware recommender system as usual is a ranked list of item suggestions. Following the usual “hide-and-predict” evaluation approach in offline experiments, common evaluation measures from information retrieval for ranking accuracy such as precision and recall can be applied. The evaluation procedure of sequenceaware recommender systems therefore involves the hiding of a number of interactions in a given sequence of events and the subsequent measurement of how good an algorithm is to predict the hidden interaction(s).

In evaluations of traditional matrix completion problem settings, the selection of the hidden elements (i.e. the test set) is often done in a randomized way, e.g., by randomly selecting 20 % of the data for testing. Moreover, cross-validation is in many cases applied to increase the confidence in the observed results. Such an entirely randomized approach is not meaningful for sequence-aware recommendation problems, and alternative procedures have to be applied.

##### 2.1.3.1.1. Data Splitting.

When splitting the data into training, validation, and test data sets, the sequential order must be considered, and the held-out interactions to be predicted must happen after those that are used for training. Different strategies are possible in the data splitting process. The following are relatively common.

- In sequential problem settings, often the splitting of the data is done per user. Commonly, the very last interaction of each user is hidden and put in the test set.1
- In session-based scenarios, it is, in contrast, more common to apply a time-based split. Commonly, research datasets cover an extended period of time, e.g., a few weeks. Often, the sessions of the very last or the last few days are used for testing (and one or more preceding days for validation).

##### 2.1.3.1.2. Making the Measurement.

In sequential recommendation problems, when the last interaction of each user is hidden, the making the measurement amounts to determining if and at which position an algorithm was ranking the hidden interaction within a top-n list. Common evaluation measures therefore include the Hit Rate, the NDCG or the MRR at a certain cut-off length.

For session-based problems, these measures can be applied as well, but the evaluation procedure is slightly different. Remember that the test dataset typically contains entire user sessions and that the specific problem in session-based recommendation is to predict the next interaction(s) given the session beginning. The options reported in the literature are to “reveal” all but the last interaction in a session, to reveal the first k elements, or to incrementally reveal one interaction after the other when measuring. Different variants also exist how the accuracy measures are applied. One can, for example, measure how good an algorithm is to predict the immediate next item in a session, using the Hit Rate, NDCG, and MRR as typical measures for a given cut-off threshold. Alternatively, and probably more importantly, one can measure how many of the subsequent (and hidden) interactions in the session are contained in the top-n list returned by a recommender. In that case, measures like precision and recall are often used.

Note that like in traditional recommendation scenarios, quality factors other than accuracy can be considered. Beyond-accuracy aspects considered in the literature in particular include the diversity of the recommendation lists, catalog coverage, popularity biases, or scalability.

Generally, observe that while we hide interactions in the data, we are usually making predictions for items. In case there is only one type of interactions in the logs, e.g., listening events on a music streaming site, an item prediction corresponds to predicting a listening event. However, real-world datasets often consist of interactions of different types, e.g., item views, add-to-cart actions, or purchases on an e-commerce site. In many academic research works, only the most frequent type of interactions is considered, for example item views. In reality, however, it might be much more important to predict item purchases, and to use all available types of interactions in the training and prediction process.

##### 2.1.3.1.3. Cross-Validation.

Given the sequential nature of the data, performing cross-validation based on random splits is not meaningful. In the literature, often only one single train-test split is applied, which, however, bears a certain risk that the obtained results are specific to that split. An alternative approach therefore is to create multiple overlapping or non-overlapping timebased splits of the data, and to repeat the measurement on these splits, see [Ludewig et al. 2021].

#### 2.1.3.2. User-Centric Evaluation

User-centric research, while not uncommon in the recommender systems literature in general, is still very rare in the context of sequential and session-based recommendation. In one study in the music domain [Kamehkhosh and Jannach 2017], researchers compared different techniques for next-track music recommendations. In this study, participants were presented with a number of alternative continuations for a given playlist. In another study [Ludewig and Jannach 2019], an online interactive radio station was created for the purpose of the experiment. Different algorithms were used to create playlist based on the a start track provided by the user and based on the feedback during the radio listening session. Both mentioned studies led to interesting insights regarding the quality perceptions of users and investigated to what extent offline accuracy measures align with user perceptions. Besides such specific research setups, the use of general frameworks for the user-centric evaluation of recommender systems can be applied, e.g., [Pu et al. 2011] or [Knijnenburg et al. 2012].

#### 2.1.3.3. Real-World Evaluation

For more traditional recommendation problems, a number of success stories from real-world deployments of recommender systems exist, see [Jannach and Jugovac 2019]. In the area of sequential and session-based recommendations, such reports are rare. An exception is the work in [Kouki et al. 2020], which reports on the use of session-based recommendation technology for an online shop in the home improvement domain. Ultimately, the use of a modern neural approach led to a significant increase in relevant business KPIs when compared to an existing commercial software for retail recommendation.2

Another interesting aspect of this work lies in how the decision was made which of several existing session-based algorithms should be put in practice. First of all, the authors benchmarked a number of techniques, including conceptually simple and more complex ones, through an offline evaluation. Reproducing previous offline experiments [Ludewig et al. 2021], they found that simple methods in many cases outperform the latest neural techniques in terms of typical accuracy measures. A subsequent evaluation with human experts however revealed that the well-performing nearest-neighbor technique might not be the best choice in the given application domain. While it is often able to correctly predict a single hidden element, other algorithms turned out to be more successful in recommending more than one possibly relevant item. In particular, other algorithms often returned items that are similar to the hidden one, which could therefore represent purchase alternatives for the customers. As a result, this study emphasizes not only the possible limitations of offline evaluations, it also stresses that in practice it is important to understand the specifics of a particular application and how a recommender is expected to create value.

### 2.1.4. Discussion and Outlook

In particular session-based and session-aware are highly relevant practical problems for which we have seen a number of technical proposals in recent years.3 However, despite the many complex algorithms that are developed, it is surprising to see that in many cases very simple techniques based on nearest-neighbors are competitive or even outperform the latest neural techniques in offline evaluations [Garg et al. 2019, Kouki et al. 2020, Ludewig et al. 2021]. Such phenomena are however not specific to session-based recommendation problems, see [Ferrari Dacrema et al. 2021] for a discussion of similar problems that were observed for traditional top-n recommendation scenarios. Overall, this points to certain methodological issues that may hamper progress in the field.

On the other hand, these observations also indicate that there is huge potential for the development of even better algorithms for session-based and sequential recommendation. In particular, the use of side information, e.g., in the form of item meta-data, the use of contextual information, and the consideration of multiple types of interactions in parallel seem to be promising areas for future work.

## 2.2. Popularity, Bias and Recurrence in Recommendation

As the view of recommendation shifted from rating prediction to item ranking, an new phenomenon was observed: offline evaluation tends to reward the recommendation of popular items, that is, items involved in frequent interaction with users [Cremonesi et al. 2010]. This becomes a strong bias to the same extent that the data distribution over items—the so-called popularity distribution—is heavily skewed in a long-tail shape. With a random data split, the popularity effect in evaluation is a clear self-fulfilling prophecy: the items with most training data—the popular items—have the most test data, which are used as relevance judgments, as discussed in Section 1.4.3. Hence, by ranking popular items highly in recommendations, we get fewer missing ratings in the ranking top, resulting in improved chances of producing high relevance metric values [Bellog´ın et al. 2017, Canamares and Castells 2018]. Even with ˜ temporal splits, the popularity distributions on the respective sides of the split point are usually correlated (to the extent that item popularity persists over time), whereby the popularity bias remains [Bellog´ın et al. 2017, Mena-Maldonado et al. 2021].

Given this effect, it seems not surprising that state of the art collaborative filtering algorithms are also heavily biased to recommend popular items, as researchers have found systematically [Canamares and Castells 2017, Jannach et al. 2015b]. This realization has fueled ˜ research in two directions: a) avoiding or countering the biases in both evaluation and algorithms, under an implicit view of bias as a potential distortion of learning and evaluation [Bellog´ın et al. 2017, Jadidinejad et al. 2021, Steck 2010, 2011]; and b) better understanding where the bias may come from, and wondering whether it might be harmless or even useful to some extent [Canamares and Castells 2018]. An additional fundamental perspective has re- ˜ cently gained momentum that considers the dual role of recommendation in satisfying users, on one side, and learning from their interactions with the delivered recommendations, on the other. We briefly discuss these perspectives in the following paragraphs.

### 2.2.1. Countering Bias

From a certain angle, popularity can be considered intrinsically undesirable from an addedvalue perspective that, as such, should be avoided. This is the viewpoint of novelty as a complementary goal of recommendation as discussed in Section 1.4.5.1. Even from this perspective, novelty should still be combined with relevance, and relevance needs to be evaluated; the relevance-seeking component, and its evaluation, are likely to be affected by popularity biases. Specific evaluation procedures have been researched aiming to remove the popularity bias from the metrics [Bellog´ın et al. 2017, Schnabel et al. 2016, Steck 2010, 2011] and the algorithms [Hernandez-Lobato et al. 2014, Jannach et al. 2017, Liu ´ et al. 2020]. Among them, counterfactual reasoning approaches for off-policy learning and evaluation are being actively researched, using techniques such as inverse propensity scoring (IPS) [Jadidinejad et al. 2021, Schnabel et al. 2016, Swaminathan et al. 2017]. IPS produces unbiased estimators for metrics and objective functions by modeling the distribution of observations over the user-item space, and compensating for it in the evaluation metrics and the learning algorithms, by downweighting observed preferences by the probability of observing them. Challenges involved in the application of IPS in this context include modeling the propensity bias from biased samples, and the boundless variance of IPS weights when observation probability can be indefinitely small.

### 2.2.2. Understanding Bias

Beyond the capability to remove or harness biases, some authors argued for a better understanding of popularity distributions and their potential harm or innocuity as a distorting factor in evaluation [Canamares and Castells 2018]. After all, recommender system applications ˜ make deliberate use of popularity signals [Amatriain and Basilico 2015], where popularity is not necessarily an entirely bad property—whereas naive popularity countering in evaluation might assess this signal as equivalent to random recommendation. Canamares and Castells ˜ [2018] theorized about the formation of popularity distributions, and found out specific conditions that determine whether or not system comparisons are preserved in evaluation under popularity biases. The key to the answer lies in the conditional dependencies between observation, relevance and items as random variables, which explain why majority choices and majority tastes can be useful in delivering effective suggestions. These findings have been extended later in experiments showing a high degree of agreement in evaluation with biased and unbiased data [Mena-Maldonado et al. 2021]. In other words, the general findings in this line are that feedback loops and Matthew effects [Merton 1968] can sometimes have a virtuous component [Ciampaglia et al. 2018]. Acting upon this awareness, Zhang et al. [2021] explore a finer handling of popularity at training and inference time, seeking to mitigate distortion while leveraging the useful signal.

### 2.2.3. The Feedback Loop

A major source of bias in offline data is the production system through which the interaction data—the input for recommendation—are collected. Recommender systems are commonly integrated in multi-component applications where items are exposed to user feedback through different pathways: search, browsing, recommendation, front page, etc. Each of such components introduces bias of its own. To the extent that a recommendation component feeds on user interaction with its own output, we have a feedback loop [Chaney et al. 2018b, Fleder and Hosanagar 2009]. This loop, in combination with offline evaluation, creates a magnification effect (also known as snowball, Matthew effect, rich-gets-richer), where the data collected for evaluation reward and reinforce the hypotheses ingrained in the production system, making popular items ever more popular, and penalizing change [Chaney et al. 2018a]. As a result the system can get stuck in its own assumptions and miss out on value and opportunities that remain hidden in unobserved choices.

Furthermore, perverse incentives can arise from such loops, where user engagement, sometimes tightly associated to short-term revenue, is blindly targeted by the recommendation strategy—recommendation might then be inducing compulsive or toxic behavior, rather than (or mixed with) serving a constructive purpose [Whittaker et al. 2021]. Such rabbit hole traps may result in suboptimal long-term business performance at best, or unwillingly contribute to toxic phenomena such as misinformation spreading, polarization, and radicalization, at worst. Preventing such risks and harmful effects is not all that simple, and to a great extent requires continual monitoring and direct specialized intervention, a revision of the business model and incentives, and dealing with non-trivial ethical questions [Stray 2020, Zuckerberg 2018].

From a more generic and algorithmic viewpoint, a promising approach to breaking feedback loops is the reinforcement learning perspective, where the algorithm is understood to be an agent that aims to please and learn simultaneously [Sutton and Barto 2021]. A recommendation is seen as an opportunity to match user interests to the best of the current system knowledge (immediate optimization), but also to extend this knowledge by exploring yet unknown or uncertain areas, so as to produce better recommendations in the future. Optimal solutions care for both the long and short term (exploration and exploitation), striving to achieve a subtle balance. After all, the mission of a recommender system is not circumscribed to a single recommendation for every user, but hopefully a recurrent, cyclic, mutually beneficial relationship with the customer over an extended time span. Much of the literature in the field builds however implicitly on the view of a once-in-a-lifetime interaction with the user.

Developments in this area have adapted so-called multi-armed bandit (MAB) approaches [Sutton and Barto 2021], where items are seen as actions to be selected (choices to be recommended) [Hill et al. 2017]. When chosen, actions return a reward (a utility or value for the user), drawn from an unknown distribution, and the goal of the system is to maximize the cumulative rewards over time. MAB algorithms break the feedback loop by injecting a controlled degree of guided stochastic exploration beyond local maxima (a controlled bit of informed randomness, to put it simply), seeking a better explore vs. exploit balance than traditional, pure exploitation approaches (which include all the algorithms discussed in Section 1.3). In order to define personalized solutions, so-called contextual variants of MAB are developed, where the context fundamentally includes the user, and the reward depends on the context [Li et al. 2016].

Further generalization beyond contextual MAB considers the effect that recommendations have on the user state, thus representing the interaction of users with the system as a Markov Decision Process (MDP) [Xin et al. 2020]. MDP are also applied as a natural fit to represent sequential recommendation [Xin et al. 2022a], that we discussed earlier in Section 2.1. MDPoriented solutions typically require large amounts of online interaction in order to be effective, which may take more time to start producing good recommendations than is affordable [Afsar et al. 2022]. This can be mitigated by complementary offline training using more abundant logged interaction data, which then introduces bias from the logging policy [Xin et al. 2022b], that needs to be dealt with as discussed earlier. Reinforcement learning in recommendation is a rapidly growing area, with tight connections to counterfactual learning and evaluation; the reader may find a good starting point towards more in-depth readings in the survey by Afsar et al. [2022].

As a final note from a broader abstract perspective, popularity biases can be seen as arising from a compound feedback loop involving manifold channels of user-item discovery and feedback, interacting with and mutually reinforcing each other. Direct novelty enhancement, bias-countering techniques, and reinforcement learning can be seen as multiple solutions in a common direction seeking a degree of healthy exploration in recommendations.

## 2.3. Impact and Value of Recommender Systems

Over the last thirty years, recommender systems have been successfully deployed in many application domains, and their value both for consumers and businesses is undisputed [Jannach and Jugovac 2019]. However, like many other areas of applied machine learning—including various subfields of information retrieval—academic research in recommender systems is mostly not based on studies with deployed systems, but largely relies on a data-centric approach. Moreover, differently from observational and analytical types of data-based research, where the goal for example is to understand certain phenomena from recorded data, recommender systems research almost exclusively focuses on developing novel prediction models. The research question in such experiments is therefore mostly not “What made a recommendation successful?” [Jannach et al. 2017] or “Are popular items, when recommended, more likely to be purchased by users?”, but almost exclusively if a new prediction model is able to more accurately predict the held-out past data.

The holy grail in recommender systems research today therefore is to improve the prediction accuracy of algorithms. The underlying and generally plausible assumption is that better accuracy leads to the effect that more relevant items appear at higher positions in the recommendation lists. As a result, the more relevant items are easier to discover by the users, thereby reducing information overload. In this context, one central feature of almost any recommender system lies in the personalization of the item suggestions.4 As such, recommender systems can seen as strategies to implement digital one-to-one marketing and hyper-segmentation.

Unfortunately, the assumed correspondence of higher prediction accuracy on past data with better overall value for the user or the provider of the recommendations is less than certain. A number of academic research works for example report that higher prediction accuracy only sometimes lead to a better quality perception by users, as obtained through user studies. Also various works from industry, e.g., by Netflix, report that the offline experimentation is often not predictive of online success [Gomez-Uribe and Hunt 2015]. The limitations of relying solely on accuracy measures can be easily illustrated. Imagine a user who liked the first two “Avengers” movies and rated them highly on an online video rental platform. An algorithm that then recommends three additional Avengers films along with related superhero movies, might even reach an accuracy of 100 %. While being 100 % accurate, the recommendation might be of little value both for the user—as the recommendation list is obvious and maybe too monotone—and the provider, because the user would have rented these movies anyway. Recommending these movies can therefore even represent a missed opportunity to promote other items.

Given these observations, it is pivotal that the field of recommender systems moves beyond research that is solely based on offline experiments and abstract accuracy measures. Such a change is highly important, in particular because it is not clear if small improvements on a specific accuracy measure for a given dataset—such things are commonly reported in published research—would even matter in practice.

In this section, we therefore first emphasize the importance of considering the human in the loop when proposing new algorithms or user interfaces for recommender systems (Section 2.3.1). Then, we discuss the various ways in which recommender systems can create value, both for consumers and recommendation providers, and why it is important to evaluate recommender systems with respect to their intended purpose (Section 2.3.2).

### 2.3.1. Understanding the Impact of Recommendations with the Human in the Loop

There are a number of relevant questions one might have to address when building a recommender system in practice, for example:

- Would users find the recommendations provided by a system useful? • Would they find them novel, diverse, or entertaining, and would they ultimately be satisfied with the recommendations?
- Would they believe that the system’s recommendations are trustworthy, would they like to see an explanation?
- Will they consider the system’s recommendations in their decisions in the future?

All these aspects might have an impact on the ultimate success of a recommender system, which depends on whether or not users adopt the system’s recommendations. Unfortunately, none of these questions can be answered with typical offline experiments. Ultimately, recommending online—to a large extent—is a problem of human-computer interaction, where a computerized, interactive system is designed to support its users in an information-finding or decision-making task. Thus, studies that involve users are a necessary means to explore questions like those mentioned above.

The most common form of such studies are experiments in the form of a randomized controlled trial, where the study participants are randomly assigned to either one or more treatment groups or to a control group. In many such studies, the participants interact with different versions of a prototype system in a controlled environment, often in a lab. For example, in an experiment on effects of providing explanations to users, one participant group would interact with a system that provides explanations, and the other one with one that does not. The existence of the explanations would therefore be the manipulated independent variable in the experiment. Depending on the specific research question, various dependent variables can be measured. These dependent variables can either be objectively measured, e.g., by recording the participants’ behavior like mouse clicks or interaction time, or they can be obtained by asking participants to self-report their perceptions in a questionnaire. If the research question, for example, is if explanations help reducing the user’s decision effort, one can both measure the time needed to make a choice from the given recommendations, and ask participants about the perceived effort of the task. After the experiment, various statistical analyses can be made to assess if the existence of explanations may have caused or is at least correlated with the observations for decision effort.

Differently from typical offline experiments, such user studies are guided by explicit research questions or hypotheses, which are commonly derived from theoretical considerations. As a consequence, each user study requires a specific experimental design that is suited to answer these research questions. This fact and the need to recruit participants for the study, which are representative for at least a subset of the target population, makes user studies typically more effortful than offline experiments, which are often not based on theoretical considerations or explicit research questions.

User-centric evaluation frameworks, as proposed in [Pu et al. 2011] or [Knijnenburg et al. 2012] are one way of standardizing such user experiments at least to some extent. These frameworks include a number of general quality factors and the potential effects of recommender systems on user behavior. Figure 2.3 provides an overview of the ResQue framework [Pu et al. 2011], which—partly inspired by the Technology Acceptance Model—proposes to assess the potential impact of user quality perceptions on their beliefs, attitudes and future behavioral intentions. Besides the identification of the relevant constructs (variables) in the model, the framework furthermore provides a set of questionnaire items that can be used to measure the participants’ subjective perceptions in an experiment.

Generally, controlled experiments with users are much more common in the Information Systems literature than in the Computer Science literature on recommender systems. Given that many important questions—in particular also regarding the user interface of recommenders [Jugovac and Jannach 2017]—cannot be answered with the predominant but narrow focus on improving prediction models, it is important that research in recommender systems more often considers evaluations with users in the loop. Clearly, user studies also have their limitations, e.g., that the study participants are not interacting with a real system or are typically not making real purchase decisions. Nonetheless, such studies can provide us with relevant indications of what might matter for users in the real world.

Finally, note that controlled experiments are not the only form of evaluations with the human in the loop. Various other types of observational studies as well as qualitative research methods, such as focus groups or interviews, are commonly applied in academia and in industry.

### 2.3.2. Consumer and Business Value of Recommender Systems

Ultimately, any recommender system is designed to create utility or value for one or more of the involved stakeholders. The academic literature historically focuses on the consumer value, at least implicitly. One underlying assumption often is that if a recommender system is able to create value for consumers, e.g., by helping them to discover novel items, this at least indirectly leads to value for the business or organization that provides the recommendations. Such considerations seem plausible, for example, for music or video streaming services that are based on flat-rate subscription rates. The more the consumers are engaged with the service—due to the constant discovery of new content—the more likely they might be to renew their subscription at the end of the month. However, the goals of the provider might not always be fully aligned with the value perspective of consumers, leading to multi-stakeholder recommendation problems.

#### 2.3.2.1. Recommendation as a Multi-Stakeholder Optimization Problem

Different types of entities can be involved in the recommendation process [Abdollahpouri et al. 2020a, Jannach and Bauer 2020].

- Consumers or End Users are the recipients of the recommendations. This can be individual users or groups of users. In the latter group recommendation setting, see [Masthoff and Delic 2022], typically the needs and preferences of different group members must ´ be balanced, i.e., there can be multiple stakeholders in the group.
- Recommendation Service Providers are the organizations that are in the control of the recommender system, which they run as part of their business. Typical examples include large online retailers or media streaming providers.
- Suppliers are the entities that create or provide the items that are offered through the recommendation services. In the e-commerce example, this could be the manufacturers; in the media domain, it could be artists or labels, or other content providers. Note that the recommendation service providers can also be the suppliers of the items they offer.
- Society: In case a recommendation service affects a large portion of a society, e.g., feed recommendations on a social media site, recommender systems can also have a societal impact, e.g., when the recommender system reinforces certain political opinions in an unbalanced way.

Consider the following example, which illustrates the potential challenges of considering multiple stakeholders interests [Jannach and Bauer 2020]. Imagine an online hotel search service which charges a commission fee to hotel owners for each booking that is made through the site. From the consumer’s perspective, the best recommendation is one that satisfies their needs, usually given certain budget constraints. The recommendation service provider, on the other hand, might try to maximize its short-term profit (commission). A recommender system might therefore select items that represent a reasonable match for the consumer, but lead to higher profit for the platform. At the same time, the hotel platform might also try to ensure that all hotel suppliers are recommended from time to time, so that the platform remains attractive for the suppliers. The hotel chains, finally, might have an interest that the recommender system pushes a certain subset of the offerings through recommendations, for example, the more expensive ones.

Today, research in multi-stakeholder recommender systems is still limited. Works exist, for example, for the specific problems of reciprocal recommendation [Palomares et al. 2021], price and profit aware recommender systems [Jannach and Adomavicius 2017], or in the area of algorithmic fairness and biases, e.g., [Abdollahpouri et al. 2020b, Mehrotra et al. 2018b]. Generally, a particular challenge in the context of multi-stakeholder recommendation lies in the fact that the most common evaluation approach—accuracy evaluation on historical datasets—are typically not sufficient to investigate these problems. Instead, alternative methodological approaches are needed, e.g., based on simulation techniques [Ghanem et al. 2022, Zhang et al. 2020]. This is particularly important because balancing multiple stakeholder goals often requires longitudinal analyses that are able to uncover the long-term effects on the involved stakeholders.

#### 2.3.2.2. Impact and Value Oriented Recommender Systems Evaluation

In practical deployments, the performance of recommender systems is determined in terms of Key Performance Indicators (KPIs) that are selected or defined by responsible organizational units. What is actually being measured primarily depends on the purpose that is intended to achieve with the recommender system. Therefore, the particular choice or design of the KPI also depends on the specific business model of the provider.

A survey of real-world success stories of recommender systems [Jannach and Jugovac 2019] identified the following main types of measurements that are taken in practice:

- Click-Through-Rates (CTR): The CTR is traditionally most suitable in ad-based business models, i.e., where page impressions matter. In recommendation scenarios, optimizing for click-rates may however be too short-sighted in many applications.
- Adoption and Conversion Measures: These measures go beyond counting click events. They, for example, only count a recommendation as successful, if there are signs that it was truly useful for consumers, e.g., when they watched a recommended movie.
- Sales and Revenue: This is the most direct measurement and can, for example, consist of determining how much additional sales was stimulated by a recommender.
- Effects on Sales Distributions: Measures of that type are useful to assess the impact of the recommendations on consumer behavior, e.g., if they could be persuaded to select different options compared to a situation with no recommender.
- User Engagement: This measure is often considered to be related with long-term customer loyalty. When consumers interact more with a service, this is considered a sign that they will come back for purchases in the future or that they will continue or renew their subscription.

The specific choice of these KPIs, as discussed, can be strongly tied to the specific market situation and individual business models of the recommendation providers. Academic research typically does not target at the idiosyncrasies of such specific situations but aims at providing generalizable insights. Nonetheless, it is important—also in academia–which kinds of value recommender systems can generate, both for consumers and providers.

Today’s research, as outlined above, is largely (implicitly) focused on the value for consumers, which is often circumscribed as helping users to find items of interest (or: “Find good items” [Herlocker et al. 2004]). Commonly, prediction accuracy metrics are used as proxies to measure this value by determining how good an algorithm is at predicting previously recorded and held-out positive interactions between a user and an item (see Section 1.4). While this abstraction is generally useful and provides as with a standardized way of operationalizing the research problem, it may represent an oversimplification of the problem and even misguide our research efforts, as discussed above. In reality, there are many different ways in which recommender systems can create value, see [Jannach and Bauer 2020].

To overcome this somewhat narrow research operationalization, researchers have proposed a purpose- and value-oriented framework for the evaluation of recommender systems [Jannach and Adomavicius 2016]. The main goal of this conceptual framework is to guide researchers—both in academia and industry—in terms of how they measure the success of a recommender system. Specifically, the framework aims to ensure that what is measured, i.e., the KPIs or metrics that are used, is aligned with the intended purpose of the system.

The proposed framework, which considers both the consumer and provider perspective, has four layers: (i) Overarching Goals, (ii) Purpose of the Recommender, (iii) Computational Task, (iv) Evaluation Approach. To understand how the framework works, consider the case of an online media streaming service. The overarching goal from a business perspective— which can be entirely independent of any recommendation technology—might be to ensure the long-term profitability of the service. The specific purpose of a recommender in that context that serves this goal might be to increase user engagement. One way to achieve this increased engagement can be to provide consumers with a mix of familiar and novel recommendations to support discovery. The computational task of the recommender system therefore is to determine such a mix. Finally, the evaluation approach must be chosen that it is aligned with the original overarching goals In the example, it could be a mixed methods approach, where (a) offline evaluations is used to gauge the accuracy and novelty levels of alternative algorithms, where (b) user studies are conducted to assess the intention of potential consumers to use the service in the future and where (c) field studies determine the impact of the recommendations on the user’s media streaming activity.

The most important point in this context—which should become obvious when using the proposed framework—is that relying solely on computational measures (such as prediction accuracy), can be too limiting. In practice, there are probably not too many cases where measures like accuracy strictly correlate with aspects that matter more in practice, such as customer retention or customer satisfaction. Therefore, as discussed in more depth in [Jannach and Bauer 2020], a paradigm shift seems needed in terms of how we evaluate recommender systems. Such a paradigm shift should ultimately lead to more impactful research in this area. To achieve this progress researchers should, for example:

- more often adopt a purpose-oriented evaluation approach, as described, where goals and evaluation measures are ensured to be better aligned;
- consider a multi-method research approach, which also includes human-in-the-loop evaluations both in terms of controlled experiments and qualitative research;
- expand their methodological repertoire also in terms of improved data-based evaluations, considering for example simulation-based approaches to investigate longitudinal effects;
- consider domain- and application specifics more often where appropriate, acknowledging that there is no “best model” and that the same set of recommendations might be good in one situation and sub-optimal in another one.

Following such advice might ultimately help to avoid a dead end in algorithms research, which is not only heavily focused on abstract metrics, but is also suffering from certain methodological problems [Ferrari Dacrema et al. 2021], which may limit the impact of current recommender research in practice.

## 2.4. Summary and Future Directions

We conclude this paper with a brief summary, pointers to further readings, and an outlook on
important future directions in this area.

### 2.4.1. Summary

Today, personalized recommendations are a central component of many online services, and various success stories are reported in the literature how such systems create value both for users and organizations. Due to their wide success in practice, research interest has correspondingly been continuously growing over the past twenty-five years. Ultimately, this led to the development an own recommender systems research field, which is historically strongly rooted in information retrieval, machine learning, and human-computer interaction. In this paper, we have first characterized the recommendation problem on an abstract and application-independent level and then reviewed the most common way of operationalizing the research problem as a matrix completion problem. After discussing the most common types of recommendation algorithms, we elaborated on how to evaluate recommender systems “in the lab” and “in the wild”. Finally, we reviewed recent technical developments in the area of session-based recommendation and approaches based on reinforcement learning.

### 2.4.2. Further Readings and Future Directions

In this paper, we largely focused on algorithmic aspects of recommender systems, which traditionally is also the most active area of research. However, building a recommender systems in practice requires much more than just a clever algorithm [Jannach et al. 2016b, Xiao and Benbasat 2007]. Questions of the user experience of recommender systems— the HCI perspective—and the impact of recommenders on users—the Information Systems perspective—seem to be underexplored compared to the large amount of algorithmic research that we observe today. More research often seems also required regarding the idiosyncrasies of particular application domains of recommender systems, acknowledging that there is no “best” application-independent algorithm and that the success of a recommender can only be evaluated relative to its intended purpose. As a result, an algorithm that works well for predicting movie relevance scores based on long-term user preferences might not be suited well for the recommendation of possible short-lived news articles or the context-dependent recommendation of music or points-of-interest in the tourism domain. Various applications of recommender systems are discussed in more depth in the Recommender Systems Handbook [Ricci et al. 2022].

In the same handbook, a variety of other timely research topics in recommender systems are discussed, which were not considered here, including the recent algorithmic developments, context-awareness, attacks on recommender systems, privacy aspects, or human factors in the evaluation process. In the remainder of this paper, we would like to discuss a small set of additional promising topics of which we believe that they require more research in the future.

#### 2.4.2.1. Conversational Recommender Systems.

Most of today’s online recommender system have a rather simple user interaction model. At defined places in the application, the system presents users with recommendations, and often the only available user action is to inspect or accept these recommendations.5 With the recent developments in natural language processing, the current progress in machine learning in general, and the increased spread of voice-enabled devices, more interactive forms of information retrieval and recommendation [Radlinski and Craswell 2017] have received renewed interest in recent years. One vision of such conversational recommender systems (CRS) is that they are able to act like a human and engage in a “natural” conversation with their users, see [Jannach et al. 2021] for a survey. Correspondingly, such future systems will be able to support a variety of user intents [Cai and Chen 2019], thus supporting interactive preference elicitation and revision, explanation, and also chit-chat. While historically such CRS were often built based on engineered knowledge, e.g., about possible dialog states, many of today’s approaches also consider “end-to-end learning” as their main approach, where a machine learning model is trained on larger collections of dialogues between humans. Considerable progress was made in that context in recent years. Still, pure learning-based approaches still have their limitations, and it is expected that future CRS will be often based on a mix of explicit domain and inference knowledge and of learning components.

#### 2.4.2.2. Fairness in Recommender Systems.

Fairness and ethical concerns have grown around the exponential development of artificial intelligence and technology, permeating and transforming all realms of modern societies—and recommendation technologies are no exception to these matters. Recommendation functionalities can determine the success of a music artist on Spotify, a seller on Amazon, a job candidate or a recruiter through LinkedIn, a research author on Google Scholar, a hotel owner on Booking.com, an Internet celebrity on YouTube, Instagram or TikTok, a political party or a world view or a value system through online news and online media. As mediators between providers and consumers, recommender systems, willingly or not, get laded with a burden of responsibility they can hardly let go of.

Fairness, and more generally, responsible AI, has indeed become a pressing issue in recommender systems research that finds reflection in the recent research literature [Mehrotra et al. 2018a], keynote talks [Baeza-Yates 2020], new dedicated research outlets [Elish et al. 2021], and public debate. Metrics are being proposed to measure fairness, and algorithms and enhancements are being researched to avoid unfair treatment of groups [Yao and Huang 2017], minorities or individuals, as both providers and consumers of recommended choices [Patro et al. 2020]. A first realization with the onset of this new research area is the complexity of fairness as a concept and the difficulty of achieving universal solutions. Studies are unanimous in stressing the importance of bias awareness and understanding as a key step (and possibly a considerable part of the solution) to prevent or mitigate unfairness. An ample strand of reflection and research efforts can be expected to continue in the times to come.

#### 2.4.2.3. Offline / Online Misalignment in Evaluation.

As mentioned in Section 1.4.1, offline evaluation results are often weakly aligned with the outcomes of online A/B testing [Garcin et al. 2014, Gomez-Uribe and Hunt 2015, Jannach and Jugovac 2019]. Known causes may partially account for this, such as a metric mismatch, bias in the offline data, or interference from external components when the evaluated techniques are integrated in complex, multistep recommendation pipelines usually involved in industrial-scale applications [Amatriain and Basilico 2015]. But a more complete explanation seems not to have been reached, and the problem may not have been fully understood yet. Important efforts are being made in this direction from different angles, as discussed in earlier sections [Gilotte et al. 2018, Jannach and Bauer 2020]. Improvements in bringing offline evaluation closer to a measure of the final effect of recommendation would have a great positive impact in the future development of recommendation technologies.
