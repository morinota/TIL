## 0.1. link

- https://paperswithcode.com/paper/recommender-systems-a-primer
- [pdf](https://arxiv.org/pdf/2302.02579v1.pdf)

## 0.2. title

Recommender Systems: A Primer

## 0.3. abstract

Personalized recommendations have become a common feature of modern online services, including most major e-commerce sites, media platforms and social networks. Today, due to their high practical relevance, research in the area of recommender systems is flourishing more than ever. However, with the new application scenarios of recommender systems that we observe today, constantly new challenges arise as well, both in terms of algorithmic requirements and with respect to the evaluation of such systems. In this paper, we first provide an overview of the traditional formulation of the recommendation problem. We then review the classical algorithmic paradigms for item retrieval and ranking and elaborate how such systems can be evaluated. Afterwards, we discuss a number of recent developments in recommender systems research, including research on session-based recommendation, biases in recommender systems, and questions regarding the impact and value of recommender systems in practice.

# 1. Basic Concepts

## 1.1. Introduction

Automated and often personalized recommendations are omnipresent in today’s online world. Nowadays, whenever we go online, there is a good chance that we will very soon receive recommendations about more things to shop, trending apps to download, or new music or artists to discover. These personalized suggestions are provided to us by recommender systems, which are software components that determine—based on statistics and machine learning models—the most suitable items that should be presented to an individual user. Due to their widespread use in practice and their demonstrated capabilities of creating value both for consumers and businesses, recommender systems can be seen as one of the most visible success stories of artificial intelligence. Today, research in this area is flourishing more than ever, both due to the practical relevance of such systems and due to the ongoing rapid developments in machine learning.

Historically, the area of recommender systems has various relationships to the field of information retrieval (IR). The problem of retrieving and ranking a set of items from a larger collection, for example, is central both in recommendation and search tasks. The two areas furthermore oftentimes rely on similar algorithms and machine learning models and they use the same or similar evaluation methodology to assess the performance of these models. Recommendation is however also different from IR in a number of ways. The selection of items in recommendation scenarios, for example, is typically based on individual user profiles and not on interactive queries. From an IR perspective, one way to look at recommendation therefore is to see it as a problem setting where the item retrieval is based on an implicit query—which may also depend on the user’s context—rather than on explicit queries. Moreover, recommendation is more often a push communication, whereas IR systems are typically reactive applications.

The roots of information retrieval can be traced to as far back as the 1950s [Sanderson and Croft 2012]. The foundations of modern-day recommender systems, in contrast, were mainly laid in the 1990s. In 1992, Tapestry, a prototype of a personalized email filtering system, was developed at Xerox PARC [Goldberg et al. 1992]. One main innovative idea of this system was to leverage the opinion of others (ratings) in the filtering process, and the term collaborative filtering (CF) was popularized with this system. Soon later, several research groups worked on automating Tapestry’s rule-based approach, and in 1994, the highly influential GroupLens system was proposed [Resnick et al. 1994]. The main idea in this system was to automatically determine like-minded “neighbors” of a user and to recommend those news items to a user that these neighbors have rated highly. In the context of this early work, the recommendation problem was operationalized as a “matrix filling” problem, where the input to a recommendation algorithm is a user-item rating matrix and the goal is to predict the missing entries in this matrix. This research operationalization and the use of collaborative filtering techniques is still predominant in today’s research and practice. Over the last almost three decades, however, many more elaborate machine learning algorithms were designed or applied for recommendation problems, including various matrix factorization techniques— which were also used first in the 1990s [Billsus and Pazzani 1998]—and, most recently, sophisticated deep learning techniques, e.g., [Liang et al. 2018].

An alternative to using collaborative preference signals is to rank and filter documents in a personalized way by looking at their content. The corresponding content-based filtering techniques were also investigated in the 1990s, often under the terms information filtering or personalized information filtering [Foltz and Dumais 1992]. These systems are often largely based on known IR techniques like TF-IDF encodings1 of the documents. In pure contentbased recommender systems, the main idea is to suggest items to the users that are similar to those that they liked in the past, i.e., without considering the opinions of others. Nowadays, however, we very often observe that hybrid systems are used in practice that combine different recommendation techniques and types of data. An early example of such a system is Fab [Balabanovic and Shoham 1997], a content-based collaborative system designed for web page ´ recommendation at Stanford.

Most of the early systems discussed here were developed as research prototypes and mostly used in academic settings. However, already before the end of the 1990s a number of successful deployments of recommendation techniques were reported for domains such as of e-commerce or music, see, e.g., [Schafer et al. 1999]. Later on, Amazon was probably one of the first organizations that relied on recommendation technology at scale [Linden et al. 2003]. Nowadays, as mentioned above, there are many online services where recommendations are central to the user experience, e.g., at Netflix [Gomez-Uribe and Hunt 2015], Spotify [Semerci et al. 2019], or YouTube [Covington et al. 2016]. Over the years, also a number of success stories can be found in the literature, where the potential business value of recommender systems is documented, see [Jannach and Jugovac 2019] for an overview.

Overall, the field has reached a certain level of maturity and standardization, in particular with respect to the operationalization of the research problem and the evaluation methodology. However, the recommendation problem is far from being solved, as there is a constant stream of new application scenarios that have not been adequately addressed so far in the research community. This paper reflects this situation and consists of two main parts. In the first part, in Sections 1.2 to 1.4, we provide an overview on the basic concepts of recommender systems and how recommender systems are commonly evaluated. Afterwards, from Section 2.1 on, we discuss a number of current and future topics in recommender systems research.

## 1.2. The Recommendation Task

The main computational task2 of any recommender system is to determine which items to show to a user in a given situation. Therefore, in any individual application setting, before an algorithm is implemented and evaluated, the question has to be answered based on which criteria the items should be selected and ranked. At the most general level, any recommender system is designed to create a certain value or utility for one or more of the involved stakeholders [Jannach and Adomavicius 2016]. In practice, there are many ways of how value can be created. From the perspective of a consumer, for example, a recommender system is usually assumed to reduce problems of information overload. From the perspective of the provider, on the other hand, providing personalized recommendations can help to increase business-related key-performance indicators (KPIs) such as sales numbers or customer retention. We will discuss related questions of the impact and the value of recommender systems in more depth later in Section 2.3.

Academic research often aims to abstract from the specifics of individual domains, particular applications, or business models of recommendation providers. The main computational task is therefore often framed in a correspondingly abstract way, usually to compute an estimate of the absolute or relative relevance of individual items for a given user. On a general level, this problem can be formalized as follows, see [Adomavicius and Tuzhilin 2005]. Given

- a set of users U,
- a set of items I and
- a utility function $f :U\times I -> R$, which maps user and items to a utility value taken from a totally ordered set R (e.g., of nonnegative real numbers),

recommend the item $i' \in I$, which maximizes the utility function, more formally:

$$
\tag{1.1}
$$

Typically, we are interested in recommending more than one item. This can be done by returning those N items that have the highest utility values. Using this definition, algorithmic research in recommender systems (see Section 1.3) amounts to defining or learning the utility function f , where f can be based on different types of additional information.

In the collaborative-filtering GroupLens system from 1994 mentioned earlier and in countless subsequent works, for example, the utility function f was designed to return a rating prediction. The additional information that is used within f was the user-item matrix of known ratings. Table 1.1 shows an example of such a rating matrix, where the specific task that is highlighted in the table is to make a recommendation for user u1. This is accomplished by predicting the ratings of user u1 for the so far unrated items i4 and i5.

The specific implementation of f in the GroupLens system was a nearest-neighbor method. In pure content-based recommendation approaches, in contrast, f is not based on the useritem rating matrix, but on the observed past behavior of an individual user and additional information about the items in I. In hybrid approaches, finally, the implementation of f might simultaneously consider the rating matrix, item meta-data, and other sources of information including demographics, various other types of observed user behavior beyond past ratings, or the users’ embedding in a social network, see also Section 1.3.6.

Collaborative filtering is the most frequently addressed problem setting in the literature and a large body of the literature aims at learning f from the usually noisy data in the user-item matrix. Correspondingly, all sorts of machine learning algorithms were applied over the years to learn f . Historically, the user-item rating matrix was considered to contain explicit, userspecified item ratings. Nowadays, research is more focused on situations where only implicit user feedback is available. In such cases, the entries in the rating matrix are not values ranging, e.g., from one to five stars, but unary. A positive value, usually represented as a 1, in the matrix therefore expresses that a user has for example viewed or purchased an item in the past. Most commonly, implicit feedback datasets do not contain negative feedback values, i.e., a value is either 1 or it is missing. Clearly, a past purchase not necessarily means that a user has liked a certain item, and implicit feedback, i.e., observed past user behavior, can be more noisy than explicit item ratings.

In principle, all sorts of information can be incorporated in the utility function f . One piece of information that received special interest in the literature is that of context, which refers to the particular situation in which a recommendation is made. The current context can have a major impact on the usefulness of a particular recommendation. One restaurant might, for example, be a good recommendation during summer but not in winter. Considering the time of the year as context may therefore be crucial. Overall, the difference to other types of side information is that the utility of an item i can be different for a specific user u, depending on the current situation. One proposal therefore is to extend the signature of f accordingly, to make this aspect explicit [Adomavicius et al. 2022]. Correspondingly, we may have

$$
f : U \times I \times C \rightarrow R
$$

as an extended utility function, where C denotes the context in which a recommendation should be made.

Generally, utility functions that make individual relevance predictions—with or without considering context—are widely used in the literature. Nowadays, however, predicting individual ratings on an absolute scale, as was done in the Netflix Prize competition [Bennett et al. 2007], is not considered the most relevant problem in practice anymore. Instead, more focus is put on creating ranked lists of suggestions, leading to the top-N recommendation task. Technically, one can use the same algorithms that were designed for rating prediction problems, and rank the items based on the predicted rating. Alternatively, learning-to-rank algorithms can be used, which do not consider the recommendable items individually, but directly aim to optimize the ranking.

As a result of this changed problem setting—creating a top-N list of items instead of making point-wise predictions—an alternative utility function f of the form

$$
f: U \times L* \rightarrow R
$$

can be defined, where L ∗ is the set of all permutations up to the length of N of the powerset of I, see also [Quadrana et al. 2018]. Given this function, we then recommend the list of items that maximizes the utility value R. Correspondingly, the problem when designing an algorithm is to define or learn a function that predicts the utility of length-restricted ranked lists of items.

Regarding the concept of utility, note that we so far have not made any assumptions regarding how utility should be defined or measured. In the past, starting with the GroupLens system, ratings were considered as proxies for utility. The goal was to predict how a user would rate a yet unseen item and to then recommend the items with the highest predicted ratings, assuming that these items are the most useful ones. Note, however, that our definitions are not generally limited to consumer value, and a utility function might consider the provider profit as well, and thus consider the value perspective of more than one stakeholder, see also [Abdollahpouri et al. 2020a, Mehrotra et al. 2018b].

In traditional settings, mostly utility functions of the form $f : U×I \rightarrow R$ were considered, where each item’s utility is considered independently from other items that might be shown to a user in a single recommendation list. Such an approach however does not allow us to assess the quality of top-N item lists as a whole, e.g., in terms of their diversity. Therefore, utility functions—and corresponding evaluation metrics—are nowadays commonly used that consider more than point-wise utility estimates, see also Section 1.4 on the evaluation of recommender systems.

## 1.3. Recommendation Algorithms

The development of recommendation algorithms has naturally mirrored the evolution of the task definition, hand in hand with the design of evaluation procedures and metrics (which we discuss later in Section 1.4) suited to the task. Recommendation can be addressed, in essence, as a supervised learning problem: given examples of observed user choices, we aim to predict present or future (yet unobserved) user interests. Variations in the task formulation give rise to different algorithmic approaches—and different metrics are appropriate to evaluate for different tasks.

As a machine learning problem, recommendation is quite unique. What makes recommendation singular in this field is, in essence, the human factor at the core of recommendation tasks. In these tasks, both the input signal and the prediction target consist of or involve user behavior at their core. This brings about a specific level of complexity compared to, for instance, recognizing shapes in an image or diagnosing a medical condition from medical tests. Furthermore, recommendation is often not just about predicting people’s actions, but about enhancing (and hence changing) such actions by bringing awareness about potentially better choices. What makes a recommendation good (and therefore the algorithm that produces it) thus involves a great deal of subjectivity and is a challenging question, that we discuss later in Section 1.4.

Recommendation algorithms have been traditionally classified into collaborative and content-based [Adomavicius et al. 2022]. The latter follow the principle that people’s tastes are related to inherent item characteristics and tend to persist over time. The former build, in a myriad different ways, on the presumed existence of regularities and structure in the distribution of user preferences and choices over the user-item space. For instance, some users may have non-random similarities to other users in their personal interests. Again, such structures are assumed to persist over time.

Different algorithmic approaches have different strengths and weaknesses. Robust and effective recommender systems therefore combine elements or entire components from different such approaches. In an aim to make algorithm categorization exhaustive, such compound solutions have been often referred to as hybrid systems. As with many machine learning or information retrieval systems, production recommender systems typically combine a core initial (commonly hybrid) algorithm that produces base item rankings, followed by post-processing algorithms, business rules (e.g. implementing marketing constraints) and fine enhancements to further improve the final recommendation ranking quality and optimizing for additional objectives [Amatriain and Basilico 2015, Covington et al. 2016].

### 1.3.1. Recommendation as a Machine Learning Problem

Modern recommendation algorithms tend to be formulated explicitly as a supervised machine learning task: a utility function f (in some of the aforementioned forms in the previous section) is learnt that minimizes a cost function, which involves the utility function within its definition. Cost can be, for instance, the sum of squared prediction error over available training ratings (the latter playing the part of labels in supervised machine learning terminology), or the binary cross-entropy loss of a sigmoid function, or the amount (measured through some monotonic function) of item pairs incorrectly ranked by the utility function to be learned. In this formulation, recommendation algorithms are distinguished from each other in the form the utility function takes (a parameterized model), the cost function, and the procedure to minimize the latter.

An example utility function is the dot product of user and item embedding vectors3 , where the coordinates of such vectors are the parameters of the model to be learned, as we shall see in Section 1.3.3.2 below. The function therefore involves some arbitrary choice (of a family of functions often referred to as the hypothesis space), and learning the function typically means learning its parameters (a “model”) by fitting them to the training data (through minimization of the cost function on training labels). Some algorithms however do not express an explicit cost function, and the utility function is defined heuristically rather than learned. This is the case, for instance, of the common nearest-neighbors collaborative filtering formulation that we overview later in Section 1.3.3.1. In either case, the utility function has hyperparameters that need to be optimized, usually taking a separate validation subset out of the training data. The ultimate goal of all algorithms is to maximize some final metric(s) of interest, such as precision, recall, or revenue—this is why some algorithms take the final metric, or a more tractable approximation thereof, as the objective function to be optimized.

Next, we provide an overview of the main algorithmic approaches and principles in the recommendation field. We start by briefly discussing different types of input data for recommendation, which determine broad differences in the algorithmic approaches developed thereupon. Each algorithmic family may deserve a paper (or an entire book) by itself. Therefore, we will provide only a broad overview here and focus on some main highlights.

### 1.3.2. Characterizing Approaches Based on their Input Data

Recommendation algorithms can be based on input data of different nature. Probably the most common data source for recommendation—and we might add, the one enabling the most powerful approaches—are records of logged user-item interactions: the rating matrix, as already discussed. When this is the only input of an algorithm we say the algorithm follows a collaborative filtering approach [Adomavicius and Tuzhilin 2005]. Collaborative filtering is based on the abstract principle that people can benefit from the experience and discoveries of other people, and not just their own, in making future choices. The simplicity of this principle and the high potential of collective data as a source for prediction are also the Achilles heel of collaborative filtering as a pure approach: where the interaction matrix is sparse, the algorithm struggles to produce reliable predictions, due to a lack of sufficient input information. Collaborative techniques may fail to deliver proper recommendations in cold start situations where little interaction has yet been recorded, as is the case at the early stages of a new deployed system, or whenever a new user or a new item enter the system. However, when sufficient interaction records are available, collaborative filtering is one of the most effective and powerful approaches.

All other input that a recommendation algorithm can take beyond user-item interactions is often referred to as side information, see Section 1.3.6. Using such information is a good complement to collaborative filtering, particularly in sparse regions of the user-item matrix. A most common type of side information is any available data and metadata directly associated to the items, to which we may refer as item features: taxonomic classifications (e.g., movie genre, music style, online store section), tags, free text associated to the items (e.g., product descriptions and reviews), etc. Algorithms that solely use such data are referred to as being content-based [Pazzani and Billsus 2007]. Instead of turning to other users for hints on what a customer might like, pure content-based approaches just focus on one user at a time, considering the interaction records of the target user4—in particular, the features of the items the user engaged with, and the features of the items to be recommended. Several other information sources can be effectively exploited by a recommender system to enhance the effectiveness and value of recommendation, such as social interactions between users [Guy 2022], data about the user (e.g., demographic information), contextual information (e.g., user and/or item geolocation, session, time, device, etc.).

When substantial interaction data is available, collaborative filtering can be more effective than other approaches as a base algorithmic core, not just in terms of delivering accurate predictions, but also in providing users with rich options beyond their individual prior experience [Vargas and Castells 2011]. These algorithms are moreover fairly general and domainagnostic, as they make no assumption about what the items are.

### 1.3.3. Collaborative Filtering

#### 1.3.3.1. Nearest Neighbors

The earliest collaborative algorithms were inspired by a typical word-of-mouth human behavior where a person takes advice from trusted friends when making a decision—one of the main sources for guidance when people make decisions and choices. Following this metaphor, socalled k-nearest-neighbor (kNN) algorithms suggest choices defined as the weighted average of the advice of the target user’s friends.

Following this heuristic intuition, recommendation may be viewed as a regression problem, where a kNN algorithm predicts the level of preference of a user u for an item i as a linear combination of the observed preferences for i in a subset of selected users: the target user’s neighbors N[u]. Following the notation in the previous section, items are ranked by the following utility function:

$$
\tag{1.2}
$$

where Ci is a normalizing term, and r(v,i) 6= 0/ means that interaction data (a ‘rating’) is available involving v and i. Typical definitions for the weights in the linear combination are based on pairwise user similarity wu,v = sim(u,s), where the similarity function can be, for instance, cosine, or Pearson correlation between the rating vectors of users u and v. The standard definitions of those measures need to be further particularized to deal with missing values in the user vectors. For instance, when using cosine similarity, missing ratings may be assigned a value of zero.

The neighborhood N[u] is usually picked as the top k most similar users to u, where k becomes a hyperparameter of the algorithm. Neighborhood selection is nonetheless a modular operation that has been the object of multiple variations and enhancements. When recommendation was addressed as rating prediction, the kNN score in Equation 1.2 above was normalized by Ci = 1/∑v∈N[u]:r(v,i)6=0/ |sim(u, v)|, in order to produce values in the rating scale. When evaluated as a ranking task, omitting normalization (i.e. Ci = 1) has often been found to produce better results [Cremonesi et al. 2010].

Multiple further variations of the kNN scheme arise from here, such as the widely used item-based version (which roughly swaps the role of users and items), multiple similarity functions, different neighbor selection approaches, and so forth. Though kNN has been developed as a heuristic scheme, Canamares and Castells [Ca ˜ namares and Castells 2017] showed ˜ that kNN can be given a probabilistic formalization where the ranking function is derived from an objective function to be maximized: the expected number of recommended items the user will like. For a more comprehensive review of kNN variants see [Nikolakopoulos et al. 2022].

kNN is an old idea [Linden et al. 2003] yet it remains a competitive approach today [Ferrari Dacrema et al. 2021, Ludewig et al. 2021]. It is very easy to understand, simple to implement, and computationally inexpensive compared to other recent, more complex approaches. As such, it is an advisable reference baseline to include in experimental recommender systems research [Canamares and Castells 2018, Cremonesi et al. 2010].

#### 1.3.3.2. Matrix Factorization

By the mid 2000s matrix factorization became popular in the field and very soon became the preferred collaborative filtering approach, due to its empirical effectiveness, relative simplicity, and flexibility as a framework enabling multiple algorithmic developments and variations. Matrix factorization is based on the assumption that the interests of users can be described in a low-dimensional space of latent factors that synthesize the subjacent properties of items that determine why a person may like them [Koren et al. 2009]. In this perspective, users and items are assumed to be describable as vectors in a common latent factor vector space, in such a way that users with similar tastes would have similar factor vectors, and so would items that please similar users. These low-dimensional vectors have come to be referred to as embeddings in the recent literature, making a connection to similar techniques in fields such as natural language processing and information retrieval [Mikolov et al. 2013, Pennington et al. 2014].

The latent factors do not necessarily correspond to actual properties of items or user traits as we would probably represent them in our human understanding. They are handled as abstract dimensions for which users and items are given values by an algorithmic approach, and are usually not interpretable by eye inspection or a clear natural intuition. Matrix factorization algorithms only require deciding how many factors we wish consider—which becomes a hyperparameter of the approach—but not what these factors are, other than axes of a multidimensional vector space.

Formally, for a k-dimensional factor space, users u ∈ U and items i ∈ I are represented by vectors pu ∈ Rk , qi ∈ Rk , where the coordinates pu,z ∈ R, pi,z ∈ R represent how interested user u is in factor z, and how much of factor z is present in item i, respectively—the higher the z coordinate value, the more the user interests are about this factor, and the more important the factor is in the nature of the item. Based on this representation, an additional assumption is made: that the interest of a user u for an item i can be captured by the dot product of their latent vectors—following our utility function notation, f(u,i) = q t i pu.

The common approach to produce the latent vectors is by minimization of a cost function R (also referred to as risk or expected loss in the empirical risk minimization machine learning framework [Vapnik 1998]), which generally involves the error in predicting the training data with some regularization. For instance, a basic formulation would be:

$$
\tag{1.3}
$$

The term added to the squared difference is a common regularization factor to avoid overfitting, where k · k represents the L2 norm, and λ is a hyperparameter that is tuned by cross validation. The minimization is often solved for by stochastic gradient descent or alternating least squares, resulting in easy implementations [Koren et al. 2009]. Once the latent vectors are learned by the algorithm, the dot product f(u,i) = q t i pu is used as the ranking function for recommendation.

The summation in the cost function can be limited to the user-item pairs for which training data are available, but it is common to extend the sum to all or a sample subset of unobserved user-item preferences [Koren et al. 2009]. In the latter case, a value needs to be imputed to the missing preference observations, which can be done in different ways (such as a constant parameter value or a randomized value). The error term in Equation 1.3 can be weighted differently for every user-item pair, often representing confidence in the corresponding (observed or imputed) preference data point. Most usually, two different weight values are used for pairs having and not having training data [Hu et al. 2008]. The combination of error weighting and missing value imputation reflect different nuances in the recommendation task and result in different algorithmic behavior (see e.g. [Steck 2013]). Besides these configuration options, matrix factorization has been rich in a myriad of further variations, such as summing item, user and global bias terms as additional parameters in the rating representation q t i pu [Koren et al. 2009], temporal awareness [Koren 2009], probabilistic formulations [Hofmann 2004, Salakhutdinov and Mnih 2007], and many other elaborations. In particular, variations in the cost function have given rise to entire new approaches, which we summarize next.

### 1.3.4. Learning to Rank

With the realization that the effectiveness of recommendation in real scenarios relies more on item ranking than on point-wise rating prediction, a shift emerged in the field towards addressing recommendation as a learning to rank (LTR) problem, drawing from a similar earlier trend in the information retrieval field [Joachims 2002]. Broadly speaking, LTR involves the introduction of loss functions that explicitly involve item ordering rather than rating error or interaction prediction error [Rendle et al. 2009]. In different possible ways, the cost functions typically represent the amount of “contradiction” between the rankings produced by the model to be learned, and how items should be ranked according to the training preference values. In other words, the model is trained to maximize order-wise agreement with the available observations. Most methods take a pairwise approach, where the “ranking error” is defined in terms of incorrectly ranked item pairs:

$$
R(\theta) = \sum_{u} \sum_i \sum_j l(u,i,j|\theta)
$$

where θ are the parameters of the recommendation model, and the loss $l(u,i, j|θ)$ quantifies how much in contradiction to the available observations the model θ ranks i and j for u (and $l$ involves the utility function f—the model—to be learned). This is summed over all pairs of items i, j that u has available training data for.

A representative and effective example in this area is Bayesian Personalized Ranking (BPR) [Rendle et al. 2009], which has become a common reference and baseline in the literature. Ranking inconsistency in BPR is expressed and developed in terms of the probability that the scores predicted by the learned model θ rank pairs of items in contradiction to the training rating data: essentially, and omitting details, $l(u,i, j) = 1_{r(u,i)>r(u, j)}P(\text{j is ranked above i for u}|θ)P(θ)$. The probability of ranking precedence is smoothed (for differentiability) as a logistic sigmoid of the ranking score difference.

For instance, for a matrix factorization model where θ consists of the latent vectors $p_u$, $q_i$ for $(u,i) \in U \times I$, and the utility scoring function is $f(u,i) = q^t_i p_u$ , we get $P(\text{j is ranked above i for u}|p,q) := 1/(1 + e^{q^t_j p_u − q^t_i p_u})$, and under normality and factor independence assumptions, the prior $P(p,q)$ results in a typical L2 regularization term. The resulting cost function can then be minimized by stochastic gradient descent. BPR can thus be seen as a layer on top of matrix factorization and a means to train the latent vectors, alternative to point-wise (i.e., user-item-wise) cost minimization.

Many alternatives to such a scheme have been proposed on a similar principle. For instance, also in a pairwise approach, RankALS [Takacs and Tikk 2012] essentially takes $l(u,i, j) = ((r(u,i) − r(u, j)) − (q^t_i p_u − q^t_j p_u))^2$ as the core pairwise ordering error to be minimized, and alternating least squares instead of gradient descent as the minimization procedure. Other approaches introduce a target ranking evaluation metric—such as nDCG, Mean Average Precision (MAP) or Mean Reciprocal Rank (MRR)—in the objective function, i.e., the objective approximates how much is lost in the metric by a suboptimal item ordering. For instance, CLiMF [Shi et al. 2012] defines the objective function as a smooth lowerbound approximation of MRR. These methods are referred to as listwise in the LTR literature [Liu 2009], because even though the loss function is still often evolved into pairwise form, it reflects a ranking goodness function (the target evaluation metric) rather than a metricagnostic item pair classification error in a binary order.

### 1.3.5. Neural Recommendation

While matrix factorization assumes the dot product of latent vectors as the scoring function, one may consider more complex, not necessarily linear5 utility scoring for recommendation. One such possibility is to use neural networks as a particular family of non-linear functions that have well-studied properties and training techniques [He et al. 2017, Zhang et al. 2022]. After revolutionizing machine learning application domains such as computer vision, speech recognition, or natural language processing, deep learning has gained blazing popularity in recent years as a basis for devising a wide variety of recommendation approaches. A comprehensive coverage of this area exceeds the scope of this paper—we provide instead a summarized overview; the reader is referred to e.g., [Zhang et al. 2022] for a wider survey.

Several technical and strategic reasons motivate an optimistic prospect for deep learning as an approach to recommendation: the ability to approximate any prediction function; efficient training (in comparison to other machine learning approaches at comparable complexity); smooth integration of heterogeneous data sources (including unstructured content), data dimensions and predictive signals; feature engineering effort savings; proved empirical effectiveness in other machine learning domains; and a growing availability of software resources and knowledge derived from a thriving activity in deep learning in many domains and fields. Furthermore, as universal function approximators, neural networks can be seen as a smooth generalization of virtually any simpler model (such as dot-product in matrix factorization as described earlier). From this perspective, a neural vs. non-neural dichotomy may become moot, strictly speaking. “Depth” in deep learning suggests however that the opportunity to grow and handle complexity is actually being leveraged by “bigger” layered models.

Neural networks provide arbitrarily high expressive power to capture complex relations in the user-item space, that matrix factor models may fail to grasp. At the top layer (the output layer), the loss function can involve a pointwise (rating or binary prediction error) [He et al. 2017] or pairwise (ranking error) element [Song et al. 2018] similar to other matrix factorization and LTR approaches discussed earlier. The generality of neural architectures further enables the smooth integration of side information in addition to user-item interaction. We discuss the use of side-information in recommendation more generally later in Section 1.3.6.3. The elaborations and combinations that can be developed in this area are as wide as imagination can afford. A huge variety of complex network architectures have been proposed in the literature using network structures such as autoencoders [Liang et al. 2018], convolutional neural networks [Tang and Wang 2018, Yuan et al. 2019], recurrent neural networks [Hidasi and Karatzoglou 2018, Wu et al. 2017], and attention layers [Kang and McAuley 2018], to name just a few. Particular architectures have been developed for specific tasks, such as session-based recommendation discussed later in Section 2.1. Interestingly, the best performance in recommendation is sometimes achieved with shallower neural models compared other domains [Anelli et al. 2022, Steck 2019].

Deep learning has proved to be empirically effective in recommendation, and steps have been taken towards its adoption in industry [Covington et al. 2016, Okura et al. 2017, Wu and Grbovic 2020]. A certain hype in this domain may have also induced a degree of noise and haste in early empirical analyses that sometimes make it difficult to get a precise perception of the actual effectiveness of neural approaches, or any conditions for which they may be best suited [Ferrari Dacrema et al. 2021, Garg et al. 2019, Kouki et al. 2020, Ludewig et al. 2021, Rendle et al. 2020, Steck 2019]. Clear effectiveness improvements have been found in specific tasks such as sequential recommendation [Sun et al. 2019b], and for specific conditions when, for instance, side information or massive data are available.

Deep learning can often also simplify and reduce feature engineering efforts. See [Steck et al. 2021] for a discussion of deep learning for recommendations at Netflix. Deep learning faces similar challenges in recommendation as it does in other domains: a large number of hyperparameters to tune, a heavy training cost tradeoff to reach the effectiveness potential, the need for massive data availability, and a black box nature involving interpretability and explainability challenges [Afchar and Hennequin 2020]. As a result, the cost and complexity of neural approaches may not necessarily always pay off in enhanced effectiveness as a universal solution for all recommendation settings and problems [Ferrari Dacrema et al. 2021, Rendle et al. 2020]. On the other hand, the data richness and volume often required to make the best of deep recommendation architectures is not always within reach of research outside corporate boundaries. Neural recommendation is nonetheless a thriving area where a profuse stream of work is being published in top tier research outlets as we speak, and industry is striving to leverage its potential [Cen et al. 2020, Covington et al. 2016, Feng et al. 2020, Steck et al. 2021]. We suggest the reader to follow through recent literature and surveys [Zhang et al. 2022] and to stay tuned to ongoing developments and future potential breakthroughs.

### 1.3.6. Content-based and Hybrid Recommender Systems

Collaborative filtering (CF) methods, as discussed in the previous section, are highly effective in many application scenarios due to their ability to detect and utilize preference or behavior patterns in a user community. Furthermore, they are particularly good at helping users to discover types of content that were previously unknown to them, e.g., by considering preferences of like-minded users. One intriguing aspect of CF methods is that they are able to accomplish this even without knowing anything about the items themselves, for example, an item’s category in an e-commerce store.

However, in cases where we dispose of such knowledge about the items and a user’s preference towards such individual item properties, it may of course make sense to consider it in the recommendation processes. For example, if we know that a user usually prefers certain movie genres or types of news, it is only intuitive to recommend more content of the same type. Moreover, in case we do not have logged transaction or rating data yet, i.e., if the user has not rated many items yet (user cold-start) or if there are new items in the catalog without purchase history (item cold-start), collaborative filtering methods may not work well.

#### 1.3.6.1. Pure Content-based Systems

In such cases, when there is little or no collaborative information available, one option is to match user-individual past preferences with knowledge about item features. Historically, this process is called content-based filtering6 . Technically, many early content-based filtering methods were based on ideas and techniques from information retrieval. Both in recommendation and retrieval scenarios, the goal is to identify and rank a set of relevant documents. While in the IR case the starting point is a (search) query, the retrieval process in a recommender system is based on a content-based user profile.

On a general level, content-based recommendation can be characterized as follows [Adomavicius and Tuzhilin 2005]:

$$
\tag{1.4}
$$

where the predicted relevance f for a given user u and item i is based on a scoring function score which matches a content-based profile of u with the content features of an item i.

Various ways of implementing the details of such a content-based approach are possible. In case where textual descriptions of an item i are available, a traditional way would be to represent the items using a TF-IDF (Term-Frequency Inverse Document Frequency) encoding. In such an implementation, the function Content(i) would return a vector of real values, where each element of the vector corresponds to a term (word), and the value indicates the importance of the term. The importance values in a TF-IDF encoding are determined by multiplying two relatively simple counting statistics, TF and IDF. The term frequency TF corresponds to the number of times a word appears in the given document i, usually normalized to account for different document lengths. The Inverse Document Frequency IDF in contrast indicates how “informative” a term is in the given document collection. The underlying logic is that if there is a term that appears in almost all documents, it carries little information to distinguish one document from each other.

The next question is how to represent the interest profile of a user u, i.e., how to implement the function ContentBasedProfile. A common goal is to use a representation that can be easily matched with the content representation of individual items. In the case of text documents, one could for example first encode all items which the user liked in the past with TF-IDF. Then, the ContentBasedProfile could be defined as being the average of the vectors of the liked items. Finally, now that the user profile and the items are represented in a compatible way, the function score can for example be implemented through the cosine similarity function.

Various alternative implementations of the different functions are of course possible. The options range from even more simple approaches, e.g., by counting overlapping features such as actors in a movie recommender systems, to more elaborate embeddings, which aim to better capture the semantics of the documents.7 The prediction function can of course be learned in a supervised approach, with input features based on item content information, and any sort of suitable shallow or deep model. Furthermore, a variety of additional pieces of information beyond the document itself were considered in the past decades to better understand the similarity or relations between different items. Besides item meta-data, various forms of exogenous information about the items, e.g., from Wikipedia, have been incorporated into content-based (or: semantics-aware) recommender systems, see [Musto et al. 2022] for an in-depth discussion.

In terms of application areas, pure content-based systems are often used when there is not sufficient collaborative information available. A typical use case is the recommendation of news, where we have to deal with a constant stream of new items, see [Kirshenbaum et al. 2012] for a case study from industry. Content-based systems have however also been applied in practice in other domains such as movies or mobile apps [Bambini et al. 2011, Jannach and Hegelich 2009], where the user preferences can be relatively stable, e.g., in terms of the preferred genres. A special use case of content-based methods are “similar item” recommendations, where the reference point to making a recommendation is not a user profile, but a specific item. Similar item recommendations are common on video streaming platforms, e.g., to recommend movies that are similar to the user just has watched, see [Trattner and Jannach 2019, Yao and Harper 2018].

#### 1.3.6.2. Hybrid Recommender Systems

Pure content-based systems can have certain limitations. An intrinsic feature of these systems is that they recommend “more of the same” by design, thus limiting the support for discovery of new types of content for users. In addition, content-based systems might surface content that is too niche. A movie recommender system that for example only considers the genre or plot descriptions may miss important quality aspects of a movie. It may thus return movies which are content-wise related to those that the user liked in the past, but in the end are not recommendations that the user will like.

A common solution to deal with such problems is to build hybrid systems, i.e., systems that not only rely on one single paradigm, e.g., collaborative filtering, but combine different approaches. This way, the shortcomings of individual methods should be avoided while the advantages are combined. In our example of content-based movie recommendations, one could apply some quality filters before the recommendations are returned, e.g., by removing all movies that have a low average community rating.

In the literature, a variety of ways of building hybrid recommender systems have been identified. In [Burke 2002], Burke identified seven such ways, which were later organized in three larger categories in [Jannach et al. 2010] as follows:

- Monolithic: In such a design, aspects of different recommendation strategies are implemented in one algorithm. A very common realization of such an approach is to design a machine learning model that includes both collaborative signals and content-based features (“side information”).
- Parallelized: In parallelized designs, the outcomes of two or more algorithms are first determined independently and then combined in some way. The combination could for example be done on the user interface level, where users are presented a list that contains personalized recommendations as well as recently trending items. Or, some weighted approach can be applied where each item receives a score from each recommender. An extreme case of such a weighting approach would be a switching hybrid, where the recommendations are always taken from one particular algorithm, depending on the situation.8
- Pipelined: In this design, one algorithm uses the outputs from another one as an input. In practice, one could for example retrieve a number of recommendation candidates based on popularity, and then only rank these items according to the assumed user preferences in a personalized way.9

#### 1.3.6.3. Collaborative Filtering with Side Information

Probably the most common hybrid approach is to enhance the power of collaborative filtering with various forms of side information. Such side information is however not limited to item-related information as in content-based filtering approaches. Wu et al. [Wu et al. 2022] categorize side information into the following categories of information that a recommender system can use in addition to the observed user-item interaction data. Figure 1.1 visualizes the different forms of information.

- User data: These are static or slowly changing features of the user, such as age, gender, or nationality. Historically, systems that leverage such information were called demographic recommender systems. In addition, researchers have also explored the consideration of personality traits in the recommendation process, see [Dhelim et al. 2022].
- Item data: These are aspects that are tied to specific items. Various forms of domain specific item-related features has been considered in the literature, including pre-structured ones like categories, unstructured ones like textual item descriptions, reviews or images, or community-provided semi-structured information in the form of tags.
- Context data: Various forms of comparably frequently changing context information have been explored in the literature, most importantly the geographical location of users, the time of the day, weather conditions or the end user device, see [Adomavicius et al. 2022].

In terms of the user-item interaction data, research has historically focused on ratings as the only form of interaction. In more recent years, recommending based on implicit feedback signals has been become predominant, see [Jannach et al. 2018] for a survey. In most published research works, only one type of implicit feedback is assumed, indicating, e.g., if a user has interacted with an item or not. In reality, multiple types of interactions are available and could be used. In an e-commerce shop, the available signals may included item views, add-to-cart-events, purchases, category navigation events, search actions and even later item returns etc. Research on leveraging combinations of such types of interaction data is somewhat limited today, probably due to the application specific nature of the interaction events. Moreover, in real-world interaction logs, information is often stored about the point in time when an interaction happened. With that information, time-aware recommender systems [Campos et al. 2014] and sequence-aware based approaches can be implemented, see also Section 2.1.

Finally, as mentioned above, various approaches exist in the literature that aim to incorporate exogenous information into the recommendation process. Examples of such information (including “world knowledge”) are Wikipedia articles, publicly accessible domain ontologies and knowledge graphs, or data that can be accessed or queried through Linked Open Data endpoints, see [Musto et al. 2022] for a discussion. Besides such types of information that refer mostly to the items, exogenous information that is tied to individual users has been explored in the literature as well, for instance, the users’ social network, their trust relationships, or reviews written by them, see [Chen et al. 2015, Dong et al. 2022, He and Chu 2010].

Recent surveys on technical approaches to build “information-rich” collaborative filtering systems can be found in [Wu et al. 2022]. and [Sun et al. 2019c]. In particular the latter work provides an overview of how various types of deep learning models incorporate side information of different types, including flat features, hierarchical features, knowledge graphs, image features and so forth.

### 1.3.7. Discussion

We have reviewed a selection of important algorithmic approaches to recommendation in this section. We emphasize that this is only a high-level rundown and the reader can find a myriad of other algorithms and variations in the literature. For instance, Rendle [2010, 2012] developed the factorization machines framework, generalizing several factor models including matrix factorization. Like neural approaches, factorization machines can smoothly leverage user and item side information in the framework for an integrated hybrid recommendation approach. Around that time and in the scope of linear models, Ning and Karypis [2011] proposed SLIM, a generalization of item-item collaborative filtering that, in a way, learns the similarity matrix as a regularized optimization problem akin to Equation 1.3 for matrix factorization in Section 1.3.3.2. This algorithm has become a common baseline in the recommender systems literature for its empirical effectiveness. Later on, Steck [2019] derived a closed form solution for computing a very similar model to SLIM. By removing the need for a costly iterative optimization procedure, the method is considerably more efficient.

Other algorithmic developments are targeted at specific recommendation scenarios such as sequential recommendation [Quadrana et al. 2018], discussed later in Section 2.1, and/or revised perspectives of the recommendation problem such as reinforcement learning [Li et al. 2016, Xin et al. 2022a], which we consider in Section 2.2.

Generally, despite the success of in particular of collaborative filtering approaches in practice, recommendation is far from a solved problem: we suggest the reader to retrospect on their own experience as a recommender system user, and form their own opinion. Many elements are involved in bringing the recommendation experience anywhere near, for instance, the effectiveness of modern search engines. Strictly speaking, this is not exactly possible—such a comparison is unfair, as search systems receive an explicit expression of intent from the user: the query. Competing against search would anyway misrepresent the purpose of recommendation: recommendation brings value in areas that search cannot solve, or for which search should not be needed. Be that as it may, room for improvement of current recommendation technology certainly remains, and different angles need to be addressed to achieve it, as we cover in the rest of this paper. The development of better algorithms is certainly one of them, and a major focus of attention and efforts in the field.

## 1.4. Evaluation of Recommender Systems

As discussed in the previous section, there is a myriad of choices available when one wants to deploy a recommender system, including a multitude of algorithms, their variants and specific configurations. Making an informed choice requires suitable and reliable evaluation methodologies. The design and selection of such reliable evaluation methodologies is therefore a central piece in recommendation technology development and research. At the same time, these evaluation methodologies drive algorithmic evolution in the field—as a fitness function, they shape the state of the art.

Evaluation generally involves a comparison between two or more recommender systems or variants. The most direct approach to compare systems in a production setting is online A/B testing, see for example [Garcin et al. 2014]. Online experiments are a finite resource though, they take time and involve a potential cost in user experience degradation by exposition to suboptimal system variants. Offline evaluation is therefore used as a complementary instrument to filter out which variants and change proposals are brought to more expensive online testing [Gruson et al. 2019, Gunawardana et al. 2022]. Offline evaluation enables, on the other hand, fast and safe hyperparameter exploration, and remains the main empirical resort widely available to academic research.

In the sections that follow we summarize the main lines of approach to recommender system evaluation. We briefly address online approaches (a perspective that will find further elaboration later in Section 2.3) to then focus on offline evaluation. More specific evaluation methodologies in the context of session-based recommendation are also discussed in Section 2.1.3.

### 1.4.1. Online Evaluation

Recommendation technologies as we experience them on Spotify, Netflix, Amazon, YouTube, Booking, Twitter, Facebook—you name it—have undergone a filtering funnel from the early conception of an algorithmic idea (perhaps in an academic research context) to the final production system [Amatriain and Basilico 2015]. This pipeline involves a combination of offline and online experimentation where new ideas are compared to each other, to established baselines, and finally, to the recommendation algorithms currently operating in a live application. By and large, offline experimentation precedes online evaluation, given the cost and bandwidth constraints of live testing [Gruson et al. 2019]. The final test in validating a new idea consists in launching it in the production platform alongside the current version, driving a fraction of user traffic to it, and comparing its performance to that of the present system— whichever wins takes over or remains the working system version.

This is called an A/B test, since two systems A and B are compared, where by A one usually means the current system (often referred to as control, borrowing from clinical trial terminology), and B is the challenger (treatment). In practice, it is common to run so-called multivariate tests comparing more than two systems simultaneously. Popular applications such as the ones mentioned at the beginning of this section are running hundreds of simultaneous A/B tests as we speak [Amatriain and Basilico 2015]. One genuine characteristic of A/B tests is that users do not know which version of the system they are facing, just like subjects in drug research do not know whether they are dispensed the actual treatment under trial (system B) or a placebo (system A). This aims to maximize the objectivity of the comparison by avoiding the introduction of confounding distortions in the experiment.

In addition to testing ideas in the most realistic possible conditions, A/B tests allow measuring the effects of algorithms on ultimate business performance and goals, beyond theoretical, more speculative effectiveness metrics. Effects can be measured on engagement (clicks, playcount, dwelling time), sales (revenue, profit, number of purchases), customer retention, and any other metric or key performance indicator the business relies on. We further elaborate on the business value of recommendation from wider perspectives in Section 2.3, also considering other possible stakeholders—beyond the customer and the system—involved in and impacted by recommendations.

A/B tests are commonly run until a statistically significant difference is found between system A and B, or until an affordable time limit has elapsed without a clear conclusion. A typical duration for an A/B test is in the order of weeks—the time needed to collect sufficient evidence to make data-driven decisions about system changes. This, and the risk involved in exposing customers to new untested changes, limits the number of experiments than can be run simultaneously on a production platform. For this reason, offline evaluation is used to shortlist a small selection of change proposals to be brought to live evaluation—we discuss offline experimentation methodology in some detail in the next section.

One open challenge to this respect is the weak or missing correlation often observed between the outcome of offline and online comparisons [Garcin et al. 2014, Gomez-Uribe and Hunt 2015, Jannach and Jugovac 2019, Kouki et al. 2020]. While this remains a major open research question in the field [Gilotte et al. 2018, Jannach and Bauer 2020, Rossetti et al. 2016], offline evaluation is routinely practiced as a selection yardstick before online testing in industry, and is by far the main resource for empirical observation in academic research.

Besides A/B testing, other forms of online experimentation and user studies can be designed with specific purposes [Knijnenburg and Willemsen 2015], not necessarily involving a production system. We discuss such studies with humans in the loop in Section 2.3.1.

### 1.4.2. Offline Evaluation

An offline experiment can be seen as a simulation of the system interacting with users, where different proxies—i.e., offline metrics—for system effectiveness are measured [Castells and Moffat 2022]. The most informative offline experiment is the one that best simulates and represents the production setting. In industrial developments, very specific production settings may be required. Therefore, in general research, a number of abstractions are usually made.

The difference between online and offline evaluation can be neatly defined as follows: whereas online evaluation acquires user feedback data (for metric computation) after recommendations are delivered to users—from these same users—in offline evaluation the test data is collected before recommendations are produced—and users are in fact never delivered the evaluated recommendations.

Setting up a basic offline experiment for evaluating a recommendation algorithm may seem a straightforward endeavor at first sight, given the wide body of well established methodologies and experimental design principles, in fields such as machine learning and information retrieval, to draw upon. The recommendation task has however peculiarities of its own that result in a fair degree of hidden complexity and pitfalls, that can produce inconsistent evaluation outcomes more easily than one might think [Canamares et al. 2020]. ˜ The experimenter is advised to carefully consider, understand and report the detailed design choices made in an experiment and their implications. We overview some of them in the following sections.

### 1.4.3. Offline Data

Offline evaluation generally distinguishes training data and test data, which should be strictly disjoint [Guy 2022]. Training data is supplied as input to the recommender system being evaluated, whereas test data is hidden from the system and is used to compute metrics on the returned recommendations. Training data in an offline experiment can be similar to the data that a production system may have available (the more similar the better), whereas test data is used in the experiment to simulate user feedback in reaction to the delivered recommendations. The training and test data for recommendation usually reflect user-item interactions as one of their major components—that is, they can be seen as disjoint subsets of the rating matrix. Training data can however also include additional side-information that specific recommendation algorithms may consume.

Training and test data can be acquired in many different ways. A good approach for the training set is to export a subset of the input data that a real recommender system is using at a certain point in time. Over the years, a number of such datasets were made publicly available, most prominently the datasets from the non-commercial MovieLens system10. Various other datasets are nowadays used by researchers containing items rating, purchase information or listening events, see also [Harper and Konstan 2015]11. Most recently, several datasets were also published that do not contain a matrix of user-item interactions, but sequential logs of recorded interactions, which can be used to evaluate session-based recommendation algorithms, see Section 2.1. Unfortunately, for many of the datasets it is not clear under which particular circumstances they were collected. For example, when an e-commerce or music streaming platform already has a recommender in place at the point in time of data collection, what we observe in the logs may be to certain extent be biased by the existing recommendation algorithm. In any case, given such a dataset of logged interactions, a trivial means to obtain test data is to subsample from the training set. This is usually referred to as splitting the data, and can be carried out in different ways, which we discuss in the next subsection.

The data typically collected for recommender system experimentation displays heavy sampling biases, originated by the working system (its algorithms, user interface and business rules) through which it was collected, external biases (marketing, fashion, social influence, etc.) and behavioral biases in user engagement. This raises issues in evaluation that we discuss later in Section 2.2. Test data can however also be obtained from a separate source from the training set. For instance, the Yahoo! R3 dataset [Marlin and Zemel 2009] includes a test set consisting of randomly sampled ratings from users for music—which means users were required to rate music that was uniformly sampled for them—whereas the training set was collected from free user interaction with music. The Coat dataset [Swaminathan et al. 2017] was built in a similar way in the clothing domain. The CM100k dataset [Canamares and Castells 2018] collected user ratings for music entirely at random, and provides a complementary label for user familiarity with the music, which is suggested as a proxy for non-uniformly distributed input training data.

#### 1.4.3.1. Data Splitting

When the data collected for offline evaluation does not include a separate test subset, the latter is usually subsampled from the available dataset. The sampling—referred to as splitting— procedure can be carried out in different ways. A first parameter of the procedure is the split ratio, i.e. the proportion of train-to-test data, typically expressed as a percentage or a ratio in [0,1]. It is common to allocate a larger data share for training (e.g. ≥ 80%) than test, given the usual data sparsity challenges when training recommendation models.

A second dimension of choice in the sampling procedure is random sampling vs. temporal splitting. For a chosen point in time, a temporal split places all the data produced prior to that point in the training subset, and the rest in the test subset [Koren 2009]. The time point is sometimes chosen in terms of meaningful time units (a number of weeks, months, etc., worth of data), or in such a way as to obtain a specific split ratio (a proportion between the amount of training and test data). More elaborate variants involving segmentation into multiple time windows have also been explored [Lathia 2010]. A temporal split generally adds to the soundness of the evaluation methodology, as it better represents a real recommendation task: predicting future (or present) usefulness based on past observed user behavior. Furthermore, it provides a cleaner data separation, since mixing past and future records can be seen as a form of data leakage. To this respect, a global time split point can be cleaner than different points for different users (as per e.g., a leave-last-out approach). Otherwise we might be predicting that a user will like some item in the “future” based on a leakage of foresight information that, for instance, the item would become popular or trendy by then (according to the training records of users with a later split time point), when in fact the item might not have even been created yet during the training period for that user.

Temporal splitting can however not always be possible, or not strictly required. For instance, meaningful data timestamps might not be available, or user needs might be relatively stable, or the experimenter might aim to focus on a specific problem where time is not a priority variable. The common alternative is to sample test and training data randomly, based on the split ratio. Random sampling is more flexible than a temporal split and enables, for instance, n-fold cross-validation, where n = 5 is a typical number (n = 10, for instance, is also usual).

A natural implementation of a random split is by i.i.d. Bernoulli sampling (coin flip) B(1, p) of data records with p = the split ratio. Other implementations have been documented that sample an exact number of data records uniformly at random without replacement, sometimes separately for each user or each item. Other authors have explored sampling an equal amount (rather than an equal ratio) of test data per item or per user, as a way to reduce evaluation biases [Bellog´ın et al. 2017]. Research on specific problems can also deploy orthogonal variations of the split procedure to simulate specific conditions such as cold-start or long-tail by placing the items or users of interest in the test set [Hurley and Zhang 2011].

#### 1.4.3.2. Candidate Item Sampling

A somewhat hidden option in the design of offline experiments is selecting the set of candidate items (sometimes referred to as target items [Canamares and Castells 2020, Sarwar et al. ˜ 2001]) that the evaluated system should rank for each target user. A natural setting for this option might consider selecting all the items in the catalog. This is not the case however when recommendation is viewed as a matrix completion problem: the known matrix cells need not be completed. In other words, an item is not included in the recommendations to users who have the item in their training records when discovery is part of the aim of recommendation, as is the case of most of the literature.

Experimenters may consider restricting candidate items to an even smaller set. Koren [2008] was the first to suggest a fixed number of target items per user. The idea caught up and is still common nowadays in the research literature [Canamares and Castells 2020, Krichene ˜ and Rendle 2020]. When small candidate sets are used, some authors refer to this option as computing sampled metrics [Krichene and Rendle 2020]. An extreme, often called condensed rankings [Buckley and Voorhees. 2004], consists in taking only the items with test ratings in the candidate set—this is the option when the error metrics are used (see Section 1.4.4.1 above), as it is not possible to compute a rating error where no rating is available.

Recent studies show that candidate sampling can have a deep impact on the outcome of offline evaluation and should be better understood [Canamares and Castells 2020, Krichene ˜ and Rendle 2020, Steck 2013]. Authors unanimously report disagreements in system comparisons arising when all vs. no test-unrated items are included in the candidate set. Canamares ˜ and Castells [2020] further show that the extremes in this settings have each their own shortcomings, and suggest that some point in between condensed and full rankings might optimize the informativeness of offline evaluation.

### 1.4.4. Recommendation Task and Metrics

In practice, every recommender system is designed to fulfil a certain purpose in order to create value for users and organizations, see also Section 2.3. Depending on the purpose, different computational tasks have to be implemented in the system [Jannach and Adomavicius 2016]. In their seminal work on recommender systems evaluation, Herlocker et al. [2004] identified tasks such as “find good items”, “annotation in context” (predict ratings), or “recommend sequence”. When designing new algorithms, the main question is to understand how effective this new algorithm is in terms of fulfilling these tasks. In offline experiments, this assessment is done with the help of computational metrics, which serve as proxies for the effectiveness of the system in its context of use.

Traditionally, the most important general aim of recommendation has been understood to involve an accurate grasp of user needs. Metrics devised to assess this have therefore been broadly referred to as accuracy metrics. Recommendation accuracy is nowadays understood as a synonym for ranking quality—metrics and evaluation protocols have therefore been borrowed and adapted from the information retrieval field [Bellog´ın et al. 2017], as we discuss next in Section 1.4.4.2. We nonetheless first discuss earlier perspectives based on rating prediction, for historical interest.

#### 1.4.4.1. Rating prediction

As mentioned in Section 1.2, the recommendation task was initially understood as a rating
prediction problem—that is, a regression task where a function f : U ×I → R is to be learned
from examples. As such, it seemed natural to evaluate recommendation by error metrics
such as Mean Average Error (MAE) and Root Mean Squared Error (RMSE). The error was
measured across the test data in the rating matrix:

$$
\text{MAE}
\\
\text{RMSE}
$$

where T ⊂ U × I denotes the subset of test data records, and the lower the value of these metrics, the better the recommender system is considered.

Such metrics were used for almost two decades in the recommender systems literature, and were the basis for such an important initiative in the growth of the field as the Netflix Prize [Liu et al. 2007]. The community has moved beyond rating prediction nonetheless, and nowadays tends to see and address recommendation as a ranking task, also in line with business models that are prominent in industry [Cremonesi et al. 2010]. Many recommendation algorithms, on the other hand, output scores for user-item pairs that do not have a meaningful interpretation as ratings, but are highly effective to select lists of top-scored items to recommend—error metrics are not meaningful when applied to such scores.

#### 1.4.4.2. Ranking Quality: Recommendation as an IR Task

During the 2000’s the view that delivering recommendations has many commonalities with delivering search results grew stronger in the community [Herlocker et al. 2004]. Both recommendation and search systems assist users in accessing relevant information or products from a large collection. One main difference lies in the absence of an explicit query in the recommendation task—still the problem can be understood to involve a user need to be satisfied, even if it is not explicitly conveyed by the user. In fact, search and recommendation often work together and complement each other in many applications. This view was recognized time before [Belkin and Croft 1992] but did not seem to catch up community-wide as—to some degree—a paradigm change in evaluation until the 2010’s [Bellog´ın et al. 2017, Cremonesi et al. 2010].

In this realization, the key for recommendation effectiveness is in the returned ranking: effective recommendations should return as many relevant items as early as possible in the ranking, and as few non-relevant items as possible in the top positions. The ranking determines what items the user will see, and how soon, when browsing recommendations. The interpretation of the system scores, by which items are ranked, as accurate rating predictions becomes irrelevant. The notion of ranking can be naturally generalized to other, not necessarily linear displays of recommendations (e.g. a “shelve” matrix), where some regions in the screen layout—equivalent to the top rank notion—are more likely than others to be examined by users.

Ranking evaluation naturally motivated researchers to borrow concepts and methodologies from the information retrieval field, where offline evaluation procedures and metrics had been researched and developed for half a century. The adaptation is however not straightforward and involves subtleties that need to be handled with care. We discuss this adaptation through the main elements involved in offline evaluation practice in the information retrieval field [Sanderson 2010].

- Collection. The set of items I can be considered an equivalent of the set of all “documents” in the IR literature. Item collections—often referred to as “item catalogs”—are in fact often the retrieval space for complementary search and recommendation functionalities in most recommender system applications.
- Query and Information Need. Search and recommendation are both motivated by a need on the user side that the system aims to help satisfy. Whereas users actively describe their need with explicit queries to a search engine, recommender systems derive user needs from observed user activity and interactions with the items. The information need representation in a recommender system is therefore considerably vague and incomplete compared to a search engine, and calls for different retrieval techniques. Search queries can have different degrees of vagueness too, and recommendation can be seen as the endpoint in a continuous spectrum of query specificity for a retrieval task, where the query is just empty.
- Relevance. The notion of relevance, central to IR, is just as meaningful in recommendation, and can be taken in quite the same sense: a recommended item is relevant to a user if they like, enjoy, are pleased by, are interested in, etc., the item. Relevance can be considered as a necessary condition for a recommendation to bring any value to the user. If a recommended item is not relevant the user will ignore it and no gain will be derived.

A major difference should be noted though as to how relevance is handled in offline experiments: while relevance is often assumed to be objectifiable—i.e. user-independent—in face of a query as a reasonable simplification, this assumption is not reasonable in recommendation, where relevance is acknowledged to be highly user-dependent, and this is typically an intrinsic characteristic of the recommendation task. This has important consequences in offline evaluation when it comes to eliciting relevance judgments: delegating item labeling as relevant or non-relevant to assessors on behalf of users is too far a stretch to enable any kind of meaningful evaluation of personalized recommendations. Offline evaluation is therefore not separable from actual end users in the way search evaluation can be.

In offline recommendation experiments, test data obtained from target users play the role of relevance judgments in IR evaluation. Compared to judgment pooling in IR [Sanderson 2010], test data can introduce extremely sharp selection biases in evaluation when data are collected in the wild, and/or by mediation of a recommender system, as is typically the case. Test data is very unevenly distributed over items, displaying a so-called popularity bias which can heavily distort evaluation outcomes [Bellog´ın et al. 2017, Canamares and Castells 2018, ˜ Jannach et al. 2015b], as we discuss further in Section 2.2.

Metrics. Once an equivalent of relevance judgments is defined and obtained, any IR metric can be applied to the output of a recommender system: precision, recall, mean average precision (MAP), mean reciprocal rank (MRR), normalized discounted cumulative gain (nDCG), are commonly used to assess the ranking quality of recommendations. Since precision and recall are rank-insensitive metrics, they are usually measured on a subset of top-n recommended items—a ranking cutoff—as P@n and Recall@n. Cutoffs can also be taken in rank-sensitive metrics such as nDCG, MAP and MRR, to further focus measurements on the ranking top.

The Area Under the ROC Curve (AUC) [Bamber 1975] can also be considered a ranking metric inasmuch as it provides a perspective of the relevant vs. non-relevant recommended items tradeoff across the ranking. AUC is commonly used as a metric for classifier evaluation, but then so are precision and recall: they view recommender systems as binary classifiers into the relevant and non-relevant classes. Focusing the measurements on top n rank positions does the trick for such measures, turning them into informative ranking metrics.

Compared to search experiments, bias and user-dependence in relevance judgments for recommendation exacerbate the sparsity of relevance labels in the ranking tops to be evaluated in offline recommendation experiments. Deciding how to handle the missing judgments becomes a key issue when importing IR ranking metrics, as discussed earlier in Section 1.4.3.2. An additional challenge is the non-random nature of the missing data, and the resulting bias in evaluation results. We discuss this further in Section 2.2.

### 1.4.5. Beyond Accuracy

While accuracy was the primary perspective on algorithmic evaluation for a long time, it eventually became apparent that this is an incomplete view of what makes a recommendation useful and profitable for consumers and providers [Ge et al. 2010, McNee et al. 2006]. For example, a fan of the Beatles would very likely love the song ‘Yesterday’—the song is highly relevant in the IR sense—but how useful would this be as a recommendation? The song is a most widely known music piece worldwide, more so for a Beatles fan. The user might as well search for it when they fancy. The song might make sense as a convenience recommendation in some scenarios, e.g., in an automatic playlist while driving on a trip, but the added-value of such a recommendation (e.g., with respect to searching and browsing) is clearly more restricted than might be, for instance, the discovery of new music or a new band or a new taste the user was not aware of and can now enjoy.

Likewise, when a streaming platform lists movies and series the user might enjoy, it may be wise to include movies from a variety of genres and directors, to better represent the diversity of interests people have—and the varying mood. While people’s tastes can be stable, we may feel like watching comedy one day and a documentary the next day, and those changes are very difficult to guess. Rather than risking a double-or-nothing bet on all-comedy or alldocumentary recommendation, a diversified offering of potential favorites would seem a more sensible approach considering this. Moreover, by diversifying recommendations the system takes opportunities to explore unseen user interests and learn about them to keep improving in the future.

Novelty and diversity are just two dimensions in the objectives beyond accuracy that can maximize the value of recommendation, and we focus on them here. It is useful to distinguish between novelty and diversity as highly related but different perspectives, a distinction we reflect in the summary that follows. Broader perspectives on the direct key measures of the value, user satisfaction, business performance and the impact at large that recommendation has on the involved stakeholders, are discussed in Section 2.3. A recommender system finds several motivations to enhance novelty and diversity, and different angles to these general notions, that can be operationalized into metrics and algorithms that enhance and measure them, as we discuss next. For a more extensive review of novelty and diversity in recommender systems, the reader is referred to [Castells et al. 2022].

#### 1.4.5.1. Novelty

At an abstract level, recommendation novelty can defined as the difference between the recommended items and a certain context of reference [Vargas and Castells 2011]. The context can typically be the past experience of the person to whom a recommendation is delivered or, considering the sparse and partial system knowledge about that, the aggregated experience of all users in the system. In the latter case, novelty equates to rarity: a recommended item is novel if it is in the long-tail of the interaction frequency distribution [Celma and Herrera 2008]. Personalized novelty, often referred to as unexpectedness [Adamopoulos and Tuzhilin 2014], would reflect how different a recommendation is from the items the target user was observed interacting with in the past. Long-tail novelty and unexpectedness can be quantified in different ways, which we summarize next.

##### 1.4.5.1.1. Long-tail novelty.

This dimension can be measured, for instance, as the average of a monotonically decreasing function φ on the amount of past engagement Li with each recommended

$$
LT(R) =
$$

where R is the returned recommendation and Li denotes the set of observed interactions involving item i. The φ function has been defined in the literature, for instance, as the complement or the negative log of the ratio of observed interactions involving the item [Vargas and Castells 2011]:

$$
(x) = 1 -\frac{x}{|L|}
$$

where L represents the set of all observed interactions. With the first definition, novelty can be read as the probability that a random user has never interacted in the past with a recommended item in R. When repeated interaction is ignored (i.e., each user-item pair is counted at most once in the above equations), the second definition is equivalent to inverse document frequency as defined in IR models for document retrieval, users being here the equivalent of documents—hence sometimes referred to asinverse user frequency [Breese et al. 1998]. When measured this way, novelty can be seen as a condition of coldness (lack of data) or unpopularity.

##### 1.4.5.1.2. Unexpectedness.

This notion is typically quantified as:

$$
Unex(R)
$$

where d denotes a set dissimilarity operator and $E_u \in I$ is a reference set of items representing “the expected” for each user u ∈U. The expected set can be, for instance, the items that the target user has interacted with, or the recommendations delivered by a supposedly conventional algorithm, or any other proxy for an unsurprising experience [Adamopoulos and Tuzhilin 2014]. Set dissimilarity can be any conventional measure such as Jaccard, Hausdorff, average item-wise distance, or other meaningful elaborations to quantify how different the two sets are. When pairwise item distance is used, feature-based dissimilarity (measured e.g., by the cosine or Jaccard distance between feature vectors/sets) is typically a good option. Other set-based dissimilarity operators can have a probabilistic interpretation, e.g. $d(R,E_u) = |R\ E_u|/|R|$ can be read as the probability that a recommended item is not expected.

##### 1.4.5.1.3. Serendipity.

An additional important notion in the scope of novelty is serendipity. While the definitions slightly vary in the literature, the dominant convention considers a recommendation as serendipitous if it is surprising (i.e. novel) and valuable [Chen et al. 2019]. If we equate value, in this sense, to relevance, a straightforward way to measure serendipity is to restrict the computation of the above novelty metrics to the recommended items that are relevant to the target user (according to the available test data), and ignore the rest.

##### 1.4.5.1.4. Further notions.

The above novelty notions are probably the most common in the literature but they are not meant to be exhaustive [Castells et al. 2022]. Freshness (how recently a recommended item was created), for instance, is often an important property by itself, and typically a signal that correlates with the above notions. Lathia and Amatriain [Lathia et al. 2010] explored finer temporal notions of novelty and diversity, involving the history of past recommendations—capturing, broadly speaking, how much a recommender system is repeating itself.

#### 1.4.5.2. Diversity

Related to but different from novelty notions, diversity is generally defined as the amount of variety covered within recommendations. For instance, ‘Orinoco Flow’ by Enya and ‘Highway to Hell’ by AC/DC can hardly be considered novelties (anyone has listened to this music sometime), but together they would make for a highly diverse music recommendation because they are very different to each other. Likewise, two very similar music rarities would not be diverse while they may be highly novel.

#### 1.4.5.3. Enhancing Novelty and Diversity

#### 1.4.5.4. User Perceptions of Diversity and Novelty

# 2. Recent Topics

## 2.1. Sequential and Session-based Recommendation

### 2.1.1. Problem Definition and Terminology

### 2.1.2. Algorithms for Sequential and Session-based Recommendation

### 2.1.3. Evaluation of Sequential and Session-based Recommender Systems

#### 2.1.3.1. Offline Evaluation

#### 2.1.3.2. User-Centric Evaluation

#### 2.1.3.3. Real-World Evaluation

### 2.1.4. Discussion and Outlook

## 2.2. Popularity, Bias and Recurrence in Recommendation

### 2.2.1. Countering Bias

### 2.2.2. Understanding Bias

### 2.2.3. The Feedback Loop

## 2.3. Impact and Value of Recommender Systems

### 2.3.1. Understanding the Impact of Recommendations with the Human in the Loop

### 2.3.2. Consumer and Business Value of Recommender Systems

#### 2.3.2.1. Recommendation as a Multi-Stakeholder Optimization Problem

#### 2.3.2.2. Impact and Value Oriented Recommender Systems Evaluation

## 2.4. Summary and Future Directions

### 2.4.1. Summary

### 2.4.2. Further Readings and Future Directions
