## 0.1. link

- [pdf](https://arxiv.org/pdf/2107.06630.pdf)
- [この方のブログを見て本論文を見つけた.](https://ayakobaba.hatenablog.com/entry/2021/09/25/190642)

## 0.2. title

Online Evaluation Methods for the Causal Effect of Recommendations

## 0.3. abstract

Evaluating the causal effect of recommendations is an important objective because the causal effect on user interactions can directly leads to an increase in sales and user engagement. To select an optimal recommendation model, it is common to conduct A/B testing to compare model performance. However, A/B testing of causal effects requires a large number of users, making such experiments costly and risky. We therefore propose the first interleaving methods that can efficiently compare recommendation models in terms of causal effects. In contrast to conventional interleaving methods, we measure the outcomes of both items on an interleaved list and items not on the interleaved list, since the causal effect is the difference between outcomes with and without recommendations. To ensure that the evaluations are unbiased, we either select items with equal probability or weight the outcomes using inverse propensity scores. We then verify the unbiasedness and efficiency of online evaluation methods through simulated online experiments. The results indicate that our proposed methods are unbiased and that they have superior efficiency to A/B testing.

# 1. Introduction

A recommendation is a treatment that can affect user behavior. An increase in user actions, such as purchases or views, by the recommendation is the treatment effect (also called the causal effect). Because this leads to improved sales or user engagement, the causal effect of recommendations is important for businesses. While most recommendation methods aim for accurate predictions of user behaviors, there may be a discrepancy between the accuracy and the causal effect of recommendations [25]. Several recent works have thus proposed recommendation methods to rank items by the causal effect of recommendations [1, 24, 25, 27, 28].

Online experiments are commonly conducted to compare model performance and select the best recommendation model. However, evaluating the causal effect is not straightforward; we cannot naively compare the outcomes of recommended items because the causal effect is the difference between the potential outcomes with and without the treatment [12, 22]. A/B testing that compares the total user actions on all items, not only recommended items, can reveal the difference in the average causal effect (see Section 3.2). Nevertheless, it suffers from large fluctuations due to the variability in natural user behaviors for non-recommended items: some users tend to purchase more items than others. A large number of users is required to compensate for such fluctuations, making online experiments costly and risky.

In this paper, we propose efficient online evaluation methods for the causal effect of recommendations based on interleaving. Interleaving generates a list from the lists ranked by the two models to be compared [3]. Whereas previous interleaving methods only measure the outcomes of items in the intersection of the original and interleaved lists, our proposed methods also measure the outcomes of items in the original lists but not in the interleaved list. We propose an interleaving method that selects items with equal probability for unbiased evaluation. With unequal selection probabilities, the evaluation might be biased due to confounding [8] between recommendation and potential outcomes, leading to inaccurate judgments of the recommendation models. We remove the possible bias by properly weighting the outcomes based on the inverse propensity score (IPS) method used in causal inference [16, 21]. This enables the use of a more general interleaving framework that only requires non-zero probabilities to be selected for any item in the original lists. As an instance of the framework, we propose a causal balanced interleaving method that balances the number of items chosen from the two compared lists. To verify the unbiasedness and efficiency of the proposed interleaving methods, we simulate online experiments to compare ranking models.

The contributions of this paper are summarized as follows.

- We propose the first interleaving methods to compare recommendation models in terms of their causal effect.
- We verify the unbiasedness and efficiency of the proposed methods through simulated online experiments

# 2. Related Work

## 2.1. Interleaving Methods

Interleaving is an online evaluation method for comparing two ranking models by observing user interactions with an interleaved list that is generated from lists ranked by the two models to be compared [3]. Several interleaving methods have been proposed for evaluating information retrieval systems. Balanced interleaving [14, 15] generates an interleaved list from two rankings to be compared such that the highest ranks in the interleaved list 𝑘𝐴 and 𝑘𝐵 from the two rankings 𝐴 and 𝐵, respectively, are the same or different by at most one. Team draft interleaving [19] alternatively selects items from compared rankings, analogously to selecting teams for a friendly team-sports match. Probabilistic interleaving [9] selects items according to probabilities that depend on the item ranks. Optimized interleaving [18] makes the properties required for interleaving in information retrieval explicit and then generates interleaved lists by solving an optimization problem to fulfill those properties. Interleaving methods have been extended to multileaving that compare multiple rankings simultaneously [30, 31]. Multileaving has been also applied to the evaluation of a news recommender system [11]. The objective of previous interleaving methods is to evaluate how accurately the rankings reflect queries or user preferences, whereas our goal is to evaluate rankings in terms of the causal effect. To the best of our knowledge, at present there are no interleaving methods for causal effects.

## 2.2. Recommendation Methods for the Causal Effect

Recommendations can affect users’ opinions [5] and induce users’ actions [6, 13]. However, users’ actions on recommended items could have occurred even without the recommendations [32]. Building recommendation models that target the causal effect is challenging because the ground truth data of causal effects are not observable [10]. One approach is to train prediction models for both recommended and non-recommended outcomes and then to rank the items based on the difference between the two predictions [1, 24]. Another approach is to optimize models directly for the causal effect. ULRMF and ULBPR [25] are respectively pointwise and pairwise optimization methods that use label transformations and training data samplings designed for causal effect optimization. DLCE [27] is an unbiased learning method for the causal effect that uses an IPS-based unbiased learning objective. There are also neighborhood methods for causal effects [28] that are based on a matching estimator in causal inference. These prior works on causal effects evaluated methods offline and did not discuss protocols for online evaluation. In this study, we develop online evaluation methods and compare some of the aforementioned recommendation methods in simulated online experiments.

Another line of works in the area of causal recommendation aims for debiasing [4]. Several methods have been proposed to learn users’ true preferences from biased (missing-not-at-random) feedback data [2, 23, 29, 33].1 These methods can be regarded as predicting interactions with recommendations (i.e., $Y_{ui}^T$, defined in the next section). Hence, we can evaluate them using previous interleaving methods.

# 3. Evaluation Methods for the Causal Effect of Recommendatios

## 3.1. Causal Effect of Recommendations

In this subsection, we define the causal effect of recommendations. Let $U$ and $I$ be sets of users and items, respectively. Let $Y_{ui} \in {0, 1}$ denote the interaction (e.g., purchase or view) of user $u \in U$ with item $i \in I$. User interactions may differ depending on whether the item is recommended or not. We denote the binary indicator for the recommendation (also called the treatment assignment) by $Z_{ui} \in {0, 1}$. Let $Y_{ui}^T$ and $Y_{ui}^C \in {0, 1}$ be hypothetical user interactions (also called potential outcomes [22]) when item $i$ is recommended to $u$ ($Z_{ui} = 1$) and when it is not recommended ($Z_{ui} = 0$), respectively. The causal effect $\tau_{ui}$ of recommending item $i$ to user $u$ is defined as the difference between the two potential outcomes: $\tau_{ui} = Y_{ui}^T − Y_{ui}^C$, that takes ternary values, $\tau_{ui} \in {−1, 0, 1}$. Using potential outcomes, the observed interaction can be expressed as

$$
Y_{ui} = Z_{ui} Y_{ui}^T +(1 -Z_{ui})Y_{ui}^C
\tag{1}
$$

$Y_{ui} = Y_{ui}^T if $i$ is recommended ($Z_{ui}= 1$) and $Y_{ui} = Y_{ui}^C$ if it is not recommended ($Z_{ui} = 0$). Note that $Y_{ui}^T$ or $Y_{ui}^C$ cannot both be observed at a specific time; hence, $\tau_{ui}$ is not directly observable.

The recommendation model $A$ generates a recommendation list $L_u^A$ for each user. The average causal effect of model $A$ is then defined as

$$
\hat{\tau}_A = \frac{1}{n|S_A|} \sum_{u \in S_A} \sum_{i \in L_{u}^A} \tau_{ui}
\tag{2}
$$

In this work, we evaluate models using the above metric.2 That is, when comparing two models, we regard $A$ to superior to $B$ when $\tau_A > \tau_B$.

## 3.2. A/B testing for the Causal Effect

For A/B testing, we randomly select non-overlapping subsets of users $S_A$ and $S_B$ (i.e., $S_A$, $S_B \in U$ and $S_A \cap S_B = \empty$) and apply models $A$ and $B$ to each subset. Let $n = |L_u^A| = |L_u^B|$ be the size of the recommendation list, which we assume to be constant. The subset average causal effect is then defined as

$$
\tag{3}
$$

This converges to $\tau_A$ as $|S_A|$ increases.

The typical evaluation metrics for A/B testing are either based on total user interactions (such as sales or user engagement) or only on interactions with recommended lists (such as click-through rates or conversion rates) [13]. Here we show that the former is a valid evaluation for the causal effect. The total user interactions divided by the number of recommendations can be expressed as

$$
\tag{4}
$$

Because the rightmost term in the final equation does not depend on the model, we can compare $\hat_{\tau_A}$ and $\hat_{\tau_B}$ by comparing $\hat{Y}_A^{total}$ and $\hat{Y}_B^{total}$. On the other hand, the average interactions with the recommended lists can be expressed as

$$
\tag{5}
$$

Hence, the evaluation based only on interactions with recommended lists is not valid testing for the causal effect.

Although A/B testing with Eq. (4) can be used for unbiased model comparisons, it may have large variance due to the variability in natural user behaviors (i.e., the potential outcomes under no recommendations, $Y_{ui}^C$). If users in $S_A$ tend to purchase more items than those in $S_B$, $\sum_{u\in S_A} \sum_{i \in I}Y_{ui}^C$ becomes larger than $\sum_{u\in S_B} \sum_{i \in I}Y_{ui}^C$, thereby altering the comparison in Eq. (4). To minimize such discrepancies, a sufficiently large number of users need to be recruited for A/B testing. We thus introduce more efficient evaluation methods in the next subsection.

## 3.3. Interleaving for the Causal Effect

In this subsection, we propose interleaving methods for the online evaluation of the causal effects of recommendations. Previous interleaving methods only measure outcomes in the interleaved lists: they only include $Y_{ui}^T$ and lack information on $Y_{ui}^C$. Further, if the item selection for the interleaved list is not randomized controlled, the naive estimate from the observed outcomes might be biased due to the confounding between recommendations and potential outcomes. We need to remedy the bias for valid comparison.

Here we describe the problem setting of interleaving for the causal effect. For each user $u$, we construct the interleaved list $L_u$ from the compared lists $L_u^A$ and $L_u^B$. We observe outcomes ${Y_{ui}}$ for all items $i \in I$. Note that $Y_{ui} = Y_{ui}^T$ if item $i$ is in the interleaved list ($i \in L_{u}$ or equivalently, $Z_{ui} = 1$) and $Y_{ui} = Y_{ui}^C$ if it is not in the list ($i \in I \setminus L_u$ or equivalently, $Z_{ui}= 0$). We want to compare the average causal effects of lists $L_u^A$ and $L_u^B$:

$$
\tag{6}
$$

We need to estimate the above values from observed outcomes because we cannot directly observe $\tau_{ui}$.

If the items in $L_u^A$ and $L_u^B$ are randomly assigned to the interleaved list independent of the potential outcomes, that is, $(Y_{ui}^T, Y_{ui}^C) \perp Z_{ui}$, the case can be regarded as a randomized controlled trial (RCT) [12, 22].3 We can then simply estimate $\tau_{L_u^A}$ as the difference in average outcomes for items on and not on the interleaved list:

$$
\tag{7}
$$

One way to realize such a randomized assignment is to select 𝑛 items from $L_u^A \cup L_u^B$ with equal probability: $p = n \setminus |L_u^A \cup L_u^B|$. We call this method equal probability interleaving (EPI).

The independence requirement heavily restricts the potential design space of interleaving methods. We thus derive estimates that are applicable to more general cases. Denote the probability (also called the propensity) of being included in the interleaved list $L_u$ by $p_{ui} = E[Z_{ui} = 1|X_{ui}]$. We assume that 1) the covariates $X_{ui}$ contain all confounders of $(Y_{ui}^T, Y_{ui}^C)$ and $Z_{ui}$, and 2) the treatment assignment is not deterministic ($0 < p_{ui} < 1$ for $i \in L_u^A \cup L_u^B$).4 Assumption 1 is equivalent to conditional independence: $(Y_{ui}^T,Y_{ui}^C) \perp Z_{ui}|X_{ui}$. When we design an interleaving method, we know the covariates that affect 𝑍𝑢𝑖 and Assumption 1 can always be satisfied.5 Therefore, the only restriction for interleaving methods is Assumption 2 (also called positivity).

Under these assumptions, we can construct an unbiased estimator using IPS weighting [16]:

$$
\tag{8}
$$

This estimator is unbiased since

$$
\tag{9}
$$

We propose a general framework for interleaving as follows.

- (1) Construct interleaved lists ${L_u}$ using an interleaving method that satisfies positivity (Assumption 2).
- (2) Conduct online experiments and obtain outcomes ${Y_{ui}}$.
- (3) Estimate $\tau_{L_u^A}$ and $\tau_{L_u^B}$ by Eq. (8) and compare them.

As an example of a valid interleaving method that satisfies positivity, we propose causal balanced interleaving (CBI), the pseudo-code for which is shown in Algorithm $1$. CBI alternatively selects items from each list to balance the items chosen from each list. The item choice in each round is not deterministic in order to satisfy the positivity required for causal effect estimates. The propensity depends on whether an item is in the intersection, $1(i \in L_u^A \cap L_u^B)$. If an item is included in both lists, it has a greater probability of being chosen. The propensity also depends on the cardinality of the union of the compared lists, $|L_u^A \cup L_u^B|$, because smaller cardinality implies that each item has a greater chance of being selected. The possible values of the covariates are limited: $1(i \in L_u^A \cap L_u^B)$ is binary and $|L_u^A \cup L_u^B| \in [n, 2n]$. Hence, we can easily compute the propensity numerically by repeating Algorithm $1$ a sufficient number of times and recording $Z_{ui}$ for each combination of covariates.

# 4. Experiments

## 4.1. Experimental Setup

We experimented with the following online evaluation methods.6

- AB-total: A/B testing evaluated by the total user interactions, as expressed in Eq. (4).
- AB-list: A/B testing evaluated by user interactions only with items on the recommended list, as in Eq. (5).
- EPI-RCT: Interleaving to select items from $L_u^A \cup L_u^B$ with equal probability and evaluation using Eq. (7).
- CBI-RCT: Interleaving by Algorithm 1 and evaluation using Eq. (7), that is, no bias correction by IPS.
- CBI-IPS: Interleaving by Algorithm 1 and evaluation using Eq. (8).

Through the experiments, we aim to answer the following research questions: RQ1) Which method produces valid (unbiased) estimates of the true differences in average causal effects (4.2.1)?, and RQ2) Are the proposed interleaving methods more efficient (do they require fewer experimental users) than AB testing (4.2.2)? We first prepared semisynthetic datasets that contain both potential outcomes $Y_{ui}^T$ and $Y_{ui}^C$ for all user-item pairs. Because we observe $Y_{ui} = Y_{ui}^T$ if $Z_{ui} = 1$ and $Y_{ui}= Y_{ui}^C$ if $Z_{ui}=0$, both potential outcomes are necessary to simulate user outcomes under various ranking models and online evaluation methods. Following the procedure described in [28], we generated two datasets: one is based on the Dunnhumby dataset,7 and the other is based on the MovieLens-1M (ML-1M) dataset [7].8 The detail and rationale of ML one are described in Section 5.1 of [28] and that of DH one are described in 5.1.1 of [27]. Each dataset is comprised of independently generated training and testing data. The testing data were used to simulate online evaluation, and the training data were used to train the following models:9 the causality-aware user-based neighborhood methods (CUBN) with outcome similarity (-O) and treatment similarity (-T) [28], the uplift-based pointwise and pairwise learning methods (ULRMF and ULBPR) [25], the Bayesian personalized ranking method (BPR) [20], and the user-based neighborhood method (UBN) [17]. We compared two models among CUBN-T, ULRMF, BPR on the Dunnhumby data and two models among CUBN-O, ULBPR, UBN on the ML-1M data.10 The average causal effect $\bar{\tau_{L_u^{model}}}$ and the average treated outcomes $\bar{Y_{L_u^{model}}^T}$ of the trained models are listed in Table 1. The superior models in terms of the average causal effect do not necessarily have higher average treated outcomes. That is, we may mistakenly select a poor model in terms of the causal effect if we only evaluate the outcomes of the recommended items.

Table 1. Averages of causal effect and potential outcomes under treatment with recommendation lists of size 𝑛 = 10.

Our protocol for simulating online experiments is the following. First, we randomly select a subset of users and generate lists $L_u^A, L_u^B$ using compared models. For the A/B testing methods (AB-total, AB-list), we further split the subset into two groups: $S_A$ and $S_B$, and {L*u^A$} and {$L_u^B$} are recommended for each group, respectively. For the interleaving methods (EPI-RCT, CBI-RCT, CBI-IPS), we generate interleaved recommendation lists using EPI or CBI. In the simulation, recommendation means that $Z*{ui}$ is set to 1, and user outcomes {$Y_{ui}$} are observed by calculating $Y_{ui} = Z_{ui} Y_{ui}^T + (1 - Z_{ui})Y_{ui}^C$ with potential outcomes $Y_{ui}^T$ and $Y_{ui}^C$. Using the observed outcomes, we estimate the difference in the average causal effects of the compared models: $\tau_A − \tau_B$. We repeated the above protocol 10, 000 times and recorded the estimated differences using each online evaluation method. The size of recommendation list was set to 10.

## 4.2. Results and Discussion

### 4.2.1. Validity of the evaluation methods

We evaluated the validity of the online evaluation methods using random subsets of 1,000 users. The means and standard deviations of the estimated differences are shown in Table 2. The means obtained by EPI-RCT and CBI-IPS are close to the true differences. The means obtained by AB-total are also close to the true value for Dunnhumby but deviate slightly for ML-1M. The AB-list often yields estimates that differ substantially from the true values but are similar to the differences in treated outcomes, $\bar{Y^T_{L_u^{model}}}$, as shown in Table 1. This is expected because the AB-list evaluates $Y_{ui}^T$, not $\tau_{ui}$, as expressed in Eq. (5). Further, the CBI-RCT estimates also deviate from the true differences in most cases.11 This is due to the bias induced by the uneven probability of recommendation in interleaving. Conversely, CBI-IPS successfully removes the bias and produces estimates centered around the true values.

Table 2. Estimated differences between the causal effects of the compared models (mean ± standard deviations for 10,000 simulated runs). The results highlighted in bold indicate that the true values are within the 95% confidence intervals of the mean estimates

### 4.2.2. Efficiency of the interleaving methods.

We compared the efficiency of AB-total, EPI-RCT, and CBI-IPS, all of which were shown to be valid in the previous section. We simulated user subsets of various sizes in {10, 14, 20, 30, 50, 70, 100, 140, 200, 300, 500, 700, 1000, 1400, 2000} and evaluated the ratio of false judgments (when the sign of the estimated difference is the opposite of the truth). Figure 1 shows the ratio of false judgments according to the number of users. As the number of users increases, the false ratios of CBI-IPS and EPI-RCT decrease more rapidly than that of AB-total does. For the Dunnhumby dataset, AB-total requires around 30 times more users than CBI-IPS and EPI-RCT to achieve the same false ratio. For the ML-1M dataset, AB-total did not reach the same false ratio in the experimental range of subset sizes. These results demonstrate the superior efficiency of the proposed interleaving methods. Furthermore, CBI-IPS tends to be slightly more efficient than EPI-RCT, as expected from the smaller standard deviations shown in Table 2. This is probably because the number of items selected from the compared lists is balanced in this interleaving method.

Fig. 1. Dependence on the number of users.

# Conclusions

In this paper, we proposed the first interleaving methods for comparing recommender models in terms of causal effects. To realize unbiased model comparisons, our methods either select items with equal probabilities or weight the outcomes using IPS. We simulated online experiments and verified that our interleaving methods and an A/B testing method are unbiased and that our interleaving methods are largely more efficient than the A/B testing method. In the future, we plan to extend our methods to multileaving. Online experimentation in real recommendation services will also be important for future work.
