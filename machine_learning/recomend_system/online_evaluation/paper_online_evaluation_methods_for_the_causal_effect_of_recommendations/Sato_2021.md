## link

- [pdf](https://arxiv.org/pdf/2107.06630.pdf)
- [„Åì„ÅÆÊñπ„ÅÆ„Éñ„É≠„Ç∞„ÇíË¶ã„Å¶Êú¨Ë´ñÊñá„ÇíË¶ã„Å§„Åë„Åü.](https://ayakobaba.hatenablog.com/entry/2021/09/25/190642)

## title

Online Evaluation Methods for the Causal Effect of Recommendations

## abstract

Evaluating the causal effect of recommendations is an important objective because the causal effect on user interactions can directly leads to an increase in sales and user engagement. To select an optimal recommendation model, it is common to conduct A/B testing to compare model performance. However, A/B testing of causal effects requires a large number of users, making such experiments costly and risky. We therefore propose the first interleaving methods that can efficiently compare recommendation models in terms of causal effects. In contrast to conventional interleaving methods, we measure the outcomes of both items on an interleaved list and items not on the interleaved list, since the causal effect is the difference between outcomes with and without recommendations. To ensure that the evaluations are unbiased, we either select items with equal probability or weight the outcomes using inverse propensity scores. We then verify the unbiasedness and efficiency of online evaluation methods through simulated online experiments. The results indicate that our proposed methods are unbiased and that they have superior efficiency to A/B testing.

# Introduction

A recommendation is a treatment that can affect user behavior. An increase in user actions, such as purchases or views, by the recommendation is the treatment effect (also called the causal effect). Because this leads to improved sales or user engagement, the causal effect of recommendations is important for businesses. While most recommendation methods aim for accurate predictions of user behaviors, there may be a discrepancy between the accuracy and the causal effect of recommendations [25]. Several recent works have thus proposed recommendation methods to rank items by the causal effect of recommendations [1, 24, 25, 27, 28].

Online experiments are commonly conducted to compare model performance and select the best recommendation model. However, evaluating the causal effect is not straightforward; we cannot naively compare the outcomes of recommended items because the causal effect is the difference between the potential outcomes with and without the treatment [12, 22]. A/B testing that compares the total user actions on all items, not only recommended items, can reveal the difference in the average causal effect (see Section 3.2). Nevertheless, it suffers from large fluctuations due to the variability in natural user behaviors for non-recommended items: some users tend to purchase more items than others. A large number of users is required to compensate for such fluctuations, making online experiments costly and risky.

In this paper, we propose efficient online evaluation methods for the causal effect of recommendations based on interleaving. Interleaving generates a list from the lists ranked by the two models to be compared [3]. Whereas previous interleaving methods only measure the outcomes of items in the intersection of the original and interleaved lists, our proposed methods also measure the outcomes of items in the original lists but not in the interleaved list. We propose an interleaving method that selects items with equal probability for unbiased evaluation. With unequal selection probabilities, the evaluation might be biased due to confounding [8] between recommendation and potential outcomes, leading to inaccurate judgments of the recommendation models. We remove the possible bias by properly weighting the outcomes based on the inverse propensity score (IPS) method used in causal inference [16, 21]. This enables the use of a more general interleaving framework that only requires non-zero probabilities to be selected for any item in the original lists. As an instance of the framework, we propose a causal balanced interleaving method that balances the number of items chosen from the two compared lists. To verify the unbiasedness and efficiency of the proposed interleaving methods, we simulate online experiments to compare ranking models.

The contributions of this paper are summarized as follows.

- We propose the first interleaving methods to compare recommendation models in terms of their causal effect.
- We verify the unbiasedness and efficiency of the proposed methods through simulated online experiments

# Related Work

## Interleaving Methods

Interleaving is an online evaluation method for comparing two ranking models by observing user interactions with an interleaved list that is generated from lists ranked by the two models to be compared [3]. Several interleaving methods have been proposed for evaluating information retrieval systems. Balanced interleaving [14, 15] generates an interleaved list from two rankings to be compared such that the highest ranks in the interleaved list ùëòùê¥ and ùëòùêµ from the two rankings ùê¥ and ùêµ, respectively, are the same or different by at most one. Team draft interleaving [19] alternatively selects items from compared rankings, analogously to selecting teams for a friendly team-sports match. Probabilistic interleaving [9] selects items according to probabilities that depend on the item ranks. Optimized interleaving [18] makes the properties required for interleaving in information retrieval explicit and then generates interleaved lists by solving an optimization problem to fulfill those properties. Interleaving methods have been extended to multileaving that compare multiple rankings simultaneously [30, 31]. Multileaving has been also applied to the evaluation of a news recommender system [11]. The objective of previous interleaving methods is to evaluate how accurately the rankings reflect queries or user preferences, whereas our goal is to evaluate rankings in terms of the causal effect. To the best of our knowledge, at present there are no interleaving methods for causal effects.

## Recommendation Methods for the Causal Effect

Recommendations can affect users‚Äô opinions [5] and induce users‚Äô actions [6, 13]. However, users‚Äô actions on recommended items could have occurred even without the recommendations [32]. Building recommendation models that target the causal effect is challenging because the ground truth data of causal effects are not observable [10]. One approach is to train prediction models for both recommended and non-recommended outcomes and then to rank the items based on the difference between the two predictions [1, 24]. Another approach is to optimize models directly for the causal effect. ULRMF and ULBPR [25] are respectively pointwise and pairwise optimization methods that use label transformations and training data samplings designed for causal effect optimization. DLCE [27] is an unbiased learning method for the causal effect that uses an IPS-based unbiased learning objective. There are also neighborhood methods for causal effects [28] that are based on a matching estimator in causal inference. These prior works on causal effects evaluated methods offline and did not discuss protocols for online evaluation. In this study, we develop online evaluation methods and compare some of the aforementioned recommendation methods in simulated online experiments.

Another line of works in the area of causal recommendation aims for debiasing [4]. Several methods have been proposed to learn users‚Äô true preferences from biased (missing-not-at-random) feedback data [2, 23, 29, 33].1 These methods can be regarded as predicting interactions with recommendations (i.e., $Y_{ui}^T$, defined in the next section). Hence, we can evaluate them using previous interleaving methods.

# Evaluation Methods for the Causal Effect of Recommendatios

## Causal Effect of Recommendations

In this subsection, we define the causal effect of recommendations. Let $U$ and $I$ be sets of users and items, respectively. Let $Y_{ui} \in {0, 1}$ denote the interaction (e.g., purchase or view) of user $u \in U$ with item $i \in I$. User interactions may differ depending on whether the item is recommended or not. We denote the binary indicator for the recommendation (also called the treatment assignment) by $Z_{ui} \in {0, 1}$. Let $Y_{ui}^T$ and $Y_{ui}^C \in {0, 1}$ be hypothetical user interactions (also called potential outcomes [22]) when item $i$ is recommended to $u$ ($Z_{ui} = 1$) and when it is not recommended ($Z_{ui} = 0$), respectively. The causal effect $\tau_{ui}$ of recommending item $i$ to user $u$ is defined as the difference between the two potential outcomes: $\tau_{ui} = Y_{ui}^T ‚àí Y_{ui}^C$, that takes ternary values, $\tau_{ui} \in {‚àí1, 0, 1}$. Using potential outcomes, the observed interaction can be expressed as

$$
Y_{ui} = Z_{ui} Y_{ui}^T +(1 -Z_{ui})Y_{ui}^C
\tag{1}
$$

$Y_{ui} = Y_{ui}^T if $i$ is recommended ($Z_{ui}= 1$) and $Y_{ui} = Y_{ui}^C$ if it is not recommended ($Z_{ui} = 0$). Note that $Y_{ui}^T$ or $Y_{ui}^C$ cannot both be observed at a specific time; hence, $\tau_{ui}$ is not directly observable.

The recommendation model $A$ generates a recommendation list $L_u^A$ for each user. The average causal effect of model $A$ is then defined as

$$
\hat{\tau}_A = \frac{1}{n|S_A|} \sum_{u \in S_A} \sum_{i \in L_{u}^A} \tau_{ui}
\tag{2}
$$

In this work, we evaluate models using the above metric.2 That is, when comparing two models, we regard $A$ to superior to $B$ when $\tau_A > \tau_B$.

## A/B testing for the Causal Effect

For A/B testing, we randomly select non-overlapping subsets of users $S_A$ and $S_B$ (i.e., $S_A$, $S_B \in U$  and $S_A \cap S_B = \empty$) and apply models $A$ and $B$ to each subset. Let $n = |L_u^A| = |L_u^B|$ be the size of the recommendation list, which we assume to be constant. The subset average causal effect is then defined as

$$
\tag{3}
$$

This converges to $\tau_A$ as $|S_A|$ increases. 

The typical evaluation metrics for A/B testing are either based on total user interactions (such as sales or user engagement) or only on interactions with recommended lists (such as click-through rates or conversion rates) [13]. Here we show that the former is a valid evaluation for the causal effect. The total user interactions divided by the number of recommendations can be expressed as

$$
\tag{4}
$$

Because the rightmost term in the final equation does not depend on the model, we can compare $\hat_{\tau_A}$ and $\hat_{\tau_B}$ by comparing $\hat{Y}_A^{total}$ and $\hat{Y}_B^{total}$. On the other hand, the average interactions with the recommended lists can be expressed as

$$
\tag{5}
$$

Hence, the evaluation based only on interactions with recommended lists is not valid testing for the causal effect. 

Although A/B testing with Eq. (4) can be used for unbiased model comparisons, it may have large variance due to the variability in natural user behaviors (i.e., the potential outcomes under no recommendations, $Y_{ui}^C$). If users in $S_A$ tend to purchase more items than those in $S_B$, $\sum_{u\in S_A} \sum_{i \in I}Y_{ui}^C$ becomes larger than $\sum_{u\in S_B} \sum_{i \in I}Y_{ui}^C$, thereby altering the comparison in Eq. (4). To minimize such discrepancies, a sufficiently large number of users need to be recruited for A/B testing. We thus introduce more efficient evaluation methods in the next subsection.

##  Interleaving for the Causal Effect

In this subsection, we propose interleaving methods for the online evaluation of the causal effects of recommendations. Previous interleaving methods only measure outcomes in the interleaved lists: they only include $Y_{ui}^T$ and lack information on $Y_{ui}^C$. Further, if the item selection for the interleaved list is not randomized controlled, the naive estimate from the observed outcomes might be biased due to the confounding between recommendations and potential outcomes. We need to remedy the bias for valid comparison. 

Here we describe the problem setting of interleaving for the causal effect. For each user $u$, we construct the interleaved list $L_u$ from the compared lists $L_u^A$ and $L_u^B$. We observe outcomes ${Y_{ui}}$ for all items $i \in I$. Note that $Y_{ui} = Y_{ui}^T$ if item $i$ is in the interleaved list ($i \in L_{u}$ or equivalently, $Z_{ui} = 1$) and $Y_{ui} = Y_{ui}^C$ if it is not in the list ($i \in I \setminus L_u$ or equivalently, $Z_{ui}= 0$). We want to compare the average causal effects of lists $L_u^A$ and $L_u^B$:

$$
\tag{6}
$$

We need to estimate the above values from observed outcomes because we cannot directly observe $\tau_{ui}$.

If the items in $L_u^A$ and $L_u^B$ are randomly assigned to the interleaved list independent of the potential outcomes, that is, $(Y_{ui}^T, Y_{ui}^C) \perp Z_{ui}$, the case can be regarded as a randomized controlled trial (RCT) [12, 22].3 We can then simply estimate $\tau_{L_u^A}$ as the difference in average outcomes for items on and not on the interleaved list:

$$
\tag{7}
$$

One way to realize such a randomized assignment is to select ùëõ items from $L_u^A \cup L_u^B$ with equal probability: $p = n \setminus |L_u^A \cup L_u^B|$. We call this method equal probability interleaving (EPI). 

The independence requirement heavily restricts the potential design space of interleaving methods. We thus derive estimates that are applicable to more general cases. Denote the probability (also called the propensity) of being included in the interleaved list $L_u$ by $p_{ui} = E[Z_{ui} = 1|X_{ui}]$. We assume that 1) the covariates $X_{ui}$ contain all confounders of $(Y_{ui}^T, Y_{ui}^C)$ and $Z_{ui}$, and 2) the treatment assignment is not deterministic ($0 < p_{ui} < 1$ for $i \in L_u^A \cup L_u^B$).4 Assumption 1 is equivalent to conditional independence: $(Y_{ui}^T,Y_{ui}^C) \perp Z_{ui}|X_{ui}$. When we design an interleaving method, we know the covariates that affect ùëçùë¢ùëñ and Assumption 1 can always be satisfied.5 Therefore, the only restriction for interleaving methods is Assumption 2 (also called positivity). 

Under these assumptions, we can construct an unbiased estimator using IPS weighting [16]:

$$
\tag{8}
$$

This estimator is unbiased since

$$
\tag{9}
$$

We propose a general framework for interleaving as follows. 

- (1) Construct interleaved lists ${L_u}$ using an interleaving method that satisfies positivity (Assumption 2). 
- (2) Conduct online experiments and obtain outcomes ${Y_{ui}}$. 
- (3) Estimate $\tau_{L_u^A}$ and $\tau_{L_u^B}$ by Eq. (8) and compare them.

As an example of a valid interleaving method that satisfies positivity, we propose causal balanced interleaving (CBI), the pseudo-code for which is shown in Algorithm $1$. CBI alternatively selects items from each list to balance the items chosen from each list. The item choice in each round is not deterministic in order to satisfy the positivity required for causal effect estimates. The propensity depends on whether an item is in the intersection, $1(i \in L_u^A \cap L_u^B)$. If an item is included in both lists, it has a greater probability of being chosen. The propensity also depends on the cardinality of the union of the compared lists, $|L_u^A \cup L_u^B|$, because smaller cardinality implies that each item has a greater chance of being selected. The possible values of the covariates are limited: $1(i \in L_u^A \cap L_u^B)$ is binary and $|L_u^A \cup L_u^B| \in [n, 2n]$. Hence, we can easily compute the propensity numerically by repeating Algorithm $1$ a sufficient number of times and recording $Z_{ui}$ for each combination of covariates.
