## 参考

- https://developers.cyberagent.co.jp/blog/archives/38632/#4__

## 基本的なモデル定義

SLIMもEASEも、以下のような式と図で表すことができる。

![](https://developers.cyberagent.co.jp/blog/wp-content/uploads/2022/12/d74d73056d85eb759d70b083f3208973.png)

図から入力も出力もfeedback行列(=rating matrix、評価行列)になっていることがわかる。

この手法では、評価行列Xにかけると再度評価行列を得るような**重み行列Bを学習**して、推薦を行う。

$$
X \in \mathbb{R}^{|U| \times |I|} \\
X \approx XB
$$

($\approx$はニアリーイコールの意味。$\sim$や$simeq$と同様。)
重み行列$B\in \mathbb{R}^{|I|\times|I|}$は、**あるアイテムを好む場合にもう片方のアイテムをどれだけ好むか**という**アイテム同士の共起行列**と捉えることができる。
また、モデルを汎化させるために、Bの対角行列を0とする。
これはBの対角成分が、あるアイテムが好まれた場合にそのアイテム自身が好まれることを示す要素だから。

ユーザ$u \in U$が与えられた時に、アイテム$i \in I$の予測スコア($\hat{X}_{u,i}$)は以下の式で定義される。

$$
\hat{X}_{u,i} = X_{u,.} \cdot B_{.,i}
$$

## SLIM: Sparse Linear Methods for Top-N Recommender Systems

これは[ICDM2011で発表された論文](https://ieeexplore.ieee.org/document/6137254)で、その後も派生となるモデルが研究されてきた。
この論文は当時のメモリベースの高速だが品質が劣る、モデルベースの高品質だが高速性が劣る、というメリデメをどちらも解決するモデルとして登場した。

### 目的関数

モデルの概要は前述したので、ここではEASEへの導入のためにも**目的関数について**説明する。

目的関数は以下のように表される。

$$
\min_{B} ||X - XB||^{2}_F
+ \frac{\lambda_2}{2} \cdot ||B||^{2}_{F}
+ \lambda_1 \cdot ||B||_{1} \\

s.t. B \geq 0, \text{diag}(B)=0
$$

- squre lossを用いており、L1正則化とL2正則化を持つ。
  - 一般的にL1正則化はよりデータをスパースにすることを目的に、L2正則化は過学習を防ぐために導入される。
  - スパース性はアイテムのsimilarityにクラスタがあることから性能や解釈性が上がったり、行列のサイズがコンパクトになることでメモリに優しく推論が速くなったりといいことが多い。
- また、$B \geq 0$と$\text{diag}(B)=0$という２つの制約がある。
  - 前者の非負制約は解釈のためによく用いられる制約だが、一般的に非負制約をはずした方が性能が高いことが知られている。
  - 後述するEASEではこの制約がなく、その点に関する比較の実験も行なっている。
  - 後者の制約は、このモデルを汎化させるために導入されている。

## SLIM から EASEへ

EASEは[WWW2019で発表された論文](https://doi.org/10.1145/3308558.3313710)である。

モデルの概要は前述したSLIMと同じだが、以下の二点が特徴的：

-
