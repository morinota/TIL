## アウトライン案

- タイトル案: 推薦システムのオフライン評価&学習を諦めない! 確率的な推薦方策への移行を考える
- TL;DR
- はじめに
  - 背景(なぜこのトピックを選んだ??)
  - 今回喋りたいこと
    - 最終的に主張したいこと: データ収集方策として確率的方策を導入してみる価値はありそう...! というか、推薦方策は基本的に確率的方策の方が評価もしやすいし継続的な訓練もしやすそう...!!
- 推薦システムが解きたい問題は予測タスクではなく、意思決定の最適化タスクである!
- 推薦システムのオフライン評価はなぜオンライン評価とズレてしまうんだっけ?? まず理論の観点から考えてみる。
  - まず推薦システムの問題を定式化してみる!
  - オンライン評価はオフライン評価の特別なケースである!
  - データ収集方策と評価方策の違いがオフライン評価を難しくする
  - データ収集方策と評価方策が違っていても何とかしたい! 王道のIPS推定量!
  - データ収集方策は確率的方策の方が都合が良いかも...!
- データ収集方策は確率的方策の方が都合良いのか?? 実験してみる!
  - open bandit pipeline を使った実験
  - 実験方法
  - 実験結果
  - 考察
- 決定的な推薦方策から確率的な推薦方策への簡単な移行アイデア
  - シンプルにepsilon-greedyを使う!
  - プラケットルースモデルに基づいてランキングの確率的サンプリング
  - プラケットルースモデルの高速化ver.:ガンベルソフトマックストリックを使う
- おわりに

# タイトル: 推薦システムのオフライン評価&学習を諦めない!確率的なデータ収集方策の有効性を実験してみた

## TL;DR

## はじめに

推薦システムを本番運用する際、オフライン評価が難しいという課題に直面した経験がある方も多いのではないでしょうか。
私自身も以前、[「NewsPicksに推薦システムを本番導入する上で一番優先すべきだったこと」](https://tech.uzabase.com/entry/2024/08/29/161828) というブログやイベントでの発表を通じて、この問題に取り組みました。結論として、「オフライン評価は一旦諦めて、まずはA/Bテスト（オンライン評価）がしやすい基盤を作ることが最優先」と考え、本番導入を進めてきました。

### 背景

- 先日のRecommendation Industry Talksというイベントで「推薦システムを本番導入する上で一番優先すべきだったこと」というタイトルで発表してきた。
  - また、同様の内容を整理して「[**NewsPicksに推薦システムを本番導入する上で一番優先すべきだったこと](https://tech.uzabase.com/entry/2024/08/29/161828)」**というタイトルでブログ化した。
  - 上記発表&ブログでの結論: **推薦システムのオフライン評価が難しいから一旦諦めて、まずはいかにA/Bテスト(オンライン評価)しやすい基盤を設計することが大事だった**...!
  - （オフライン評価とオンライン評価については、ブログの説明などを参照してください）

<https://speakerdeck.com/morinota/tui-jian-sisutemuwoben-fan-dao-ru-surushang-de-fan-you-xian-subekidatutakoto-newspicksji-shi-tui-jian-ji-neng-nogai-shan-shi-li-woyuan-ni>

一方で、もしオフライン評価を適切に行えるのであれば、それはめちゃめちゃ嬉しいわけです。
例えば次のようなメリットがあるでしょう。

- **高速なフィードバック**: オンライン評価に比べ、実験結果を即座に確認できる。
- **リスクゼロ**: 悪い推薦方策をオンライン環境に出してしまうような、ユーザ体験を毀損するリスクがない。

さらに、オフライン評価がうまく機能しないと、MLOpsにおける「継続的な訓練（Continuous Training）」にも支障をきたす可能性があると思います。「MLOps」とは「機械学習の成果をスケールさせるための様々な取り組み」です。
その中で「継続的な訓練」は、DevOpsの原則である「継続的な改善（Continuous Improvement）」を機械学習に応用する概念ですが、もしオフライン評価で推薦方策の良し悪しを正しく評価できない場合、「継続的な訓練」によって不適切な方向へ方策を学習・最適化してしまうリスクが高まります。

以上を踏まえると、**オフライン評価 & 学習という技術は、MLOpsにおける「継続的な訓練」を正しく機能させるため、ひいては機械学習の成果をスケールさせるために非常に重要な技術なのでは**、と思っているわけです...!! :thinking:

- 一方で、やっぱりオフライン評価できた方が嬉しい...!
  - A/Bテストなどのオンライン評価と比べて、高速にフィードバックを得られる。ユーザ体験を毀損するリスクがない、という利点。
- というか、オフライン評価が難しいってことは、MLOpsにおける「継続的な訓練」も上手く機能しないのでは…??
  - MLOps (=機械学習の成果をスケールさせるための様々な取り組み) では、DevOpsの原則の一つである「継続的な改善(Continuous Improvement)」を「継続的な訓練(Continuous Training)」によって行っていく。
  - 推薦モデルの良し悪しをオフライン環境で正しく評価できなかったら、「継続的な訓練」をしても適切ではない方向に最適化していってしまいそう??
  - → MLOpsにおける「継続的な訓練」によって「継続的な改善」を達成するためには、ひいては**機械学習の成果をスケールさせるためには、オフライン評価 & 学習って非常に重要な技術なのでは…!!**

### 今回喋りたいこと

- 前述の経緯で、オフライン評価のアプローチをいろいろ調べてると…
  - 決定的な推薦方策によって収集したログデータだと、かなり打ち手がなさそうな印象…!!
  - 確率的な推薦方策によって、ある程度探索的に収集したデータを使えば、だいぶ打ち手が増えそう。
- というわけで、確率的な推薦方策を本番導入することを色々考えてみた!
- ちなみに、決定的な推薦方策と確率的な推薦方策

- 決定的(deterministic)な方策
  - 常に同じ行動を選択する方策。
  - ex.) ユーザ1に対して、アイテムAを100%の確率で推薦する。
- 確率的(stochastic)な方策
  - ある確率でランダムに行動を選択する方策。
  - ex.) ユーザ1に対して、アイテムAを50%、アイテムBを30%、アイテムCを20%の確率で推薦する。

今回は、推薦方策のオフライン評価・学習に使用しやすいログデータを収集したいというモチベーションで、理論面での理解と、open bandit pipelineを用いた合成データによる実験を通して、「データ収集方策は決定的方策であるべきか、確率的方策であるべきか」の考えを深めることを目的とする。

## 推薦システムが解きたい問題は予測タスクではなく、意思決定の最適化タスクである

- 推薦システムは、意思決定の最適化問題を解くタスクである
  - 例えば、ニュース推薦の場合
    - 何らかの方策 (i.e. ロジック) によって、(ユーザ, 記事) ペアの関連度や嗜好度などのスコアを予測
    - スコアの予測値に基づいて、ユーザにどんな記事を推薦するかの意思決定を行う

> 予測というよりもむしろデータに基づいた意思決定の最適化問題
> (*書籍「反実仮想機械学習」より引用*)

- データに基づいて導かれる意思決定の規則 = 意思決定方策(decision-making policy)。
  - オフライン評価のモチベーションは、この意思決定方策の性能をログデータに基づいて正確に評価できるようになること。でもこの**意思決定方策のオフライン評価がなかなかに難しい…!** 🤔
  - (今回は特に推薦タスクの話がしたいので、以降では意思決定方策ではなく「推薦方策」という用語を使います)

以下の図は、Booking.comさんのMLアプリケーション開発運用の教訓に関する論文で、以下の「相関がなかった」という有名な相関図。

![](https://i.imgur.com/TZEaYJu.png)

*引用元: 150 Successful Machine Learning Models: 6 Lessons Learned at Booking com, Bernardi et al., 2019*

オフライン評価は難しいが、やっぱりできたらかなり嬉しい。

- オフライン評価が確度高くできる世界とそうでない世界の比較（何回も言っちゃってるかもしれないのでトグルに入れました…!）（オフライン学習ができる世界も含めてしまってるかも）
  - 前者の世界
    - A/Bテストなどのオンライン評価の前に有効そうな方策を絞り込む事ができ、仮説検証をより高速に回す事ができる。
    - ユーザ体験を悪化させ得る施策を、オンライン環境に出す前に検知する事ができる。
    - 「継続的な訓練」によって推薦方策を我々が望む方向に改善 & 最適化しやすい
  - 後者の世界
    - 全ての仮説に対してA/Bテストする必要があり、仮説検証の速度が下がる。(A/Bテストの運用コストを下げても、どうしてもオフライン実験するよりは時間がかかる。10倍くらい?)
    - ユーザ体験を悪化させ得る方策を、オンライン環境に出してしまう可能性。
    - 「継続的な訓練」をしても、誤った方向に最適化してしまう可能性。

## 推薦システムのオフライン評価はなぜオンライン評価とズレてしまうんだっけ?? まず理論の観点から考えてみる

### まず推薦システムの問題を定式化してみる

必要に応じて記号や数式などを使って、「推薦システムってどんな問題を解こうとしてるんだっけ??」「なんでオフライン評価が難しくなっちゃうんだっけ??」ということを整理してみます。

- まずニュース推薦システムの問題設定
  - 全ユーザの累積報酬を最大化することを目指し、システムは候補ニュース集合から、各ユーザ向けにニュースを逐次的に選択し表示していく。
- 上述のような推薦戦略を設計する方法は、contexual banditの分野で研究されている**sequential decision-making problem(逐次的な意思決定問題)**として定式化できる。
  - ニュース推薦システムは意思決定を行うエージェントであり、意思決定の指針として推薦方策（ポリシー）を持つ
  - ニュースアイテムは行動（選択肢）
  - ユーザやニュースアイテムの特徴や状態は、意思決定の際に考慮されるコンテキスト（文脈）
  - 選択した行動に対して、クリックや購入などの報酬が観測される
  - 各試行(roundやtimestep等とも言われる印象...!)において、エージェントはコンテキストを受け取り、方策に従って行動を選択し、報酬を受け取る。

概念図で表すとこんな感じですね...!!

<!-- 後でfigmaとかで作ってみる! -->

数式記号を使って表すと…

- 特徴量(feature, context)のベクトル: $x$ (ex. ユーザ特徴量など)
- 離散的な行動(action): $a \in A$ (ex. 推薦アイテムなど)
- 行動の結果として観測される報酬(即時報酬?): $r$

また、推薦方策 $\pi$ を行動空間 $A$ 上の条件付き確率分布とする (i.e. あるコンテキスト $x$ を持つユーザに対して、行動 $a$ を推薦する確率)

$$
\pi(a|x) := p_{\pi}(a|x)
$$

また、推薦方策 $\pi$ の良し悪しを表す性能(policy value) $V(\pi)$ を、以下のような(x,a,r)の同時分布に対する報酬 $r$ の期待値で定義する:

```math
V(\pi) := \mathbb{E}_{p(x, a, r)}[r] =\mathbb{E}_{p(x)\pi(a|x)p(r|a,x)}[r]
```

（例えば、推薦システムの導入目的が「サービスの課金率を向上させたい！」という場合は、報酬 $r$ を「そのユーザが課金したか否かを表す二値変数」として定義し、その期待値 $\mathbb{E}{p(x, a, r)}[r]$ は課金率になるでしょう）

**さて、この推薦方策の性能 $V(\pi)$ は、実環境においては我々は知ることはできません**。なぜかというと、$V(\pi)$ の定義式に出てくる $p(r|a,x)$ を我々は知り得ないからですね。
この $p(r|a,x)$ というのは、「あるコンテキスト $x$ を持つユーザに対して、ある行動 $a$ を選択したときに得られる報酬 $r$ の確率分布」を表しています。ざっくり「このユーザにこのニュースを推薦したら、課金してくれる確率がxx％やで！」みたいな情報です...!!
方策の性能 $V(\pi)$ を計算するためには、この $p(r|a,x)$ を、可能性がある全てのコンテキストと、方策が選び得る全ての行動に対して知っている必要があるわけなので、無理です...!! （逆にこの情報が既知だったら、簡単に課金率を最大化できそうですね...!!:thinking:）
（コンテキストの確率分布 $p(x)$ も未知で、推薦方策の行動の確率分布 $\pi(a|x)$ は既知ですね!:thinking:）

というわけで、推薦方策の（真の）性能 $V(\pi)$ がわからない中で、どのように我々はよりビジネスに有効な推薦方策を設計し改善していくのでしょうか。
そのための方法が、オンライン評価やオフライン評価なんです。
<!-- より具体的には、評価したい方策そのものを本番稼働させて得られた観測データを使って性能 $V(\pi)$ を推定するのがオンライン評価、評価したい方策とは異なる方策を本番稼働させて得られた観測データを使って性能 $V(\pi)$ を推定するのがオフライン評価です。 -->
より具体的には、本番稼働させた推薦方策（データ収集方策と呼びます）によって得られたログデータを使って、推薦方策の性能 $V(\pi)$ を推定するのがオンライン評価、データ収集方策とは異なる推薦方策（評価方策と呼びます）によって得られたログデータを使って、推薦方策の性能 $V(\pi)$ を推定するのがオフライン評価です。

### オンライン評価はオフライン評価の特別なケース（簡単ver.）である

例えばオンライン評価の場合、A/Bテストなどで新しい推薦方策 $\pi$ を本番稼働させて、収集したログデータ $D_{\pi}:=\{(x_i, a_i, r_i)\}_{i=1}^{n}$ を元に"なんらかの指標"を計算します。それを既存の推薦方策と比較して「テストパターン（treatment群）の方が課金率が良い!」「現行パターン（control群）の方がCTRが良い!」などと判断するわけです。

A/Bテストなどのオンライン評価時の"なんらかの指標"として最も一般的に利用されているのは、以下のようなAVG推定量になります。
（例えば我々がA/Bテストの際に計算しているCTRや課金率などは、 方策の性能 $V(\pi)$ そのものではなく、実際には観測データをもとに下記の方法で推定した $V(\pi)$ の推定値と言えるわけですね...!:thinking:）

```math
\hat{V}_{AVG}(\pi;D_{\pi}) = \frac{1}{n} \sum_{i=1}^{n} r_i
```

例えば推薦方策 $\pi$ を本番稼働させて、以下のような観測データを得られたとします。

<!-- テーブル(カラムはtimestep, context, action, reward) レコード数は5個くらい -->

| timestep | context | action | reward |
| --- | --- | --- | --- |
| 0 | [年齢30歳, エンジニア, ...] | 記事A | 1 |
| 1 | [年齢20歳, 学生, ...] | 記事B | 0 |
| 2 | [年齢40歳, 経営者, ...] | 記事C | 1 |
| 3 | [年齢50歳, 医師, ...] | 記事A | 0 |
| 4 | [年齢60歳, 自営業, ...] | 記事D | 1 |

この観測データを使って $\pi$ の性能 $V(\pi)$ をAVG推定量で推定すると、以下のようになります。

$$
\hat{V}_{AVG}(\pi;D_{\pi}) = \frac{1}{5} \sum_{i=1}^{5} r_i
\\
= \frac{1}{5} \times (1 + 0 + 1 + 0 + 1) = 0.6
$$

オンライン評価の場合、言い換えると性能を**評価したい方策自身によって観測データが収集された場合**（OPEの分野では「**データ収集方策と評価方策が一致する場合**」と表現されます...!）には、AVG推定量によって推薦方策の真の性能 $V(\pi)$ をバイアスなく推定できます。

<details>

<summary>ちなみに、オンライン評価の場合にAVG推定量が真の性能に対してバイアスがなくなることは、以下のように導出できます(メモメモ...!)</summary>

---

オンライン評価の場合に、AVG推定量が推薦方策の真の性能 $V(\pi)$ に対してバイアスのない推定量であることは、AVG推定量 $\hat{V}_{AVG}(\pi;D_{\pi})$ の観測データ $D_{\pi}$ に対する期待値を計算することで証明できます。

```math
\mathbf{E}_{D_{\pi}}[\hat{V}_{AVG}(\pi;D_{\pi})] = \mathbb{E}_{D_{\pi}}[\frac{1}{n} \sum_{i=1}^{n} r_i]
```

(ここで、期待値は線形性(E[a+b] = E[a] + E[b])を持つので...!:thinking:)

```math
= \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}*{p(x)\pi(a|x)p(r|a,x)}[r]
```

(ここで、各試行 i の(x,a,r)は独立同一分布に従うと仮定すると、$E_{x,a,r}[r]$ はiに依存しないから、sumの外に出せるはずなので...! :thinking:)

```math
= \mathbb{E}_{p(x)\pi(a|x)p(r|a,x)}[r]
```

(方策の真の性能の定義式を思い出すと...! :thinking:)

```math
= V(\pi)
```

上記の通り、観測データに対するAVG推定量の期待値が、推薦方策の真の性能 $V(\pi)$ に一致するので、バイアス $V(\pi) - \mathbf{E}_{D_{\pi}}[\hat{V}_{AVG}(\pi;D_{\pi})]$ はゼロであると言えますね (少しわかってきた...!:thinking:)

---

</details>

この「観測された報酬を平均するだけ」というシンプルな考え方で推薦方策の性能を確度高く評価できるからこそ、A/Bテストなどのオンライン評価は非常に有用な手法として広く利用されているんだなぁと思いました:thinking:

### データ収集方策と評価方策の違いがオフライン評価を難しくする

さてオンライン評価では、(暗黙的に)AVG推定量を使って推薦方策の性能を推定していることがわかったところで、続いてオフライン評価のことを考えてみます。
オンライン評価とは異なりオフライン評価の場合には、評価したい方策 $\pi$ とは別の方策によって収集されたログデータ $D_{\pi_0}$ を使って、推薦方策の性能 $V(\pi)$ を推定することになります。(OPEの分野では、「**データ収集方策と評価方策が異なる場合**」とも表現されます...!)

| timestep | context | action | reward | 評価したい方策が選ぶaction |
| --- | --- | --- | --- | --- |
| 0 | [年齢30歳, エンジニア, ...] | 記事A | 1 | 記事A |
| 1 | [年齢20歳, 学生, ...] | 記事B | 0 | 記事A |
| 2 | [年齢40歳, 経営者, ...] | 記事C | 1 | 記事B |
| 3 | [年齢50歳, 医師, ...] | 記事A | 0 | 記事C |
| 4 | [年齢60歳, 自営業, ...] | 記事D | 1 | 記事D |

オンライン評価の例と同じ観測データが得られた場合を考えてみましょう。この観測データは、評価したい方策 $\pi$ とは異なる方策 $\pi_0$ によって収集されたものだとします。
この場合は評価方策 $\pi$ の性能をAVG推定量を使って計算することはできません。
なぜなら、そもそも評価したい方策 $\pi$ は、同じコンテキストに対して、本番環境で稼働した方策 $\pi_0$ とは異なる意思決定をするかもしれないからですね。
上の例では、データ収集方策は年齢30歳の職業エンジニアのユーザに対して記事Aを推薦していますが、評価方策は年齢30歳のエンジニアのユーザに対して記事Bを推薦するとしたら、AVG推定量をどう計算したらいいんだ??ってなってしまいます:thinking:

最もシンプルな考え方としては、「**データ収集方策と評価方策が同じ意思決定をした時の報酬の平均値を取って、AVG推定量っぽく計算しよう!**」という戦略が考えられます。それが以下のようなnaive推定量として知られるものです(ReplayMethodとも呼ばれるようです...!!)

$$
\hat{V}_{naive}(\pi;D_{\pi_0}) = \frac{1}{\sum_{i=1}^{n} \mathbb{I}[\pi(a_i|x_i) = 1.0]} \sum_{i=1}^{n} \mathbb{I}[\pi(a_i|x_i) = 1.0] r_i
$$

ここで、$\mathbb{I}[hoge]$ は、中の条件式がTrueの場合に1、Falseの場合に0を返す関数です(一般的に指示関数、indicator functionと言うようです...!:thinking:)。
なので、$\mathbb{I}[\pi(a_i|x_i) = 1.0]$ は、$i$ 番目の試行において、評価方策 $\pi$ がコンテキスト $x_i$ に対してデータ収集方策と同じ行動 $a_i$ を選択する場合に1、選択しなかった場合に0を返す関数です。

なので上の例の場合、評価方策 $\pi$ の性能をnaive推定量を使って計算すると、以下のようになります。

$$
\hat{V}_{naive}(\pi;D_{\pi_0}) = \frac{1}{(1 + 0 + 0 + 0 + 1)} \times (1 \times 1 + 0 \times 0 + 0 \times 1 + 0 \times 0 + 1 \times 1) = 1.0
$$

データ収集方策と評価方策が同じ行動を選択したのはtimestep 0とtimestep 4のみで、その時は両方とも報酬1を得ているので、naive推定量による評価方策の性能は1.0になります。

ちなみに、評価方策が確率的な場合には、naive推定量の定義式はたぶん以下のようになるのかなと思ってます。前述の式を一般化した形ですね...!:thinking:

$$
\hat{V}_{naive}(\pi;D_{\pi_0}) = \frac{\sum_{i=1}^{n} \pi(a_i|x_i) r_i}{\sum_{i=1}^{n} \pi(a_i|x_i)}
$$

さて、このnaive推定量は、データ収集方策と評価方策が同じ行動を選択した場合のみの報酬の平均値を取ることで、評価方策の性能を推定しようとしているわけですが、実際のところこの推定量は確度高く真の性能を推定できるのでしょうか。
（ここまでの話の流れを踏まえても、体感的にも、なんとなくあんまり正しく真の性能を推定できなさそうな感じがしますね。特に、データ収集方策と評価方策の意思決定の傾向が異なれば異なるほど正しく評価できなさそう...!:thinking:）

実際、このnaive推定量は、真の性能に対してバイアス $hoge$ を持つことが知られています。

ちなみに、naive推定量が真の性能に対してバイアスを持つことは、以下のように導出できます(メモメモ...!)

---

AVG推定量の場合と同様に、naive推定量の観測データに対する期待値を計算して、真の性能の定義と比較してみます。

$$
\mathbf{E}_{D_{\pi_0}}[\hat{V}_{naive}(\pi;D_{\pi_0})] = \mathbb{E}_{D_{\pi_0}}[\frac{1}{\sum_{i=1}^{n} \mathbb{I}[\pi(a_i|x_i) = 1.0]} \sum_{i=1}^{n} \mathbb{I}[\pi(a_i|x_i) = 1.0] r_i]
\\
% 期待値は線形性を持つ (E[a+b] = E[a] + E[b]) ので...!
\\
= \frac{1}{\sum_{i=1}^{n} \mathbb{I}[\pi(a_i|x_i) = 1.0]} \sum_{i=1}^{n} \mathbb{I}[\pi(a_i|x_i) = 1.0] \mathbb{E}_{p(x)\pi_0(a|x)p(r|a,x)}[r]
\\

$$
---

</details>

### データ収集方策と評価方策が違っていても何とかしたい! 王道のIPS推定量

AVG推定量では、データ収集方策と評価方策が異なるほど大きなバイアスを持つ。
AVG推定量のバイアスを打ち消す最も基本的な戦略が、以下のIPS(Inverse Propensity Score)推定量。

```math
\hat{V}_{IPS}(\pi_1;D_{\pi_0}) := \frac{1}{n} \sum_{i=1}^{n} \frac{\pi_1(a_i|x_i)}{\pi_0(a_i|x_i)} r_i = \frac{1}{n} \sum_{i=1}^{n} w(x_i, a_i) r_i
```

なお、$w(x_i, a_i) := \frac{\pi_1(a_i|x_i)}{\pi_0(a_i|x_i)}$ を重要度重み(importance weight)と呼び、評価方策 $\pi_1$ とデータ収集方策 $\pi_0$ による行動選択確率の比を表す。

**「反実仮想機械学習」を読んでいてもOPE推定量の多くがIPS推定量の拡張ver.っぽく、最も基本的なオフライン評価の戦略の1つと言ってもよさそう**です...!:thinking:

### IPS推定量を使う上で満たすべき仮定: 共通サポート仮定

そしてIPS推定量は、以下の**共通サポート仮定**を満たす場合に真の性能に対して不偏になります:

$$
\pi_1(a|x) > 0 -> \pi_0(a|x) > 0, \forall a \in A, \forall x \in X
$$

つまり、「**評価方策 $\pi_1$ が特徴 $x$ に対してサポートする(=選択する可能性がある)全ての行動 $a$ を、データ収集方策 $\pi_0$ もサポートしていてくれ!**」という仮定です。

### データ収集方策は確率的方策の方が都合が良さそう

さて、IPS推定量（あるいはその拡張ver.たち）を活用するためには、上記の共通サポート仮定をなるべく満たしたい訳です。

ここで、決定的方策と確率的方策の話が出てきます!
データ収集方策 $\pi_0$ と評価方策 $\pi_1$ がそれぞれ、決定的方策/確率的方策であるケース（2^2 = 4通り）を考えてみましょう。

![](https://pbs.twimg.com/media/GVufdqfXkAEkE6W?format=png&name=4096x4096)

- 1. pi_0 = 決定的方策、pi_1 = 決定的方策のケース
  - -> 共通サポート過程を満たせなさそう。唯一満たせるのは、$\pi_0 = \pi_1$ の場合のみなので。
- 2. pi_0 = 決定的方策、pi_1 = 確率的方策のケース
  - -> 共通サポート過程を満たせなさそう。
- 3. pi_0 = 確率的方策、pi_1 = 決定的方策のケース
  - -> 共通サポート過程を満たせそう!
- 4. pi_0 = 確率的方策、pi_1 = 確率的方策のケース
  - -> 共通サポート過程を満たせそう!

これを踏まえると、データ収集方策 $\pi_0$ が決定的方策の場合、共通サポート仮定を全然満たせなさそうですよね。
一方で、**データ収集方策 $\pi_0$ が確率的方策でさえあれば**、評価方策 $\pi_1$ が決定的方策だろうと確率的方策だろうと、共通サポート仮定

抽象的な表現ですが**方策の探索の度合いが「データ収集方策 ≧ 評価方策」であれば**、共通サポート仮定を満たしやすく、IPS推定量（あるいはその拡張ver.たち）を使ってオフライン評価 & 学習しやすいという認識です...!:thinking:

（ちなみに、共通サポート仮定を満たしてIPS推定量が真の性能に対して不偏になっただけで、オフライン評価難しい問題が全て解決するとは限りません! **バイアスを除去できたとしてもバリアンスの懸念があるから**です。特に推薦タスクにおいて推薦アイテム候補の数が多かったり、推薦アイテムリストの長さが長かったりすると、方策が取りうる行動の選択肢が爆発的に増え、今度はIPS推定量のバリアンスが大きくなる問題に直面していくようです:thinking:）

## 実験: データ収集方策は本当に確率的方策の方が都合良いのか??

### Open Bandit Pipelineを使ったシミュレーション

Open Bandit Pipeline (OBP) は、意思決定の最適化タスクにおけるオフライン評価や学習を簡単に実験できるPythonパッケージです。今回のシミュレーションでは、このOBPを活用して、推薦システムにおけるデータ収集方策とその影響を分析します。

### 実験のふんわり設定を考える

今回の実験では、架空のプッシュ通知ニュースの推薦システムを想定しました。このシステムの目標は、ユーザごとに4種類の候補ニュースから一つを選んで配信し、通知の開封率を最大化することです。ユーザとニュースごとに以下のような情報が与えられています。

- ユーザごとに異なる **コンテキスト**（例: 年齢、職種、過去の閲覧履歴）。
  - 実験では、3次元のコンテキストベクトル $x \in \mathbb{R}^3$ をランダムに生成。
- ニュースごとに異なる **アイテムコンテキスト**（例: 所属カテゴリ、紐づけられたタグ）。
  - 同じく、3次元のコンテキストベクトル $e \in \mathbb{R}^3$ をランダムに生成。

また、プッシュ通知を開封するかどうかを示す **報酬 $r$** をバイナリ変数（1: 開封、0: 未開封）で表します。

各方策 $\pi$ の真の性能 $V(\pi)$ は、その方策を稼働させた場合の開封率の期待値として定義されます。
今回はシミュレーションの都合上、ニュースごとの報酬の期待値は以下のように固定しています。

- ニュース $a_0$: 開封率の期待値 $0.4$
  - (具体的には、E_{p(r|a=a_0, x)}[r] = E_{p(r|a=a_0)}[r] = 0.4)
- ニュース $a_1$: 開封率の期待値 $0.3$
  - (具体的には、E_{p(r|a=a_1, x)}[r] = E_{p(r|a=a_1)}[r] = 0.3)
- ニュース $a_2$: 開封率の期待値 $0.2$
  - (具体的には、E_{p(r|a=a_2, x)}[r] = E_{p(r|a=a_2)}[r] = 0.2)
- ニュース $a_3$: 開封率の期待値 $0.1$
  - (具体的には、E_{p(r|a=a_3, x)}[r] = E_{p(r|a=a_3)}[r] = 0.1)

これにより、実環境では知り得ないはずの各方策の真の性能 $V(\pi)$ を直接計算することができます。
（ちなみに、繰り返しになりますが、実環境では各行動 & 各コンテキストで条件付けた報酬の期待値 $E_{p(r|a, x)}[r]$ は一般的に未知なので、各方策の真の性能 $V(\pi)$ も知ることはできません。それゆえにオンライン評価やオフライン評価で推定するわけです）

ちなみに、open bandit pipelineパッケージの`SyntheticBanditDataset`を元に上記の設定を入力して、合成データを作成しています。

```python
dataset = SyntheticBanditDataset(
        n_actions=4,
        dim_context=3,
        action_context=np.random.random((n_actions, dim_context)),
        reward_type="binary",
        reward_function="E_{p(r|a, x)}[r]を返す関数",
        behavior_policy_function=データ収集方策の振る舞いを定義した関数,
    )
# 設定に基づいて合成データを生成
bandit_feedback = dataset.obtain_batch_bandit_feedback(n_rounds=10000)
```

### 実験1: データ収集方策の選択がオフライン評価に与える影響

この実験で検証したい質問は、「**どのようなデータ収集方策を使うと、新しい方策の性能をより確度高く推定できるのか??**」です。
この実験では、以下の5種類のデータ収集方策を使用しました。それぞれの方策が異なる意思決定の特徴を持っています。

- 1つ目 $\pi_1$: パーソナライズなしの決定的方策
  - 全てのユーザに対し、ニュース $a_3$ を確率1.0で推薦。
- 2つ目 $\pi_2$: パーソナライズなしの確率的方策。ただし探索的でない。
  - 全てのユーザに対し、ニュース $a_2$ を確率0.5、ニュース $a_3$ を確率0.5で推薦。確率的方策だが、全ての候補ニュースを推薦するわけではない。
- 3つ目 $\pi_3$: パーソナライズなしの確率的かつ探索的な方策。
  - 全てのユーザに対し、ニュース $a_0$ を確率0.025、ニュース $a_1$ を確率0.025、ニュース $a_2$ を確率0.025、ニュース $a_3$ を確率0.925で推薦する確率的方策。
  - （言い換えると、**コンテキスト考慮なしでepsilon=0.1のepsilon-greedy方策**）
- 4つ目 $\pi_4$: パーソナライズありの決定的方策
  - ユーザとニュースのコンテキストを考慮し、コンテキストベクトル $x$ とアイテムコンテキストベクトル $e$ の内積が最大となるニュースを確率1.0で推薦。
- 5つ目 $\pi_5$: パーソナライズありの確率的かつ探索的な方策。
  - ユーザとニュースのコンテキストを考慮し、コンテキストベクトル $x$ とアイテムコンテキストベクトル $e$ の内積が最も大きいニュースを確率0.925で推薦し、それ以外のニュースを確率0.025で推薦する確率的方策。
  - （言い換えると、**コンテキスト考慮ありでepsilon=0.1のepsilon-greedy方策**）

これらのデータ収集方策を用いてログデータを収集し、新しい方策 $\pi_6$ を評価します。なお、$\pi_6$ は **「コンテキストベクトル $x$ とアイテムコンテキストベクトル $e$ の内積が最小のニュースを確率1.0で推薦する」方策** であり、$\pi_4$ や $\pi_5$ とは真逆の意思決定を行います。

#### 実験結果と気付き

図1は、各データ収集方策によって収集されたログデータを使って、新しい方策 $\pi_6$ の性能をオフライン評価した結果です。横軸はログデータ数、縦軸は$\pi_6$の真の性能とオフライン評価によって推定された性能の差を表しています。

![]()

図1から次のような結果が得られました。

- 1. $\pi_3$ や $\pi_5$ のような確率的方策で収集したログデータを使用すると、ログデータ数を増やすにつれて、新しい方策 $pi_6$ のオフライン評価による推定誤差が0に収束していく。
- 2. 一方で、$\pi_1$ や $\pi_4$ のような決定的方策、また $\pi_2$ のような確率的方策ではあるが全てのニュースが選ばれうるわけではない方策で収集したログデータを使用すると、ログデータ数を増やしても、推定誤差は0にならず、$pi_6$ の性能を過小に評価し続けた。

この実験結果から得られた気づきは以下の通りです。

- 1. パーソナライズ有無によらず、データ収集方策が確率的かつ探索的（i.e. 全ての候補が選ばれる確率がそれぞれ0より大きい）であることが、オフライン評価の精度向上に重要である。
- 2. 決定的方策、あるいは確率的方策であっても探索的でない場合は、ログデータ数を増やしても、オフライン評価の精度は向上しない。
- 2. 探索度合いが小さい場合でも (ex. 実験では $\epsilon = 0.1$ のepsilon-greedy戦略。つまり10回中9回は決定的に意思決定する方策) 、ログデータ数が増えるほど、オフライン評価の精度が向上し誤差が0に近づいていく。

### 実験2: データ収集方策の選択がオフライン学習に与える影響

続いて、オフライン学習の実験を行います。
この実験で検証したい質問は、「**どのようなデータ収集方策を使うと、新しい方策をより良く学習させられるのか??**」です。
実験には、オフライン評価の実験と同様の、以下の5種類のデータ収集方策を使用しました。
（再掲なので概要のみ記載しています）

- 1つ目 $\pi_1$: パーソナライズなしの決定的方策
- 2つ目 $\pi_2$: パーソナライズなしの確率的方策。ただし探索的でない。
- 3つ目 $\pi_3$: パーソナライズなしの確率的かつ探索的な方策。
- 4つ目 $\pi_4$: パーソナライズありの決定的方策
- 5つ目 $\pi_5$: パーソナライズありの確率的かつ探索的な方策。

新モデルの学習には`scikit-learn`の`LogisticRegression` を用い、収集データをIPW（Inverse Probability Weighting）で重み付けした目的関数（ざっくりIPS推定量の目的関数ver...!!:thinking:）を使用しています。
ちなみに、open bandit pipelineパッケージを使うと以下のような雰囲気で実装できます。

```python
from sklearn.linear_model import LogisticRegression
from obp.policy import IPWLearner

new_policy = IPWLearner(
        n_actions=4,
        base_classifier=LogisticRegression(),
    )
new_policy.fit(
    context=bandit_feedback_train["context"],
    action=bandit_feedback_train["action"],
    reward=bandit_feedback_train["reward"],
    pscore=bandit_feedback_train["pscore"],
)
```

#### 実験結果と気付き

図2は、各データ収集方策によって収集されたログデータを使って、全く新しい方策を0からオフライン学習し、その真の性能の推移を描画したものです。
横軸はログデータ数、縦軸はオフライン学習後の新方策の真の性能を表しています。

![]()

図2から次のような結果が得られました。

- 1. $\pi_3$ や $\pi_5$ のような確率的方策で収集したログデータを使用すると、学習に使うログデータ数が増えるにつれて、新方策の真の性能が上限値0.4に近づくように学習が進む。すなわち、本設定における最適な方策（全ユーザにニュース $a_0$ を選択する方策）に辿り着く。
- 2. 一方で、$\pi_1$ や $\pi_4$ のような決定的方策、また $\pi_2$ のような確率的方策ではあるが全てのニュースが選ばれうるわけではない方策で収集したログデータを使用すると、学習に使うログデータ数が増えても、新方策の真の性能が上限値0.4に収束しない。すなわち、本設定における最適な方策に辿り着けなかった。

この実験結果から得られた気づきは以下の通りです。

- オフライン評価と同様に、パーソナライズ有無によらず、**データ収集方策が確率的かつ探索的（i.e. 全ての候補が選ばれる確率が0ではない）であることが、オフライン学習で方策を最適化するために重要**。
- 決定的方策、あるいは確率的方策でも探索的でない (i.e. 全ての候補が選ばれない) 場合は、オフライン学習で最良のモデルまで辿り着けない。
- 探索的な度合いが小さくepsilon = 0.1 (i.e. 10回中9回は決定的な意思決定!) でも、オフライン学習で最良のモデルまで辿り着ける。

オフライン評価の実験の気づきとほぼ同じになりました! まあオフライン学習って、まず方策の良し悪しを正しくオフライン評価できた上で、より良くなる方向にパラメータ更新していくイメージなので、似た結論になって当然と言えば当然な気もしますね...!:thinking:

## おわりに

本記事では、Open Bandit Pipeline を活用したシミュレーションによって、**データ収集方策の選択がオフライン評価やオフライン学習に与える影響**を検証しました。その結果、以下の重要な知見を得ることができました。

- 1. **確率的かつ探索的なデータ収集方策の有効性**
  決定的な方策や、探索的でない確率的方策では、十分なログデータを収集しても、オフライン評価や学習の精度が限界に達してしまうことが確認されました。一方で、確率的かつ探索的な方策を採用すれば、ログデータ数が増えるほど、オフライン評価の精度やオフライン学習による新モデルの性能は改善することができました。
- 2. **探索度合いが小さくても有効である**
   探索的な度合いを小さく設定（例: $\epsilon = 0.1$ のepsilon-greedy）しても、ログデータ数が増えるにつれてオフライン評価・学習の精度向上が期待できることがわかりました。この結果は、実務でデータ収集方策として確率的方策を導入する際のハードルを下げるものと言えるはずです。
- 3. **オフライン評価と学習の成功はデータ収集方策次第**
  - オフライン評価と学習はいずれも収集されたログデータの品質に強く依存する。したがって、評価方策や学習モデルを改善する以前に、データ収集方策の設計を見直すことが、推薦システムの継続的な改善を達成する鍵なのでは。

今回の実験で示した通り、「データ収集方策が決定的方策か確率的方策か」が推薦システムの性能評価や学習プロセスに与える影響は非常に大きいように思います。
**epsilon-greedy戦略などのアプローチであれば既存の決定的なデータ収集方策に簡単に探索の要素を追加**することができますし、**探索度合いが小さくてもオフライン評価&学習の精度向上が期待できる**のでリスクも小さくコントロールできますし、実務での活用可能性は十分にあると思っています。

最後までお読みいただきありがとうございました!!
