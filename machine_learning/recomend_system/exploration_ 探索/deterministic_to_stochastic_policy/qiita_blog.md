## アウトライン案

- タイトル案: 推薦システムのオフライン評価&学習を諦めない! 確率的な推薦方策への移行を考える
- TL;DR
- はじめに
  - 背景(なぜこのトピックを選んだ??)
  - 今回喋りたいこと
    - 最終的に主張したいこと: データ収集方策として確率的方策を導入してみる価値はありそう...! というか、推薦方策は基本的に確率的方策の方が評価もしやすいし継続的な訓練もしやすそう...!!
- 推薦システムが解きたい問題は予測タスクではなく、意思決定の最適化タスクである!
- 推薦システムのオフライン評価はなぜオンライン評価とズレてしまうんだっけ?? まず理論の観点から考えてみる。
  - まず推薦システムの問題を定式化してみる!
  - オンライン評価はオフライン評価の特別なケースである!
  - データ収集方策と評価方策の違いがオフライン評価を難しくする
  - データ収集方策と評価方策が違っていても何とかしたい! 王道のIPS推定量!
  - データ収集方策は確率的方策の方が都合が良いかも...!
- データ収集方策は確率的方策の方が都合良いのか?? 実験してみる!
  - open bandit pipeline を使った実験
  - 実験方法
  - 実験結果
  - 考察
- 決定的な推薦方策から確率的な推薦方策への簡単な移行アイデア
  - シンプルにepsilon-greedyを使う!
  - プラケットルースモデルに基づいてランキングの確率的サンプリング
  - プラケットルースモデルの高速化ver.:ガンベルソフトマックストリックを使う
- おわりに

# タイトル: 推薦システムのオフライン評価&学習を諦めない! 確率的な推薦方策への移行を考える

## TL;DR

## はじめに

### 背景

- 先日のRecommendation Industry Talksというイベントで「推薦システムを本番導入する上で一番優先すべきだったこと」というタイトルで発表してきた。
  - また、同様の内容を整理して「[**NewsPicksに推薦システムを本番導入する上で一番優先すべきだったこと](https://tech.uzabase.com/entry/2024/08/29/161828)」**というタイトルでブログ化した。
  - 上記発表&ブログでの結論: **推薦システムのオフライン評価が難しいから一旦諦めて、まずはいかにA/Bテスト(オンライン評価)しやすい基盤を設計することが大事だった**...!
  - （オフライン評価とオンライン評価については、ブログの説明などを参照してください）

<https://speakerdeck.com/morinota/tui-jian-sisutemuwoben-fan-dao-ru-surushang-de-fan-you-xian-subekidatutakoto-newspicksji-shi-tui-jian-ji-neng-nogai-shan-shi-li-woyuan-ni>

- 一方で、やっぱりオフライン評価できた方が嬉しい...!
  - A/Bテストなどのオンライン評価と比べて、高速にフィードバックを得られる。ユーザ体験を毀損するリスクがない、という利点。
- というか、オフライン評価が難しいってことは、MLOpsにおける「継続的な訓練」も上手く機能しないのでは…??
  - MLOps (=機械学習の成果をスケールさせるための様々な取り組み) では、DevOpsの原則の一つである「継続的な改善(Continuous Improvement)」を「継続的な訓練(Continuous Training)」によって行っていく。
  - 推薦モデルの良し悪しをオフライン環境で正しく評価できなかったら、「継続的な訓練」をしても適切ではない方向に最適化していってしまいそう??
  - → MLOpsにおける「継続的な訓練」によって「継続的な改善」を達成するためには、ひいては**機械学習の成果をスケールさせるためには、オフライン評価 & 学習って非常に重要な技術なのでは…!!**

### 今回喋りたいこと

- 前述の経緯で、オフライン評価のアプローチをいろいろ調べてると…
  - 決定的な推薦方策によって収集したログデータだと、かなり打ち手がなさそうな印象…!!
  - 確率的な推薦方策によって、ある程度探索的に収集したデータを使えば、だいぶ打ち手が増えそう。
- というわけで、確率的な推薦方策を本番導入することを色々考えてみた!
- ちなみに、決定的な推薦方策と確率的な推薦方策

- 決定的(deterministic)な方策
  - 常に同じ行動を選択する方策。
  - ex.) ユーザ1に対して、アイテムAを100%の確率で推薦する。
- 確率的(stochastic)な方策
  - ある確率でランダムに行動を選択する方策。
  - ex.) ユーザ1に対して、アイテムAを50%、アイテムBを30%、アイテムCを20%の確率で推薦する。

今回は、推薦方策のオフライン評価・学習に使用しやすいログデータを収集したいというモチベーションで、前者から後者への移行について考えてみる。

## 推薦システムが解きたい問題は予測タスクではなく、意思決定の最適化タスクである

- 推薦システムは、意思決定の最適化問題を解くタスクである
  - 例えば、ニュース推薦の場合
    - 何らかの方策 (i.e. ロジック) によって、(ユーザ, 記事) ペアの関連度や嗜好度などのスコアを予測
    - スコアの予測値に基づいて、ユーザにどんな記事を推薦するかの意思決定を行う

> 予測というよりもむしろデータに基づいた意思決定の最適化問題
> (*書籍「反実仮想機械学習」より引用*)

- データに基づいて導かれる意思決定の規則 = 意思決定方策(decision-making policy)。
  - オフライン評価のモチベーションは、この意思決定方策の性能をログデータに基づいて正確に評価できるようになること。でもこの**意思決定方策のオフライン評価がなかなかに難しい…!** 🤔
  - (今回は特に推薦タスクの話がしたいので、以降では意思決定方策ではなく「推薦方策」という用語を使います)

以下の図は、Booking.comさんのMLアプリケーション開発運用の教訓に関する論文で、以下の「相関がなかった」という有名な相関図。

![](https://i.imgur.com/TZEaYJu.png)

*引用元: 150 Successful Machine Learning Models: 6 Lessons Learned at Booking com, Bernardi et al., 2019*

オフライン評価は難しいが、やっぱりできたらかなり嬉しい。

- オフライン評価が確度高くできる世界とそうでない世界の比較（何回も言っちゃってるかもしれないのでトグルに入れました…!）（オフライン学習ができる世界も含めてしまってるかも）
  - 前者の世界
    - A/Bテストなどのオンライン評価の前に有効そうな方策を絞り込む事ができ、仮説検証をより高速に回す事ができる。
    - ユーザ体験を悪化させ得る施策を、オンライン環境に出す前に検知する事ができる。
    - 「継続的な訓練」によって推薦方策を我々が望む方向に改善 & 最適化しやすい
  - 後者の世界
    - 全ての仮説に対してA/Bテストする必要があり、仮説検証の速度が下がる。(A/Bテストの運用コストを下げても、どうしてもオフライン実験するよりは時間がかかる。10倍くらい?)
    - ユーザ体験を悪化させ得る方策を、オンライン環境に出してしまう可能性。
    - 「継続的な訓練」をしても、誤った方向に最適化してしまう可能性。

## 推薦システムのオフライン評価はなぜオンライン評価とズレてしまうんだっけ?? まず理論の観点から考えてみる

### まず推薦システムの問題を定式化してみる

必要に応じて記号や数式などを使って、「推薦システムってどんな問題を解こうとしてるんだっけ??」「なんでオフライン評価が難しくなっちゃうんだっけ??」ということを整理してみます。

- まずニュース推薦システムの問題設定
  - 全ユーザの累積報酬を最大化することを目指し、システムは候補ニュース集合から、各ユーザ向けにニュースを逐次的に選択し表示していく。
- 上述のような推薦戦略を設計する方法は、contexual banditの分野で研究されている**sequential decision-making problem(逐次的な意思決定問題)**として定式化できる。
  - ニュース推薦システムは意思決定を行うエージェントであり、意思決定の指針として推薦方策（ポリシー）を持つ
  - ニュースアイテムは行動（選択肢）
  - ユーザやニュースアイテムの特徴や状態は、意思決定の際に考慮されるコンテキスト（文脈）
  - 選択した行動に対して、クリックや購入などの報酬が観測される
  - 各試行(roundやtimestep等とも言われる印象...!)において、エージェントはコンテキストを受け取り、方策に従って行動を選択し、報酬を受け取る。

概念図で表すとこんな感じですね...!!

<!-- 後でfigmaとかで作ってみる! -->

数式記号を使って表すと…

- 特徴量(feature, context)のベクトル: $x$ (ex. ユーザ特徴量など)
- 離散的な行動(action): $a \in A$ (ex. 推薦アイテムなど)
- 行動の結果として観測される報酬(即時報酬?): $r$

また、推薦方策 $\pi$ を行動空間 $A$ 上の条件付き確率分布とする (i.e. あるコンテキスト $x$ を持つユーザに対して、行動 $a$ を推薦する確率)

$$
\pi(a|x) := p_{\pi}(a|x)
$$

また、推薦方策 $\pi$ の良し悪しを表す性能(policy value) $V(\pi)$ を、以下のような(x,a,r)の同時分布に対する報酬 $r$ の期待値で定義する:

```math
V(\pi) := \mathbb{E}_{p(x, a, r)}[r] =\mathbb{E}_{p(x)\pi(a|x)p(r|a,x)}[r]
```

（例えば、推薦システムの導入目的が「サービスの課金率を向上させたい！」という場合は、報酬 $r$ を「そのユーザが課金したか否かを表す二値変数」として定義し、その期待値 $\mathbb{E}{p(x, a, r)}[r]$ は課金率になるでしょう）

**さて、この推薦方策の性能 $V(\pi)$ は、実環境においては我々は知ることはできません**。なぜかというと、$V(\pi)$ の定義式に出てくる $p(r|a,x)$ を我々は知り得ないからですね。
この $p(r|a,x)$ というのは、「あるコンテキスト $x$ を持つユーザに対して、ある行動 $a$ を選択したときに得られる報酬 $r$ の確率分布」を表しています。ざっくり「このユーザにこのニュースを推薦したら、課金してくれる確率がxx％やで！」みたいな情報です...!!
方策の性能 $V(\pi)$ を計算するためには、この $p(r|a,x)$ を、可能性がある全てのコンテキストと、方策が選び得る全ての行動に対して知っている必要があるわけなので、無理です...!! （逆にこの情報が既知だったら、簡単に課金率を最大化できそうですね...!!:thinking:）
（コンテキストの確率分布 $p(x)$ も未知で、推薦方策の行動の確率分布 $\pi(a|x)$ は既知ですね!:thinking:）

というわけで、推薦方策の（真の）性能 $V(\pi)$ がわからない中で、どのように我々はよりビジネスに有効な推薦方策を設計し改善していくのでしょうか。
そのための方法が、オンライン評価やオフライン評価なんです。
<!-- より具体的には、評価したい方策そのものを本番稼働させて得られた観測データを使って性能 $V(\pi)$ を推定するのがオンライン評価、評価したい方策とは異なる方策を本番稼働させて得られた観測データを使って性能 $V(\pi)$ を推定するのがオフライン評価です。 -->
より具体的には、本番稼働させた推薦方策（データ収集方策と呼びます）によって得られたログデータを使って、推薦方策の性能 $V(\pi)$ を推定するのがオンライン評価、データ収集方策とは異なる推薦方策（評価方策と呼びます）によって得られたログデータを使って、推薦方策の性能 $V(\pi)$ を推定するのがオフライン評価です。

### オンライン評価はオフライン評価の特別なケース（簡単ver.）である

例えばオンライン評価の場合、A/Bテストなどで新しい推薦方策 $\pi$ を本番稼働させて、収集したログデータ $D_{\pi}:=\{(x_i, a_i, r_i)\}_{i=1}^{n}$ を元に"なんらかの指標"を計算します。それを既存の推薦方策と比較して「テストパターン（treatment群）の方が課金率が良い!」「現行パターン（control群）の方がCTRが良い!」などと判断するわけです。

A/Bテストなどのオンライン評価時の"なんらかの指標"として最も一般的に利用されているのは、以下のようなAVG推定量になります。
（例えば我々がA/Bテストの際に計算しているCTRや課金率などは、 方策の性能 $V(\pi)$ そのものではなく、実際には観測データをもとに下記の方法で推定した $V(\pi)$ の推定値と言えるわけですね...!:thinking:）

```math
\hat{V}_{AVG}(\pi;D_{\pi}) = \frac{1}{n} \sum_{i=1}^{n} r_i
```

例えば推薦方策 $\pi$ を本番稼働させて、以下のような観測データを得られたとします。

<!-- テーブル(カラムはtimestep, context, action, reward) レコード数は5個くらい -->

| timestep | context | action | reward |
| --- | --- | --- | --- |
| 0 | [年齢30歳, エンジニア, ...] | 記事A | 1 |
| 1 | [年齢20歳, 学生, ...] | 記事B | 0 |
| 2 | [年齢40歳, 経営者, ...] | 記事C | 1 |
| 3 | [年齢50歳, 医師, ...] | 記事A | 0 |
| 4 | [年齢60歳, 自営業, ...] | 記事D | 1 |

この観測データを使って $\pi$ の性能 $V(\pi)$ をAVG推定量で推定すると、以下のようになります。

$$
\hat{V}_{AVG}(\pi;D_{\pi}) = \frac{1}{5} \sum_{i=1}^{5} r_i
\\
= \frac{1}{5} \times (1 + 0 + 1 + 0 + 1) = 0.6
$$

オンライン評価の場合、言い換えると性能を**評価したい方策自身によって観測データが収集された場合**（OPEの分野では「**データ収集方策と評価方策が一致する場合**」と表現されます...!）には、AVG推定量によって推薦方策の真の性能 $V(\pi)$ をバイアスなく推定できます。

<details>

<summary>ちなみに、オンライン評価の場合にAVG推定量が真の性能に対してバイアスがなくなることは、以下のように導出できます(メモメモ...!)</summary>

---

オンライン評価の場合に、AVG推定量が推薦方策の真の性能 $V(\pi)$ に対してバイアスのない推定量であることは、AVG推定量 $\hat{V}_{AVG}(\pi;D_{\pi})$ の観測データ $D_{\pi}$ に対する期待値を計算することで証明できます。

```math
\mathbf{E}_{D_{\pi}}[\hat{V}_{AVG}(\pi;D_{\pi})] = \mathbb{E}_{D_{\pi}}[\frac{1}{n} \sum_{i=1}^{n} r_i]
```

(ここで、期待値は線形性(E[a+b] = E[a] + E[b])を持つので...!:thinking:)

```math
= \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}*{p(x)\pi(a|x)p(r|a,x)}[r]
```

(ここで、各試行 i の(x,a,r)は独立同一分布に従うと仮定すると、$E_{x,a,r}[r]$ はiに依存しないから、sumの外に出せるはずなので...! :thinking:)

```math
= \mathbb{E}_{p(x)\pi(a|x)p(r|a,x)}[r]
```

(方策の真の性能の定義式を思い出すと...! :thinking:)

```math
= V(\pi)
```

上記の通り、観測データに対するAVG推定量の期待値が、推薦方策の真の性能 $V(\pi)$ に一致するので、バイアス $V(\pi) - \mathbf{E}_{D_{\pi}}[\hat{V}_{AVG}(\pi;D_{\pi})]$ はゼロであると言えますね (少しわかってきた...!:thinking:)

---

</details>

この「観測された報酬を平均するだけ」というシンプルな考え方で推薦方策の性能を確度高く評価できるからこそ、A/Bテストなどのオンライン評価は非常に有用な手法として広く利用されているんだなぁと思いました:thinking:

### データ収集方策と評価方策の違いがオフライン評価を難しくする

さてオンライン評価では、(暗黙的に)AVG推定量を使って推薦方策の性能を推定していることがわかったところで、続いてオフライン評価のことを考えてみます。
オンライン評価とは異なりオフライン評価の場合には、評価したい方策 $\pi$ とは別の方策によって収集されたログデータ $D_{\pi_0}$ を使って、推薦方策の性能 $V(\pi)$ を推定することになります。(OPEの分野では、「**データ収集方策と評価方策が異なる場合**」とも表現されます...!)

| timestep | context | action | reward | 評価したい方策が選ぶaction |
| --- | --- | --- | --- | --- |
| 0 | [年齢30歳, エンジニア, ...] | 記事A | 1 | 記事A |
| 1 | [年齢20歳, 学生, ...] | 記事B | 0 | 記事A |
| 2 | [年齢40歳, 経営者, ...] | 記事C | 1 | 記事B |
| 3 | [年齢50歳, 医師, ...] | 記事A | 0 | 記事C |
| 4 | [年齢60歳, 自営業, ...] | 記事D | 1 | 記事D |

オンライン評価の例と同じ観測データが得られた場合を考えてみましょう。この観測データは、評価したい方策 $\pi$ とは異なる方策 $\pi_0$ によって収集されたものだとします。
この場合は評価方策 $\pi$ の性能をAVG推定量を使って計算することはできません。
なぜなら、そもそも評価したい方策 $\pi$ は、同じコンテキストに対して、本番環境で稼働した方策 $\pi_0$ とは異なる意思決定をするかもしれないからですね。
上の例では、データ収集方策は年齢30歳の職業エンジニアのユーザに対して記事Aを推薦していますが、評価方策は年齢30歳のエンジニアのユーザに対して記事Bを推薦するとしたら、AVG推定量をどう計算したらいいんだ??ってなってしまいます:thinking:

最もシンプルな考え方としては、「**データ収集方策と評価方策が同じ意思決定をした時の報酬の平均値を取って、AVG推定量っぽく計算しよう!**」という戦略が考えられます。それが以下のようなnaive推定量として知られるものです(ReplayMethodとも呼ばれるようです...!!)

$$
\hat{V}_{naive}(\pi;D_{\pi_0}) = \frac{1}{\sum_{i=1}^{n} \mathbb{I}[\pi(a_i|x_i) = 1.0]} \sum_{i=1}^{n} \mathbb{I}[\pi(a_i|x_i) = 1.0] r_i
$$

ここで、$\mathbb{I}[hoge]$ は、中の条件式がTrueの場合に1、Falseの場合に0を返す関数です(一般的に指示関数、indicator functionと言うようです...!:thinking:)。
なので、$\mathbb{I}[\pi(a_i|x_i) = 1.0]$ は、$i$ 番目の試行において、評価方策 $\pi$ がコンテキスト $x_i$ に対してデータ収集方策と同じ行動 $a_i$ を選択する場合に1、選択しなかった場合に0を返す関数です。

なので上の例の場合、評価方策 $\pi$ の性能をnaive推定量を使って計算すると、以下のようになります。

$$
\hat{V}_{naive}(\pi;D_{\pi_0}) = \frac{1}{(1 + 0 + 0 + 0 + 1)} \times (1 \times 1 + 0 \times 0 + 0 \times 1 + 0 \times 0 + 1 \times 1) = 1.0
$$

データ収集方策と評価方策が同じ行動を選択したのはtimestep 0とtimestep 4のみで、その時は両方とも報酬1を得ているので、naive推定量による評価方策の性能は1.0になります。

ちなみに、評価方策が確率的な場合には、naive推定量の定義式はたぶん以下のようになるのかなと思ってます。前述の式を一般化した形ですね...!:thinking:

$$
\hat{V}_{naive}(\pi;D_{\pi_0}) = \frac{\sum_{i=1}^{n} \pi(a_i|x_i) r_i}{\sum_{i=1}^{n} \pi(a_i|x_i)}
$$

さて、このnaive推定量は、データ収集方策と評価方策が同じ行動を選択した場合のみの報酬の平均値を取ることで、評価方策の性能を推定しようとしているわけですが、実際のところこの推定量は確度高く真の性能を推定できるのでしょうか??
（ここまでの話の流れを踏まえても、体感的にも、なんとなくあんまり正しく真の性能を推定できなさそうな感じがしますね。特に、データ収集方策と評価方策の意思決定の傾向が異なれば異なるほど正しく評価できなさそう...!:thinking:）

実際、このnaive推定量は、真の性能に対してバイアス $hoge$ を持つことが知られています。

<details>

<summary>ちなみに、naive推定量が真の性能に対してバイアスを持つことは、以下のように導出できます(メモメモ...!)</summary>

---

AVG推定量の場合と同様に、naive推定量の観測データに対する期待値を計算して、真の性能の定義と比較してみます。

$$
\mathbf{E}_{D_{\pi_0}}[\hat{V}_{naive}(\pi;D_{\pi_0})] = \mathbb{E}_{D_{\pi_0}}[\frac{1}{\sum_{i=1}^{n} \mathbb{I}[\pi(a_i|x_i) = 1.0]} \sum_{i=1}^{n} \mathbb{I}[\pi(a_i|x_i) = 1.0] r_i]
\\
% 期待値は線形性を持つ (E[a+b] = E[a] + E[b]) ので...!
\\
= \frac{1}{\sum_{i=1}^{n} \mathbb{I}[\pi(a_i|x_i) = 1.0]} \sum_{i=1}^{n} \mathbb{I}[\pi(a_i|x_i) = 1.0] \mathbb{E}_{p(x)\pi_0(a|x)p(r|a,x)}[r]
\\

$$
---

</details>

### データ収集方策と評価方策が違っていても何とかしたい! 王道のIPS推定量

AVG推定量では、データ収集方策と評価方策が異なるほど大きなバイアスを持つ。
AVG推定量のバイアスを打ち消す最も基本的な戦略が、以下のIPS(Inverse Propensity Score)推定量。

```math
\hat{V}_{IPS}(\pi_1;D_{\pi_0}) := \frac{1}{n} \sum_{i=1}^{n} \frac{\pi_1(a_i|x_i)}{\pi_0(a_i|x_i)} r_i = \frac{1}{n} \sum_{i=1}^{n} w(x_i, a_i) r_i
```

なお、$w(x_i, a_i) := \frac{\pi_1(a_i|x_i)}{\pi_0(a_i|x_i)}$ を重要度重み(importance weight)と呼び、評価方策 $\pi_1$ とデータ収集方策 $\pi_0$ による行動選択確率の比を表す。

**「反実仮想機械学習」を読んでいてもOPE推定量の多くがIPS推定量の拡張ver.っぽく、最も基本的なオフライン評価の戦略の1つと言ってもよさそう**です...!:thinking:

### IPS推定量を使う上で満たすべき仮定: 共通サポート仮定

そしてIPS推定量は、以下の**共通サポート仮定**を満たす場合に真の性能に対して不偏になります:

$$
\pi_1(a|x) > 0 -> \pi_0(a|x) > 0, \forall a \in A, \forall x \in X
$$

つまり、「**評価方策 $\pi_1$ が特徴 $x$ に対してサポートする(=選択する可能性がある)全ての行動 $a$ を、データ収集方策 $\pi_0$ もサポートしていてくれ!**」という仮定です。

### データ収集方策は確率的方策の方が都合が良いかも

$$
\pi_1(a|x) > 0 \rightarrow \pi_0(a|x) > 0, \forall a \in A, \forall x \in X
$$

IPS推定量（あるいはその拡張ver.たち）を活用するためには、上記の共通サポート仮定をなるべく満たしたい訳です。

ここで、決定的方策と確率的方策の話が出てきます!
データ収集方策 $\pi_0$ と評価方策 $\pi_1$ がそれぞれ、決定的方策/確率的方策であるケース（2^2 = 4通り）を考えてみましょう。

![](https://pbs.twimg.com/media/GVufdqfXkAEkE6W?format=png&name=4096x4096)

- 1. pi_0 = 決定的方策、pi_1 = 決定的方策のケース
  - -> 共通サポート過程を満たせなさそう。唯一満たせるのは、$\pi_0 = \pi_1$ の場合のみなので。
- 2. pi_0 = 決定的方策、pi_1 = 確率的方策のケース
  - -> 共通サポート過程を満たせなさそう。
- 3. pi_0 = 確率的方策、pi_1 = 決定的方策のケース
  - -> 共通サポート過程を満たせそう!
- 4. pi_0 = 確率的方策、pi_1 = 確率的方策のケース
  - -> 共通サポート過程を満たせそう!

これを踏まえると、データ収集方策 $\pi_0$ が決定的方策だと、共通サポート仮定を全然満たせなさそう...!:scream:
一方で、**データ収集方策 $\pi_0$ が確率的方策でさえあれば**、評価方策 $\pi_1$ が決定的方策でも共通サポート仮定を結構満たせそう...!:thinking:

抽象的な表現ですが**方策の探索の度合いが「データ収集方策 ≧ 評価方策」であれば**、共通サポート仮定を満たしやすく、IPS推定量（あるいはその拡張ver.たち）を使ってオフライン評価 & 学習しやすいという認識です...!:thinking:

（ちなみに、共通サポート仮定を満たしてIPS推定量が真の性能に対して不偏になっただけで、オフライン評価難しい問題が全て解決するとは限りません! **バイアスを除去できたとしてもバリアンスの懸念があるから**です。特に推薦タスクにおいて推薦アイテム候補の数が多かったり、推薦アイテムリストの長さが長かったりすると、方策が取りうる行動の選択肢が爆発的に増え、今度はIPS推定量のバリアンスが大きくなる問題に直面していくようです:thinking:）

## 実験: データ収集方策は本当に確率的方策の方が都合良いのか??

### Open Bandit Pipelineを使ったシミュレーション

OBPパッケージについて少し共有する。

### 実験のふんわり設定を考える

推薦タスク(意思決定の最適化タスク)の架空の問題設定:

- 推薦システムの目的は、プッシュ通知の開封率を最大化すること。
- プッシュ通知で4つの配信候補のニュースがある。
- 各ユーザは、ユーザごとに異なるコンテキスト(ex. 年齢や職種、過去の記事の閲読履歴など)を持つ。
  - (シミュレーションでは、3次元のコンテキストベクトルをランダムに生成する)
- 各ニュースは、ニュースごとに異なるアイテムコンテキスト(ex. 所属するカテゴリ、紐づけられたタグなど)を持つ。

これを定式化すると...

- 報酬 $r$ は、プッシュ通知を開封してくれたか(1)否か(0)のbinary変数とする。
- 任意の推薦方策 $\pi$ の真の性能 $V(\pi)$ は、開封率の期待値として定義される。
- 行動空間(選択肢): $A = \{a_0, a_1, a_2, a_3\}$ (配信候補のニュース)
- ユーザのコンテキストベクトル: $x \in \mathbb{R}^{3}$
- ニュースのアイテムコンテキストベクトル: $e \in \mathbb{R}^{3}$

また、今回はシミュレーションなので、実環境では知り得ない報酬rの真の確率分布 $p(r|a,x)$ も設定しておく。

- これにより、実環境では知り得ない、各方策 $\pi$ の真の性能 $V(\pi)$ を計算することができる。
- 今回は単純化するため、コンテキスト $x$ に関係なく各ニュースごとに報酬rの期待値が異なると仮定する。
  - 具体的には以下のように設定する:
  - ニュース $a_0$ の開封率(の期待値)はコンテキストによらず0.4: $E_{p(r|a_0, x)}[r] = E_{p(r|a_0)}[r] = 0.4$
  - ニュース $a_1$ の開封率(の期待値)はコンテキストによらず0.3: $E_{p(r|a_1, x)}[r] = E_{p(r|a_1)}[r] = 0.3$
  - ニュース $a_2$ の開封率(の期待値)はコンテキストによらず0.2: $E_{p(r|a_2, x)}[r] = E_{p(r|a_2)}[r] = 0.2$
  - ニュース $a_3$ の開封率(の期待値)はコンテキストによらず0.1: $E_{p(r|a_3, x)}[r] = E_{p(r|a_3)}[r] = 0.1$

### オフライン評価の実験方法

- 以下の5種類の推薦方策の1つをデータ収集方策として採用して、**どの方策で集めたデータを使うと、新モデルの性能をより確度高く推定できるか (オフライン評価の問題)** を実験した。
  - 1つ目 $\pi_1$: パーソナライズなしの決定的方策
    - コンテキストを考慮せず、全てのユーザに対してニュース $a_3$ を確率1.0で推薦する決定的方策。
  - 2つ目 $\pi_2$: パーソナライズなしの確率的方策。ただし探索的ではない。
    - コンテキストを考慮せず、全てのユーザに対してニュース $a_2$ を確率0.5、ニュース $a_3$ を確率0.5で推薦する確率的方策(確率的方策だが、全ての候補ニュースを推薦するわけではない)。
    - (i.e. コンテキスト考慮なしの確率的方策だが、全ての候補ニュースを推薦するわけではないケース)。
  - 3つ目 $\pi_3$: パーソナライズなしの確率的かつ探索的な方策。
    - コンテキストを考慮せず、全てのユーザに対してニュース $a_0$ を確率0.1、ニュース $a_1$ を確率0.1、ニュース $a_2$ を確率0.1、ニュース $a_3$ を確率0.7で推薦する確率的方策
    - (i.e. コンテキスト考慮なしのepsilon-greedy方策)
  - 4つ目 $\pi_4$: パーソナライズありの決定的方策
    - ユーザとニュースのコンテキストを考慮し、コンテキストベクトル $x$ とアイテムコンテキストベクトル $e$ の内積が最も大きいニュースを確率1で推薦する決定的方策。
    - (i.e. コンテキスト考慮ありの決定的方策)。
  - 5つ目 $\pi_5$: パーソナライズありの確率的 かつ探索的な方策。
    - ユーザとニュースのコンテキストを考慮し、コンテキストベクトル $x$ とアイテムコンテキストベクトル $e$ の内積が最も大きいニュースを確率0.7で推薦し、その他のニュースを均等に確率0.1で推薦する確率的方策。
    - (i.e. コンテキスト考慮ありのepsilon-greedy方策)。

- この5種類の推薦方策を、それぞれデータ収集方策として使う。
- データ収集方策が集めたログデータを使って、全く新しい方策 $\pi_6$ の真の性能をIPS推定量を使って推定する。
  - ここで、方策 $\pi_6$ は、コンテキストベクトル $x$ とアイテムコンテキストベクトル $e$ の**内積が最も小さいニュースを確率1.0で推薦する**決定的方策とする。つまり、**方策 $\pi_4$ や $\pi_5$ とは真逆の意思決定をする方策**である。
  
- open bandit pipelineパッケージの`SyntheticBanditDataset`を元に上記の設定を入力して、合成データを作成する。

```python
dataset = SyntheticBanditDataset(
        n_actions=4,
        dim_context=3,
        action_context=np.random.random((n_actions, dim_context)),
        reward_type="binary",
        reward_function=true_expected_reward_function,
        behavior_policy_function=logging_policy_function,
    )
# 設定に基づいて合成データを生成
bandit_feedback = dataset.obtain_batch_bandit_feedback(n_rounds=10000)
```

### オフライン評価の実験結果と気付き

結果として...
pi_3もしくはpi_5で集めたデータを使うと、新モデルの性能を精度高くオフライン評価できる。
気付き
パーソナライズ有無によらず、データ収集方策が確率的方策 (かつ探索的方策!) であれば、精度高く新モデルのオフライン評価ができる。
データ収集方策と評価したい方策が、それぞれ全く逆の意思決定をするパーソナライズ方策だとしても、データ収集方策が少しでも探索的 (epsilon = 0.1 のepsilon-greedy戦略。つまり10回中9回は決定的な意思決定!) であれば、確度高くオフライン評価できる。

### オフライン学習の実験方法

- オフライン評価の実験と同様の5種類の推薦方策の1つをデータ収集方策として採用して、**どの方策で集めたデータを使うと、より良い新モデルを学習させられるか (オフライン学習の問題) を実験**した!
  - 1つ目 $\pi_1$: パーソナライズなしの決定的方策
  - 2つ目 $\pi_2$: パーソナライズなしの確率的方策。ただし探索的ではない。
  - 3つ目 $\pi_3$: パーソナライズなしの確率的かつ探索的な方策。
  - 4つ目 $\pi_4$: パーソナライズありの決定的方策
  - 5つ目 $\pi_5$: パーソナライズありの確率的かつ探索的な方策。
- なお、新モデルには `scikit-learn` の `LogisticRegression` を使い、データ収集方策が集めたログデータに対して、IPWによって重み付けされた目的関数を使って学習を行う。
- open bandit pipelineパッケージを使うと以下のように実装できる。

```python
from sklearn.linear_model import LogisticRegression
from obp.policy import IPWLearner

new_policy = IPWLearner(
        n_actions=4,
        base_classifier=LogisticRegression(),
    )
new_policy.fit(
    context=bandit_feedback_train["context"],
    action=bandit_feedback_train["action"],
    reward=bandit_feedback_train["reward"],
    pscore=bandit_feedback_train["pscore"],
)
```

### オフライン学習の実験結果と気付き

## おわりに
