---
format:
  html:
    theme: cosmo
  # revealjs:
  #   # incremental: false
  #   # theme: [default, custom_lab.scss]
  #   theme: [default, custom_lab.scss]
  #   slide-number: true
  #   scrollable: true
  #   logo: https://s3-ap-northeast-1.amazonaws.com/qiita-image-store/0/1697279/dfa905d1c1e242b4e39be182ae21a2b6ac72c0ad/large.png?1655951919
  #   footer: ⇒ [https://qiita.com/morinota](https://qiita.com/morinota)
from: markdown+emoji

fig-cap-location: bottom

title: 確率的(探索的)な推薦方策を考える ~オフライン評価&学習しやすい優しい世界?~
subtitle: y-tech-ai ワクワク勉強会
date: 2024/08/13
author: モーリタ
title-slide-attributes:
  #   data-background-image: https://i.imgur.com/nTazczH.png
  data-background-size: contain
  data-background-opacity: "0.5"
---

## (下書き) 書きたい内容をラフに列挙する

<!-- ## TL;DR -->

- 意思決定最適化問題のオフライン評価が難しい話を少し
  - 「反実仮想機械学習」などを読んだ感じでは、データ収集方策が決定的だと、オフライン評価がかなり困難になりそう。
  - (確率的な方策を採用したらそれで完結するわけではないが、決定的な方策だともう厳しすぎるので)
- 評価・学習に使用しやすいログデータを収集するというモチベーションで、決定的な推薦方策から確率的な推薦方策への移行アイデアをいくつか考えてみる。
  - まず問題設定:
  - アイデア0: 決定的な方策
  - アイデア1: プラケットルースモデルに基づくランキングの確率的サンプリング
  - アイデア2: ガンベルソフトマックストリックを使って同様のランキングを高速に生成
  - アイデア3: epsilon-greedyを使う
  - ちなみに...「探索」がユーザ体験にポジティブな影響を与えて長期的なengagementに繋がるという主張の論文もあった。
  - 論文の結論としては、単に多様性を増やすだけではダメで、セレンディピティを生むような探索を推薦システムに実行させることができた場合に有効という結論だったはず。
- 懸念: オンライン推論の場合、確率的な方策はレイテンシーとか大丈夫??
- 実験してみる:

## はじめに

### なんでこのトピックを??

- 先日のRecommendation Industry Talksというイベントで「推薦システムを本番導入する上で一番優先すべきだったこと」というタイトルで発表してきましたー!
  - 結論: **推薦システムのオフライン評価が難しいから一旦諦めて、まずはいかにA/Bテスト(オンライン評価)しやすい基盤を設計することが大事だった**...!
  - オフライン評価が難しかった -> オフライン-オンライン評価の結果が相関しなかった
- 一方で、やっぱりオフライン評価できた方が嬉しい...!
  - A/Bテストなどのオンライン評価と比べて、高速にフィードバックを得られる。ユーザ体験を毀損するリスクがない、という利点。
  - というか、オフライン評価が難しいってことは、ハイパーパラメータチューニングやオフライン学習も難しいのでは...!:scared:
- オフライン評価のアプローチを調べていると...
  - 決定的な推薦方策によって収集した観測データだと、かなり打ち手がなさそうな印象。
  - **確率的な推薦方策によってある程度探索的に収集したデータを使えば、だいぶ打ち手が増えそう**...!:thinking:
- というわけで、確率的な推薦方策を本番導入することを色々考えてみた!

## ということで、決定的な推薦方策と確率的な推薦方策

- 決定的(deterministic)な方策
  - 常に同じ行動を選択する方策。
  - ex.) ユーザ1に対して、アイテムAを100%の確率で推薦する。
- 確率的(stochastic)な方策
  - ある確率でランダムに行動を選択する方策。
  - ex.) ユーザ1に対して、アイテムAを50%、アイテムBを30%、アイテムCを20%の確率で推薦する。

今回は、推薦モデルの評価・学習に使用しやすいログデータを収集したいというモチベーションで、前者から後者への移行について考えてみます!

## 推薦システムのオフライン評価の話を少し

### 意思決定の最適化問題とオフライン評価

- 推薦システムは意思決定の最適化問題!
  - 例えば、ニュース推薦の場合
    - 何らかのモデルによって、(ユーザ, 記事)のペアの相性やクリック確率などのスコアを予測
    - スコアの予測値に基づいて、ユーザにどんな記事を推薦するかの意思決定を行う

> 予測というよりもむしろデータに基づいた意思決定の最適化問題
> (*書籍「反実仮想機械学習」より引用*)

データに基づいて導かれる意思決定の規則 = 意思決定方策(decision-making policy)。
オフライン評価のモチベーションは、この意思決定方策の性能を、ログデータに基づいて正確に評価できるようになること!

- オフライン評価が正確にできる場合
  - A/Bテストなどのオンライン評価の前に、有効そうな方策を絞り込む事ができ、仮説 -> 実験 -> フィードバックをより高速に回す事ができる。
  - ハイパーパラメータチューニングやオフライン学習もしやすくなる。
  - ユーザ体験を悪化させ得る施策を、オンライン環境に出す前に検知する事ができる。
- 逆に、オフライン評価の確度が低い場合
  - 全ての仮説に対してABテストする必要があり、仮説 -> 施策 -> 検証の速度が下がる。(ABテストの運用コストを下げても、どうしてもオフライン実験するよりは時間がかかる。10倍くらい?)
  - 誤った基準でハイパーパラメータチューニングしてしまう可能性。
  - ユーザ体験を悪化させ得る施策を、オンライン環境に出す可能性。

### オフライン評価がなかなか難しい問題

## データ収集方策が決定的だと、オフライン評価がかなり困難になりそう

## 決定的な推薦方策から確率的な推薦方策への移行アイデア達

- プラケットルースモデルに基づくランキングの確率的サンプリング
- ガンベルソフトマックストリックを使って同様のランキングを高速に生成
- epsilon-greedyを使う

## 懸念: オンライン推論の場合、確率的な方策はレイテンシーとか大丈夫??
