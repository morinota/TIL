## アウトライン案

- タイトル案: 推薦システムのオフライン評価&学習を諦めない! 確率的な推薦方策への移行を考える
- TL;DR
- はじめに
  - 背景(なぜこのトピックを選んだ??)
  - 今回喋りたいこと
    - 最終的に主張したいこと: データ収集方策として確率的方策を導入してみる価値はありそう...! というか、推薦方策は基本的に確率的方策の方が評価もしやすいし継続的な訓練もしやすそう...!!
- 推薦システムが解きたい問題は予測タスクではなく、意思決定の最適化タスクである!
- 推薦システムのオフライン評価はなぜオンライン評価とズレてしまうんだっけ?? まず理論の観点から考えてみる。
  - まず推薦システムの問題を定式化してみる!
  - オンライン評価はオフライン評価の特別なケースである!
  - データ収集方策と評価方策の違いがオフライン評価を難しくする
  - データ収集方策と評価方策が違っていても何とかしたい! 王道のIPS推定量!
  - データ収集方策は確率的方策であるべきかも...!!
- データ収集方策は確率的方策の方が都合良いのか?? 実験してみる!
  - open bandit pipeline を使った実験
  - 実験方法
  - 実験結果
  - 考察
- 決定的な推薦方策から確率的な推薦方策への簡単な移行アイデア
  - シンプルにepsilon-greedyを使う!
  - プラケットルースモデルに基づいてランキングの確率的サンプリング
  - プラケットルースモデルの高速化ver.:ガンベルソフトマックストリックを使う
- おわりに

# タイトル: 推薦システムのオフライン評価&学習を諦めない! 確率的な推薦方策への移行を考える

## TL;DR

## はじめに

### 背景

- 先日のRecommendation Industry Talksというイベントで「推薦システムを本番導入する上で一番優先すべきだったこと」というタイトルで発表してきた。
  - また、同様の内容を整理して「[**NewsPicksに推薦システムを本番導入する上で一番優先すべきだったこと](https://tech.uzabase.com/entry/2024/08/29/161828)」**というタイトルでブログ化した。
  - 上記発表&ブログでの結論: **推薦システムのオフライン評価が難しいから一旦諦めて、まずはいかにA/Bテスト(オンライン評価)しやすい基盤を設計することが大事だった**...!
  - （オフライン評価とオンライン評価については、ブログの説明などを参照してください）

<https://speakerdeck.com/morinota/tui-jian-sisutemuwoben-fan-dao-ru-surushang-de-fan-you-xian-subekidatutakoto-newspicksji-shi-tui-jian-ji-neng-nogai-shan-shi-li-woyuan-ni>

- 一方で、やっぱりオフライン評価できた方が嬉しい...!
  - A/Bテストなどのオンライン評価と比べて、高速にフィードバックを得られる。ユーザ体験を毀損するリスクがない、という利点。
- というか、オフライン評価が難しいってことは、MLOpsにおける「継続的な訓練」も上手く機能しないのでは…??
  - MLOps (=機械学習の成果をスケールさせるための様々な取り組み) では、DevOpsの原則の一つである「継続的な改善(Continuous Improvement)」を「継続的な訓練(Continuous Training)」によって行っていく。
  - 推薦モデルの良し悪しをオフライン環境で正しく評価できなかったら、「継続的な訓練」をしても適切ではない方向に最適化していってしまいそう??
  - → MLOpsにおける「継続的な訓練」によって「継続的な改善」を達成するためには、ひいては**機械学習の成果をスケールさせるためには、オフライン評価 & 学習って非常に重要な技術なのでは…!!**

### 今回喋りたいこと

- 前述の経緯で、オフライン評価のアプローチをいろいろ調べてると…
  - 決定的な推薦方策によって収集したログデータだと、かなり打ち手がなさそうな印象…!!
  - 確率的な推薦方策によって、ある程度探索的に収集したデータを使えば、だいぶ打ち手が増えそう。
- というわけで、確率的な推薦方策を本番導入することを色々考えてみた!
- ちなみに、決定的な推薦方策と確率的な推薦方策

- 決定的(deterministic)な方策
  - 常に同じ行動を選択する方策。
  - ex.) ユーザ1に対して、アイテムAを100%の確率で推薦する。
- 確率的(stochastic)な方策
  - ある確率でランダムに行動を選択する方策。
  - ex.) ユーザ1に対して、アイテムAを50%、アイテムBを30%、アイテムCを20%の確率で推薦する。

今回は、推薦方策のオフライン評価・学習に使用しやすいログデータを収集したいというモチベーションで、前者から後者への移行について考えてみる。

## 推薦システムが解きたい問題は予測タスクではなく、意思決定の最適化タスクである

- 推薦システムは、意思決定の最適化問題を解くタスクである
  - 例えば、ニュース推薦の場合
    - 何らかの方策 (i.e. ロジック) によって、(ユーザ, 記事) ペアの関連度や嗜好度などのスコアを予測
    - スコアの予測値に基づいて、ユーザにどんな記事を推薦するかの意思決定を行う

> 予測というよりもむしろデータに基づいた意思決定の最適化問題
> (*書籍「反実仮想機械学習」より引用*)

- データに基づいて導かれる意思決定の規則 = 意思決定方策(decision-making policy)。
  - オフライン評価のモチベーションは、この意思決定方策の性能をログデータに基づいて正確に評価できるようになること。でもこの**意思決定方策のオフライン評価がなかなかに難しい…!** 🤔
  - (今回は特に推薦タスクの話がしたいので、以降では意思決定方策ではなく「推薦方策」という用語を使います)

以下の図は、Booking.comさんのMLアプリケーション開発運用の教訓に関する論文で、以下の「相関がなかった」という有名な相関図。

![](https://i.imgur.com/TZEaYJu.png)

*引用元: 150 Successful Machine Learning Models: 6 Lessons Learned at Booking com, Bernardi et al., 2019*

オフライン評価は難しいが、やっぱりできたらかなり嬉しい。

- オフライン評価が確度高くできる世界とそうでない世界の比較（何回も言っちゃってるかもしれないのでトグルに入れました…!）（オフライン学習ができる世界も含めてしまってるかも）
  - 前者の世界
    - A/Bテストなどのオンライン評価の前に有効そうな方策を絞り込む事ができ、仮説検証をより高速に回す事ができる。
    - ユーザ体験を悪化させ得る施策を、オンライン環境に出す前に検知する事ができる。
    - 「継続的な訓練」によって推薦方策を我々が望む方向に改善 & 最適化しやすい
  - 後者の世界
    - 全ての仮説に対してA/Bテストする必要があり、仮説検証の速度が下がる。(A/Bテストの運用コストを下げても、どうしてもオフライン実験するよりは時間がかかる。10倍くらい?)
    - ユーザ体験を悪化させ得る方策を、オンライン環境に出してしまう可能性。
    - 「継続的な訓練」をしても、誤った方向に最適化してしまう可能性。

## 推薦システムのオフライン評価はなぜオンライン評価とズレてしまうんだっけ?? まず理論の観点から考えてみる

### まず推薦システムの問題を定式化してみる

必要に応じて記号や数式などを使って、「推薦システムってどんな問題を解こうとしてるんだっけ??」「なんでオフライン評価が難しくなっちゃうんだっけ??」ということを整理してみます。

- まずニュース推薦システムの問題設定
  - 全ユーザの累積報酬を最大化することを目指し、システムは候補ニュース集合から、各ユーザ向けにニュースを逐次的に選択し表示していく。
- 上述のような推薦戦略を設計する方法は、contexual banditの分野で研究されている**sequential decision-making problem(逐次的な意思決定問題)**として定式化できる。
  - ニュース推薦システムは意思決定を行うエージェントであり、意思決定の指針として推薦方策（ポリシー）を持つ
  - ニュースアイテムは行動（選択肢）
  - ユーザやニュースアイテムの特徴や状態は、意思決定の際に考慮されるコンテキスト（文脈）
  - 選択した行動に対して、クリックや購入などの報酬が観測される
  - 各試行(roundやtimestep等とも言われる印象...!)において、エージェントはコンテキストを受け取り、方策に従って行動を選択し、報酬を受け取る。

概念図で表すとこんな感じですね...!!

<!-- 後でfigmaとかで作ってみる! -->

数式記号を使って表すと…

- 特徴量(feature, context)のベクトル: $x$ (ex. ユーザ特徴量など)
- 離散的な行動(action): $a \in A$ (ex. 推薦アイテムなど)
- 行動の結果として観測される報酬(即時報酬?): $r$

また、推薦方策 $\pi$ を行動空間 $A$ 上の条件付き確率分布とする (i.e. あるコンテキスト $x$ を持つユーザに対して、行動 $a$ を推薦する確率)

$$
\pi(a|x) := p_{\pi}(a|x)
$$

また、推薦方策 $\pi$ の良し悪しを表す性能(policy value) $V(\pi)$ を、以下のような(x,a,r)の同時分布に対する報酬 $r$ の期待値で定義する:

```math
V(\pi) := \mathbb{E}_{p(x, a, r)}[r] =\mathbb{E}_{p(x)\pi(a|x)p(r|a,x)}[r]
```

（例えば、推薦システムの導入目的が「サービスの課金率を向上させたい！」という場合は、報酬 $r$ を「そのユーザが課金したか否かを表す二値変数」として定義し、その期待値 $\mathbb{E}{p(x, a, r)}[r]$ は課金率になるでしょう）

**さて、この推薦方策の性能 $V(\pi)$ は、実環境においては我々は知ることはできません**。なぜかというと、$V(\pi)$ の定義式に出てくる $p(r|a,x)$ を我々は知り得ないからですね。
この $p(r|a,x)$ というのは、「あるコンテキスト $x$ を持つユーザに対して、ある行動 $a$ を選択したときに得られる報酬 $r$ の確率分布」を表しています。ざっくり「このユーザにこのニュースを推薦したら、課金してくれる確率がxx％やで！」みたいな情報です...!!
方策の性能 $V(\pi)$ を計算するためには、この $p(r|a,x)$ を、可能性がある全てのコンテキストと、方策が選び得る全ての行動に対して知っている必要があるわけなので、無理です...!! （逆にこの情報が既知だったら、簡単に課金率を最大化できそうですね...!!:thinking:）
（コンテキストの確率分布 $p(x)$ も未知で、推薦方策の行動の確率分布 $\pi(a|x)$ は既知ですね!:thinking:）

というわけで、推薦方策の（真の）性能 $V(\pi)$ がわからない中で、どのように我々はよりビジネスに有効な推薦方策を設計し改善していくのでしょうか。
そのための方法が、オンライン評価やオフライン評価なんです。
<!-- より具体的には、評価したい方策そのものを本番稼働させて得られた観測データを使って性能 $V(\pi)$ を推定するのがオンライン評価、評価したい方策とは異なる方策を本番稼働させて得られた観測データを使って性能 $V(\pi)$ を推定するのがオフライン評価です。 -->
より具体的には、本番稼働させた推薦方策（データ収集方策と呼びます）によって得られたログデータを使って、推薦方策の性能 $V(\pi)$ を推定するのがオンライン評価、データ収集方策とは異なる推薦方策（評価方策と呼びます）によって得られたログデータを使って、推薦方策の性能 $V(\pi)$ を推定するのがオフライン評価です。

### オンライン評価はオフライン評価の特別なケース（簡単ver.）である

例えばオンライン評価の場合、A/Bテストなどで新しい推薦方策 $\pi$ を本番稼働させて、収集したログデータ $D_{\pi}:=\{(x_i, a_i, r_i)\}_{i=1}^{n}$ を元に"なんらかの指標"を計算します。それを既存の推薦方策と比較して「テストパターン（treatment群）の方が課金率が良い!」「現行パターン（control群）の方がCTRが良い!」などと判断するわけです。

A/Bテストなどのオンライン評価時の"なんらかの指標"として最も一般的に利用されているのは、以下のようなAVG推定量になります。
（例えば我々がA/Bテストの際に計算しているCTRや課金率などは、 方策の性能 $V(\pi)$ そのものではなく、実際には観測データをもとに下記の方法で推定した $V(\pi)$ の推定値と言えるわけですね...!:thinking:）

```math
\hat{V}_{AVG}(\pi;D_{\pi}) = \frac{1}{n} \sum_{i=1}^{n} r_i
```

このオンライン評価（データ収集方策 $\pi_0$ と評価方策 $\pi_1$ が同じ）の場合は、AVG推定量（シンプルにログデータの報酬を平均するだけ）で推薦方策の性能 $V(\pi)$ をバイアスなく推定できる。
（具体的には、ログデータ $D$ に対するAVG推定量の期待値が、 $V(\pi)$ に一致する）

```math
\mathbf{E}_{D_{\pi}}[\hat{V}_{AVG}(\pi;D_{\pi})] = \mathbb{E}_{D}[\frac{1}{n} \sum_{i=1}^{n} r_i]
\\
= \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}_{p(x)\pi(a|x)p(r|a,x)}[r] = V(\pi)
\\
\because (x,a,r) \sim^{i.i.d.} p(x)\pi(a|x)p(r|a,x), \text{期待値の線形性より}
\\
= \mathbb{E}_{p(x)\pi(a|x)p(r|a,x)}[r] = V(\pi)
```

### データ収集方策と評価方策の違いがオフライン評価を難しくする

しかし一方で、オフライン評価（データ収集方策 $\pi_0$ と評価方策 $\pi_1$ が異なる）の場合は、このAVG推定量は推薦方策の性能 $V(\pi)$ に対してバイアスを持ってしまう。
（具体的には、$\pi_0(a|x)$ が大きいほどその報酬の期待値を過剰に評価してしまう)

```math
\mathbb{E}_{D_{\pi_0}}[\hat{V}_{AVG}(\pi_1;D_{\pi_0})] = \mathbb{E}_{D_{\pi_0}}[\frac{1}{n} \sum_{i=1}^{n} r_i]
\\
= \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}_{p(x)\pi_0(a|x)p(r|a,x)}[r]
\\
\because (x,a,r) \sim^{i.i.d.} p(x)\pi(a|x)p(r|a,x), \text{期待値の線形性より}
\\
= \mathbb{E}_{p(x)}[\sum_{a \in A} \pi_0(a|x) \mathbb{E}_{p(r|a,x)}[r]]
\neq V(\pi_1)
```

### データ収集方策と評価方策が違っていても何とかしたい! 王道のIPS推定量

AVG推定量では、データ収集方策と評価方策が異なるほど大きなバイアスを持つ。
AVG推定量のバイアスを打ち消す最も基本的な戦略が、以下のIPS(Inverse Propensity Score)推定量。

```math
\hat{V}_{IPS}(\pi_1;D_{\pi_0}) := \frac{1}{n} \sum_{i=1}^{n} \frac{\pi_1(a_i|x_i)}{\pi_0(a_i|x_i)} r_i = \frac{1}{n} \sum_{i=1}^{n} w(x_i, a_i) r_i
```

なお、$w(x_i, a_i) := \frac{\pi_1(a_i|x_i)}{\pi_0(a_i|x_i)}$ を重要度重み(importance weight)と呼び、評価方策 $\pi_1$ とデータ収集方策 $\pi_0$ による行動選択確率の比を表す。

**「反実仮想機械学習」を読んでいてもOPE推定量の多くがIPS推定量の拡張ver.っぽく、最も基本的なオフライン評価の戦略の1つと言ってもよさそう**です...!:thinking:

### IPS推定量を使う上で満たすべき仮定: 共通サポート仮定

そしてIPS推定量は、以下の**共通サポート仮定**を満たす場合に真の性能に対して不偏になります:

$$
\pi_1(a|x) > 0 -> \pi_0(a|x) > 0, \forall a \in A, \forall x \in X
$$

つまり、「**評価方策 $\pi_1$ が特徴 $x$ に対してサポートする(=選択する可能性がある)全ての行動 $a$ を、データ収集方策 $\pi_0$ もサポートしていてくれ!**」という仮定です。

### データ収集方策は確率的方策であるべきかも

$$
\pi_1(a|x) > 0 \rightarrow \pi_0(a|x) > 0, \forall a \in A, \forall x \in X
$$

IPS推定量（あるいはその拡張ver.たち）を活用するためには、上記の共通サポート仮定をなるべく満たしたい訳です。

ここで、決定的方策と確率的方策の話が出てきます!
データ収集方策 $\pi_0$ と評価方策 $\pi_1$ がそれぞれ、決定的方策/確率的方策であるケース（2^2 = 4通り）を考えてみましょう。

![](https://pbs.twimg.com/media/GVufdqfXkAEkE6W?format=png&name=4096x4096)

- 1. pi_0 = 決定的方策、pi_1 = 決定的方策のケース
  - -> 共通サポート過程を満たせなさそう。唯一満たせるのは、$\pi_0 = \pi_1$ の場合のみなので。
- 2. pi_0 = 決定的方策、pi_1 = 確率的方策のケース
  - -> 共通サポート過程を満たせなさそう。
- 3. pi_0 = 確率的方策、pi_1 = 決定的方策のケース
  - -> 共通サポート過程を満たせそう!
- 4. pi_0 = 確率的方策、pi_1 = 確率的方策のケース
  - -> 共通サポート過程を満たせそう!

これを踏まえると、データ収集方策 $\pi_0$ が決定的方策だと、共通サポート仮定を全然満たせなさそう...!:scream:
一方で、**データ収集方策 $\pi_0$ が確率的方策でさえあれば**、評価方策 $\pi_1$ が決定的方策でも共通サポート仮定を結構満たせそう...!:thinking:

抽象的な表現ですが**方策の探索の度合いが「データ収集方策 ≧ 評価方策」であれば**、共通サポート仮定を満たしやすく、IPS推定量（あるいはその拡張ver.たち）を使ってオフライン評価に打つ手が出てくるのでは...!:thinking:

（ちなみに補足すると、共通サポート仮定を満たしてIPS推定量が真の性能に対して不偏になっただけで、オフライン評価難しい問題が全て解決するとは限りません! **バイアスを除去できたとしてもバリアンスの懸念があるから**です。特に推薦タスクにおいて推薦アイテム候補の数が多かったり、推薦アイテムリストの長さが長かったりすると、方策が取りうる行動の選択肢が爆発的に増え、IPS推定量のバリアンスが大きくなる問題があります。）

## オフライン評価しやすさの観点で、データ収集方策は本当に確率的方策の方が都合良いのか?? 実験してみる

### Open Bandit Pipelineを使ったシミュレーション

OBPパッケージについて軽く説明する。

### 実験方法

- 推薦システムの問題設定
  - 架空の問題設定:
    - 推薦システムの目的は、プッシュ通知の開封率を最大化すること。
    - プッシュ通知で4つの配信候補のニュースがある。
    - 各ユーザは、年齢や職種、過去の記事の閲読履歴など、ユーザごとに異なるコンテキストを持つ。
    - 各ニュースは、所属するカテゴリや紐づけられたタグなど、ニュースごとに異なるアイテムコンテキストを持つ。
  - これを定式化すると...
    - 報酬 $r$ は、プッシュ通知を開封してくれたか(1)否か(0)のbinary変数とする。
    - 任意の推薦方策 $\pi$ の真の性能 $V(\pi)$ は、開封率の期待値として定義される。
    - 行動空間: $A = \{a_0, a_1, a_2, a_3\}$ (配信候補のニュース)
    - ユーザのコンテキストベクトル: $x \in \mathbb{R}^{5}$
    - ニュースのアイテムコンテキストベクトル: $e \in \mathbb{R}^{5}$

- 今回はシミュレーションなので、実環境では知り得ない報酬の確率分布 $p(r|a,x)$ も定義する。
  - これにより、実環境では知り得ない、各方策 $\pi$ の真の性能 $V(\pi)$ を計算することができる。
  - 今回は単純化するため、コンテキスト $x$ に関係なく各ニュースごとに期待値が異なると仮定する。
    - 具体的には以下のように設定する:
    - ニュース $a_0$ の開封率(の期待値)はコンテキストによらず0.1: $E_{p(r|a_0, x)}[r] = E_{p(r|a_0)}[r] = 0.1$
    - ニュース $a_1$ の開封率(の期待値)はコンテキストによらず0.2: $E_{p(r|a_1, x)}[r] = E_{p(r|a_1)}[r] = 0.2$
    - ニュース $a_2$ の開封率(の期待値)はコンテキストによらず0.3: $E_{p(r|a_2, x)}[r] = E_{p(r|a_2)}[r] = 0.3$
    - ニュース $a_3$ の開封率(の期待値)はコンテキストによらず0.4: $E_{p(r|a_3, x)}[r] = E_{p(r|a_3)}[r] = 0.4$

- k種類の推薦方策を定義する:
  - 1つ目 $\pi_1$: コンテキストを考慮せず、全てのユーザに対してニュース $a_0$ を確率1で推薦する決定的方策。
  - 2つ目 $\pi_2$: コンテキストを考慮せず、全てのユーザに対してニュース $a_1$ を確率1で推薦する決定的方策。
  - 3つ目 $\pi_3$: コンテキストを考慮せず、全てのユーザに対してニュース $a_0$ を確率0.4、ニュース $a_1$ を確率0.3、ニュース $a_2$ を確率0.2、ニュース $a_3$ を確率0.1で推薦する確率的方策。
  - 4つ目 $\pi_4$: ユーザとニュースのコンテキストを考慮し、コンテキストベクトル $x$ とアイテムコンテキストベクトル $e$ の内積が最も大きいニュースを確率1で推薦する決定的方策。
  - 5つ目 $\pi_5$: ユーザとニュースのコンテキストを考慮し、コンテキストベクトル $x$ とアイテムコンテキストベクトル $e$ の内積が最も大きいニュースを確率0.7で推薦し、その他のニュースを均等に確率0.1で推薦する確率的方策。

- このk種類の推薦方策を、それぞれデータ収集方策と評価方策として使って、オフライン評価を行う。
  - それぞれの推薦方策に対して、1000ラウンドのシミュレーションを行い、各ラウンドでの報酬を記録する。
  - この報酬データを使って、naive推定量 $\hat{V}_{naive}(\pi;D)$ とIPS推定量 $\hat{V}_{IPS}(\pi;D)$ の値を計算し、各推薦方策の真の性能 $V(\pi)$ とどの程度一致するかを確認する。

### 実験結果

### 考察

## 決定的な推薦方策から確率的な推薦方策への簡単な移行アイデア

### シンプルにepsilon-greedyを使う

### プラケットルースモデルに基づいてランキングの確率的サンプリング

### プラケットルースモデルの高速化ver.:ガンベルソフトマックストリックを使う

## おわりに

決定的な推薦方策から確率的な推薦方策への移行アイデアをいくつか検討してみます。

推薦タスクの問題設定としては、推薦候補のアイテム集合 $A$ から、ユーザ特徴量 $x$ に基づいて、長さ $k$ の推薦アイテムリスト $\mathbf{a} = [a_1, a_2, \ldots, a_k]$ を生成するというものを考えます。

すでに、任意のユーザ特徴量 $x$ とアイテム $a \in A$ の組み合わせに対してスコアを割り当てる関数 $f_{\theta}(x, a)$ が何らか存在しているとします。（$f_{\theta}$ は例えば線形モデルやNN、あるいはシンプルな内積などで実装されているでしょう）

そして現在、このスコア関数 $f_{\theta}(x,a)$ に基づいてスコアが最も高い(or低い)アイテムk個を選択するような決定的な推薦方策が稼働しているとします。

例えば図にするとこういう決定的な方策! (xに対するスコアが高い順にitem=1,2,...と並んでる想定)

![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1697279/a5e8e5d2-4e14-6ffe-5404-6eec9462e1b1.png)

この**決定的な方策を確率的な方策に移行するアイデア**たちを考えてみます。

- 1. プラケットルースモデルに基づくランキングの確率的サンプリング
- 2. ガンベルソフトマックストリックを使う
- 3. epsilon-greedyを使う

### プラケットルースモデルに基づくランキングの確率的サンプリング

- アイテム集合 $A$ から 長さ $k$ の推薦アイテムリスト　$\mathbf{a} = [a_1, a_2, \ldots, a_k]$ を確率的にサンプリングする際は、リスト内で同一アイテムの重複を生まないような工夫が必要。
- よく用いられる方法として「反実仮想機械学習」で紹介されてたのが、**プラケットルースモデル(Pleckett-Luce model、PLモデル)**という方法。

PLモデルの流れは、以下のお気持ち実装を参照:

```python
def sampling_with_plackett_luce_model(f_theta: ScoreFunction, x: FeatureVector, A: set[Item], k: int)->list[Item]:
    """
    PLモデルに基づく推薦アイテムリストの確率的サンプリング
    """
    ranking = []
    # スコア関数 f_theta(x,a) を使って、個別のアイテムaについてスコアを割り当てる
    score_by_item = {a: f_theta(x, a) for a in A}
    for _ in range(k):
        # スコア f_theta(x,a) をソフトマックス関数に渡して、確率分布 p(a|x) に変換
        pmf = calculate_pmf_by_softmax(score_by_item, temperature_param=1.0)
        
        # 確率分布に基づいてi番目のポジションの推薦アイテムをサンプリング
        a_i = np.random.choice(A, p=pmf)
        ranking.append(a_i)

        # 同一アイテムが重複して選ばれないように、選択されたアイテムを推薦候補から除外して、次のポジションのサンプリングへ
        A.remove(a_i)
    return ranking
```

たぶん分布はこんな感じになりそう??

![スクリーンショット 2024-10-15 17.08.04.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1697279/3691787f-a1eb-aa89-f038-5f470de5032d.png)

ただ書籍によると、上記のアルゴリズムは現場の推薦・検索システムに適用するには計算量の問題で難しい(**ソフトマックス関数を何度も計算する必要がある**ため...!:scream:)とのこと。

#### 計算量ってどれくらいなんだろ？

- 変数:
  - $|A| = n$ (アイテム集合のサイズ)
  - $k$ (推薦リストの長さ)
- 各処理の計算量
  - スコアの計算: Aの各アイテムに対してスコアを計算してる。なので計算量はO(n)になる。
  - メインのループ処理: サンプリングする推薦リストの長さ $k$ 回ループする。ループの計算量はO(k)になる。
    - 確率分布の計算: Aの各アイテムに対してソフトマックス関数で確率質量を計算してる。計算量はO(n)になる。
    - サンプリング: `np.random.choice(A, p=pmf)`は `A`の要素数に依らずにかかる時間は一定。計算量はO(1)になる。
    - サンプリングしたアイテムを推薦リストに追加: `ranking`はリスト型。リストの末尾に要素を追加する`append`は既存の要素数に関係なくO(1)でできる。計算量はO(1)になる。
    - サンプリングしたアイテムを推薦候補から除外: `A`はset型。`set`はハッシュテーブルで実装されているので、要素の追加・削除がO(1)でできる。計算量はO(1)になる。

よって、全体の計算量はO(n + k *(n + 1 + 1 + 1)) -> O(n + k* n) -> O(k * n)になる??
(スコア関数による推論がめっちゃ軽い前提??:thinking:)

### ガンベルソフトマックストリックを使う

- 前述のように、originalのPLモデルのアルゴリズムは計算量が大きく、実用的ではないっぽい。
- **ガンベルソフトマックストリック(Gumbel-Softmax trick)**と呼ばれるテクニックを使うことで、PLモデルに基づいた確率的なランキング方策を効率化して高速化できることが知られてるみたい。

ガンベルソフトマックストリックが行うこと:

- 1. 各アイテムのスコア $f_{\theta}(x,a)$ を用意する(ここは同じ)。
- 2. スコアに対して**ガンベル分布に従うノイズ**を加える。
- 3. スコア+ノイズの値が大きい順に、アイテムを並び替えて推薦アイテムリストを生成する。

数式で表すと以下。
($Gumbel(0,1)$ は標準ガンベル分布)

$$
\mathbf{a} = argsort_{a \in A} \{ f_{\theta}(x,a) + \epsilon_{a} \}
\\
\epsilon_{a} \sim Gumbel(0,1)
\tag{2}
$$

- 実は、**ガンベルソフトマックストリックでランキングを生成することは、PLモデルに基づくランキングの確率的サンプリングと同等(?)であることが知られているっぽい**。
- ガンベルソフトマックストリックを使うことで、ソフトマックス関数を適用することなくスコア関数に基づいたランキングを確率的に生成できるため、計算効率の意味で非常に嬉しい。

お気持ち実装だとこんな感じ

```python
def sampling_with_gumbel_softmax_trick(f_theta: ScoreFunction, x: FeatureVector, A: set[Item], k: int)->list[Item]:
    """
    ガンベルソフトマックストリックを使った推薦アイテムリストの確率的サンプリング
    """
    # 各アイテムのスコア $f_{\theta}(x,a)$ を用意する(ここは同じ)。
    score_by_item = {a: f_theta(x, a) for a in A}
    # スコアに対してガンベル分布に従うノイズを加える。
    gumbele_noises = np.random.gumbel(
        size=len(preference_score_by_id), loc=0.0, scale=1.0
    )
    perturbed_score_by_item = {
        a: score + noise for (a, score), noise in zip(score_by_item.items(), gumbele_noises)
    }
    # スコア+ノイズの値が大きい順のアイテムk個を、,推薦アイテムリストとして返す
    return sorted(perturbed_score_by_item.items(), key=lambda item_score: item_score[1], reverse=True)[:k]
```

プラケットルースモデルと同等、とのことなので、たぶん分布は同じくこんな感じになりそう??

![スクリーンショット 2024-10-15 17.08.04.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1697279/3691787f-a1eb-aa89-f038-5f470de5032d.png)

#### 計算量ってどれくらいなんだろ？

- 変数:
  - $|A| = n$ (アイテム集合のサイズ)
  - $k$ (推薦リストの長さ)
- 各処理の計算量
  - スコアの計算: Aの各アイテムに対してスコアを計算してる。なので計算量はO(n)になる。
  - ガンベルノイズの生成: ガンベルノイズは標準ガンベル分布から生成している。これも計算量はO(n)になる。
  - スコアにノイズを加える: Aの各アイテムに対してスコアにノイズを加えている。計算量はO(n)になる。
  - スコア+ノイズの値が大きい順にソートして、上位k個を取り出す: `sorted`関数は計算量がO(n log n)。計算量はO(n log n)になる。

よって全体としては、計算量はO(n + n + n + n * log n) -> O(n log n)になりそう??:thinking:

### epsilon-greedyを使う

- 最後に、epsilon-greedyを使ったアイデアを考えてみます。
- epsilon-greedyの概要:
  - 強化学習でよく使われる探索戦略の1つ。
  - 行動を選択するたびに、確率 $\epsilon$ でランダムに行動を選択し、確率 $1-\epsilon$ でスコアが最も高い行動を選択する。

お気持ち実装だとこんな感じ

```python
def sampling_with_epsilon_greedy(f_theta: ScoreFunction, x: FeatureVector, A: set[Item], k: int, epsilon: float = 0.1)->list[Item]:
    """
    epsilon-greedyを使った推薦アイテムリストの確率的サンプリング
    """
    ranking = []
    # スコア関数 f_theta(x,a) を使って、個別のアイテムaについてスコアを計算してる
    score_by_item = {a: f_theta(x, a) for a in A}
    for _ in range(k):
        # 確率 epsilon で探索(ランダムに推薦アイテムを選択)
        if np.random.rand() < epsilon:
            a_i = np.random.choice(A)
        # 確率 1-epsilon で活用(最適なアイテムを選択)
        else:
            # スコアが最も高いアイテムを選択
            a_i = max(score_by_item, key=score_by_item.get)
        ranking.append(a_i)

        # 同一アイテムが重複して選ばれないように、選択されたアイテムを推薦候補から除外して、次のポジションのサンプリングへ
        A.remove(a_i)
    return ranking
```

なので、なんとなくこんな感じの分布になりそう。

![スクリーンショット 2024-10-15 17.10.21.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1697279/d3f37134-f164-a268-c529-e951cfa81016.png)

#### 計算量ってどれくらいなんだろ？

- 変数:
  - $|A| = n$ (アイテム集合のサイズ)
  - $k$ (推薦リストの長さ)
- 各処理の計算量
  - スコアの計算: Aの各アイテムに対してスコアを計算してる。なので計算量はO(n)になる。
  - メインのループ処理: サンプリングする推薦リストの長さ $k$ 回ループする。ループの計算量はO(k)になる。
    - 探索か活用かの判定: `np.random.rand()`は定数時間で計算できる。計算量はO(1)になる。
    - 探索(ランダムに推薦アイテムを選択): `np.random.choice(A)`は `A`の要素数に依らずにかかる時間は一定。計算量は定数時間 O(1) になりそう。
    - 活用(最適なアイテムを選択): スコアが最も高いアイテムを選択する処理は、`max`関数を使っている。計算量はO(n)になる。
    - 選択されたアイテムを推薦候補から除外: `A`はset型。計算量はO(1)になる。

全体の計算量はO(n + k *(1 + n + 1)) -> O(n + k*n) -> O(k * n)になりそう??:thinking:
（プラケットルースモデルと同じくらい??:thinking:）

## 懸念: オンライン推論の場合、確率的な方策はレイテンシーとか大丈夫??

### 決定的な推薦方策の場合

お気持ち実装

```python
def sampling_with_deterministic_policy(f_theta: ScoreFunction, x: FeatureVector, A: set[Item], k: int)->list[Item]:
    """
    決定的な推薦アイテムリストの生成
    """
    # スコア関数 f_theta(x,a) を使って、個別のアイテムaについてスコアを計算してる
    score_by_item = {a: f_theta(x, a) for a in A}

    # スコアが最も高いアイテムk個を推薦アイテムリストとして返す
    return sorted(score_by_item.items(), key=lambda item_score: item_score[1], reverse=True)[:k]
```

この計算量は...

- 各処理の計算量
  - スコアの計算: Aの各アイテムに対してスコアを計算してる。なので計算量はO(n)になる。
  - スコアが最も高いアイテムを選択: `sorted`関数は計算量がO(n log n)。計算量はO(n log n)になる。

全体の計算量はO(n + n log n)になりそう??:thinking:
(gumbel softmax trickと同じくらい??:thinking:)

## 実際に実験してみる

### リアルタイム推論を想定した実験設定

- 問題設定
  - あるユーザに対して、n個のアイテム集合からk個のアイテムを推薦するタスクを考える。
  - クライアント側からuser_idを指定したリクエストを受け取り、そのユーザに対してk個のアイテムを推薦する。
  - スコア関数 $f_{theta}(x,a)$ について
    - 今回は、ユーザとアイテムの埋め込み表現をもとに、内積をスコアとするシンプルなスコア関数を想定する。
  - リアルタイム推論時には事前に作成された特徴量（ユーザ、アイテム埋め込み）を使うケースを想定する。(特徴量作成はバッチ、推論だけリアルタイムのケース)

### ちなみに、今回の実験のための準備方法

- リアルタイムで推薦結果を作って返す推論サーバの作り方:
  - 今回はなるべく簡単にシンプルに推論サーバを提供するために、Sagemaker EndpointとAWS API Gatewayを紐付ける方法を試してみました!
  - 参考にしたAWS公式のブログ: [Creating a machine learning-powered REST API with Amazon API Gateway mapping templates and Amazon SageMaker](https://aws.amazon.com/jp/blogs/machine-learning/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker/)

![アーキテクチャ図を作って追加する](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2020/03/04/api-gateway-sagemaker-1.gif)

- こんな感じのエンドポイントを作ってみました!

```shell
curl -X GET "https://xxxxxxxxx.execute-api.{aws-region}.amazonaws.com/1/recommended-items/{user_id}?k={推薦アイテムリストの長さ}&inference_type={推薦結果の作り方の種類}" 
```

- ちなみにSagemaker推論エンドポイントの設定は以下です:
  - instance_type: `ml.m5.large` (利用料金は 0.149 USD/hour)
    - 今回は $f_{\theta}(x,a)$ の計算がシンプルな内積計算なので、m5.largeで十分です!
    - 参考: <https://aws.amazon.com/jp/sagemaker/pricing/>
  - instance_count: 1

それぞれリクエスト投げてみる。

- クエリパラメータ
  - k=10&inference_type=deterministic
  - k=10&inference_type=stochastic_plackett_luce
  - k=10&inference_type=stochastic_gumbel_softmax_trick
  - k=10&inference_type=stochastic_epsilon_greedy

パッと数回、リクエストを投げてみた結果...。体感的には、どれが遅いとか早いとかなさそうな感じ...:thinking:

- `inference_type=deterministic`
  - 80msくらい
- `inference_type=stochastic_plackett_luce`
  - 80msくらい
- `inference_type=stochastic_gumbel_softmax_trick`
  - 80msくらい
- `inference_type=stochastic_epsilon_greedy`
  - 80msくらい

## もう少しだけちゃんとレイテンシー計測してみたい

以下の資料を参考に、**locustという負荷試験ツール**を使って、レイテンシーを計測してみます。

- 参考: [SageMaker Endpointのレイテンシーを負荷試験ツールlocustで計測する](https://nsakki55.hatenablog.com/entry/2022/12/14/091430)
- locustとは
  - 参考: [【初心者でも安心】Locustで始める負荷テスト](https://zenn.dev/secondselection/articles/locust_sample)
  - オープンソースのPython製の負荷試験ツールの1つ。
    - Pythonistaに使いやすくシンプルなUIで負荷試験ができる
  - locust=イナゴ、という意味らしい。
    - イナゴのように群れて大量発生したリクエストをサーバ（アプリケーション）がどれだけ対処できるかをテストする感じ...!:thinking:

簡易的に実験するため、前述のSagemaker Endpointと同じコンテナをローカル環境で起動させました。
locustを使ってリクエストを何度も投げて、レイテンシーを計測しました。

![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1697279/e379a18f-a33e-0ac1-e4ba-61354d665a2c.png)

結果としてはあんまり変わらなさそうに見えた。

だったら確率的な推薦方策にした方が、オフライン評価&学習しやすさを考えると良いのかも...??
