---
format:
  html:
    theme: cosmo
  # revealjs:
  #   # incremental: false
  #   # theme: [default, custom_lab.scss]
  #   theme: [default, custom_lab.scss]
  #   slide-number: true
  #   scrollable: true
  #   logo: https://s3-ap-northeast-1.amazonaws.com/qiita-image-store/0/1697279/dfa905d1c1e242b4e39be182ae21a2b6ac72c0ad/large.png?1655951919
  #   footer: ⇒ [https://qiita.com/morinota](https://qiita.com/morinota)
from: markdown+emoji

fig-cap-location: bottom

title: 確率的(探索的)な推薦方策を考える ~オフライン評価&学習しやすい優しい世界?~
subtitle: y-tech-ai ワクワク勉強会
date: 2024/08/13
author: モーリタ
title-slide-attributes:
  #   data-background-image: https://i.imgur.com/nTazczH.png
  data-background-size: contain
  data-background-opacity: "0.5"
---

## (下書き) 書きたい内容をラフに列挙する

<!-- ## TL;DR -->

- 意思決定最適化問題のオフライン評価が難しい話を少し
  - 「反実仮想機械学習」などを読んだ感じでは、データ収集方策が決定的だと、オフライン評価がかなり困難になりそう。
  - (確率的な方策を採用したらそれで完結するわけではないが、決定的な方策だともう厳しすぎるので)
- 評価・学習に使用しやすいログデータを収集するというモチベーションで、決定的な推薦方策から確率的な推薦方策への移行アイデアをいくつか考えてみる。
  - まず問題設定:
  - アイデア0: 決定的な方策
  - アイデア1: プラケットルースモデルに基づくランキングの確率的サンプリング
  - アイデア2: ガンベルソフトマックストリックを使って同様のランキングを高速に生成
  - アイデア3: epsilon-greedyを使う
  - ちなみに...「探索」がユーザ体験にポジティブな影響を与えて長期的なengagementに繋がるという主張の論文もあった。
  - 論文の結論としては、単に多様性を増やすだけではダメで、セレンディピティを生むような探索を推薦システムに実行させることができた場合に有効という結論だったはず。
- 懸念: オンライン推論の場合、確率的な方策はレイテンシーとか大丈夫??
- 実験してみる:

## はじめに

### なんでこのトピックを??

- 先日のRecommendation Industry Talksというイベントで「推薦システムを本番導入する上で一番優先すべきだったこと」というタイトルで発表してきましたー!
  - 結論: **推薦システムのオフライン評価が難しいから一旦諦めて、まずはいかにA/Bテスト(オンライン評価)しやすい基盤を設計することが大事だった**...!
  - オフライン評価が難しかった -> オフライン-オンライン評価の結果が相関しなかった
- 一方で、やっぱりオフライン評価できた方が嬉しい...!
  - A/Bテストなどのオンライン評価と比べて、高速にフィードバックを得られる。ユーザ体験を毀損するリスクがない、という利点。
  - というか、オフライン評価が難しいってことは、ハイパーパラメータチューニングやオフライン学習も難しいのでは...!:scared:
- オフライン評価のアプローチを調べていると...
  - 決定的な推薦方策によって収集した観測データだと、かなり打ち手がなさそうな印象。
  - **確率的な推薦方策によってある程度探索的に収集したデータを使えば、だいぶ打ち手が増えそう**...!:thinking:
- というわけで、確率的な推薦方策を本番導入することを色々考えてみた!

## ということで、決定的な推薦方策と確率的な推薦方策

- 決定的(deterministic)な方策
  - 常に同じ行動を選択する方策。
  - ex.) ユーザ1に対して、アイテムAを100%の確率で推薦する。
- 確率的(stochastic)な方策
  - ある確率でランダムに行動を選択する方策。
  - ex.) ユーザ1に対して、アイテムAを50%、アイテムBを30%、アイテムCを20%の確率で推薦する。

今回は、推薦モデルの評価・学習に使用しやすいログデータを収集したいというモチベーションで、前者から後者への移行について考えてみます!

## 推薦システムのオフライン評価の話を少し

### 意思決定の最適化問題とオフライン評価

- 推薦システムは意思決定の最適化問題!
  - 例えば、ニュース推薦の場合
    - 何らかのモデルによって、(ユーザ, 記事)のペアの相性やクリック確率などのスコアを予測
    - スコアの予測値に基づいて、ユーザにどんな記事を推薦するかの意思決定を行う

> 予測というよりもむしろデータに基づいた意思決定の最適化問題
> (*書籍「反実仮想機械学習」より引用*)

データに基づいて導かれる意思決定の規則 = 意思決定方策(decision-making policy)。
オフライン評価のモチベーションは、この意思決定方策の性能を、ログデータに基づいて正確に評価できるようになること!

- オフライン評価が正確にできる場合
  - A/Bテストなどのオンライン評価の前に、有効そうな方策を絞り込む事ができ、仮説 -> 実験 -> フィードバックをより高速に回す事ができる。
  - ハイパーパラメータチューニングやオフライン学習もしやすくなる。
  - ユーザ体験を悪化させ得る施策を、オンライン環境に出す前に検知する事ができる。
- 逆に、オフライン評価の確度が低い場合
  - 全ての仮説に対してABテストする必要があり、仮説 -> 施策 -> 検証の速度が下がる。(ABテストの運用コストを下げても、どうしてもオフライン実験するよりは時間がかかる。10倍くらい?)
  - 誤った基準でハイパーパラメータチューニングしてしまう可能性。
  - ユーザ体験を悪化させ得る施策を、オンライン環境に出す可能性。

### オフライン評価がなかなか難しい問題

- naive推定量のバイアスの話を少し

### 　最も基本的な戦略　IPS推定量を使う上で満たすべき仮定: 共通サポート仮定

- naive推定量では、データ収集方策と評価方策が異なるほど大きなバイアスを持つ。
- naive推定量のバイアスを打ち消す最も基本的な戦略 = IPS(Inverse Propensity Score)推定量

$$
IPS推定量の定義式
$$

「反実仮想機械学習」を読んでいてもOPE推定量の多くがIPS推定量の拡張ver.っぽく、最も基本的なOPE推定量の1つと言ってもよさそうです...!:thinking:

そしてIPS推定量は、以下の**共通サポート仮定**を満たす場合に真の性能に対して不偏になります:

$$
\pi_1(a|x) > 0 -> \pi_0(a|x) > 0, \forall a \in A, \forall x \in X
$$

つまり、「評価方策 $\pi_1$ が特徴 $x$ に対してサポートする(=選択する可能性がある)全ての行動 $a$ を、データ収集方策 $\pi_0$ もサポートしていてくれ!」という仮定です。

### データ収集方策が決定的方策だと、オフライン評価難しそう

$$
\pi_1(a|x) > 0 \rightarrow \pi_0(a|x) > 0, \forall a \in A, \forall x \in X
$$

IPS推定量（あるいはその拡張ver.たち）を活用するためには、上記の共通サポート仮定をなるべく満たしたい訳です。

ここで、決定的方策と確率的方策の話が出てきます!
データ収集方策 $\pi_0$ と評価方策 $\pi_1$ がそれぞれ、決定的方策/確率的方策であるケース（2^2 = 4通り）を考えてみましょう。

![](https://pbs.twimg.com/media/GVufdqfXkAEkE6W?format=png&name=4096x4096)

- 1. pi_0 = 決定的方策、pi_1 = 決定的方策のケース
  - -> 共通サポート過程を満たせなさそう。唯一満たせるのは、$\pi_0 = \pi_1$ の場合のみなので。
- 2. pi_0 = 決定的方策、pi_1 = 確率的方策のケース
  - -> 共通サポート過程を満たせなさそう。
- 3. pi_0 = 確率的方策、pi_1 = 決定的方策のケース
  - -> 共通サポート過程を満たせそう!
- 4. pi_0 = 確率的方策、pi_1 = 確率的方策のケース
  - -> 共通サポート過程を満たせそう!

これを踏まえると、データ収集方策 $\pi_0$ が決定的方策だと、共通サポート仮定を全然満たせなさそう...!:scream:
一方で、**データ収集方策 $\pi_0$ が確率的方策でさえあれば**、評価方策 $\pi_1$ が決定的方策でも共通サポート仮定を結構満たせそう...!:thinking:

抽象的な表現ですが**方策の探索の度合いが「データ収集方策 ≧ 評価方策」であれば**、共通サポート仮定を満たしやすく、IPS推定量（あるいはその拡張ver.たち）を使ってオフライン評価に打つ手が出てくるのでは...!:thinking:

## 決定的な推薦方策から確率的な推薦方策への移行アイデア達

決定的な推薦方策から確率的な推薦方策への移行アイデアをいくつか検討してみます。

- プラケットルースモデルに基づくランキングの確率的サンプリング
- ガンベルソフトマックストリックを使う
- epsilon-greedyを使う

### プラケットルースモデルに基づくランキングの確率的サンプリング

### ガンベルソフトマックストリックを使う

### epsilon-greedyを使う

## 懸念: オンライン推論の場合、確率的な方策はレイテンシーとか大丈夫??
