## REINFORCE推薦システムについて

- REINFORCE推薦システムとは?
  - k個の推薦セットを生成する問題をマルコフ決定過程(MDP) $(S, A, P, R, \rho_0, \gamma)$ として定式化した手法。
    - 強化学習の考えに基づいて、過去の観測データから方策をオフライン学習する手法??:thinking:
- 以下は、REINFORCE推薦システムの定義に必要な記号一覧
  - $\mathcal{S}$ はユーザの関心やcontextを表す状態空間(=特徴量xの空間と言っても良いはず!:thinking:) 
    - (stateのS、強化学習の文脈ではユーザ特徴量が時間や選択した行動によって変化する想定なので、xじゃなくてsが使われがち...!:thinking:)
  - $\mathcal{A}$ は推薦可能なアイテムを含む離散的な行動空間
  - $P : S \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$ は状態遷移確率
    - たぶん条件付き確率 $P(s_{t+1}|s_{t}, a_{t})$ みたいなイメージだよね...!:thinking:
    - ユーザが state $s_{t}$ にいるときに、アクション $a_{t}$ を選択した場合に、次のstate $s_{t+1}$ に遷移する確率...!:thinking:
  - $R : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ は報酬関数。$r(s, a)$ は状態$s$の下でのアクション$a$の即時報酬を示す
    - これは、条件付き確率 $P(r_{t}|s_{t}, a_{t})$ みたいなやつ...!:thinking:
  - $\rho_0$ は初期状態分布 (initial state distribution)
    - ユーザの最初のstate (i.e. 特徴量) がどれであるかの確率分布。p(s_0) みたいな感じ...!:thinking:
  - $\gamma$ は将来の報酬の割引率。
    - これは割引率とか難しい言葉を使ってるが、結局はある方策を動かしたときに得られる期待累積報酬を評価する上で、近い未来(i.e. time step!)に対して遠い未来の報酬をどれだけ重要視するかを調整するためのパラメータ...!:thinking:
    - 全期間の報酬を一律に合計して累積報酬を定義する場合は、$\gamma=1$ となる。
- REINFORCEがやりたいこと = 強化学習っぽい方策を、オフラインバッチ学習すること??:thinking:
- REINFORCEの学習手順
  - ＄H_{t} = \{(A_{0}, a_{0}, r_{0}), (A_{1}, a_{1}, r_{1}), \ldots, (A_{t-1}, a_{t-1}, r_{t-1})\}$ を取得する。これはtime step $t=0, 1, \ldots, t-1$ までの**アクションと得られた即時報酬の軌跡(履歴)**を表す。
    - (要するに、オフライン学習における過去の観測データ $D$ みたいなもの...??:thinking:)
    - $A_{t}$ は、time step $t$ で、あるユーザに推薦されたアイテム集合。(i.e. 方策が選んだ行動!)
    - $a_{t}$ は、time step $t$ で、ユーザがinteractしたアイテム。
    - $r_{t}$ は、time step $t$ で、その結果得られた即時報酬。
  - 軌跡 $H_{t}$ を使って、ユーザ潜在状態ベクトル $u_{s_t}$ をencodeする。
    - $u_{s_t} = RNN_{\theta}(H_{t})$
  - ユーザ潜在状態ベクトル $u_{s_t}$ を入力として、方策 $\pi_{\theta}$ は推薦アイテム $a_{t}$ を以下の確率分布に従って生成する。(ソフトマックス方策と言うっぽい...!:thinking:)
    - ユーザ状態 $s_t$ で条件づけたアイテム $a_{t}$ の選択確率。
    - (**じゃあアイテムk個の推薦セットを推論するためには、PLモデルに従うことになりそう**...?? オンライン推論ならばガンベルソフトマックストリックを使わねば...!:thinking:)

$$
\pi_{\theta}(a_{t}|s_t) = P_{\pi_{\theta}}(a_{t}|s_t)
\\ 
= \frac{\exp(f_{\theta}(u_{s_t}, a_{t}))}{\sum_{a \in \mathcal{A}} \exp(f_{\theta}(u_{s_t}, a))}
\tag{1}
$$
      

- REINFORCEでは、方策 $\pi_{\theta}$ を学習するために、以下の目的関数 $J$ の最大化を目指す。
  - i.e. ユーザの軌跡に対する累積報酬の期待値を、方策が最大化すべき報酬とみなしてる...! なので何を報酬と定義するかは、usecaseによってカスタムすることになるはず...!:thinking: 

$$
J(\pi_{\theta}) = \mathbb{E}_{s_t \sim \rho_0, a_t \sim \pi_{\theta}}[R(s_t, a_t)]
$$

## 推薦の品質を測定するための評価指標

### 指標に必要なアイテム属性を定義する

surprise因子を定義するために使用される、**2つのアイテム属性**を紹介:

- 1. **Topic cluster(トピッククラスタ)**
  - 各アイテムのトピッククラスタは、次の手順で生成される:
    - 1. アイテムの共起行列を作成する
      - **要素 (i, j) は、アイテム i と j が同じユーザによって連続してinteractされた回数**をカウントする。
      - "連続して"って条件は厳しくない?? まあこの条件は、プロダクトやusecaseに応じて柔軟に変更しても良さそう...!:thinking:
    - 2. 共起行列に対してMatrix Factorizationを適用し、各アイテムに1つの埋め込みを生成する。
    - 3. k-meansを使用して、学習された埋め込みをk=10K(=10,000)個のクラスタに割り当てる。
      - まあここのクラスタ数kも、プロダクトやusecaseに応じて柔軟に変更して良さそう...!:thinking:
    - 4. 各アイテムに最も近いクラスタを、そのアイテムのトピッククラスタとして割り当てる。
- 2. Content Provider(コンテンツ提供者)
  - コンテンツ提供者は、以下の理由から興味深いアイテム属性の1つになってる:
    - 1. 同じプロバイダによって作られたコンテンツには一貫性がみられることが多い.
      - ex. food blogerは、特定の料理について頻繁に書く傾向がある。
    - 2. 長期的なユーザ体験に影響を与える、コンテンツ提供者の多様性や新規性の重要性を理解したい。

### Accuracy metrics(精度指標)

- 推薦システムの最も重要な特性は、ユーザが消費したいと思うようなコンテンツをretrieveできること。
- 本論文では、推薦セット $A^{\pi_{\theta}}$ において、ユーザが消費したいと思うアイテムを識別する平均精度を測定するために、MAP@K=50 (mean average precision) を使用する。
  - オンラインテストで評価するならOKだけど、オフラインテストで評価するなら、データ収集方策を考慮する必要があるよね:thinking:

### Diversity metrics(多様性指標)





