## REINFORCE推薦システムについて

- REINFORCE推薦システムとは?
  - k個の推薦セットを生成する問題をマルコフ決定過程(MDP) $(S, A, P, R, \rho_0, \gamma)$ として定式化した手法。
    - 強化学習の考えに基づいて、過去の観測データから方策をオフライン学習する手法??:thinking:
- 以下は、REINFORCE推薦システムの定義に必要な記号一覧
  - $\mathcal{S}$ はユーザの関心やcontextを表す状態空間(=特徴量xの空間と言っても良いはず!:thinking:) 
    - (stateのS、強化学習の文脈ではユーザ特徴量が時間や選択した行動によって変化する想定なので、xじゃなくてsが使われがち...!:thinking:)
  - $\mathcal{A}$ は推薦可能なアイテムを含む離散的な行動空間
  - $P : S \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$ は状態遷移確率
    - たぶん条件付き確率 $P(s_{t+1}|s_{t}, a_{t})$ みたいなイメージだよね...!:thinking:
    - ユーザが state $s_{t}$ にいるときに、アクション $a_{t}$ を選択した場合に、次のstate $s_{t+1}$ に遷移する確率...!:thinking:
  - $R : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ は報酬関数。$r(s, a)$ は状態$s$の下でのアクション$a$の即時報酬を示す
    - これは、条件付き確率 $P(r_{t}|s_{t}, a_{t})$ みたいなやつ...!:thinking:
  - $\rho_0$ は初期状態分布 (initial state distribution)
    - ユーザの最初のstate (i.e. 特徴量) がどれであるかの確率分布。p(s_0) みたいな感じ...!:thinking:
  - $\gamma$ は将来の報酬の割引率。
    - これは割引率とか難しい言葉を使ってるが、結局はある方策を動かしたときに得られる期待累積報酬を評価する上で、近い未来(i.e. time step!)に対して遠い未来の報酬をどれだけ重要視するかを調整するためのパラメータ...!:thinking:
    - 全期間の報酬を一律に合計して累積報酬を定義する場合は、$\gamma=1$ となる。
- REINFORCEがやりたいこと = 強化学習っぽい方策を、オフラインバッチ学習すること??:thinking:
  - (REINFORCEって、強化学習的な方策をOPLしてるってことっぽい...!:thinking:)
- REINFORCEの学習手順
  - ＄H_{t} = \{(A_{0}, a_{0}, r_{0}), (A_{1}, a_{1}, r_{1}), \ldots, (A_{t-1}, a_{t-1}, r_{t-1})\}$ を取得する。これはtime step $t=0, 1, \ldots, t-1$ までの**アクションと得られた即時報酬の軌跡(履歴)**を表す。
    - (要するに、オフライン学習における過去の観測データ $D$ みたいなもの...??:thinking:)
      - **いや、どうやらここでの$H$ は、半日仮想機械学習では $\tau$ で表記されてる軌跡に近い...! ちなみにtauの場合は状態と状態繊維もタプルに含まれてる...!**
      - 定義は $\tau = \{(s_h, a_h, r_h, s_{h+1})\}_{h=0}^{H}$
      - $\tau$ が従う確率分布は $p_{\pi}(\tau) = p(s_0) \prod_{h=0}^{H} \pi(a_h|s_h) p(r_h|s_h, a_h) p(s_{h+1}|s_h, a_h)$
      - オフライン強化学習の文脈で、データ収集方策により与えられるログデータ $D$ の形は以下: $D := \{\tau_i\}_{i=1}^{n} \sim p(D) = \prod_{i=1}^{n} p_{\pi_{0}}(\tau_i)$
    - $A_{t}$ は、time step $t$ で、あるユーザに推薦されたアイテム集合。(i.e. 方策が選んだ行動!)
    - $a_{t}$ は、time step $t$ で、ユーザがinteractしたアイテム。
    - $r_{t}$ は、time step $t$ で、その結果得られた即時報酬。
  - 軌跡 $H_{t}$ を使って、ユーザ潜在状態ベクトル $u_{s_t}$ をencodeする。
    - $u_{s_t} = RNN_{\theta}(H_{t})$
  - ユーザ潜在状態ベクトル $u_{s_t}$ を入力として、方策 $\pi_{\theta}$ は推薦アイテム $a_{t}$ を以下の確率分布に従って生成する。(ソフトマックス方策と言うっぽい...!:thinking:)
    - ユーザ状態 $s_t$ で条件づけたアイテム $a_{t}$ の選択確率。
    - (**じゃあアイテムk個の推薦セットを推論するためには、PLモデルに従うことになりそう**...?? オンライン推論ならばガンベルソフトマックストリックを使わねば...!:thinking:)

$$
\pi_{\theta}(a_{t}|s_t) = P_{\pi_{\theta}}(a_{t}|s_t)
\\ 
= \frac{\exp(f_{\theta}(u_{s_t}, a_{t}))}{\sum_{a \in \mathcal{A}} \exp(f_{\theta}(u_{s_t}, a))}
\tag{1}
$$
      

- REINFORCEでは、方策 $\pi_{\theta}$ を学習するために、以下の目的関数 $J$ の最大化を目指す。
  - i.e. ユーザの軌跡に対する累積報酬の期待値を、方策が最大化すべき報酬とみなしてる...! なので何を報酬と定義するかは、usecaseによってカスタムすることになるはず...!:thinking: 

$$
J(\pi_{\theta}) = \mathbb{E}_{s_0 \sim \rho_0, a_t \sim \pi_{\theta}}[\sum_{t=0}^{T} r(s_t, a_t)
\tag{2}
$$

ちなみに、上式が何の確率分布に基づく期待値か、の表記は、以下の同時確と同義のはず...!
$$
E_{s_0 \sim \rho_0, A_t \sim \pi_{\theta}(\cdot|s_t), s_{t+1} \sim P(s_t, A_t)}[\cdot] = E_{P(s_0, A_t, s_{t+1})}[\cdot]
$$

(今回のREINFORCEの問題設定では、即時報酬 r が確率変数ではないと仮定してるっぽい...!:thinking:)


- Googleの既存文献(多分読んだことある!)の貢献は、REINFORCEをオフラインバッチ学習に適用させたこと!
  - (強化学習は、元々オンライン学習のparadimだったのに、それを実践しやすいオフラインバッチ学習として使えるようにしたってことか...!：thinking_face:)
  - 目的関数の勾配の推定式を以下のように定義した! (まさにOPL推定量っぽいじゃん...!:thinking:)
    - 著者らは、オフライン学習によって引き起こされる分布シフトに対処するために、**重要度サンプリング**の一次近似[2]を適用してる。
      - なんかIPS推定量みたいな感じで、データ収集方策(論文ではbehavior policy $\beta$)と学習方策の行動分布の比率を使って、学習方策の勾配をバイアスなく推定しようとしてる...!:thinking:
      - だから、実際には真の勾配の推定値なので $\hat{\Delta}_{\theta} J(\pi_{\theta})$ のはず...!:thinking:

$$
\Delta_{\theta} J(\pi_{\theta}) = \mathbb{E}_{s \sim d_{\beta}_{t}, a_t \sim \beta(\cdot|s_t)} \left[ \frac{\pi_{\theta}(a|s)}{\beta_{t}(a|s)} \nabla_{\theta} \log \pi_{\theta}(a|s) \cdot R_{t} \right]
$$

- ここで、
  - $d^{\beta}_{t}(s)$ は、方策 $\beta$　のもとでのdiscountedな状態遷移確率である。(discountedの意味がわからず...!)
  - この重要度重みは、セット推薦(=k個の長さのランキング推薦!)の設定に対応するようにさらに調整される。(はいはい、ランキング問題のための調整ね!)

- 本論文では、探索(exploration)と活用(exploitation)をバランスさせるために、推論時には、方策の出力する確率 $\pi_{\theta}$ に従って、上位 $K'$ 個の最も確率が高いアイテムを返す一方で、残りの $K - K'$ 個のアイテムをサンプリングするハイブリッドアプローチ（ボルツマン探索[13]）が採用される。
  - 全部が確率的推薦ではなく、上位K'個は決定的推薦ってことか...!:thinking:

## 学習中にユーザの探索を増やすための3つの簡単な方法

### 方法1 エントロピー正則化

- 学習時の最適化関数に、方策の行動確率分布 $\pi_{\theta}$ のエントロピーを高くなるように強制する、エントロピー正則化項を追加する。

### 方法2　reward shaping(即時の報酬関数rの変形？)

- reward shapingとは?
  - MDPが提供する報酬以外の報酬を変換したり追加したりすること。
    - (要するに即時報酬関数rの定義を変更することっぽい...!:thinking:)
  - RL分野において、アルゴリズム設計者が望む方策を生成するために、学習を導くのに非常に効果的らしい。
- RLの探索に関する既存手法は、大きく2つのカテゴリーに分けられる:
  - 1. **state-actionペアの価値関数(=reward function r?)の不確実性**を定量化し、エージェントが最も不確実性の高い領域の探索を指示できるようにするアプローチ。
  - 2. **好奇心(curiosity)や内発的動機づけ(interinsic motivation)という定性的な概念**を用いて、エージェントが環境を探索し、後で役に立つかもしれないスキルを学ぶように促すアプローチ。
- ただどちらのカテゴリーの手法も共通したアプローチをとる: 
  - 内発的報酬(intrinsic reward)を、元々の報酬関数、言い換えれば外発的報酬(extrinsic reward)に追加するように、(即時)報酬関数を変換すること!
    - 内発的報酬 $r^{i}(s, a)$ は、エージェントが環境を探索するための動機づけを提供する。(上述した2つのカテゴリーの手法は、内発的報酬関数の定義が異なるだけっぽい...?:thinking:)

$$
r(s, a) = c \cdot r^{i}(s, a) + r^{e}(s, a)
\tag{5}
$$

- ここで、
  - ハイパーパラメータ $c$ は、内発的報酬の相対的な重要度合いを制御する。
- 本手法は、方策が環境(ユーザ)の未知のパターンを発見したときに、より多くの報酬を与えるという原理を採用してる。

### 方法3

## 推薦の品質を測定するための評価指標

### 指標に必要なアイテム属性を定義する

surprise因子を定義するために使用される、**2つのアイテム属性**を紹介:

- 1. **Topic cluster(トピッククラスタ)**
  - 各アイテムのトピッククラスタは、次の手順で生成される:
    - 1. アイテムの共起行列を作成する
      - **要素 (i, j) は、アイテム i と j が同じユーザによって連続してinteractされた回数**をカウントする。
      - "連続して"って条件は厳しくない?? まあこの条件は、プロダクトやusecaseに応じて柔軟に変更しても良さそう...!:thinking:
    - 2. 共起行列に対してMatrix Factorizationを適用し、各アイテムに1つの埋め込みを生成する。
    - 3. k-meansを使用して、学習された埋め込みをk=10K(=10,000)個のクラスタに割り当てる。
      - まあここのクラスタ数kも、プロダクトやusecaseに応じて柔軟に変更して良さそう...!:thinking:
    - 4. 各アイテムに最も近いクラスタを、そのアイテムのトピッククラスタとして割り当てる。
- 2. Content Provider(コンテンツ提供者)
  - コンテンツ提供者は、以下の理由から興味深いアイテム属性の1つになってる:
    - 1. 同じプロバイダによって作られたコンテンツには一貫性がみられることが多い.
      - ex. food blogerは、特定の料理について頻繁に書く傾向がある。
    - 2. 長期的なユーザ体験に影響を与える、コンテンツ提供者の多様性や新規性の重要性を理解したい。

### Accuracy metrics(精度指標)

- 推薦システムの最も重要な特性は、ユーザが消費したいと思うようなコンテンツをretrieveできること。
- 本論文では、推薦セット $A^{\pi_{\theta}}$ において、ユーザが消費したいと思うアイテムを識別する平均精度を測定するために、MAP@K=50 (mean average precision) を使用する。
  - オンラインテストで評価するならOKだけど、オフラインテストで評価するなら、データ収集方策を考慮する必要があるよね:thinking:

### Diversity metrics(多様性指標)





