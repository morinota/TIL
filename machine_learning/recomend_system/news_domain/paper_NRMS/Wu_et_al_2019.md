## link

https://aclanthology.org/D19-1671/

## title

Neural News Recommendation with Multi-Head Self-Attention

## abstract

News recommendation can help users find interested news and alleviate information overload. Precisely modeling news and users is critical for news recommendation, and capturing the contexts of words and news is important to learn news and user representations. In this paper, we propose a neural news recommendation approach with multi-head selfattention (NRMS). The core of our approach is a news encoder and a user encoder. In the news encoder, we use multi-head self-attentions to learn news representations from news titles by modeling the interactions between words. In the user encoder, we learn representations of users from their browsed news and use multihead self-attention to capture the relatedness between the news. Besides, we apply additive attention to learn more informative news and user representations by selecting important words and news. Experiments on a realworld dataset validate the effectiveness and efficiency of our approach.

# Introduction

Online news platforms such as Google News1 and MSN News2 have attracted many users to read news online (Das et al., 2007). Massive news articles are generated everyday and it is impossible for users to read all news to find their interested content (Phelan et al., 2011). Thus, personalized news recommendation is very important for online news platforms to target user interests and alleviate information overload (IJntema et al., 2010). Learning accurate representations of news and users are two core tasks in news recommendation (Okura et al., 2017). Several deep learning based methods have been proposed for these tasks (?Kumar et al., 2017; Khattar et al., 2018; Wu et al., 2019b,c,a; Zhu et al., 2019; An et al., 2019). For example, Okura et al. (2017) proposed to learn news representations from news bodies via auto-encoders, and learn representations of users from their browsed news via GRU. However, GRU is quite time-consuming, and their method cannot capture the contexts of words. Wang et al. (2018) proposed to learn news representations from news titles via a knowledge-aware convolutional neural network (CNN), and learn representations of users based on the similarities between candidate news and their browsed news. However, CNN cannot capture the long-distance contexts of words, and their method cannot model the relatedness between browsed news. Our work is motivated by several observations. First, the interactions between words in news title are important for understanding the news. For example, in Fig. 1, the word “Rockets” has strong relatedness with “Bulls”. Besides, a word may interact with multiple words, e.g., “Rockets” also has semantic interactions with “trade”. Second, different news articles browsed by the same user may also have relatedness. For example, in Fig. 1 the second news is related to the first and the third news. Third, different words may have different importance in representing news. In Fig. 1, the word “NBA” is more informative than “2018”. Besides, different news articles browsed by the same user may also have different importance in representing this user. For example, the first three news articles are more informative than the last one. In this paper, we propose a neural news recommendation approach with multi-head selfattention (NRMS). The core of our approach is a news encoder and a user encoder. In the news encoder, we learn news representations from news titles by using multi-head self-attention to model the interactions between words. In the user encoder, we learn representations of users from their browsing by using multi-head self-attention to capture their relatedness. Besides, we apply additive attentions to both news and user encoders to select important words and news to learn more informative news and user representations. Extensive experiments on a real-world dataset show that our approach can effectively and efficiently improve the performance of news recommendation.

# Our Approach

Our NRMS approach for news recommendation is shown in Fig. 2. It contains three modules, i.e., news encoder, user encoder and click predictor.

## News Encoder

The news encoder module is used to learn news representations from news titles. It contains three layers. The first one is word embedding, which is used to convert a news title from a sequence of words into a sequence of low-dimensional embedding vectors. Denote a news title with M words as [w1, w2, ..., wM]. Through this layer it is converted into a vector sequence [e1, e2, ..., eM]. The second layer is a word-level multi-head self-attention network (Vaswani et al., 2017; Wu et al., 2018). The interactions between words are important for learning news representations. For example, in the news title “Rockets Ends 2018 with a Win”, the interaction between “Rockets” and “Win” is useful for understanding this news, and such long-distance interactions usually cannot be captured by CNN. In addition, a word may interact with multiple words in the same news. For instance, in above example the word “Rockets” has interactions with both “Ends” and “Win”. Thus, we propose to use multi-head self-attention to learn contextual representations of words by capturing their interactions. The representation of the ith word learned by the kth attention head is computed as:

where Qw k and Vw k are the projection parameters in the kth self-attention head, and α k i,j indicates the relative importance of the interaction between the ith and jth words. The multi-head representation h w i of the ith word is the concatenation of the representations produced by h separate self-attention heads, i.e., h w i = [h w i,1 ; h w i,2 ; ...; h w i,h]. The third layer is an additive word attention network. Different words in the same news may have different importance in representing this news. For example, in the second news of Fig. 1, the word “NFL” is more informative than “Today” for understanding this news. Thus, we propose to use attention mechanism to select important words in news titles for learning more informative news representations. The attention weight α w i of the i-th word in a news title is computed as:

where Vw and vw are projection parameters, and qw is the query vector. The final representation of a news is the weighted summation of the contextual word representations, formulated as:

## User Encoder

The user encoder module is used to learn the representations of users from their browsed news. It contains two layers. The first one is a newslevel multi-head self-attention network. Usually, news articles browsed by the same user may have some relatedness. For example, in Fig. 1, the first two news articles are related. In addition, a news article may interact with multiple news articles browsed by the same user. Thus, we propose to apply multi-head self-attention to enhance the representations of news by capturing their interactions. The representation of the ith news learned by the kth attention head is formulated as follows:

where Qn k and Vn k are parameters of the kth news self-attention head, and β k i,j represents the relative importance of the interaction between the jth and the kth news. The multi-head representation of the ith news is the concatenation of the representations output by h separate self-attention heads, i.e., h n i = [h n i,1 ; h n i,2 ; ...; h n i,h]. The second layer is an additive news attention network. Different news may have different informativeness in representing users. For example, in Fig. 1 the first news is more informative than the fourth news in modeling user interest, since the latter one is usually browsed by massive users. Thus, we propose to apply the additive attention mechanism to select important news to learn more informative user representations. The attention weight of the ith news is computed as:

where Vn, vn and qn are parameters in the attention network, and N is the number of the browsed news. The final user representation is the weighted summation of the representations of the news browsed by this user, which is formulated as:

## Click Predictor

The click predictor module is used to predict the probability of a user clicking a candidate news. Denote the representation of a candidate news Dc as r c . Following (Okura et al., 2017), the click probability score yˆ is computed by the inner product of the user representation vector and the news representation vector, i.e., yˆ = u T r c . We also explored other kinds of scoring methods such as perception, but dot product shows the best performance and efficiency.

## Model Training

Motivated by (Huang et al., 2013), we use negative sampling techniques for model training. For each news browsed by a user (regarded as a positive sample), we randomly sample K news which are shown in the same impression but not clicked by the user (regarded as negative samples). We shuffle the orders of these news to avoid possible positional biases. Denote the click probability score of the positive and the K negative news as yˆ + and [ˆy − 1 , yˆ − 2 , ..., yˆ − K] respectively. These scores are normalized by the softmax function to compute the posterior click probability of a positive sample as follows:

$$
\tag{11}
$$

We re-formulate the news click probability prediction problem as a pseudo (K + 1)-way classification task, and the loss function for model training is the negative log-likelihood of all positive samples S, which is formulated as follows:

$$
\tag{12}
$$

# Experiments

## Datasets and Experimental Settings

We conducted experiments on a real-world news recommendation dataset collected from MSN News3 logs in one month (Dec. 13, 2018 to Jan. 12, 2019). The detailed statistics are shown in Table 1. The logs in the last week were used for test, and the rest were used for training. We randomly sampled 10% of training data for validation.

In our experiments, the word embeddings are 300-dimensional and initialized by the Glove embedding (Pennington et al., 2014). The selfattention networks have 16 heads, and the output of each head is 16-dimensional. The dimension of the additive attention query vectors is 200. Following (Wu et al., 2019b), the negative sampling ratio K is 4. Adam (Kingma and Ba, 2014) is used for model optimization. We apply 20% dropout to the word embeddings to mitigate overfitting. The batch size is 64. These hyperparameters are tuned on validation set. We conducted experiments on a machine with Xeon E5-2620 v4 CPUs and a GTX1080Ti GPU. We independently repeated each experiment 10 times and reported average results in terms of AUC, MRR, nDCG@5 and nDCG@10.

## Performance Evaluation

We evaluate the performance of our approach by comparing it with several baseline methods, including: (1) LibFM (Rendle, 2012), a matrix factorization based recommendation method; (2) DSSM (Huang et al., 2013), deep structured semantic model; (3) Wide&Deep (Cheng et al., 2016), a popular neural recommendation method; (4) DeepFM (Guo et al., 2017), another popular neural recommendation method; (5) DFM (Lian et al., 2018), deep fusion model for news recommendation; (6) DKN (Wang et al., 2018), deep knowledge-aware network for news recommendation; (7) Conv3D (Khattar et al., 2018), a neural news recommendation method with 3-D CNNs to learn user representations; (8) GRU (Okura et al., 2017), a neural news recommendation method using GRU to learn user representations; (9) NRMS, our approach. In methods (1) and (3-5), we use one-hot encoded user ID, news ID and the TF-IDF features extracted from news titles as the model input. In methods (6-9), we all use news titles for fair comparison. The results of these methods are summarized in Table 2. We have several observations from Table 2. First, neural recommendation methods such as DSSM and NRMS outperform traditional recommendation methods such as LibFM on news recommendation. This may be because neural networks can learn better representations of news and users than matrix factorization methods. Thus, it may be more appropriate to learn news and user representations via neural networks rather than craft them manually. Second, among the deep learning based methods, the methods which exploit the relatedness between news (e.g., Conv3D, GRU and NRMS) can outperform other methods. This may be because the news browsed by the same user usually have relatedness, and capturing the news relatedness is useful for understanding these news and modeling user interests. Third, our approach performs better than all baseline methods. This is because our approach can capture the interactions between both words and news via multi-head self-attention to enhance representation learning of news and users. Besides, our approach employs additive attention to select important words and news for learning informative news and user representations. These results validate the effectiveness of our approach. We also conducted experiments to compare the time efficiency of our approach with several popular news recommendation methods. The results are shown in Table 3. From the results, we find our approach has a smaller parameter size and a lower time complexity in learning news and user representations than existing news recommendation methods. In addition, different from DKN, our approach does not need to memorize the news browsing histories of users when computing the click probability scores. In addition, since our approach can be further accelerated by computing the hidden representations of different attention heads in parallel, our approach is more suitable for being deployed in large-scale news recommendation scenarios. These results validate the efficiency of our approach.

## Effectiveness of Attention Mechanism

Next we explore the effectiveness of attentions in our approach. First, we verify the word- and news-level attentions. The results are shown in Fig. 3(a). We find the word-level attention is very useful. This is because modeling the interactions between words and selecting important words can help learn informative news representations. Besides, the news-level attention is also useful. This is because capturing the relatedness of news and selecting important news can benefit the learning of user representations. Moreover, combining both word- and news-level attentions can further improve the performance of our approach. We also study the influence of additive and selfattentions on our approach. The results are shown in Fig. 3(b). From these results, we find the selfattentions are very useful. This is because the interactions between words and news are important for understanding news and modeling users. In addition, the additive attentions are also helpful. This is because different words and news may usually have different importance in representing news and users. Thus, selecting important words and news can help learn more informative news and user representations. Combining both additive and self-attention can further improve our approach. Thus, these results validate the effectiveness of the attention mechanism in our approach.

# Conclusion and Future Work

In this paper we propose a neural news recommendation approach with multi-head self-attention. The core of our approach is a news encoder and a user encoder. In both encoders we apply multihead self-attentions to learn contextual word and news representations by modeling the interactions between words and news. In addition, we use additive attentions to select important words and news to learn more informative news and user representations. Extensive experiments validate the effectiveness and efficiency of our approach. In our future work, we will try to improve our approach in the following potential directions. First, in our framework we do not consider the positional information of words and news, but they may be useful for learning more accurate news and user representations. We will explore position encoding techniques to incorporate the word position and the time-stamps of news clicks to further enhance our approach. Second, we will explore how to effectively incorporate multiple kinds of news information in our framework, especially long sequences such as news body, which may challenge the efficiency of typical self-attention networks.
