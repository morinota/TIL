## link

- [pdf](https://web-ainf.aau.at/pub/jannach/files/Journal_IPM_2018.pdf)
- 
## title

News Recommender Systems - Survey and Roads Ahead

## abstract

More and more people read the news online, e.g., by visiting the websites of their favorite newspapers or by navigating the sites of news aggregators. However, the abundance of news information that is published online every day through different channels can make it challenging for readers to locate the content they are interested in. The goal of News Recommender Systems (NRS) is to make reading suggestions to users in a personalized way. Due to their practical relevance, a variety of technical approaches to build such systems have been proposed over the last two decades. In this work, we review the state-of-the-art of designing and evaluating news recommender systems over the last ten years. One main goal of the work is to analyze which particular challenges of news recommendation (e.g., short item life times and recency aspects) have been well explored and which areas still require more work. Furthermore, in contrast to previous surveys, the paper specifically discusses methodological questions and today’s academic practice of evaluating and comparing different algorithmic news recommendation approaches based on accuracy measures. 

# Introduction

The newspaper industry has experienced a substantial transformation during the last twenty years. Today, readers can find various sources of news online, e.g., on the web presences of traditional newspaper companies, on digitalonly news sites, or on news aggregation platforms provided, for example, by Google2 or Yahoo!3 . Additionally, the digital form of information delivery allows publishers to distribute new or updated content in real-time, leading to an increased speed of publication. The availability of the various (often free) online news sources has led to a constant increase of users of such platforms [138]. At the same time, however, the abundance of available information and the constant update cycle make it increasingly challenging for readers to keep track of news that are most relevant to them. 

Recommender Systems have shown to be a valuable tool to help users in such situations of information overload [84]. The main tasks of such systems are typically to filter incoming streams of information according to the users’ preferences or to point them to additional items of interest in the context of a given object. During the past decades, significant advances in recommendation technology have been made. Recommenders have been successfully applied in a variety of domains , and the recommendable objects include movies, books, travel and tourism services, research articles, search queries, and many more [9, 15, 144]. 

News recommendation in general represents another application domain in which several of the known techniques for building automated recommendations can be applied. In fact, there have been a number of reported instances where such systems are being used in live environments to deliver news recommendations on popular websites (see, e.g., [32, 39, 93, 115, 118]). However, news recommendation problems often have certain characteristics that are either not present at all or that are at least less pronounced in other domains. For example, in contrast to other domains like movie recommendation, the relevance of news items can change very rapidly (i.e., decline or re-increase due to recent real-life events) [5, 141] and the “item churn” is generally high [39]. In fact, since news web sites are often continuously updated, some articles can be superseded by a “breaking news” article on the same topic several times during a single day, which might require constant updates to the recommendation models. Another typical challenge in the news domain is that a user’s interest can dynamically change, depending on different contextual factors like the time of the day, the features of the user’s device (e.g., mobile phone vs. desktop), or the user’s current location [20, 90, 125].

Due to the high practical relevance of the news recommendation problem and its specific challenges, a considerable number of research works has been published on this topic in particular within the last ten years. Many of these works propose novel algorithmic approaches to generate personalized recommendations. These algorithms are typically evaluated using offline experimental designs and existing log data. In recent years, however, recommender systems research in a variety of domains has shown that it is also important to evaluate recommendations in a more user- or system utility-oriented way [48, 78, 83, 96, 151]. These developments have also led to a number of alternative forms of assessing the quality of the recommendations, e.g., in terms of diversity [80, 161, 172]. 

With this work, we consider these recent developments and provide an overview of what has been achieved in the last ten years with respect to the different challenges in the news recommendation domain. Based in this analysis, we identify existing research gaps and potential areas for future research. In contrast to some previous overview works like [14, 109, 141], we however focus not only on the underlying algorithmic approaches used to create the recommendations, but also on questions related to the empirical evaluation and the user perception of such systems. To provide a starting point for future research, we finally also report insights from a set of experiments, which highlight the importance of short-term model updates when standard accuracy measures are applied in the evaluation. 

The paper is organized as follows. Next, in Section 2, we briefly summarize how we selected existing research works for consideration in our survey. Afterwards, in Section 3, we discuss the various challenges of news recommendation in more detail and review existing algorithmic approaches to deal with these challenges. Then, in Section 4, we provide a survey of today’s academic practice of benchmarking and evaluating different technical approaches. Our paper ends with an outlook on possible directions for future research in Section 5.

# Survey Method and Research Scope

To develop the survey part of the paper, we investigated, in a structured way, more than 140 research articles that appeared between 2005 and 2016 in relevant computer science and information systems publication outlets. Figure 1 shows how many NRS papers were considered in this time frame per year. The trend of the graph indicates that research interest in the topic of news recommendation has grown steadily over the years to the point that news has become an important sub-topic in the RS research field.

Figure 1: Number of NRS papers per year. As the survey only covered 2016 until August, this year was excluded.


Regarding the scope of our review, we focus on papers that describe approaches for “classical” news recommendation scenarios, e.g., on news aggregation sites. Similar technical approaches can in many cases be applied for “news feed filtering” problems, for example on Facebook or Twitter, where the content of social news feeds are personalized to the users. These scenarios feature many of the same challenges, e.g., an emphasis on freshness and an overwhelming number of new items every day. However, news feed filtering on social networks has some unique aspects that are different from news recommendation. For example, the recommendable items are not necessarily news items but can be some currently trending viral videos that are not related to a recent event in the real world. Also, on news sites, information about a visitor’s social network friends and general interest profile is in many cases non-existent. However, this information oftentimes plays a central role in news feed personalization. 

The identification of papers for our survey was done according to the following strategy. We first scanned the proceedings and volumes of a set of relevant conference series (RecSys, WWW, SIGIR, and KDD) and journals (Expert Systems with Applications and Knowledge-Based Systems) for articles that fall into the above-described scope. Additionally, we used the keywords “news recommendation” and “news recommender” to search for papers in Google Scholar. Using the resulting set of articles as a starting point, we followed the references of the retrieved articles to find additional papers on the topic. The survey methodology is therefore not that of a traditional “systematic review” in the sense of, e.g., [94], where pre-defined search queries and inclusion and exclusion criteria are used. Nonetheless, as we followed a defined search strategy, had a defined research scope, and used pre-structured forms to classify the papers along different dimensions, we are confident that the risk of the introduction of a researcher bias into the survey is low. 

A few other survey works on the topic of news recommendation already exist. When looking at these existing works, we find that some challenges of news recommendation that we identify in our work were also summarized by Ozg ¨ obek et al. [141]. The authors of this work however only consider a small number of (twelve) papers and do not ¨ consider evaluation aspects in their survey. Li et al. [109] also discuss challenges of news recommendation, but do not discuss existing approaches to deal with these issues in depth. The survey paper by Borges and Lorena [14], finally, discusses news recommendation mostly from a quite general perspective and focuses the analysis on different aspects of six specific papers.

# News Recommendation – Challenges and Algorithmic Approaches

Before we discuss the challenges of the domain in more detail along with algorithmic approaches that were proposed in the literature, we will briefly review which types of algorithms are generally used for the news recommendation problem.

## Recommendation Paradigms in News Recommendation

Recommender systems are often classified into four main categories [84, 108]: collaborative filtering (CF), contentbased filtering (CB), knowledge-based techniques (KB), and hybrid approaches. In academic settings, collaborative filtering is the most common approach in the recommender systems literature according to the survey presented in [85], while all other approaches are much less frequently employed. The general dominance of collaborative filtering is in some sense not surprising because this method, whose recommendations are based on the “wisdom of the crowd”, is domain-independent and does not require detailed (and domain-specific) information about the recommendable items. Furthermore, a number of public benchmark data sets, e.g., from MovieLens4 , are available, which has been a driving factor for the academic community.

In the news recommendation domain, however, things are different. An analysis of the 112 (of 144) papers in our survey that propose one or more recommendation algorithms reveals the following.5 Content-based filtering, which is, roughly speaking, based on analyzing a reader’s past documents of interest and recommending “more of the same”, was used as an underlying paradigm in 59 of the analyzed papers. Approaches, like the Google News personalization system [39], that rely on collaborative filtering and on interest patterns in a community without considering the content of a news article are only proposed in 19 of the 112 papers. However, 45 of the papers use a hybrid approach and combine content-based filtering and collaborative filtering. This means that although collaborative filtering alone is not the method of choice for most researchers, relying on patterns in the community behavior in addition to contentbased user profiles is often considered to be promising. Knowledge-based techniques, which are based on explicit domain knowledge to map user preferences to article features, were not applied in any of the analyzed papers, neither in isolation nor in combination with other techniques. This is, however, not very surprising, since such approaches are generally only employed in high-involvement product domains with a long life cycle. 

Since the recommendable objects in the news domain are text documents, which can be automatically analyzed with standard techniques from Information Retrieval (IR), it is in fact not too surprising that many researchers rely on content-based techniques. However, in general, content-based techniques are considered not to be very accurate in offline experiments when using IR measures like precision and recall [80]. As in other domains, the main dilemma of academic research – and to some extent also for recommendation service providers – is that it is not always clear to what extent high accuracy in offline experiments translates into online success [60]. Only very few studies are available that compare the offline and online performance of recommendation algorithms, e.g., [10] in the field of research paper recommendation. In the news domain, a comparative offline/online experiment was recently conducted by Garcin et al. [55], who also found that offline performance is not necessarily a predictor of online success. In their case, a popularity-based method performed best offline, but, in the end, showed the worst results in an online test, placing far behind a more sophisticated algorithm. 

As we will discuss later, the effectiveness of pure collaborative filtering methods might be overestimated in offline experiments, which calls for the design of novel hybrid approaches that combine multiple sources of preference signals and which are able to balance prediction accuracy with other quality factors like novelty or diversity.

## Challenges of User Modeling for News Recommendation

To be able to personalize the reading suggestions, recommender systems have to create and maintain user profiles that capture each user’s reading preferences over time. How these user profiles are built and which information is collected and used is usually related to the chosen recommendation paradigm. Similar to other application domains of recommender systems, one can in general try to stimulate users to provide explicit preference information, e.g., in the form of “like” or “dislike” statements, or monitor and interpret the user’s past behavior (implicit feedback) over time. Typical implicit feedback signals in the news domain include reading an article, sharing it, printing it, or commenting on it. On a more fine-grained level, it is also possible to record dwelling times or mouse movements as interest indicators. Generally, different types of information can be used to construct the user profiles. Many papers rely mainly on the content of the news articles inspected by a user to infer the interest profiles. Some works, in addition, consider features of the users and the community they belong to to create the profiles. Other approaches finally use various types of information in parallel. An example of a system that relies mostly on content information is the Athena news recommendation system [74], where the user profile is based on the keywords of the articles that were read by a user. New or not yet viewed articles are then ranked according to the similarity of their content with the aggregated user profile. A similar approach was used by Rao et al. [153], who employ user profiles that have a “bag-of-concepts” format with DBPedia as a knowledge base in the background. Standard distance measures like cosine similarity or the Jaccard coefficient can then be used to assess the relevance of a news article for a given profile. Similar to these approaches, Chu and Park [31] rely on the content of the read articles for profile construction, and additionally consider demographic aspects to assess the relevance of new articles for a user. Another work that considers demographic information as a key factor for user profiling is [103]. In this approach, the users were first segmented according to their demographics and article access patterns. The relevance of a new article for a given user was then calculated based on various factors, including the estimated interest of the user’s segment in the topic of the article or the number of users in the segment who have already read the article. Instead of clustering the users, Li et al. [112] applied a graph-based model based on which the logical relationship and similarities between a newly posted article and user comments about the article on a social media site were calculated. This topicbased model was then used to retrieve other relevant news articles. The basis for the user modeling approach in [5] is a weighted term vector, which is generated by considering the topics of the articles that were read by a user. Different from other approaches, however, the authors propose to maintain a short-term and a long-term model, where the short-term model only considers the 20 most recently read articles of each user. Besides being able to capture long-term and short-term interests for recommending, the topicbased approach of Ahn et al. [5] was designed in a way that users can be put in control of their recommendations. For each recommended article, the relevant topics were displayed and users could then update their profile accordingly when they did not agree with the system’s assumptions about the preferences. An example of a work that solely relies on click behavior and does not consider article content is presented by Das et al. [39], who describe the inner workings of the Google News Personalization system at that time. Similar to Lee and Park [103], they clustered the user community, this time however based mainly on the past article reading behavior. To predict the relevance of an item, they then used both a long-term collaborative filtering model and a short-term model based on article co-visitations. In a later paper, Liu et al. [115] present an alternative approach that was also implemented for the Google News service. In contrast to the previous approach, their newer model takes both content information as well as the reading patterns in the community into account. Their proposed Bayesian framework tries to remove the community bias from the user’s consumption behavior in each topic to extract the user’s “genuine” long-term interests. Furthermore, it is also designed to take short-term changes in the user’s interests into account. To predict the relevance of new articles, the proposed approach finally also considers the user’s location and the reading behavior of the user community in the past hour. Generally, considering long-term and short-term preferences and balancing their importance represent an important problem in news recommendation [72, 110]; see also the discussion later in Section 3.4 on recency aspects. One of the key design questions is whether two separate models should be maintained (as, e.g., in [6]) or an integrated model with a time decay factor (as in [111]). A comparison of different strategies of considering the long-term and short-term preferences was done by Huy et al. [72], who observed that a combination of both models in a hybrid approach led to the best results.


## Cold-Start Problems and Data Sparsity Issues in News Recommendation

Cold-start situations are a common problem in many application domains of recommender systems. User coldstart refers to situations where little is known about the preferences of the user for which a recommendation is requested; item cold-start means that a new recommendable item (i.e., an article) was added that was not viewed or rated by many people yet. The problem of general data sparsity is often connected to this problem and in particular relevant for collaborative filtering approaches.

### The Permanent Cold-Start Problem

In principle, many of the technical and domain-independent approaches that were proposed over the years to deal with cold-start situations can be applied in the news recommendation context. One general approach is to try to incorporate additional information (e.g., about the user’s context) into the recommendation process when little is known about the current user. In the news recommendation domain, for example, works exist that consider the user’s location [49, 132, 166], the time of the day [40, 49, 132], or demographic information [103]. Other examples of contextual information explored in the literature include the website (or domain) that the user has visited before landing on the news page or the type of the device used to access the news [40, 168]. Instead of considering only the limited information of the user’s history, Lin et al. [113] propose to construct an implicit social network of other users with similar reading behavior. The authors then suggest to determine a set of “experts” whose opinion is used to make recommendations for users for which only a short reading history is available. To identify these experts, they considered other users who have read the same articles in a given time frame, and they subsequently involved these users more heavily in the factorization process if they have a high probability of being a good proxy for a cold-start user. Along the same lines, Zheng et al. [186] propose to enrich the profiles of cold-start users with the profiles of a group of similar other users based on the general news topics they seem to be interested in. An alternative to incorporating additional aspects related to the user is to consider certain features of the news articles themselves when assessing their relevance for a cold-start user, e.g., the freshness of the article or its general popularity. Also, one can default to a non-personalized strategy in case of too little information. The “YourNews” system [5], for example, first shows a list of recently published articles and starts the personalization process after the first article has been read. In another approach, Montes-Garc´ıa et al. [132] consider the geographical proximity of the news event to the user and the credibility of a news source as factors that can determine the relevance of an item to a cold-start user. In their case, the credibility of the news source was determined manually by a team of journalists. The context-tree approach by Garcin et al. [53] also takes the recency of the news items into account, but in addition factors the item’s popularity into the recommendation process. By design, their method is also suitable for one-time or first-time users, which are not uncommon for news platforms. Finally, Tavakolifard et al. [166] propose to use external data sources – in their case Twitter – to estimate an item’s current popularity more precisely and correspondingly obtain a better picture of its potential relevance for the current user. For the item cold-start problem, adopting a content-based recommendation strategy already solves a major part of the problem. In content-based approaches, items are generally recommended based on the past content-wise preferences of individual users, which are usually determined by their past navigation or item rating behavior. Therefore, the required information about a news article can be extracted easily, e.g., from its keywords, making it instantly recommendable without the need for a detailed click history. Different alternative technical approaches are possible to infer the user preferences. Li et al. [108], for example, propose a hybrid approach to construct the user profiles which considers factors such as similarities between access patterns for different news items. In addition, their approach utilizes item properties that are specific to news, like the news content itself, as well as the user’s preferences for named entities appearing in the news item.

### Data Sparsity Issues

When applying collaborative filtering approaches, the recommendation problem is often considered as a matrix completion task, with users and items as rows and columns, respectively, and matrix entries representing the relevance of items for users. In many domains, including the news domain, this matrix can be extremely sparse and only a tiny fraction of the matrix entries are known. Finding a sufficient number of similar users, for example, in a nearestneighbors collaborative filtering approach, can therefore be very challenging [143]. Correspondingly, a number of approaches have been proposed in the context of news recommender systems to deal with the sparsity problem, e.g., by incorporating additional sources of knowledge, predominantly for collaborative filtering, but also for other approaches. Mannens et al. [127], for example, propose a recommendation method that mitigates sparsity by complementing binary consumption values in the matrix with “potential consumption” values between 0 and 1 based on a collaborative filtering algorithm, which is then re-executed until the matrix is dense enough. Furthermore, to reduce the uncertainty introduced by this probabilistic approach, they also suggest to post-filter the news articles based on Linked Data sources. In contrast, Morales et al. [133] propose to use information from social network sites to deal with the sparsity problem. In their work, they analyzed the posts of users and their social friends on Twitter and generated a mapping between tweets and news articles based on the entities in the articles. Leveraging information about the relationships between users is also the idea of the work presented in [114] and [113], which has already been mentioned in Section 3.3.1. In contrast to other approaches, the authors create an implicit virtual network, based on the users’ access logs. In their approach, which combines content information and collaborative filtering, the idea is to identify (two) expert users who are considered as influencers for users without a sufficiently large consumption history. These suspected influence relationships are then used to make the rating matrix denser. Instead of using social network information, Lu et al. [120] propose to combine different forms of user behavior, context, and content features of the news items to find similar users in sparse-data situations. Specifically, they consider the user’s browsing, commenting, and publishing behavior as implicit feedback signals. An even richer set of preference signals was combined in the News@hand system presented in [11, 24, 25]. The authors use ontologies and Semantic Web technology to represent the knowledge about users and items and utilize a variety of preference indicators, such as page clicks, rating, and comments. To deal with the sparsity problem, they then propose a graph-based method to spread the information via the semantic relations of the network of concepts. Li et al. [105] address the data sparsity issue by framing the recommendation task as a so called contextual bandit problem. In such a problem formulation [101], every recommendation task is represented as a choice between k possible “arms” of a multi-armed bandit. The recommendation strategy makes this choice only based on a context vector, for which Li et al. [105] chose a compact representation of users and articles (e.g., based on demographic information, location, and news categories). Afterwards, a reward is observed based on the user reaction (click). The advantage of contextual bandit approaches is that they can be used to balance exploration of the item space and exploitation of previous knowledge even in sparse data situations. Generally, one of several well-studied optimization strategies can be used to optimize the expected reward, i.e., in this case, the overall number of clicks. To evaluate the usefulness of their approach, the authors tested it on data sets with various sparsity levels. Generally, when considering recommendation as a matrix completion problem, different forms of rating imputation and other forms of advanced statistical methods can be applied. Agarwal et al. [3], for example, analyze the correlations between different post-read actions (such as sharing, commenting, printing, or emailing article links) and are thus able to interchange the information across actions types to derive additional data points to feed into the recommendation process. Overall, a number of techniques for dealing with the data sparsity issue in the news domain were presented over the years. Since data sparsity is such a central problem in news recommendation, many papers evaluate their algorithmic proposals – even when they are not explicitly focusing on data augmentation – on data sets of different densities to analyze their behavior in such situations.


## The Importance of Considering the Recency of News Articles

Whatever algorithm is used for making personalized reading suggestions, the recency or “freshness” of an article will impact its relevance for the readers in many cases. Clearly, there are recommendation scenarios where a reader might be interested in older news as well, e.g., when investigating the development of a story over time or when looking for articles related to the currently read one. On typical landing pages of news sites, however, the recency of the articles is typically one important ranking criterion besides other factors such as the estimated general attractiveness of an article. Technically, the recency of an article can be considered in the recommendation process at three stages. We can filter assumedly outdated news before computing relevance predictions or an item ranking (pre-filtering); we can incorporate the recency factor into the algorithms themselves (recency modeling); or we can filter or downrank articles after the main ranking process (post-filtering). However, one key design question in this context is how we balance the possible trade-off between article freshness and assumed relevance for the individual user. Examples of works that apply a pre-filtering strategy are [39, 41], and [159]. In one of the news recommendation approaches by Desarkar and Shinde [41], for example, the authors selected the 100 most recent articles from the user’s preferred publisher websites and considered only these articles in the subsequent ranking process. Das et al. [39], on the other hand, considered recency as one of several factors for item pre-filtering; other factors being, for example, the user’s language preferences or user-defined topics of interest. In recency modeling approaches, the freshness of an article is considered simultaneously in conjunction with different other features of the item or the user. In principle, such an approach has the advantage that the different factors can more easily be balanced in an integrated way compared to when assumedly outdated items are filtered in a separate process and with separate heuristics. Examples of recency modeling approaches include [12, 148], and [182]. In the work by Pon et al. [148], freshness is considered along with a “multiple topic tracking” technique for users with several interests to train a news classifier. To this end, users are represented by a number of vectors instead of just a single average Term Frequency-Inverse Document Frequency (TF-IDF) vector, which is commonly used to weight the importance of keywords. In this way, short-term topic interest can be accounted for by giving preference to user profile vectors that have the highest similarity to the recently consumed news articles. Yeung and Yang [182] propose an approach that relies on a variety of features to predict the relevance of news articles (e.g., user features, item features, usage patterns, and context factors). The freshness of an article is considered as one of the item features and more recent articles receive a higher weight in the ranking process. Similarly, Bielikova et al. [12] integrated the ´ publication time of an article as one of the factors in their tree-based model for computing the relevance of the articles. Post-filtering based on article recency was applied, for example, in [108, 186], and [129]. In the method by Zheng et al. [186], news articles are first ranked according to their estimated relevance based on the interest groups the user belongs to. In a second step, the ranking is adjusted based on item freshness and popularity. A similar two-stage approach was adopted by Li et al. [108], which was already explained in detail in Section 3.3.1. A decay-based method has been proposed by Medo et al. [129]. In their work, they tested different score decay strategies for old news (no decay, medium decay, strong decay) and evaluated the impact of these strategies on the performance of the news recommender system. Generally, to what extent older articles should be filtered can depend on different factors. First, in case of cold-start users with a very short reading history, we can resort to focusing the recommendations on articles that were recently added and which are relatively popular [49, 53, 166]. Such non-personalized recommendations might however lead to lower user satisfaction. Filtering out older articles can also be problematic if there are many readers who are interested in investigating developing stories and in reading older news articles on the same topic. Finally, whether or not it is good to focus strongly on the latest news in the recommendations can even depend on the individual user and his or her current context and goals. One might be interested to catch up with the latest news in the morning during the week, but be more interested in longer (and perhaps older) stories when visiting the news site on the weekend. In any case, an open question seems to be how to select a threshold to filter old news or how to set a decay factor. When reviewing the literature, a number of different strategies and threshold values are used and it is not clear if an approach that is working well in one setting will also be suitable for another use case. In many cases, details about the specific settings are not reported in the research papers, which makes these works hard to reproduce. Table 1 gives an overview of approaches that consider item recency as a relevant factor. In the last two columns, we show which thresholds were applied in case they were reported and which time windows were used to decrease the relevance of an article based on its recency. We can observe significant differences. Some works only focus on articles of the last six hours [49], whereas others consider articles of the last twenty days [136]. Some authors add a decay penalty for every second since the article was published [38], whereas others do this only for each hour [183]. In many cases, however, this detailed information is unfortunately not reported at all.

Table 1: Overview of news recommendation approaches that consider recency as a factor. The first column categorizes the papers according to their recency strategy. The next two columns list the paper’s author(s) and its publication year. The fourth column shows the employed recommendation strategy (CB = Content-based filtering, CF = Collaborative filtering, H = Hybrid). The fifth column lists the data sets used in the paper. The last two columns show (a) how old articles can be at most to be considered recent and (b) the granularity of applying a decay factor, e.g., for every hour after the initial publication.

##  Considering Quality Factors Beyond Prediction Accuracy

The main optimization goal of researchers in academia is to accurately predict the relevance of a news item for a user. Most commonly, classification or ranking accuracy measures are used for that purpose (see Section 4 for more details). However, in practice, predicting the relevance of an item is in many cases not enough. If, for example, a user is interested in politics and has shown strong interest in articles about an ongoing presidential election in the past, recommending more articles about this topic is probably a good choice. However, recommending solely articles about the election, or solely about politics, might be too monotonous for users and would probably not lead to high user engagement in the future. In case of a news aggregation site, it is furthermore important that the recommended news are not too similar to each other. Presenting three articles from three different sources about, e.g., the same plane accident might be of little value for users. Therefore, one additional challenge, besides accurately predicting whether an article is relevant for a user or not, is to take additional quality factors into account. In the recommender systems literature, diversity, novelty, and serendipity are often considered as such quality factors that have to be balanced with prediction accuracy (see, e.g., [29]).

### Diversity

Users of news recommender systems can be interested in a variety of topics. A recommender system should therefore be able to address these varied tastes and generate diversified recommendation lists [171]. Empirical research suggests that increasing the diversity of the recommendations can lead to a better quality perception by users [48, 151, 188]. An example for a work that considers diversity in the news recommendation domain is presented in [108], which we already mentioned earlier in the context of recency filtering and cold-start techniques. In the proposed two-stage approach, articles were clustered in the first phase, and the recommendations were personalized and adapted, e.g., with respect to recency, in the second phase. In both stages, the authors propose to add some level of diversification. According to the authors, the first-level clustering strategy implicitly introduces some level of topic diversity, while the second stage implements a greedy approach that explicitly minimizes the similarity among items in a recommendation set. In the latter case, the diversity was measured by computing the average pairwise (dis-)similarity of documents, and an experimental evaluation showed that their approach was beneficial in terms of both accuracy and diversity when compared to the works presented in [31, 39, 105, 115]. Later on, Li and Li [107] followed an alternative and more advanced approach to diversify news recommendations in the context of a learning-to-rank optimization scheme. In their work, they propose a framework that relies on a hypergraph to model the relations between different “media objects”, which included users, news articles, topics, and named entities. Using the same experimental setup as in their previous work [108], the authors were able to demonstrate further improvements both in terms of accuracy and diversity. Typically, there is a trade-off between accuracy, e.g., in terms of classification accuracy, and diversity, e.g., in terms of intra-list similarity [188]. To address this issue, Desarkar and Shinde [41] analyzed two possible approaches of balancing these goals in the news domain and tested them for a special privacy-targeted scenario where no past interaction history is available for the users and the relevance of each candidate item is consequently estimated only by its popularity. In their case, diversification is described as a bi-criteria optimization problem where both the relevance and the dissimilarity between the news objects should be high. Outside the domain of news recommendation, a number of works have been published over the past ten years on the topic of diversifying recommendations [29]. Besides the question of how to balance diversity with accuracy, researchers investigated to what extent the level of diversification should be determined by the preferences of the individual user [162] or how diversity preferences can depend on time aspects [102]. In the news domain, these aspects have not been investigated much yet, even though some of these aspects – like the consideration of the time of the day when recommending [40] – might have an effect on the user’s short-term diversity preferences.

### Novelty

Novelty, as a quality criterion for recommendations, was defined in terms of the non-obviousness of the item suggestions by Herlocker et al. [68]. Informally, novel items are, according to their definition, those that the user has not seen yet, but which are relevant to him or her. An alternative definition was provided by Vargas and Castells [172], who defined novelty as the inverse general popularity of an item. The assumption here is that less popular items are more likely to be unknown to users and recommending long-tail items will, in general, lead to higher novelty levels. Several ways of numerically quantifying the degree of novelty are possible, including alternative ways of considering popularity information [187] or the distance of a recommendable item to the user’s profile [137, 154]. Like diversity and accuracy, novelty and accuracy aspects can represent a trade-off. In their work on news recommendation, Garcin et al. [53] relied on the definition by Herlocker et al. [68] and tested different configurations of weighting the two factors to achieve both high accuracy and high novelty. Rao et al. [154] followed an approach where the novelty of an item is defined by its distance to the user’s profile. Specifically, they compared the distance of the concepts appearing in the user profile and the news article, where the taxonomy was constructed from encyclopedic knowledge. The evaluation results of their user study show that this approach cannot only be used to increase accuracy but also reduce the perceived redundancy among news articles. Generally, as for diversity, the “optimal” degree of novelty can depend on the user’s current situation and context, as discussed recently by Kapoor et al. [89] in the context of the music domain. Determining the right balance of recommending (a) things that are most likely relevant for the user and (b) recommending items that help the user discover new things remains challenging. Limited research on these aspects exists in the news recommendation domain. As discussed in Section 3.3.1, the news domain has the special characteristic that most of the recommendable items are usually new and unknown to the readers. In terms of some of the definitions from above, all recommendations are novel. Therefore, novelty can probably not be determined on the basis of the individual item itself, but rather in terms of the content the news item is about, e.g., based on whether or not the user already knows about the real-world event the news item is reporting on.

### Serendipity

Serendipity is sometimes also mentioned as a desirable quality characteristic of recommendations. Herlocker et al. [68] describe serendipitous recommendations as those that help users find surprising and interesting items that they might not have noticed otherwise. Ge et al. [57], based on an earlier approach by Murakami et al. [135], defined serendipitous recommendations as items that are not only novel but also positively surprising for the user and proposed a generic metric based on the concepts of unexpectedness and usefulness. Zhang et al. [185] finally proposed a related conceptualization in the context of a music recommendation application in which the level of serendipity is determined based on the distance between the recommendations and the user’s expected content. To our knowledge, only limited research on serendipity aspects of the news recommendation problem exist so far. In principle, one can apply techniques based on Latent Dirichlet Allocation (LDA) models – as proposed by Zhang et al. [185] for music recommendation – to the news domain and focus on recommending news items that are content-wise dissimilar to the user’s typical latent topics. However, whether or not this will lead to higher levels of user satisfaction in this domain, still has to be explored. Overall, one key question today is how to quantify the level of serendipity of a recommendation list and how such serendipity measures would be related to existing novelty measures. Again, adding serendipitous items to the recommendations comes with the risk of decreasing the average relevance of the recommendation lists. Furthermore, Kotkov et al. [99] recently discussed the problem that today in many (general RS) research data sets used for offline experimentation not enough information about the user’s context (e.g., time, location, or mood) is available. The user’s context can however have a significant impact on the user’s perception of serendipitous item recommendations. In our view, more research is therefore required also outside the field of news recommendations to better understand the concept of serendipity in recommender systems and how to measure it.

## Scalability Issues

Scalability is the last of the main challenges of news recommendation that we discuss in this section. Large news websites like Google News can have hundreds of millions of users per month. At the same time, the number of articles that can be searched and recommended can be huge as well, with thousands of new items appearing every day.6 A common approach to deal with these huge amounts of data is to apply clustering techniques of different types. Clustering can in many cases help to speed up the computations; it can however also lead to compromises with respect to the accuracy of the relevance predictions. In addition to such algorithmic approaches that, e.g., allow us to do the computations on a more coarse-grained level, researchers often propose to rely on distributed computing mechanisms and to execute the calculations on multiple servers in parallel. The application of various forms of clustering techniques for different aspects of the news recommendation problem has been proposed, e.g., in [39, 120, 129], and [174]. In some of these cases, the main goal is to search for similar users (neighbors) only within a smaller cluster of users without the need to scan the entire database. In the case of the Google News system, as described in [39], the authors use a combination of (MinHash) clustering and distributed computing based on the MapReduce framework to make the approach scalable. Lu et al. [120] also combine MapReduce and a clustering technique to increase the scalability. Specifically, they use a “Jaccard–Kmeans” based clustering technique where the similarity of users is computed in multiple dimensions, e.g., based on their past behavior and the content the users preferred in the past. Other examples of works on news recommendation that rely on distributed computing include [156] and [159]. Besides clustering the users, one can also try to cluster the available news articles and group them based on their features and the user’s preferences. In [108], for example, the authors propose a two-stage recommendation approach in which they apply different clustering techniques to create a hierarchy of news clusters. This hierarchy can then be efficiently traversed from top to bottom to find a set of news articles similar to the user’s reading interest. Medo et al. [129] adopt a different approach to achieve scalability in the context of a news platform where users can post news items. In their method, they construct a local neighborhood network of users based on the similarity of their rating behavior (likes/dislikes). In contrast to traditional k-nearest-neighbor (kNN) approaches, they do not store the whole neighborhood, but only the most similar neighbors for each user. Once a user posts a new article on the system, it is propagated along the directed edges of the graph to their “followers”, who can spread it further by liking it. The final ranking of the news items is then determined by a score, which is based on the like/dislike statements of the target user’s neighbors. In a follow-up work, Wei et al. [174], expand the method by adapting it to take the users’ reading patterns into account. Additionally, they propose different stochastic solutions to the problem of updating the neighborhood that improve the scalability of their approach further, e.g., by randomly selecting new neighbors. Many of above-mentioned approaches describe how to achieve scalability on a theoretical level or give an account of how real-world systems implement large-scale operations, as in the case of [39]. What sets the news domain apart from other domains that are investigated in recommender systems research, however, are the live news recommendation challenges organized by Plista, which we will explain in detail in Section 4.2. Such challenges give researchers the opportunity to test the scalability of their approaches “in the wild”. An example is the work by Doychev et al. [45], whose content-based recommender system addresses the requirement of the Plista challenge to respond quickly to a large amount of recommendation requests using a variety of state-of-the-art data processing and management technologies. These range from a two-tiered load balancing stage (Nginx, Tornado), to query execution using a nonrelational database and full-text search engine (MongoDB, ElasticSearch), to a recommendation cache based on an in-memory database (Redis).7 In practice, this results in average response times of about only 3 ms. In conclusion, the amount of literature on this issue shows that the scalability problem has been addressed to a certain degree by the news recommender research community. Nonetheless, because of the constant increase in the amount of available online news articles, scalability remains one of the key problems in this domain, and, thus, more sophisticated solutions and further scalability-oriented evaluations are necessary.

## Discussion

The review of recent works in this section shows that during the last ten years substantial efforts went into the development of new techniques to deal with the particular challenges of news recommendation. Figure 2 shows the distribution of topics and challenges that were addressed in the reviewed works. The diagram shows that the largest number of papers was devoted to the problems of user profiling, which is a common challenge for all recommender systems domains. Additionally, a number of papers focus on the specific problems of data sparsity and user and item cold-start. This is in fact expected, given that news recommendation systems face a continuous item cold-start situation, and personalization techniques that work well, e.g., for movie recommendation, do not always work well under such circumstances. Also, it is not very surprising that recency aspects received a lot of interest, due to the unique short-livedness of the items in the domain.

Figure 2: Distribution of research challenges (other than accuracy) addressed by news recommendation papers published in the last decade. Each paper can fall into multiple categories.

Beginning around the year 2011, we observe more and more papers on beyond-accuracy quality aspects like diversity, novelty, and serendipity, i.e., topics that were not strongly in the focus of researchers in earlier years. This recent trend can be considered as a positive development, since it has already been shown in other domains in which recommenders have been applied that focusing solely on accuracy measures to compare algorithms can be insufficient and potentially misleading [80, 128]. Nonetheless, more work is needed to better understand how to balance the sometimes competing goals of achieving high prediction accuracy and, for example, list diversity in parallel. Looking at the different challenges discussed in this section so far, we can identify a number of additional areas in which more work is still required. In the context of user modeling, for example, a number of papers differentiate between long-term preferences and short-term interests. In our view, it is however not fully clear yet how these aspects should be balanced, how we can deal with an interest drift over longer periods of time, or how we should deal with exceptional events and specific contextual conditions, which induce a probably only short-lived interest at the user’s side.

Furthermore, in terms of user modeling, today’s news sites also provide only limited ways for users to explicitly inform the recommender about their preferences or to give the system feedback on individual recommendations. More research is therefore required to put users into control of their recommendations. Today, Amazon.com implements a small set of such features on their website and provides users with explanations for the recommendations. Many users however seem to make limited use of these functionalities [88]. From the data perspective, more and more types of information are becoming accessible regarding the user’s current context, e.g., the current location, weather conditions, other people nearby etc. Also, continuously new data sources like DBPedia and other sources containing structured information, e.g., in terms of ontologies, become available. Some works exist which try to leverage structured information about named entities or other concepts mentioned in the news articles to find similar articles. Still, we see a lot of remaining potential to build even better news recommenders that rely on such additional data sources. As discussed in the section on aspects of item freshness, there is a number of further open questions. Looking at existing works, it is not fully clear yet how quickly older news should be discarded in the recommendation process. In fact, this aspect might even depend on the category of the news items. Some breaking news article might be outdated with the next incoming article on the same topic. An article about a sports event might not be very relevant anymore after the weekend. In contrast, an in-depth news report on a certain topic might be an interesting read even after months. In addition, whether the focus should be more on recent articles or not could also depend on the preferences of the individual user and the time of the day. Some first steps toward a better understanding of these aspects have been taken, but in our view a substantial amount of further investigation is still required.

# Evaluating and Benchmarking News Recommender Systems

Having reviewed the general challenges and current algorithmic approaches in news recommendation, we will discuss methodological questions related to the evaluation of news recommendation approaches in this section. To this end, we will review today’s academic practice of evaluating and benchmarking different algorithms and then reflect on possible limitations in this context caused by the availability of public evaluation data sets. Finally, we will compare open source frameworks that can be used for news recommendation evaluation.

## Evaluation Approaches

Figure 3: Distribution of evaluation approaches used in news recommendation papers published in the last decade. Each paper can fall into multiple categories.

Recommender systems are typically evaluated in one of the following three approaches: through offline experimentation and simulation based on historical data, through laboratory studies, or through A/B (field) tests on real-world websites. Sometimes, parts of the theoretic properties of the proposed algorithms can be demonstrated using a formal proof. Figure 3 shows the distribution of the different evaluation approaches for the examined papers. Our analysis shows that offline experimentation is the predominant way of evaluating and comparing different algorithms in the field of news recommendation. User studies are done to a much smaller extent and field tests (A/B studies) are still comparably rare, even though we observe more of such papers in the recent past. Comparable analyses have been conducted by Jannach et al. [85] on recommender systems in general and by Beel et al. [9] in the domain of research article recommendation, which both revealed a similar distribution of evaluation methods. Figure 4 shows which measures were used by researchers to quantify the performance and characteristics of the evaluated approaches. The results show that most of the works focus on accuracy measures of one type or another, including information retrieval measures like precision and recall, rank-based measures like Mean Reciprocal Rank or Normalized Discounted Cumulative Gain, or prediction measures like the Root Mean Square Error.

Figure 4: Distribution of evaluation measures/metrics used in news recommendation papers published in the last decade. Each paper can fall into multiple categories.

Among the 112 surveyed papers that introduce one or more new algorithms, only 19 used click-based metrics (such as the Click-Through-Rate) as an evaluation measure, even though such metrics are typically considered as success measures in practice, e.g., in the advertising industry. Measuring click rates, however, requires access to past navigation data and/or to a real-world system, which might explain the infrequent usage of these measures. An even smaller number of papers (about 10 out of 112) also considered aspects like diversity, novelty, or serendipity in their evaluations. Scalability, even though an important problem in practice, was also in the focus of only a few research works. Note in this context that while some papers take aspects like diversity or novelty into consideration in the design process of their algorithm, they do not explicitly quantify any improvements w.r.t. these aspects with standard metrics in their experimental evaluation. A complete list of challenges addressed by each paper and the metrics that were used in the evaluation process can be found in the appendix in Table A.3.

## Research Data Sets

The types of evaluations that can be done in academic research often depend on the characteristics of publicly available data sets, e.g., their size or the amount of user and item information. In the following, we will review a number of such data sets used in the literature and discuss their limitations in terms of what kind of research can be done with them. A list of the papers that present or analyze news recommendation data sets can be found in the appendix in Table A.4. 

### Yahoo!’s Data Sets. 

A number of research works in news recommendation are based on different data sets provided by Yahoo! (see, e.g., [124, 126, 139]). One prominent example is the “Yahoo! Front Page Today Module (R6A)” data set, which contains information about news articles that were displayed on the “Featured Tab” on Yahoo!’s landing page and (a fraction of) the click data for these articles.8 The data was collected during the first ten days of May 2009 and contains over 45 million user visits. For each visit, the timestamp and the displayed article ID is available, as well as a six-dimensional feature vector that was extracted from the item-user interactions via a specific form of regression analysis [32]. Later on, an alternative data set taken from the same website was published (named R6B), which contains data collected during fourteen days and comprising over 130 anonymized user features like age, gender, extracted behavioral variables etc. The two Front Page data sets were specifically constructed in a way so that an unbiased evaluation of recent explore-exploit based techniques (using contextual bandits) is possible [31, 106]. In 2016, Yahoo! released a huge “News Feed” data set containing interactions of about 20 million users collected on different Yahoo! websites over the period of three months in February 2015. Again, different user and item features are part of this data set, which contained about 110 billion lines. Currently, however, the data is no longer available, perhaps due to issues related to the privacy of users.9 While very valuable for researchers, one of the limitations of the still available “Front Page” data sets is that the data collection period is very short (i.e., at most 14 days), making it impossible to conduct research on personalization based on long-term user models. Also, given that the data was sampled only during one specific period, certain events that happened exactly during this period might have lead to certain biases in the data.

### SmartMedia Adressa Data Set. 

Recently, a click log data set with approximately 20 million page visits from a Norwegian news portal as well as a sub-sample with 2.7 million clicks (referred to as “light version”) were released.10 The data set was collected during ten weeks in the first quarter of 2017 and contains the click events of about 2 million users and about 13 thousand articles (light version: one week, 15 thousand users, one thousand articles). However, according to Gulla et al. [64], the clicks exhibit a strong concentration bias on only small subset of the items. In addition to the click logs, the data set contains some contextual information about the users such as geographical location, active time (time spent reading an article), and session boundaries. Furthermore, subscribed users who pay for access to the “pluss” category of articles can be identified; they make up 10.2 % of the user base. This subscription information could enable future investigations that focus on the willingness to pay for news content and the effect of a paid subscription on the consumption and interest patterns of users. Finally, even though the public data set contains some basic information about the news articles themselves in the form of keywords, the article content is only available upon request.

### Outbrain Data Set. 

In the context of the 2016/2017 Outbrain Click Prediction challenge hosted on Kaggle11, participants were asked to predict clicks on targeted content ads. The data set that was used for this challenge contains roughly 2 billion page visits for about 560 US publishers generated by a user base of approximately 700 million visitors during two weeks in June 2016. Even though the challenge focused on ad click prediction, the main click log of page visits, which contains the unique item and user IDs, timestamps, geolocations, etc., can be used for news recommendation evaluation as well. In contrast to other news recommendation data sets, however, the content of the news articles is not released. Instead only keywords, categories, and entities are provided as article metadata in the form of IDs. In fact, since even the publisher can only be identified by ID, researchers cannot be sure which type of news (or other textual content) the respective publishers provide. Therefore, while the possibilities to evaluate content-based recommendation schemes on this data set are rather limited, the massive amount of data offers a great opportunity to test more complex machine learning models.

### Proprietary and Non-Public Data Sets. 

Unfortunately, much research in the field of news recommendation is based on proprietary and non-public data sets. This in particular holds for research works like [39, 55, 93], which investigate the relationship between offline performance (measured in terms of accuracy metrics) and online success (measured in terms of click-through-rates). Often, additional application-specific ways of evaluating the performance of the algorithms in the live setting are used in these papers. In some cases, like in [55], the used real-world data sets were again collected only during a short period of time and contain a comparably small number of user interactions.

In some cases, researchers who use their own proprietary data sets in addition demonstrate the effectiveness of their methods on non-news data sets (e.g., from MovieLens as done in [39]). Such data sets are however often very different in terms of their characteristics (e.g., with respect to their sparsity or the fact that there exists no constant item cold-start situation), and it therefore remains unclear whether or not evaluations on such non-news data sets are truly representative of the news domain [142].

### The Plista Data Set and the NewsREEL Challenge. 

In 2013, the Plista12 data set was released [90] for research purposes. Plista is a German company that provides content and advertisement recommendations for a large number of websites. The data set contains different types of activities by news editors (Create, Update) and news readers (Impression, Click) recorded for about one dozen web portals during June 2013. The data set is comparably large and contains about 80 million article impressions and about 1 million clicks on recommended items. A small number of features of the users and the news items are provided as well. The various challenges of news recommendations mentioned in previous sections – e.g., data sparsity, recency biases, very few recorded interactions for many users – become once again apparent when analyzing the data set. In addition to making the data set publicly available, Plista holds regular contests (the CLEF-NewsREEL challenge13) in which researchers can test their recommendation algorithms in a live environment, which is a unique opportunity in the entire research field. During the contest, participants receive recommendation requests, which they have to answer within a limited time frame. If the recommendations are generally considered “recommendable”, e.g., because they are not too old, they are served to real users, and feedback is given to the participating researchers in case their recommendation was clicked. One main insight of these contests is that simply recommending those items that have been published within the last few minutes and which have been popular in this time frame seems to be a hard-to-beat approach when the click-through-rate is taken as a success measure [121]. A recent analysis of the performance of other approaches that took part in the NewsREEL challenge can be found in [46].

## Open Source News Evaluation Frameworks

As discussed above, the availability and quality of public news-related data sets can be a problem when evaluating news recommendation algorithms. However, even if researchers have access to a data set that fulfills their requirements, they need to compare their own algorithm implementation with other well-known approaches in a reproducible way to demonstrate the viability of their strategy. To this end, a small number of open source frameworks are available that are either specifically targeted towards evaluating news recommendation algorithms or can be adapted to this purpose. In theory, any recommender system evaluation framework, such as the popular LensKit14 or LibRec15 libraries, could be used to benchmark news recommendation algorithms. However, as discussed earlier, one unique aspect of the news domain is the importance of article freshness. Since most general purpose RS evaluation frameworks use cross-validation or sliding window schemes, the freshness problem cannot be treated adequately. However, a few frameworks exist that implement a “streaming” evaluation scheme, which aims to simulate real-life recommendation scenarios more closely. In such an evaluation scheme, clicks in the data set are processed in a chronological way, and after every click in the data set, algorithms have to generate a recommendation list that is then compared with the actual next clicks of the user. Table 2 shows a summary of the above-mentioned streaming evaluation frameworks for news recommendation algorithms. Of the four available frameworks, only one is designed explicitly for the news domain and, as such, includes special-purpose algorithms for news recommendation. The variety of algorithms in the IR & E framework, however, is rather low, with only a few baselines and one contextual bandit approach. In contrast, the general-purpose frameworks FluRS and Alpenglow offer a number of more complex algorithm implementations, such as incremental learning versions of kNN and matrix factorization. However, as discussed earlier, these general-purpose collaborative filtering strategies can often not deal with new items quickly enough. Idomaar does not provide any algorithms of its own except for a few simple baselines. The idea behind this framework is instead to offer a standardized evaluation environment. In contrast to the other frameworks, where algorithms are implemented as drop-in classes in Python or C++ respectively, in Idomaar each recommendation strategy has to be implemented as a web service endpoint to provide maximum separation between implementation and evaluation and to offer flexibility in the programming language, at the possible cost of efficiency. Finally, StreamingRec, an open-source news recommendation framework developed by the authors of this paper, offers a range of baseline approaches and more advanced algorithms in an extensible Java environment. Similar to Idomaar, StreamingRec’s evaluation procedure aims to simulate real-life click behavior in the news domain, but without the overhead of implementing algorithms as web-service endpoints. In our view, in particular Idomaar and the more recent StreamingRec framework represent the most valuable starting points for researchers. Idomaar has the advantage of defining language-agnostic interfaces. The framework does, however, not include any pre-implemented algorithms. StreamingRec, on the other hand, while being tied to the Java language, offers a wide range of pre-implemented news recommendation algorithms that can be used as baselines in comparative evaluations. Many of these algorithms are also capable of immediately considering new articles without costly model retraining.

## Discussion

Research on the topic of news recommendation was historically often subject to a number of constraints and limitations with respect to the evaluation methodology. Up to the very recent past, only very few public data sets containing user-item interactions for news specific applications were available. While some of the data sets are comparably large, many only cover the user interactions of a few consecutive days, which makes learning of long-term preference models more or less impossible. A further practical problem in this context is that researchers often use subsamples of the larger data sets to test their often computationally complex methods. Since these subsamples or the specific procedures to create them are usually not publicly shared, the reproducibility of the research results often remains limited. In some cases, researchers resort to data sets from other domains (e.g., movies) to test their news recommendation methods. Such approaches however come with a number of limitations as discussed in [142]. In particular, the aspects of recency and general popularity play a much less pronounced role in other domains, which makes it unclear to what extent algorithms that work well for other domains are suitable for news recommendation. To our knowledge, only one data set in the news recommendation domain is available which contains explicit feedback on news articles.21 The YOW data set22, which is described in detail by Ozg ¨ obek et al. [142], contains ¨ ratings for about 380 articles, which were collected from 21 paid users over a period of four weeks. The data set is therefore comparably small and in addition does not contain information about the news articles themselves, which makes experimentation with content-based or hybrid approaches infeasible. From a methodological perspective, classical IR accuracy measures like precision or recall dominate the research landscape. These measures are often, like in the CLEF-NewsREEL challenge, based on click-through data. While these measures are therefore based on real-world user interactions, a number of limitations remain. First, it is not fully clear today whether being able to successfully predict clicks in an offline experiments will translate into longterm business success in a deployed application [55], which can be a general problem of offline evaluations [10, 78]. Past live studies in the domain of mobile game recommendation show for example that recommending comparably popular items leads to high click-through-rates in an online shop but not to the strongest effect in sales [79]. In addition, the click-through-rate might only be an indicator of the user’s attention, but not necessarily a sign of genuine interest in the topic and thus proof of the recommender system’s success. A second potential issue in the context of measuring in offline experiments is that high values for precision and recall can in different domains be achieved by recommending mostly popular items [80]. In some cases, and depending on the specific variant of the measures, popularity-based baseline methods are often hard to beat by more complex methods [36]. In practice, however, users can easily recognize that the recommendations of such a popularity-based method are not based on the currently viewed news article and not tailored to their personal viewing history, which might lead to limited acceptance of the recommendations. This intuition is also supported by the experiments in Garcin et al. [55], where a popularity-based method was the winner in the offline evaluation, but performed poorly in an online scenario. Finally, a number of protocols variations can be applied in offline experiments. The simplest approach is to apply a time-agnostic cross-validation protocol, which is often done in the literature to evaluate recommenders, e.g., in the movie domain. However, splitting up user interactions into training and test sets without considering the consumption order disregards the importance of item freshness in the news domain. Therefore, the obtained results might be not very representative of an algorithm’s true performance. Another approach is to split the data using one of several time-aware evaluation protocols (see [20] for an overview), and use several consecutive days for training and the last one for testing. The outcomes of such an evaluation might depend on the specific protocol variant, which, as a result, might limit the comparability of the results of different researchers. Furthermore, the results of the 2017 CLEFNewsREEL challenge indicate [122] that being able to very quickly react to incoming news articles, e.g., within a few minutes, is very important to achieve high click-through rates. It therefore remains unclear how effective complex recommendation algorithms are that can only be re-trained overnight.

# Conclusions and Future Directions

Our review has shown that news recommendation is an active topic of research and that in recent years significant advances have been made in different directions. In this concluding section, we summarize some key challenges (as discussed in more detail in Sections 3.7 and Section 4.4) and sketch possible directions for future work. In our analysis of algorithmic approaches in Section 3, we found that content-based methods are quite frequently used in the academic literature, i.e., in almost half of the papers. The observations from our experimental evaluation and from news recommendation challenges in the real world, however, suggest that relying solely on content-based techniques can be insufficient. Factors like general article popularity and recency are highly important in the domain and collaborative-content-based hybrid techniques, as used in the field study by Kirshenbaum et al. [93], are therefore the method of choice when it comes to optimizing IR accuracy measures or click-through rates. However, in this context, more research is needed to better understand how factors like diversity, novelty, or popularity contribute to the quality perception of users, as focusing only on accuracy measures might lead to mostly non-personalized recommendations and little discovery. Better personalization is however only possible if more is known about the preferences of the individual users. It is therefore important to further investigate the relative importance of long-term and short-term (contextual) preference models. Research in that direction is however still hampered by the lack of data sets that contain longer user interaction histories. From an algorithmic perspective, questions regarding scalability were addressed in a number of the reviewed papers. Nowadays, the tools for collecting and storing big amounts of interaction data seem to have reached a high enough maturity level to support basic data processing in real-time. However, training and continuously updating the machine learning models (as done in other domains, e.g., in [81, 82]), remains a challenge, in particular when the models are based on deep learning strategies. In the news domain, new articles that are possibly relevant for a larger number of users can appear several times a day, and these articles have to be immediately considered by the recommendation methods. Otherwise, the performance of such models can be limited. This calls for more research on hybrid methods that are, for example, based on sophisticated long-term models and computationally efficient shortterm adaptation heuristics. With respect to the data perspective, today’s research data sets comprise a number of user features and item features, and quite recently, two large news recommendation data sets have been made publicly available. These new data sets represent a promising starting point for further reproducible research into scalability, sparsity, and user modeling aspects of news recommendation. Additionally, researchers are nowadays able to test their approaches on a variety of data set and publishers, which can give further insight into the generalizability of newly proposed algorithms; a crucial aspect of RS design, which has not yet been given enough attention in the literature. At the same time, with the continuous development of the Social Web, the Semantic Web, and the Internet of Things (IoT), a variety of additional data sources will continue to become available in the future. These data sources, for example, include information about the user’s behavior on Social Web platforms, additional information about the relationships between concepts and named entities in the news articles, or contextual information about the user’s situation derived from smartphone sensors or future IoT devices. A number of research avenues, therefore, exist on how to leverage these possibly huge amounts of additional types of information in the news recommendation process. Looking at methodological aspects, we found that IR measures and click-through rates are the prevalent evaluation measures in the academic literature. Whether or not optimizing algorithms for such measures leads to the best value for the different stakeholders (consumer, news provider, news aggregator) has not been investigated to a large extent in the recommender systems literature in general. More research and more public data sets that include explicit information about the business value are therefore required to understand, e.g., the economic perspective of news recommenders (“RECO-nomics”) [78] and how to balance the interests of the involved stakeholders. In this context, extended cooperations with news publishers or third-party recommendation providers are also required to better understand the business value of news recommendations.
