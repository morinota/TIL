## 0.1. link ãƒªãƒ³ã‚¯

- https://arxiv.org/pdf/2104.07413.pdf https://arxiv.org/pdf/2104.07413.pdf

## 0.2. title ã‚¿ã‚¤ãƒˆãƒ«

Empowering News Recommendation with Pre-trained Language Models
äº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã®å¼·åŒ–

## 0.3. abstract æŠ„éŒ²

Personalized news recommendation is an essential technique for online news services.
ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µãƒ¼ãƒ“ã‚¹ã«ã¨ã£ã¦ä¸å¯æ¬ ãªæ‰‹æ³•ã§ã‚ã‚‹ã€‚
News articles usually contain rich textual content, and accurate news modeling is important for personalized news recommendation.
ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã«ã¯é€šå¸¸ã€è±Šå¯Œãªãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã«ã¯æ­£ç¢ºãªãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãŒé‡è¦ã§ã‚ã‚‹ã€‚
Existing news recommendation methods mainly model news texts based on traditional text modeling methods, which is not optimal for mining the deep semantic information in news texts.
æ—¢å­˜ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦æ‰‹æ³•ã¯ã€ä¸»ã«ä¼çµ±çš„ãªãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°æ‰‹æ³•ã«åŸºã¥ã„ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ãŒã€ã“ã‚Œã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹æ·±ã„æ„å‘³æƒ…å ±ã®ãƒã‚¤ãƒ‹ãƒ³ã‚°ã«ã¯æœ€é©ã§ã¯ãªã„ã€‚
Pre-trained language models (PLMs) are powerful for natural language understanding, which has the potential for better news modeling.
äº‹å‰ã«è¨“ç·´ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆPLMï¼‰ã¯è‡ªç„¶è¨€èªç†è§£ã«å¼·åŠ›ã§ã‚ã‚Šã€ã‚ˆã‚Šè‰¯ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®å¯èƒ½æ€§ã‚’ç§˜ã‚ã¦ã„ã‚‹ã€‚
However, there is no public report that show PLMs have been applied to news recommendation.
ã—ã‹ã—ã€PLMãŒãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã«é©ç”¨ã•ã‚ŒãŸã¨ã„ã†å…¬çš„ãªå ±å‘Šã¯ãªã„ã€‚
In this paper, we report our work on exploiting pre-trained language models to empower news recommendation.
æœ¬ç¨¿ã§ã¯ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã‚’å¼·åŒ–ã™ã‚‹ãŸã‚ã«ã€äº‹å‰ã«è¨“ç·´ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã™ã‚‹ç ”ç©¶ã«ã¤ã„ã¦å ±å‘Šã™ã‚‹ã€‚
Offline experimental results on both monolingual and multilingual news recommendation datasets show that leveraging PLMs for news modeling can effectively improve the performance of news recommendation.
å˜è¨€èªã¨å¤šè¨€èªã®ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãŸã‚ªãƒ•ãƒ©ã‚¤ãƒ³å®Ÿé¨“ã®çµæœã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«PLMã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’åŠ¹æœçš„ã«æ”¹å–„ã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚
Our PLM-empowered news recommendation models have been deployed to the Microsoft News platform, and achieved significant gains in terms of both click and pageview in both English-speaking and global markets.
å½“ç¤¾ã®PLMã‚’æ´»ç”¨ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã¯ã€Microsoft Newsãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã«å°å…¥ã•ã‚Œã€è‹±èªåœã¨ã‚°ãƒ­ãƒ¼ãƒãƒ«å¸‚å ´ã®ä¸¡æ–¹ã§ã€ã‚¯ãƒªãƒƒã‚¯æ•°ã¨ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼æ•°ã®ä¸¡æ–¹ã§å¤§å¹…ãªå¢—åŠ ã‚’é”æˆã—ãŸã€‚

# 1. Introduction ã¯ã˜ã‚ã«

News recommendation techniques have played critical roles in many online news platforms to alleviate the information overload of users [15].
ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦æŠ€è¡“ã¯ã€ãƒ¦ãƒ¼ã‚¶ã®æƒ…å ±éå¤šã‚’ç·©å’Œã™ã‚‹ãŸã‚ã«ã€å¤šãã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¦ã„ã‚‹[15]ã€‚
News modeling is an important step in news recommendation, because it is a core technique to understand the content of candidate news and a prerequisite for inferring user interests from clicked news.
ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°(=è¦ã¯ã‚¢ã‚¤ãƒ†ãƒ ã®ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã€ã£ã¦èªè­˜:thinking:)ã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã®é‡è¦ãªã‚¹ãƒ†ãƒƒãƒ—ã§ã‚ã‚‹ã€‚ãªãœãªã‚‰ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹å€™è£œã®å†…å®¹ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®æ ¸ã¨ãªã‚‹æŠ€è¡“ã§ã‚ã‚Šã€ã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ã®èˆˆå‘³ã‚’æ¨æ¸¬ã™ã‚‹ãŸã‚ã®å‰ææ¡ä»¶ã ã‹ã‚‰ã§ã‚ã‚‹ã€‚
Since news articles usually have rich textual information, news texts modeling is the key for understanding news content for news recommendation.
**é€šå¸¸ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã¯è±Šå¯Œãªãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’æŒã£ã¦ã„ã‚‹ãŸã‚ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã¯ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã®ãŸã‚ã«ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®éµã¨ãªã‚‹**ã€‚
Existing news recommendation methods usually model news texts based on traditional NLP models [15, 19, 20, 22, 23, 25, 26].
æ—¢å­˜ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦æ‰‹æ³•ã¯é€šå¸¸ã€ä¼çµ±çš„ãªNLPãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ã„ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹[15, 19, 20, 22, 23, 25, 26]ã€‚
For example, Wang et al.[20] proposed to use a knowledge-aware CNN network to learn news representations from embeddings of words and entities in news title.
ä¾‹ãˆã°ã€Wangã‚‰[20]ã¯ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã‚‹å˜èªã¨ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®åŸ‹ã‚è¾¼ã¿ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹è¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã€knowledge-aware CNNãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚
Wu et al.[23] proposed to use multi-head self-attention network to learn news representations from news titles.
Wuã‚‰[23]ã¯ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹è¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã€multi-head self-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ææ¡ˆã—ãŸã€‚
However, it is difficult for these shallow models to understand the deep semantic information in news texts [18].
ã—ã‹ã—ã€ã“ã®ã‚ˆã†ãª**æµ…ã„ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹æ·±ã„æ„å‘³æƒ…å ±ã‚’ç†è§£ã™ã‚‹ã“ã¨ã¯é›£ã—ã„**[18]ã€‚
In addition, their models are only learned from the supervisions in the news recommendation task, which may not be optimal for capturing the semantic information.
åŠ ãˆã¦ã€å½¼ã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã‚¿ã‚¹ã‚¯ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒ“ã‚¸ãƒ§ãƒ³(=æ•™å¸«ãƒ©ãƒ™ãƒ«?)ã‹ã‚‰å­¦ç¿’ã•ã‚Œã‚‹ã ã‘ã§ã‚ã‚Šã€æ„å‘³æƒ…å ±ã‚’æ‰ãˆã‚‹ã«ã¯æœ€é©ã§ã¯ãªã„ã‹ã‚‚ã—ã‚Œãªã„ã€‚

Pre-trained language models (PLMs) have achieved great success in NLP due to their strong ability in text modeling [2, 5, 6, 11, 12, 14, 28].
äº‹å‰è¨“ç·´ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆPLMï¼‰ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ãŠã‘ã‚‹å¼·åŠ›ãªèƒ½åŠ›ã«ã‚ˆã‚Šã€è‡ªç„¶è¨€èªå‡¦ç†ã«ãŠã„ã¦å¤§ããªæˆåŠŸã‚’åã‚ã¦ã„ã‚‹[2, 5, 6, 11, 12, 14, 28]ã€‚
Different from traditional models that are usually directly trained with labeled data in specific tasks, PLMs are usually first pre-trained on a large unlabeled corpus via self-supervision to encode universal text information [5].
é€šå¸¸ã€ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã§ç›´æ¥è¨“ç·´ã•ã‚Œã‚‹å¾“æ¥ã®ãƒ¢ãƒ‡ãƒ«ã¨ã¯ç•°ãªã‚Šã€**PLMã¯é€šå¸¸ã€æ™®éçš„ãªãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’ç¬¦å·åŒ–ã™ã‚‹ãŸã‚ã«ã€è‡ªå·±æ•™å¸«ã‚’é€šã˜ã¦ãƒ©ãƒ™ãƒ«ãªã—ã®å¤§è¦æ¨¡ãªã‚³ãƒ¼ãƒ‘ã‚¹ã§äº‹å‰ã«è¨“ç·´ã•ã‚Œã‚‹**[5]ã€‚(ã†ã‚“ã†ã‚“:thinking:)
Thus, PLMs can usually provide better initial point for finetuning in downstream tasks [16].
ã—ãŸãŒã£ã¦ã€PLMã¯é€šå¸¸ã€**ä¸‹æµã®ã‚¿ã‚¹ã‚¯ã§fine-tuningã‚’è¡Œã†ãŸã‚ã®ã€ã‚ˆã‚Šè‰¯ã„åˆæœŸãƒã‚¤ãƒ³ãƒˆã‚’æä¾›ã™ã‚‹ã“ã¨ãŒã§ãã‚‹**[16]ã€‚(ã†ã‚“ã†ã‚“...!)
In addition, different from many traditional NLP methods with shallow models [9, 10, 24, 29], PLMs are usually much deeper with a huge number of parameters.
ã•ã‚‰ã«ã€æµ…ã„ãƒ¢ãƒ‡ãƒ« [9, 10, 24, 29] ã‚’æŒã¤å¤šãã®ä¼çµ±çš„ãªNLPæ‰‹æ³•ã¨ã¯ç•°ãªã‚Šã€PLMã¯é€šå¸¸ã€è†¨å¤§ãªæ•°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ã€ã‚ˆã‚Šæ·±ã„ã‚‚ã®ã§ã‚ã‚‹ã€‚
For example, the BERT-Base model contains 12 Transformer layers and up to 109M parameters [5].
ä¾‹ãˆã°ã€BERT-Base ãƒ¢ãƒ‡ãƒ«ã«ã¯ã€12 ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼å±¤ã¨æœ€å¤§ 109M ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹[5]ã€‚
Thus, PLMs may have greater ability in modeling the complicated contextual information in news text, which have the potentials to improve news text modeling for news recommendation.
ã“ã®ã‚ˆã†ã«ã€PLMã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹è¤‡é›‘ãªæ–‡è„ˆæƒ…å ±ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹èƒ½åŠ›ãŒé«˜ã„å¯èƒ½æ€§ãŒã‚ã‚Šã€ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã®ãŸã‚ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ¢ãƒ‡ãƒ«åŒ–ã‚’æ”¹å–„ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

In this paper, we present our work on empowering large-scale news recommendation with pre-trained language models.
Different from existing news recommendation methods that use shallow NLP models for news modeling, we explore to model news with pre-trained language models and finetune them with the news recommendation task.
æœ¬ç¨¿ã§ã¯ã€**äº‹å‰å­¦ç¿’ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦å¤§è¦æ¨¡ãªãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã‚’å®Ÿç¾**ã™ã‚‹æˆ‘ã€…ã®ç ”ç©¶ã‚’ç´¹ä»‹ã™ã‚‹ã€‚(ã‚³ãƒ¼ãƒ‰:t https://github.com/wuch15/PLM4NewsRec.)
ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«æµ…ã„è‡ªç„¶è¨€èªå‡¦ç†ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹æ—¢å­˜ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦æ‰‹æ³•ã¨ã¯ç•°ãªã‚Šã€æˆ‘ã€…ã¯**äº‹å‰å­¦ç¿’ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã—ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã‚¿ã‚¹ã‚¯ã«åˆã‚ã›ã¦ãã‚Œã‚‰ã‚’fine-tuningã™ã‚‹ã“ã¨ã‚’æ¢æ±‚ã—ã¦ã„ã‚‹**ã€‚
Offline experiments on real-world English and multilingual news recommendation datasets validate that incorporating PLMs into news modeling can consistently improve the news recommendation performance.
å®Ÿä¸–ç•Œã®è‹±èªãŠã‚ˆã³å¤šè¨€èªãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãŸ**ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å®Ÿé¨“**(ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‹ã...)ã«ã‚ˆã‚Šã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«PLMã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ä¸€è²«ã—ã¦æ”¹å–„ã§ãã‚‹ã“ã¨ãŒæ¤œè¨¼ã•ã‚ŒãŸã€‚
In addition, our PLM-empowered news recommendation models have been deployed to the Microsoft News platform.
ã•ã‚‰ã«ã€ç§ãŸã¡ã®PLMã‚’æ´»ç”¨ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã¯ã€**Microsoft Newsãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã«å°å…¥ã•ã‚Œã¦ã„ã‚‹**(ã»ã†ã»ã†ã€‚https://www.msn.com/ja-jp/feed æœ¬ç•ªå°å…¥ã•ã‚Œã¦ã‚‹ã®ã‹ã€‚)ã€‚
To our best knowledge, this is the first reported effort to empower large-scale news recommender systems with PLMs.
ç§ãŸã¡ã®çŸ¥ã‚‹é™ã‚Šã€ã“ã‚Œã¯å¤§è¦æ¨¡ãªãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚’PLMã§å¼·åŒ–ã™ã‚‹æœ€åˆã®å–ã‚Šçµ„ã¿ã§ã‚ã‚‹ã€‚
The online flight experiments show that our PLM-empowered news recommendation models achieved 8.53% click and 2.63% pageview gains in English-speaking markets, and 10.68% click and 6.04% pageview gains in other 43 global markets.
**ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å®Ÿé¨“ã«ã‚ˆã‚‹ã¨ã€ç§ãŸã¡ã®PLMã‚’åˆ©ç”¨ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã¯ã€è‹±èªåœã®å¸‚å ´ã§8.53%ã®ã‚¯ãƒªãƒƒã‚¯ã¨2.63%ã®ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼å¢—åŠ ã‚’é”æˆã—ã€ãã®ä»–ã®43ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¸‚å ´ã§10.68%ã®ã‚¯ãƒªãƒƒã‚¯ã¨6.04%ã®ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼å¢—åŠ ã‚’é”æˆã—ãŸ**ã€‚(ã‚ã€ã¡ã‚ƒã‚“ã¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆã‚‚ã‚„ã£ã¦ã‚‹ã®ã­ã€‚)

# 2. Methodology æ–¹æ³•è«–

In this section, we introduce the details of PLM-empowered news recommendation.
ã“ã“ã§ã¯ã€PLMã‚’æ´»ç”¨ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã®è©³ç´°ã‚’ç´¹ä»‹ã™ã‚‹ã€‚
We first introduce the general news recommendation model framework, and then introduce how to incorporate PLMs into this framework to empower news modeling.
ã¾ãšä¸€èˆ¬çš„ãªãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ç´¹ä»‹ã—ã€æ¬¡ã«ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«PLMã‚’çµ„ã¿è¾¼ã‚“ã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’å¼·åŒ–ã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã™ã‚‹ã€‚

## 2.1. General News Recommendation Framework ä¸€èˆ¬çš„ãªãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

![fig1]()

The general framework of news recommendation used in many existing methods [1, 15, 21, 23] is shown in Fig.1.
å¤šãã®æ—¢å­˜æ‰‹æ³•[1, 15, 21, 23]ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã®ä¸€èˆ¬çš„ãªæ çµ„ã¿ã‚’å›³1ã«ç¤ºã™ã€‚
The core components in this framework include a news encoder that aims to learn the embeddings of news from their texts, a user encoder that learns user embedding from the embeddings of clicked news, and a click prediction module that computes personalized click score for news ranking based on the relevance between user embedding and candidate news embedding.
ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ä¸­å¿ƒçš„ãªæ§‹æˆè¦ç´ ã«ã¯ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿ã‚’ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å­¦ç¿’ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ãŸ**news encoder**ã€ã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã®åŸ‹ã‚è¾¼ã¿(ã®interaction sequence or set!)ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ã®åŸ‹ã‚è¾¼ã¿ã‚’å­¦ç¿’ã™ã‚‹**user encoder**ã€ãƒ¦ãƒ¼ã‚¶ã®åŸ‹ã‚è¾¼ã¿ã¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®åŸ‹ã‚è¾¼ã¿å€™è£œã®é–“ã®é–¢é€£æ€§ã«åŸºã¥ã„ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ãŸã‚ã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸã‚¯ãƒªãƒƒã‚¯ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹**click prediction module**ãŒå«ã¾ã‚Œã‚‹[1, 15, 21, 23]ã€‚
We assume a user has ğ‘‡ historical clicked news, which are denoted as $[D_1, D_2, \cdots,  D_T]$.
ã‚ã‚‹ãƒ¦ãƒ¼ã‚¶ãŒéå»ã«ã‚¯ãƒªãƒƒã‚¯ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ $[D_1, D_2, \cdots,  D_T]$ ã¨ã—ã¦ã€$T$ å€‹æŒã£ã¦ã„ã‚‹ã¨ä»®å®šã™ã‚‹ã€‚
The news encoder processes these clicked news of a user and each candidate news $D_c$ to obtain their embeddings, which are denoted as $[h_1, h_2, \cdots, h_T]$ and $h_c$, respectively.
ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯ã€ãƒ¦ãƒ¼ã‚¶ã®ã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã¨å„å€™è£œãƒ‹ãƒ¥ãƒ¼ã‚¹ $D_c$ ã‚’å‡¦ç†ã—ã€ãã‚Œã‚‰ã®åŸ‹ã‚è¾¼ã¿ã‚’æ±‚ã‚ã€ãã‚Œãã‚Œã‚’ $[h_1, h_2, \cdots, h_T]$ ãŠã‚ˆã³ $h_c$ ã¨ã™ã‚‹ã€‚
It can be implemented by various NLP models, such as CNN [10] and self-attention [18].
(news encoderã¯)CNN[10]ã‚„self-attention[18]ãªã©ã€ã•ã¾ã–ã¾ãªNLPãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å®Ÿè£…ã§ãã‚‹ã€‚

The user encoder receives the sequence of clicked news embeddings as input, and outputs a user embedding $\mathbf{u}$ that summarizes user interest information.
ãƒ¦ãƒ¼ã‚¶ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯ã€ã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã®åŸ‹ã‚è¾¼ã¿**ã‚·ãƒ¼ã‚±ãƒ³ã‚¹**(=setã§ã¯ãªãsequenceã¨ã—ã¦æ‰±ã†äº‹ã‚’æƒ³å®šã—ã¦ã‚‹ã®ã‹ãª...!)ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€ãƒ¦ãƒ¼ã‚¶ã®é–¢å¿ƒæƒ…å ±ã‚’è¦ç´„ã—ãŸãƒ¦ãƒ¼ã‚¶åŸ‹ã‚è¾¼ã¿ $\mathbf{u}$ ã‚’å‡ºåŠ›ã™ã‚‹ã€‚
It can also be implemented by various models, such as the GRU network used in [15], the attention network used in [21] and the combination of multi-head self-attention and additive attention networks used in [23].
ã¾ãŸã€[15]ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹GRUãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€[21]ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€[23]ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹multi-head self-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨additive attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®çµ„ã¿åˆã‚ã›ãªã©ã€æ§˜ã€…ãªãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å®Ÿè£…ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

The click prediction module takes the user embedding u and hğ‘ as inputs, and compute the click score ğ‘¦Ë† by evaluating their relevance.
ã‚¯ãƒªãƒƒã‚¯äºˆæ¸¬ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ãƒ¦ãƒ¼ã‚¶åŸ‹ã‚è¾¼ã¿ $\mathbf{u}$ ã¨ $\mathbf{h}_{c}$ ã‚’å…¥åŠ›ã¨ã—ã€ãã‚Œã‚‰ã®é–¢é€£æ€§ã‚’è©•ä¾¡ã™ã‚‹ã“ã¨ã§ã‚¯ãƒªãƒƒã‚¯ã‚¹ã‚³ã‚¢ $\hat{y}$ ã‚’è¨ˆç®—ã™ã‚‹ã€‚
It can also be implemented by various methods such as inner product [15], neural network [20] and factorization machine [7].
ã¾ãŸã€å†…ç©[15]ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯[20]ã€å› æ•°åˆ†è§£ãƒã‚·ãƒ³[7]ãªã©ã€ã•ã¾ã–ã¾ãªæ–¹æ³•ã§å®Ÿè£…ã™ã‚‹ã“ã¨ã‚‚ã§ãã‚‹ã€‚(**dot-productä»¥å¤–ã®click prediction moduleæ°—ã«ãªã‚‹...!**:thinking:)

## 2.2. PLM Empowered News Recommendation PLM

Next, we introduce the framework of PLM empowered news recommendation, as shown in Fig.2.
æ¬¡ã«ã€å›³2ã«ç¤ºã™ã‚ˆã†ã«ã€PLMã‚’æ´»ç”¨ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ç´¹ä»‹ã™ã‚‹ã€‚
We instantiate the news encoder with a pre-trained language model to capture the deep contexts in news texts and an attention network to pool the output of PLM.
ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã«ã¯ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹æ·±ã„æ–‡è„ˆã‚’æ‰ãˆã‚‹ãŸã‚ã«äº‹å‰å­¦ç¿’ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã€**PLMã®å‡ºåŠ›ã‚’ãƒ—ãƒ¼ãƒ«ã™ã‚‹ãŸã‚ã®attetionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯**ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã™ã‚‹ã€‚(PLMã§å‡ºåŠ›ã•ã‚Œã‚‹ word embeddingé”ã‚’é›†ç´„ã—ã¦news text embeddingã‚’ä½œã‚‹ç‚ºã¨ã„ã†èªè­˜:thinking: ã“ã®æ–¹æ³•ã‚’æ¡ç”¨ã—ãŸç†ç”±ã¯å®Ÿé¨“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§å¾Œè¿°ã•ã‚Œã¦ã„ãŸ...!)
We denote an input news text with $M$ tokens as $[w_1,w_2, \cdots,w_M]$.
ã“ã“ã§ã¯ã€$M$ å€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŒã¤å…¥åŠ›ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã‚’ $[w_1,w_2, \cdots,w_M]$ ã¨ã—ã¾ã™ã€‚
The PLM converts each token into its embedding, and then learns the hidden representations of words through several Transformer [18] layers.
PLMã¯å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åŸ‹ã‚è¾¼ã¿ã«å¤‰æ›ã—ã€ã„ãã¤ã‹ã®Transformer [18]å±¤ã‚’é€šã—ã¦å˜èªã®hidden representationã‚’å­¦ç¿’ã™ã‚‹ã€‚
We denote the hidden token representation sequence as $[r_1, r_2, \cdots, r_M]$.
hiddenãƒˆãƒ¼ã‚¯ãƒ³è¡¨ç¾sequenceã‚’ $[r_1, r_2, \cdots, r_M]$ ã¨ã™ã‚‹ã€‚
We use an attention [29] network to summarize the hidden token representations into a unified news embedding.
ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³[29]ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã£ã¦ã€hiddenãƒˆãƒ¼ã‚¯ãƒ³è¡¨ç¾sequenceã‚’çµ±ä¸€ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿ã«è¦ç´„ã™ã‚‹ã€‚
(ãªã‚“ã¨ãªãã€ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ $[CLS]$ ã®hiddenãƒˆãƒ¼ã‚¯ãƒ³è¡¨ç¾ã‚’ã€æ–‡ã®åŸ‹ã‚è¾¼ã¿ã¨ã¿ãªã—ã¦ãã‚Œã‚’ãã®ã¾ã¾ãƒ‹ãƒ¥ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿ã¨ã—ã¦ä½¿ã†æƒ³åƒã ã£ãŸãŒã€äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¨ã¯åˆ¥ã§attentionã«é€šã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿ã‚’ä½œã‚‹ã®ã‹ãª...?:thinking:ã“ã®æ–¹æ³•ã‚’æ¡ç”¨ã—ãŸç†ç”±ã¯å¾Œè¿°ã•ã‚Œã¦ãŸã€‚)
The news embeddings learned by the PLM and attention network are further used for user modeling and candidate matching.
PLMã¨ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã£ã¦å­¦ç¿’ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿ã¯ã€ã•ã‚‰ã«ãƒ¦ãƒ¼ã‚¶ãƒ¢ãƒ‡ãƒªãƒ³ã‚°(=fig2ã®å³å´?)ã¨å€™è£œãƒãƒƒãƒãƒ³ã‚°(=fig2ã®å·¦å´?)ã«ä½¿ç”¨ã•ã‚Œã‚‹ã€‚

## 2.3. Model Training ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

Following [22, 23], we also use negative sampling techniques to build labeled samples from raw news impression logs, and we use the cross-entropy loss function for model training by classifying which candidate news is clicked.
ã¾ãŸã€[22, 23]ã«å€£ã„ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æŠ€è¡“(=æ•™å¸«ã‚ã‚Šå­¦ç¿’ã«ãŠã‘ã‚‹negative exampleã‚’ä½œã‚‹æ‰‹æ³•ã€‚)ã‚’ç”¨ã„ã¦ç”Ÿã®ãƒ‹ãƒ¥ãƒ¼ã‚¹impression (i.e. interaction?:thinking:) ãƒ­ã‚°ã‹ã‚‰ãƒ©ãƒ™ãƒ«ä»˜ãã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆã—ã€ã©ã®å€™è£œã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸã‹ã‚’åˆ†é¡ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±é–¢æ•°ã‚’ç”¨ã„ã‚‹ã€‚(=**next item predictionçš„ãªã‚¿ã‚¹ã‚¯ã‚’å­¦ç¿’ã•ã›ã‚‹æƒ³å®šãªã®ã‹ãª**??:thinking:)
By optimizing the loss function via backward-propagation, the parameters in the recommendation model and PLMs can be tuned for the news recommendation task.
é€†èª¤å·®ä¼æ¬æ³•ã«ã‚ˆã£ã¦æå¤±é–¢æ•°ã‚’æœ€é©åŒ–ã™ã‚‹ã“ã¨ã§ã€**æ¨è–¦ãƒ¢ãƒ‡ãƒ«(=user encoderã¨ click prediction module?) ã¨PLMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã‚¿ã‚¹ã‚¯ã«åˆã‚ã›ã¦ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

# 3. Experiments å®Ÿé¨“

## 3.1. Datasets and Experimental Settings ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨å®Ÿé¨“è¨­å®š

Our offline experiments are conducted on two real-world datasets.
æˆ‘ã€…ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å®Ÿé¨“ã¯ã€2ã¤ã®å®Ÿä¸–ç•Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¡Œã‚ã‚ŒãŸã€‚
The first one is MIND [27], which is an English dataset for monolingual news recommendation.
æœ€åˆã®ã‚‚ã®ã¯ MIND [27]ã§ã‚ã‚Šã€**å˜è¨€èªãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã®ãŸã‚ã®è‹±èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**ã§ã‚ã‚‹ã€‚
It contains the news click logs of 1 million users on Microsoft News in six weeks.3
ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆãƒ‹ãƒ¥ãƒ¼ã‚¹ã®6é€±é–“ã«ãŠã‘ã‚‹100ä¸‡ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¯ãƒªãƒƒã‚¯ãƒ­ã‚°ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã€‚

The second one is a multilingual news recommendation dataset (denoted as Multilingual) collected by ourselves on MSN News platform from Dec.1, 2020 to Jan.14, 2021.
2ã¤ç›®ã¯ã€2020å¹´12æœˆ1æ—¥ã‹ã‚‰2021å¹´1æœˆ14æ—¥ã¾ã§ã€MSN Newsãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§åé›†ã—ãŸ**å¤šè¨€èªãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**ï¼ˆMultilingualã¨è¡¨è¨˜ï¼‰ã§ã‚ã‚‹ã€‚
It contains users from 7 countries with different language usage, and their market language codes are EN-US, DE-DE, FR-FR, IT-IT, JA-JP, ES-ES and KO-KR, respectively.
ç•°ãªã‚‹è¨€èªã‚’ä½¿ç”¨ã™ã‚‹7ã‚«å›½ã®ãƒ¦ãƒ¼ã‚¶ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€ãã®å¸‚å ´è¨€èªã‚³ãƒ¼ãƒ‰ã¯ãã‚Œãã‚ŒEN-USã€DE-DEã€FR-FRã€IT-ITã€JA-JPã€ES-ESã€KO-KRã§ã‚ã‚‹ã€‚
We randomly sample 200,000 impression logs in each market.
å„å¸‚å ´ã§20ä¸‡ä»¶ã®ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ãƒ»ãƒ­ã‚°ã‚’ç„¡ä½œç‚ºã«æŠ½å‡ºã€‚
The logs in the last week are used for test and the rest are used for training and validation (9:1 split).
ç›´è¿‘1é€±é–“ã®ãƒ­ã‚°ã‚’ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨ã—ã€æ®‹ã‚Šã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¤œè¨¼ã«ä½¿ç”¨ã™ã‚‹ï¼ˆ9:1ã®å‰²åˆï¼‰ã€‚
The detailed statistics of the two datasets are shown in Table 1.
2ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è©³ç´°ãªçµ±è¨ˆã¯è¡¨1ã«ç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚

![table1]()

In our experiments, we used the â€œBaseâ€ version of different pretrained language models if not specially mentioned.
å®Ÿé¨“ã§ã¯ã€ç‰¹ã«è¨€åŠã•ã‚Œã¦ã„ãªã„é™ã‚Šã€ã•ã¾ã–ã¾ãªäº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ã®ã€Œãƒ™ãƒ¼ã‚¹ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ãŸã€‚
We finetuned the last two Transformer layers because we find there is only a very small performance difference between finetuning all layers and the last two layers.
ã™ã¹ã¦ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’fine-tuningã™ã‚‹ã®ã¨ã€æœ€å¾Œã®2ã¤ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’fine-tuningã™ã‚‹ã®ã¨ã§ã¯ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«ã”ãã‚ãšã‹ãªå·®ã—ã‹ãªã„ã“ã¨ãŒã‚ã‹ã£ãŸã‹ã‚‰ã ã€‚
Following [27], we used the titles of news for news modeling.
[27]ã«å¾“ã„ã€**ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ä½¿ç”¨**ã—ãŸã€‚
We used Adam [3] as the optimization algorithm and the learning rate was 1e-5.
æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¯Adam [3]ã‚’ä½¿ç”¨ã—ã€å­¦ç¿’ç‡ã¯1e-5ã¨ã—ãŸã€‚
The batch size was 128.4 These hyperparameters are developed on the validation sets.
ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯128.4ã§ã€ã“ã‚Œã‚‰ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯æ¤œè¨¼ã‚»ãƒƒãƒˆã§é–‹ç™ºã•ã‚ŒãŸã€‚
We used average AUC, MRR, nDCG@5 and nDCG@10 over all impressions as the performance metrics.
ã™ã¹ã¦ã®impressionã®å¹³å‡AUCã€MRRã€NDCG@5ã€NDCG@10ã‚’ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã¨ã—ã¦ä½¿ç”¨ã—ãŸã€‚
We repeated each experiment 5 times independently and reported the average performance.
å„å®Ÿé¨“ã‚’ç‹¬ç«‹ã«5å›ç¹°ã‚Šè¿”ã—ã€ãã®å¹³å‡å€¤ã‚’å ±å‘Šã—ãŸã€‚

## 3.2. Offline Performance Evaluation ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ€§èƒ½è©•ä¾¡

We first compare the performance of several methods on the MIND dataset to validate the effectiveness of PLM-based models in monolingual news recommendation.
ã¾ãšã€å˜è¨€èªãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã«ãŠã‘ã‚‹PLMãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã«ã€MINDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã„ãã¤ã‹ã®æ‰‹æ³•ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¯”è¼ƒã™ã‚‹ã€‚
We compared several recent news recommendation methods including EBNR [15], NAML [21], NPA [22], LSTUR [1], NRMS [23] and their variants empowered by different pre-trained language models, including BERT [5], RoBERTa [14] and UniLM [2].
EBNR[15]ã€NAML[21]ã€NPA[22]ã€LSTUR[1]ã€NRMS[23]ã€ãŠã‚ˆã³BERT[5]ã€RoBERTa[14]ã€UniLM[2]ãªã©ã€ã•ã¾ã–ã¾ãªäº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å¼·åŒ–ã•ã‚ŒãŸãã‚Œã‚‰ã®å¤‰ç¨®(=æ‰‹æ³•+news encoderã‚’PLMã«!)ã‚’å«ã‚€ã€æœ€è¿‘ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦æ‰‹æ³•ã‚’æ¯”è¼ƒã—ãŸã€‚

![table2]()

The results are shown in Table 2.
çµæœã‚’è¡¨2ã«ç¤ºã™ã€‚
Referring to this table, we find that incorporating pre-trained language models can consistently improve the performance of basic models.5
ã“ã®è¡¨ã‚’å‚ç…§ã™ã‚‹ã¨ã€PLMã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ä¸€è²«ã—ã¦å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã“ã¨ãŒã‚ã‹ã‚‹5ã€‚
This is because pre-trained language models have stronger text modeling ability than the shallow models learned from scratch in the news recommendation.
ã“ã‚Œã¯ã€**PLMãŒãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã§ã‚¼ãƒ­ã‹ã‚‰å­¦ç¿’ã•ã‚ŒãŸæµ…ã„ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚å¼·åŠ›ãªãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°èƒ½åŠ›ã‚’æŒã¤ãŸã‚**ã§ã‚ã‚‹ã€‚
In addition, we find that the models based on RoBERTa are better than those based on BERT.
ã•ã‚‰ã«ã€RoBERTa(=PLMã®ä¸€ã¤)ã«åŸºã¥ããƒ¢ãƒ‡ãƒ«ã¯ã€BERTã«åŸºã¥ããƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸã€‚
This may be because RoBERTa has better hyperparameter settings than BERT and is pre-trained on larger corpus for a longer time.
ã“ã‚Œã¯ã€RoBERTaãŒBERTã‚ˆã‚Šã‚‚å„ªã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã‚’æŒã£ã¦ãŠã‚Šã€ã‚ˆã‚Šå¤§ããªã‚³ãƒ¼ãƒ‘ã‚¹ã§ã‚ˆã‚Šé•·ã„æ™‚é–“äº‹å‰è¨“ç·´ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€‚
Besides, the models based on UniLM achieve the best performance.
ã•ã‚‰ã«ã€UniLM(=ã“ã‚Œã‚‚PLMã®ä¸€ã¤)ã«åŸºã¥ããƒ¢ãƒ‡ãƒ«ã¯æœ€é«˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ãŸã€‚
This may be due to UniLM can exploit the self-supervision information in both text understanding and generation tasks, which can help learn a higher-quality PLM.
ã“ã‚Œã¯ã€UniLMãŒãƒ†ã‚­ã‚¹ãƒˆç†è§£ã¨ç”Ÿæˆã‚¿ã‚¹ã‚¯ã®ä¸¡æ–¹ã§è‡ªå·±æ•™å¸«æƒ…å ±ã‚’åˆ©ç”¨ã§ãã‚‹ãŸã‚ã€ã‚ˆã‚Šè³ªã®é«˜ã„PLMã‚’å­¦ç¿’ã§ãã‚‹ãŸã‚ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€‚

In addition, we conduct experiments on the Multilingual dataset to validate the effectiveness of PLMs in multilingual news recommendation.
ã•ã‚‰ã«ã€å¤šè¨€èªãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã«ãŠã‘ã‚‹PLMã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã€å¤šè¨€èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãŸå®Ÿé¨“ã‚’è¡Œã†ã€‚
We compare the performance of EBNR, NAML, NPA, LSTUR and NRMS with different multilingual text modeling methods, including:
æˆ‘ã€…ã¯ã€EBNRã€NAMLã€NPAã€LSTURã€NRMSã®æ€§èƒ½ã‚’ã€ä»¥ä¸‹ã®ã‚ˆã†ãªç•°ãªã‚‹**å¤šè¨€èªãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°æ‰‹æ³•(=å¤šè¨€èªå¯¾å¿œã®PLMã£ã¦æ„å‘³?:thinking:)**ã¨æ¯”è¼ƒã™ã‚‹:
(1) MUSE [13], using modularizing unsupervised sense embeddings; (2) Unicoder [8], a universal language encoder pre-trained by cross-lingual self-supervision tasks; and (3) InfoXLM [4], a contrastively pre-trained cross-lingual language model based on information-theoretic framework.
(1)æ•™å¸«ãªã—ã‚»ãƒ³ã‚¹åŸ‹ã‚è¾¼ã¿(?)ã‚’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã—ãŸMUSE [13]ã€
(2)ãƒ¦ãƒ‹ã‚³ãƒ¼ãƒ€[8]ã€ã‚¯ãƒ­ã‚¹ãƒªãƒ³ã‚¬ãƒ«è‡ªå·±æ•™å¸«ã‚¿ã‚¹ã‚¯ã§äº‹å‰è¨“ç·´ã•ã‚ŒãŸãƒ¦ãƒ‹ãƒãƒ¼ã‚µãƒ«è¨€èªã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã€
(3)æƒ…å ±ç†è«–çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«åŸºã¥ãcontrastiveã«äº‹å‰å­¦ç¿’ã•ã‚ŒãŸã‚¯ãƒ­ã‚¹ãƒªãƒ³ã‚¬ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ«ã€InfoXLM [4]ã€‚
In these methods, following [8] we mix up the training data in different languages.
ã“ã‚Œã‚‰ã®æ–¹æ³•ã§ã¯ã€[8]ã«å¾“ã„ã€ç•°ãªã‚‹è¨€èªã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æ··åœ¨ã•ã›ã‚‹ã€‚
In addition, we also compare the performance of independently learned monolingual models based on MUSE for each market (denoted as Single).
ã•ã‚‰ã«ã€MUSEã‚’ãƒ™ãƒ¼ã‚¹ã«ç‹¬è‡ªã«å­¦ç¿’ã—ãŸå„å¸‚å ´ã®å˜è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆSingleã¨è¡¨è¨˜ï¼‰ã®æ€§èƒ½ã‚‚æ¯”è¼ƒã—ãŸã€‚

![table3]()

The results of different methods in terms of AUC are shown in Table 3.
AUCã®è¦³ç‚¹ã‹ã‚‰ã®ç•°ãªã‚‹æ–¹æ³•ã®çµæœã‚’è¡¨3ã«ç¤ºã™ã€‚
We find that multilingual models usually outperform the independently learned monolingual models.
æˆ‘ã€…ã¯ã€**å¤šè¨€èªãƒ¢ãƒ‡ãƒ«ãŒé€šå¸¸ã€ç‹¬ç«‹ã—ã¦å­¦ç¿’ã•ã‚ŒãŸå˜è¨€èªãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç™ºè¦‹**ã—ãŸã€‚
This may be because different languages usually have some inherent relatedness and users in different countries may also have some similar interests.
ã“ã‚Œã¯ã€ç•°ãªã‚‹è¨€èªã«ã¯å›ºæœ‰ã®é–¢é€£æ€§ãŒã‚ã‚Šã€ç•°ãªã‚‹å›½ã®ãƒ¦ãƒ¼ã‚¶ã‚‚ã¾ãŸã€åŒã˜ã‚ˆã†ãªé–¢å¿ƒã‚’æŒã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã‹ã‚‰ã ã‚ã†ã€‚
Thus, jointly training models with multilingual data can help learn a more accurate recommendation model.
ã“ã®ã‚ˆã†ã«ã€**å¤šè¨€èªãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å…±åŒå­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šç²¾åº¦ã®é«˜ã„æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã‚‹**ã€‚
It also provides the potential to use a unified recommendation model to serve users in different countries with diverse language usage (e.g., IndoEuropean and Altaic), which can greatly reduce the computation and memory cost of online serving.
ã¾ãŸã€ç•°ãªã‚‹market(=ç•°ãªã‚‹è¨€èªåŸŸ)ã§çµ±åˆã•ã‚ŒãŸæ¨è–¦ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€å¤šæ§˜ãªè¨€èªï¼ˆä¾‹ãˆã°ã€å°æ¬§èªã¨ã‚¢ãƒ«ã‚¿ã‚¤èªï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã•ã¾ã–ã¾ãªå›½ã®ãƒ¦ãƒ¼ã‚¶ã«ã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€**ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ã®è¨ˆç®—ã‚³ã‚¹ãƒˆã¨ãƒ¡ãƒ¢ãƒªãƒ¼ãƒ»ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã‚‹**ã€‚(å„marketæ¯ã«ã€è¨€ã„æ›ãˆã‚Œã°ç•°ãªã‚‹usecaseæ¯ã«ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é‹ç”¨ã™ã‚‹ã‚ˆã‚Šã‚‚...!:thinking:)
In addition, the performance methods based on multilingual PLMs are better than those based on MUSE embeddings.
ã•ã‚‰ã«ã€å¤šè¨€èªPLMã«åŸºã¥ãæ–¹æ³•ã¯ã€MUSEåŸ‹ã‚è¾¼ã¿(=å˜è¨€èªPLMã ã£ã‘?)ã«åŸºã¥ãæ–¹æ³•ã‚ˆã‚Šã‚‚æ€§èƒ½ãŒè‰¯ã„ã€‚
This may be because PLMs are also stronger than word embeddings in capturing the complicated multilingual semantic information.
ã“ã‚Œã¯ã€PLMãŒè¤‡é›‘ãªå¤šè¨€èªã®æ„å‘³æƒ…å ±ã‚’æ‰ãˆã‚‹ä¸Šã§ã€å˜èªåŸ‹ã‚è¾¼ã¿ã‚ˆã‚Šã‚‚å¼·ã„ã‹ã‚‰ã‹ã‚‚ã—ã‚Œãªã„ã€‚(MUSEãŒã©ã‚“ãªæ‰‹æ³•ãªã®ã‹è‰¯ãã‚ã‹ã£ã¦ãªã„...!:thinking:)
In addition, InfoXLM can better empower multilingual news recommendation than Unicoder.
ã•ã‚‰ã«ã€InfoXLMã¯ã€Unicoderã‚ˆã‚Šã‚‚å¤šè¨€èªãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã‚’å¼·åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
This may be because InfoXLM uses better contrastive pre-training strategies than Unicoder to help learn more accurate models.
ã“ã‚Œã¯ã€InfoXLMãŒUnicoderã‚ˆã‚Šã‚‚å„ªã‚ŒãŸcontrastiveäº‹å‰å­¦ç¿’æˆ¦ç•¥ã‚’ä½¿ã£ã¦ã€ã‚ˆã‚Šæ­£ç¢ºãªãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ãŸã‚ã¨æ€ã‚ã‚Œã‚‹ã€‚

## 3.3. Influence of Model Size ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®å½±éŸ¿

![fig3]()

Next, we explore the influence of PLM size on the recommendation performance.
æ¬¡ã«ã€**PLMã®ã‚µã‚¤ã‚ºãŒæ¨è–¦æ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿**ã‚’æ¢ã‚‹ã€‚
We compare the performance of two representative methods (i.e., NAML and NRMS) with different versions of BERT, including BERT-Base (12 layers), BERT-Medium (8 layers), BERTSmall (4 layers) and BERT-Tiny (2 layers).
BERT-Baseï¼ˆ12 å±¤ï¼‰ã€BERT-Mediumï¼ˆ8 å±¤ï¼‰ã€BERTSmallï¼ˆ4 å±¤ï¼‰ã€ãŠã‚ˆã³ BERT-Tinyï¼ˆ2 å±¤ï¼‰ã‚’å«ã‚€ BERT ã®ã•ã¾ã–ã¾ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ã€ä»£è¡¨çš„ãª 2 ã¤ã®æ‰‹æ³•ï¼ˆNAML ãŠã‚ˆã³ NRMSï¼‰ã®æ€§èƒ½ã‚’æ¯”è¼ƒã™ã‚‹ã€‚
The results on MIND are shown in Fig.3.
MINDã§ã®çµæœã‚’å›³3ã«ç¤ºã™ã€‚
We find that using larger PLMs with more parameters usually yields better recommendation performance.
**é€šå¸¸ã€ã‚ˆã‚Šå¤§ããªPLMã¨ã‚ˆã‚Šå¤šãã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šè‰¯ã„æ¨è–¦æ€§èƒ½ãŒå¾—ã‚‰ã‚Œã‚‹**ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
This may be because larger PLMs usually have stronger abilities in capturing the deep semantic information of news, and the performance may be further improved if more giant PLMs (e.g., BERT-Large) are incorporated.
ã“ã‚Œã¯ã€**é€šå¸¸ã€å¤§å‹ã® PLM ã®æ–¹ãŒã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æ·±ã„æ„å‘³æƒ…å ±ã‚’æ•æ‰ã™ã‚‹èƒ½åŠ›ãŒé«˜ã„**ãŸã‚ã§ã‚ã‚Šã€ã‚ˆã‚Šå·¨å¤§ãª PLMï¼ˆBERT-Large ãªã©ï¼‰ã‚’çµ„ã¿è¾¼ã‚ã°ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒã•ã‚‰ã«å‘ä¸Šã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
However, since huge PLMs are too cumbersome for online applications, we prefer the base version of PLMs.
**ã—ã‹ã—ã€å·¨å¤§ãªPLMã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¯é¢å€’ãªã®ã§ã€ç§ãŸã¡ã¯åŸºæœ¬ç‰ˆã®PLMã‚’å¥½ã‚€**ã€‚

## 3.4. Influence of Different Pooling Methods ç•°ãªã‚‹ãƒ—ãƒ¼ãƒªãƒ³ã‚°æ–¹æ³•ã®å½±éŸ¿

We also explore using different pooling methods for learning news embeddings from the hidden states of PLMs.
ã¾ãŸã€PLMã®éš ã‚Œãƒˆãƒ¼ã‚¯ãƒ³è¡¨ç¾ã®sequenceã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®åŸ‹ã‚è¾¼ã¿ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã€ç•°ãªã‚‹ãƒ—ãƒ¼ãƒªãƒ³ã‚°æ–¹æ³•ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚æ¤œè¨ã™ã‚‹ã€‚(PLMã®å‡ºåŠ›ã‚’attentionã§é›†ç´„ã™ã‚‹ã‚„ã¤!:thinking:)
We compare three methods, including:
ä»¥ä¸‹ã®3ã¤ã®æ–¹æ³•ã‚’æ¯”è¼ƒã™ã‚‹ï¼š

(1) CLS, using the representation of the â€œ[CLS]â€ token as news embedding, which is a widely used method for obtaining sentence embedding;
(1)CLSï¼šãƒ‹ãƒ¥ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿ã¨ã—ã¦"[CLS]"ãƒˆãƒ¼ã‚¯ãƒ³ã®è¡¨ç¾ã‚’ç”¨ã„ã‚‹ã€‚ã“ã‚Œã¯ã€æ–‡ã®åŸ‹ã‚è¾¼ã¿ã‚’å¾—ã‚‹ãŸã‚ã«åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹æ–¹æ³•ã§ã‚ã‚‹ã€‚(=PLMã‚’ä½¿ã†æ–¹æ³•ã¨ã—ã¦çœŸã£å…ˆã«ã“ã‚Œã‚’æƒ³åƒã—ãŸ!:thinking:)
(2) Average, using the average of hidden states of PLM;
(2) PLMã®éš ã•ã‚ŒãŸçŠ¶æ…‹ã®å¹³å‡ã‚’ä½¿ç”¨ã—ãŸå¹³å‡ï¼›
(3) Attention, using an attention network to learn news embeddings from hidden states.
(3) ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã£ã¦ã€éš ã‚ŒãŸçŠ¶æ…‹ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®åŸ‹ã‚è¾¼ã¿ã‚’å­¦ç¿’ã™ã‚‹ã€‚(ã“ã‚ŒãŒæœ¬è«–æ–‡ã§ææ¡ˆã—ã¦ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã ã‚ˆã­??)

![fig4]()

The results of NAML-BERT and NRMS-BERT on MIND are shown in Fig.4.
MIND ã«ãŠã‘ã‚‹ NAML-BERT ãŠã‚ˆã³ NRMS-BERT ã®çµæœã‚’å›³ 4ã«ç¤ºã™ã€‚
We find it is very interesting that the CLS method yields the worst performance.
æˆ‘ã€…ã¯ã€**CLSæ³•ãŒæœ€æ‚ªã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ã‚‚ãŸã‚‰ã—ãŸã“ã¨ãŒéå¸¸ã«èˆˆå‘³æ·±ã„**ã“ã¨ã«æ°—ã¥ã„ãŸã€‚
This may be because it cannot exploit all output hidden states of the PLM.
ã“ã‚Œã¯ã€PLMã®ã™ã¹ã¦ã®å‡ºåŠ›hidden state(=å…¨ã¦ã®tokenã®éš ã‚Œãƒˆãƒ¼ã‚¯ãƒ³è¡¨ç¾ã®äº‹ã ã‚ˆã­??)ã‚’åˆ©ç”¨ã§ããªã„ã‹ã‚‰ã‹ã‚‚ã—ã‚Œãªã„ã€‚(ãã†ãªã®...??)
In addition, Attention outperforms Average.
ã•ã‚‰ã«ã€**ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¯ã‚¢ãƒ™ãƒ¬ãƒ¼ã‚¸ã‚’ä¸Šå›ã‚‹**ã€‚
This may be because attention networks can distinguish the informativeness of hidden states, which can help learn more accurate news representations.
ã“ã‚Œã¯ã€self-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒ**hidden stateã®æƒ…å ±æ€§ã‚’åŒºåˆ¥ã™ã‚‹**ã“ã¨ãŒã§ãã€ã‚ˆã‚Šæ­£ç¢ºãªãƒ‹ãƒ¥ãƒ¼ã‚¹è¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹ã®ã«å½¹ç«‹ã¤ã‹ã‚‰ã‹ã‚‚ã—ã‚Œãªã„ã€‚(=ãŸã¶ã‚“multi-headã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãªã®ã‹ãª??)
Thus, we choose attention mechanism as the pooling method.
ã‚ˆã£ã¦ã€ãƒ—ãƒ¼ãƒªãƒ³ã‚°æ–¹æ³•ã¨ã—ã¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’é¸æŠã™ã‚‹ã€‚

## 3.5. Visualization of News Embedding ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®åŸ‹ã‚è¾¼ã¿ã®å¯è¦–åŒ–

We also study the differences between the news embeddings learned by shallow models and PLM-empowered models.
ã¾ãŸã€**æµ…ã„ãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿ã¨ã€PLMã‚’åˆ©ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®é•ã„**ã«ã¤ã„ã¦ã‚‚ç ”ç©¶ã—ã¦ã„ã‚‹ã€‚
We use t-SNE [17] to visualize the news embeddings learned by NRMS and NRMSUniLM, and the results are shown in Fig.5.
NRMSã¨NRMSUniLMã«ã‚ˆã£ã¦å­¦ç¿’ã•ã‚ŒãŸ**ãƒ‹ãƒ¥ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿ã‚’å¯è¦–åŒ–ã™ã‚‹ãŸã‚ã«t-SNE [17]ã‚’ç”¨ã„**(t-SNEãŒæ°—ã«ãªã‚‹...!!:thinking:)ã€ãã®çµæœã‚’å›³5ã«ç¤ºã™ã€‚
We find an interesting phenomenon that the news embeddings learned by NRMS-UniLM are much more discriminative than NRMS.
**NRMS-UniLMã«ã‚ˆã£ã¦å­¦ç¿’ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿ã¯ã€NRMSã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«è­˜åˆ¥æ€§ãŒé«˜ã„ã¨ã„ã†èˆˆå‘³æ·±ã„ç¾è±¡ã‚’ç™ºè¦‹ã—ãŸ**ã€‚(=ã“ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®è­˜åˆ¥æ€§ã®é«˜ã•ã®è©•ä¾¡ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–çµæœã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ã‹è©•ä¾¡ã¨ã‹ã«è‰¯ã„ã‹ã‚‚...! booking.comãŒæ¡ç”¨ã—ã¦ãŸå¿œç­”åˆ†å¸ƒåˆ†æã¨åŒæ§˜ã«ã€æ•™å¸«ãƒ©ãƒ™ãƒ«ã«ä¾å­˜ã—ã¦ãªã„è©•ä¾¡æ–¹æ³•ã ã—...!!:thinking:)
This may be because the shallow self-attention network in NRMS cannot effectively model the semantic information in news texts.
ã“ã‚Œã¯ã€NRMSã®æµ…ã„self-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã¯ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã®æ„å‘³æƒ…å ±ã‚’åŠ¹æœçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã§ããªã„ã‹ã‚‰ã‹ã‚‚ã—ã‚Œãªã„ã€‚
Since user interests are also inferred from embeddings of clicked news, it is difficult for NRMS to accurately model user interests from non-discriminative news representations.
ãƒ¦ãƒ¼ã‚¶ã®èˆˆå‘³ã¯ã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã®åŸ‹ã‚è¾¼ã¿ã‹ã‚‰ã‚‚æ¨æ¸¬ã•ã‚Œã‚‹ãŸã‚ã€**NRMSã®ã¯ã£ãã‚Šã¨è­˜åˆ¥ã§ãã¦ãªã„ãƒ‹ãƒ¥ãƒ¼ã‚¹è¡¨ç¾ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ã®èˆˆå‘³ã‚’æ­£ç¢ºã«ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ã“ã¨ã¯é›£ã—ã„**ã€‚
In addition, we observe that the news embeddings learned by NRMS-UniLM form several clear clusters.
ã•ã‚‰ã«ã€NRMS-UniLMã«ã‚ˆã£ã¦å­¦ç¿’ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿ã¯ã€ã„ãã¤ã‹ã®**æ˜ç¢ºãªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’å½¢æˆ**ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
This may be because the PLM-empowered model can disentangle different kinds of news for better user interest modeling and news matching.
ã“ã‚Œã¯ã€PLMã‚’æ´»ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ãŒã€ã‚ˆã‚Šè‰¯ã„ãƒ¦ãƒ¼ã‚¶ã®é–¢å¿ƒãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒãƒƒãƒãƒ³ã‚°ã®ãŸã‚ã«ã€ç•°ãªã‚‹ç¨®é¡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åŒºåˆ¥ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ãŸã‚ã§ã‚ã‚ã†ã€‚
These results demonstrate that deep PLMs have greater ability than shallow NLP models in learning discriminative text representations, which is usually beneficial for accurate news recommendation.
ã“ã‚Œã‚‰ã®çµæœã¯ã€ãƒ‡ã‚£ãƒ¼ãƒ—PLMãŒæµ…ã„NLPãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚**è­˜åˆ¥çš„ãªãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹èƒ½åŠ›ãŒé«˜ã„**ã“ã¨ã‚’ç¤ºã—ã¦ãŠã‚Šã€ã“ã‚Œã¯é€šå¸¸ã€**æ­£ç¢ºãªãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã«æœ‰ç›Š**ã§ã‚ã‚‹ã€‚

## 3.6. Online Flight Experiments ã‚ªãƒ³ãƒ©ã‚¤ãƒ³é£›è¡Œå®Ÿé¨“

We have deployed our PLM-empowered news recommendation models into the Microsoft News platform.
ç§ãŸã¡ã¯ã€PLMã‚’æ´»ç”¨ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã‚’Microsoft Newsãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã«å°å…¥ã—ã¾ã—ãŸã€‚
Our NAML-UniLM model was used to serve users in English-speaking markets, including EN-US, EN-GB, EN-AU, EN-CA and EN-IN.
å½“ç¤¾ã®NAML-UniLMãƒ¢ãƒ‡ãƒ«ã¯ã€EN-USã€EN-GBã€EN-AUã€EN-CAã€EN-INã‚’å«ã‚€**è‹±èªåœå¸‚å ´**ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚ŒãŸã€‚
The online flight experimental results have shown a gain of 8.53% in click and 2.63% in pageview against the previous news recommendation model without pre-trained language model.
ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ•ãƒ©ã‚¤ãƒˆã®å®Ÿé¨“çµæœã¯ã€PLMã‚’ä½¿ç”¨ã—ãªã„å¾“æ¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ã€ã‚¯ãƒªãƒƒã‚¯æ•°ã§8.53%ã€ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼ã§2.63%ã®åˆ©å¾—ã‚’ç¤ºã—ã¾ã—ãŸã€‚
In addition, our NAML-InfoXLM model was used to serve users in other 43 markets with different languages.
ã•ã‚‰ã«ã€æˆ‘ã€…ã®NAML-InfoXLMãƒ¢ãƒ‡ãƒ«ã¯ã€è¨€èªã®ç•°ãªã‚‹ä»–ã®43ã®å¸‚å ´ã®ãƒ¦ãƒ¼ã‚¶ã«ã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã™ã‚‹ãŸã‚ã«ã‚‚ä½¿ç”¨ã•ã‚ŒãŸã€‚
The online flight results show an improvement of 10.68% in click and 6.04% in pageview.
ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ»ãƒ•ãƒ©ã‚¤ãƒˆã®çµæœã¯ã€ã‚¯ãƒªãƒƒã‚¯æ•°ã§10.68%ã€ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼ã§6.04%ã®æ”¹å–„ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
These results validate that incorporating pre-trained language models into news recommendation can effectively improve the recommendation performance and user experience of online news services.
ã“ã‚Œã‚‰ã®çµæœã¯ã€PLMã‚’ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã«çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µãƒ¼ãƒ“ã‚¹ã®æ¨è–¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹ã‚’åŠ¹æœçš„ã«æ”¹å–„ã§ãã‚‹ã“ã¨ã‚’æ¤œè¨¼ã—ã¦ã„ã‚‹ã€‚

# 4. Conclusion çµè«–

In this paper, we present our work on empowering personalized news recommendation with pre-trained language models.
æœ¬ç¨¿ã§ã¯ã€äº‹å‰ã«å­¦ç¿’ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®æˆ‘ã€…ã®ç ”ç©¶ã‚’ç´¹ä»‹ã™ã‚‹ã€‚
We conduct extensive offline experiments on both English and multilingual news recommendation datasets, and the results show incorporating pre-trained language models can effectively improve news modeling for news recommendation.
è‹±èªã¨å¤šè¨€èªã®ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å®Ÿé¨“ã‚’è¡Œã£ãŸçµæœã€äº‹å‰ã«è¨“ç·´ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã®ãŸã‚ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’åŠ¹æœçš„ã«æ”¹å–„ã§ãã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸã€‚
In addition, our PLM-empowered news recommendation models have been deployed to a commercial news platform, which is the first public reported effort to empower real-world large-scale news recommender systems with PLMs.
ã•ã‚‰ã«ã€ç§ãŸã¡ã®PLMã‚’åˆ©ç”¨ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã¯ã€å•†æ¥­çš„ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã«å°å…¥ã•ã‚Œã¦ãŠã‚Šã€ã“ã‚Œã¯ã€PLMã‚’åˆ©ç”¨ã—ã¦å®Ÿéš›ã®å¤§è¦æ¨¡ãªãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚’å¼·åŒ–ã™ã‚‹æœ€åˆã®å–ã‚Šçµ„ã¿ã¨ã—ã¦å…¬ã«å ±å‘Šã•ã‚Œã¦ã„ã‚‹ã€‚
The online flight results show significant improvement in both click and pageview in a large number of markets with different languages.
ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ»ãƒ•ãƒ©ã‚¤ãƒˆã®çµæœã¯ã€ã‚¯ãƒªãƒƒã‚¯æ•°ã€ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼æ•°ã¨ã‚‚ã«ã€è¨€èªã®ç•°ãªã‚‹å¤šãã®å¸‚å ´ã§å¤§å¹…ãªæ”¹å–„ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

## å‚è€ƒ

- EBNR: Embedding-based news recommendation for millions of users
- NAML: Neural News Recommendation with Attentive Multi-View
Learning. 
- NPA: Neural news recommendation with personalized attention.
- LSTUR: Neural News Recommendation with Long-and Short-term User Representations.
- NRMS: Neural News Recommendation with Multi-Head Self-Attention.

ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ç”¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã‚’ã‚ã‚“ã¾ã‚ŠçŸ¥ã‚‰ãªã„ã®ã§ã€å¹´æœ«å¹´å§‹ã§ã–ã£ãã‚Šèª¿ã¹ã‚ˆã†ã¨æ€ã£ã¦ã‚‹ã€‚
2021ã®https://arxiv.org/pdf/2104.07413.pdf ã«ã¦ã€æœ€è¿‘ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä»¥ä¸‹ã®5ã¤ã‚’æŒ™ã’ã¦ã„ãŸã®ã§ã€ã“ã®è¾ºã‚Šã‚’æ¼ã£ã¦ã¿ã‚ˆã†ã‹ãªã€‚

- [EBNR: Embedding-based news recommendation for millions of users](http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p1933.pdf)
- [NAML: Neural News Recommendation with Attentive Multi-View Learning.](https://arxiv.org/abs/1907.05576)
- [NPA: Neural news recommendation with personalized attention.](https://arxiv.org/abs/1907.05559)
- [LSTUR: Neural News Recommendation with Long-and Short-term User Representations.](https://aclanthology.org/P19-1033/)
- [NRMS: Neural News Recommendation with Multi-Head Self-Attention.](https://aclanthology.org/D19-1671/)

ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®åŸ‹ã‚è¾¼ã¿ã‚’ä½œã‚‹éƒ¨åˆ†ã¨ã„ã†ã‚ˆã‚Šã¯ã€ãƒ¦ãƒ¼ã‚¶ã®åŸ‹ã‚è¾¼ã¿ã‚’ä½œã‚‹éƒ¨åˆ† (i.e. user encoder) ã¨ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¨ãƒ¦ãƒ¼ã‚¶ã®åŸ‹ã‚è¾¼ã¿ã‹ã‚‰æ¨è–¦ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºã™ã‚‹éƒ¨åˆ† (i.e. click prediction module) ã‚’å‚è€ƒã«ã—ãŸã„...!