# タイトル: 「反実仮想機械学習」を参考に、推薦タスクの実務でよく使われてるtwo-towerモデルを勾配ベースのオフ方策学習で最適化したいんだー！

## ブログ構成案

- 3行まとめ
- はじめに: 本ブログを書く意味付け
- two-towerモデルはこんな感じの雰囲気
- 勾配ベースのオフ方策学習はこんな感じの雰囲気
- two-towerモデルを勾配ベースのオフ方策学習するための作戦をまとめる
- pytorchでいざ実装!
- open bandit pipelineパッケージの合成データを使って、性能を評価してみる。
- おわりに

# 以下、本文

## はじめに: 本ブログを書く意味付け

## two-towerモデルはこんな感じの雰囲気

## 勾配ベースのオフ方策学習はこんな感じの雰囲気

- そもそもオフ方策学習 (Off-policy learning, OPL) とは??
  - ログデータを使って、新しい方策(policy)を学習する手法。
  - ex. 既存の推薦システムが「ニュース記事A」を50%、「ニュース記事B」を30%の確率で推薦しているとする。
    - その推薦結果とユーザから得られた報酬（ex. クリックしたか否か）を記録し、そのログデータ(i.e. バンディットフィードバック)を使って、より良い (ex. 累積報酬の期待値を最大化する, etc.) 推薦方策を学習する。

- OPLの基本的な2種類のアプローチ:
  - OPLには、2つの基本的なアプローチとして、回帰ベースと勾配ベースの2つがある。
    -  (両者を混ぜたようなアプローチなども近年はあるみたい)
  - 1. 回帰ベース(Regression-based)
    - 従来の教師あり機械学習っぽいアプローチ。
    - 各アクションの報酬期待値 $E_{p(r|x,a)}[r] = q(x,a)$ を推定するモデル $\hat{q}(a,x)$ を学習し、予測関数 $\hat{q}(a,x)$ の出力値を直接使って意思決定を行う!
      - 例: hogehgoe
    - (すなわち、CTR予測モデルなど、教師あり学習を使った推薦モデルのアプローチの多くは、ほぼほぼ回帰ベースのアプローチに分類できそう...!!:thinking:)
  - 2. 勾配ベース(Gradient-based)
    - 事前に定義した方策性能 (policy value) $V(\pi_{\theta})$ が高くなるように、方策のパラメータ $\theta$ を最適化する方法。
    - 以下のようなパラメータ更新則で方策のパラメータを更新していくというのが、勾配ベースアプローチの基本的なアイデア。
      - $\theta_{t+1} \leftarrow \theta_{t} + \nu \nabla_{\theta} V(\pi_{\theta})$
      - ここで、$\nu$ は学習率。$\nabla_{\theta} V(\pi_{\theta})$ は方策性能の勾配 (方策勾配、policy gradient)。
    - ただし、真の方策性能 $V(\pi_{\theta})$ は未知であるため、その**方策勾配 $\nabla_{\theta} V(\pi_{\theta})$ も当然未知**になる。
    - -> そこで勾配ベースのオフ方策学習では、**何らかの方法で方策勾配を推定して、その推定値を使って方策のパラメータを更新していく!**（仕方なく！本当は真の方策勾配でパラメータ更新をしたいが...!!）
      - ex. オフ方策評価(OPE, Off-policy evaluation)でも出てくるIPS推定量, DR推定量などを使う

- 方策性能(Policy value)と、その方策勾配(Policy gradient)の関係
  - 例えば、任意の方策 $\pi_{\theta}$ に対して、その性能を以下のように定義する場合を考える
    - （意味合いは「ある環境 $p(x)$ において、方策 $\pi_{\theta}$ を稼働させて行動を選択した場合に得られる報酬の期待値」みたいなイメージ...! オフ方策評価でもよく使われてる印象...! :thinking:）
    - $V(\pi_{\theta}) = E_{p(x) \pi_{\theta}(a|x) p(r|x,a)}[r] = E_{p(x)\pi_{\theta}(a|x)}[q(x, a)]$
  - この場合、方策性能に対するモデルパラメータ $\theta$ の勾配、すなわち方策勾配 $\nabla_{\theta} V(\pi_{\theta})$ は以下のように表現できる。
    - (要するに、勾配の式に方策性能の定義を代入しただけ!)
    - $\nabla_{\theta} V(\pi_{\theta}) = \nabla_{\theta} E_{p(x)\pi_{\theta}(a|x)}[q(x, a)]$
  - 上記の方策勾配の定義の右辺を式変形していくといくと、最終的に以下のようになる!
    - $\nabla_{\theta} V(\pi_{\theta}) = E_{p(x)\pi_{\theta}(a|x)}[q(x, a) \nabla_{\theta} \log \pi_{\theta}(a|x)]$
    - (元の定義式に対して、「期待値の勾配」=「勾配の期待値」の性質だったり、「同時分布の期待値」=「条件付き期待値の期待値」の性質だったり、ログトリック(条件付き確率の勾配の変形)を使ったり、...などなどしていい感じに式変形していく...!)
  - この真の方策勾配の式を元に、IPS推定量だったりDR推定量だったりを駆使して、いい感じの推定量を作って、方策のモデルパラメータをいい感じに更新していく！というのが勾配ベースのオフ方策学習の基本的な流れ...!:thinking:

- 方策勾配の代表的な推定方法1: IPS推定量
  - hoge
- 方策勾配の代表的な推定方法2: DR推定量
  - hoge






## two-towerモデルを勾配ベースのオフ方策学習するための作戦をまとめる

## pytorchでいざ実装!

## open bandit pipelineパッケージの合成データを使って、性能を評価してみる。
