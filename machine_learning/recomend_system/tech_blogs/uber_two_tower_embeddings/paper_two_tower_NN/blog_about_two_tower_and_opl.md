# タイトル: 「反実仮想機械学習」を参考に、推薦タスクの実務でよく使われてるtwo-towerモデルを勾配ベースのオフ方策学習で最適化したいんだー！

## ブログ構成案

- 3行まとめ
- はじめに: 本ブログを書く意味付け
- two-towerモデルはこんな感じの雰囲気
- 勾配ベースのオフ方策学習はこんな感じの雰囲気
- two-towerモデルを勾配ベースのオフ方策学習するための作戦をまとめる
- pytorchでいざ実装!
- open bandit pipelineパッケージの合成データを使って、性能を評価してみる。
- おわりに

# 以下、本文

## はじめに: 本ブログを書く意味付け

## two-towerモデルはこんな感じの雰囲気

- two-towerモデルとは?
  - 推薦システムや検索タスクなどでよく使われる、**深層学習ベースのモデルの一つ**。
  - 特に**大規模なコーパス (数百万~数億規模のアイテム)から関連アイテムを高速に取得するretreivalフェーズ**に強みを持つ。
    - 2-stages推薦でいうところの1ステージ目「retrieval」の部分を担当するモデルとして使われることが多い印象。2ステージ目の「reranking」の部分は別のリッチなモデルが使われたりするイメージ...!:thinking:
- 構造上の特徴
  - **2つの encoder (i.e. tower)** から構成されるモデル。
    - (ちなみにNLP分野では「**デュアルエンコーダ(dual encoder)**」と呼ばれているらしい...!:thinking:)
  - 1つ目の encoder (ユーザタワー, クエリタワーとか呼ばれる) : クエリ(ユーザ情報やコンテキスト)を入力として受け取り、埋め込みに変換。
  - 2つ目の encoder (アイテムタワーとか呼ばれる) : 推薦や検索の候補となるアイテム(商品、ニュース記事、etc.)を入力として受け取り、埋め込みに変換。
  - two-towerモデルは**クエリとアイテムを共通の埋め込み空間**にマッピングする。推論時はその埋め込み空間上で内積で類似度を計算 -> 上位K個のアイテムを取得することで推薦や検索を行う。
- two-towerモデルの利点
  - スケーラブルで高速:
    - 大規模なデータセットでも高速に推論できる (事前にアイテム埋め込みをインデックス化し、近傍探索でretreiveできる)
    - **推論時(検索時)の計算コストが低い (クエリの埋め込みを計算 -> 事前計算されたアイテム埋め込みと内積計算するだけ)**
  - 特徴量の柔軟な活用
    - 任意のユーザやアイテムの特徴量を多層NNで非線形変換して埋め込みを作るので、**特徴量エンジニアリングの自由度が高い**
      - 画像特徴量、NLP的な特徴量なども組み込みやすい。
      - 扱いたい特徴量に応じて各towerの構造も柔軟にアレンジできる。ニュース推薦の分野ではアイテムタワー内部にAttention機構を採用したりしてた...!:thinking:
    - また純粋な行動履歴ベースの行列分解(Matrix Factorization)モデルよりも、アイテムやユーザの特徴量を柔軟に組み込めるので、コールドスタートアイテム、コールドスタートユーザに対応しやすい。
- 参考資料:
  - [Deep Neural Networks for YouTube Recommendations]()
    - 2016年のYoutubeの動画推薦システムについて紹介してる論文。
    - DNNを使った2-stages推薦を採用しており、candidate retrieve用のモデルも candidate ranking用のモデルもtwo-towerモデルを採用してた。
    - 「two-towerモデルを提案!」という内容ではなく、「Youtubeの推薦の困難さをこういう工夫で対処してますよ!」的なtipsを紹介してくれてる様な印象の論文:thinking:
  - [Mixed Negative Sampling for Learning Two-tower Neural Networks in Recommendations]()
    - 2020年のGoogle Playの推薦システムを取り扱った論文。こちらは主に2-stages推薦の1ステージ目candidate retrieveのモデルとしてtwo-towerモデルを採用してた。論文のメイントピックは、two-towerモデル学習時のバイアス問題に対処するためのネガティブサンプリング方法について。
  - [Innovative Recommendation Applications Using Two Tower Embeddings at Uber](https://www.uber.com/en-JP/blog/innovative-recommendation-applications-using-two-tower-embeddings/)
    - こちらは論文ではないが、Uberさんのテックブログでtwo-towerモデルを使った推薦システムの応用事例が紹介されてた。
    - ちなみに自分はこのブログを読んでtwo-towerアーキテクチャを知り、一つ目の論文に辿り着きました!感謝...!:pray:
  - [Two-Towerモデルと近似最近傍探索による候補生成ロジックの導入](https://blog.recruit.co.jp/data/articles/two-tower-model/)
    - リクルートさんのテックブログ。
    - ちなみに自分はこのブログから、二つ目の論文に辿り着きました!感謝...!:pray:

## 勾配ベースのオフ方策学習はこんな感じの雰囲気

- そもそもオフ方策学習 (Off-policy learning, OPL) とは??
  - ログデータを使って、新しい方策(policy)を学習する手法。
  - ex. 既存の推薦システムが「ニュース記事A」を50%、「ニュース記事B」を30%の確率で推薦しているとする。
    - その推薦結果とユーザから得られた報酬（ex. クリックしたか否か）を記録し、そのログデータ(i.e. バンディットフィードバック)を使って、より良い (ex. 累積報酬の期待値を最大化する, etc.) 推薦方策を学習する。

- OPLの基本的な2種類のアプローチ:
  - OPLには、2つの基本的なアプローチとして、回帰ベースと勾配ベースの2つがある。
    -  (両者を混ぜたようなアプローチなども近年はあるみたい)
  - 1. 回帰ベース(Regression-based)
    - 従来の教師あり機械学習っぽいアプローチ。
    - 各アクションの報酬期待値 $E_{p(r|x,a)}[r] = q(x,a)$ を推定するモデル $\hat{q}(a,x)$ を学習し、予測関数 $\hat{q}(a,x)$ の出力値を直接使って意思決定を行う!
      - 例: hogehgoe
    - (すなわち、CTR予測モデルなど、教師あり学習を使った推薦モデルのアプローチの多くは、ほぼほぼ回帰ベースのアプローチに分類できそう...!!:thinking:)
  - 2. 勾配ベース(Gradient-based)
    - 事前に定義した方策性能 (policy value) $V(\pi_{\theta})$ が高くなるように、方策のパラメータ $\theta$ を最適化する方法。
    - 以下のようなパラメータ更新則で方策のパラメータを更新していくというのが、勾配ベースアプローチの基本的なアイデア。
      - $\theta_{t+1} \leftarrow \theta_{t} + \nu \nabla_{\theta} V(\pi_{\theta})$
      - ここで、$\nu$ は学習率。$\nabla_{\theta} V(\pi_{\theta})$ は方策性能の勾配 (方策勾配、policy gradient)。
    - ただし、真の方策性能 $V(\pi_{\theta})$ は未知であるため、その**方策勾配 $\nabla_{\theta} V(\pi_{\theta})$ も当然未知**になる。
    - -> そこで勾配ベースのオフ方策学習では、**何らかの方法で方策勾配を推定して、その推定値を使って方策のパラメータを更新していく!**（仕方なく！本当は真の方策勾配でパラメータ更新をしたいが...!!）
      - ex. オフ方策評価(OPE, Off-policy evaluation)でも出てくるIPS推定量, DR推定量などを使う

- 方策性能(Policy value)と、その方策勾配(Policy gradient)の関係
  - 例えば、任意の方策 $\pi_{\theta}$ に対して、その性能を以下のように定義する場合を考える
    - （意味合いは「ある環境 $p(x)$ において、方策 $\pi_{\theta}$ を稼働させて行動を選択した場合に得られる報酬の期待値」みたいなイメージ...! オフ方策評価でもよく使われてる印象...! :thinking:）
    - $V(\pi_{\theta}) = E_{p(x) \pi_{\theta}(a|x) p(r|x,a)}[r] = E_{p(x)\pi_{\theta}(a|x)}[q(x, a)]$
  - この場合、方策性能に対するモデルパラメータ $\theta$ の勾配、すなわち方策勾配 $\nabla_{\theta} V(\pi_{\theta})$ は以下のように表現できる。
    - (要するに、勾配の式に方策性能の定義を代入しただけ!)
    - $\nabla_{\theta} V(\pi_{\theta}) = \nabla_{\theta} E_{p(x)\pi_{\theta}(a|x)}[q(x, a)]$
  - 上記の方策勾配の定義の右辺を式変形していくといくと、最終的に以下のようになる!
    - $\nabla_{\theta} V(\pi_{\theta}) = E_{p(x)\pi_{\theta}(a|x)}[q(x, a) \nabla_{\theta} \log \pi_{\theta}(a|x)]$
    - (元の定義式に対して、「期待値の勾配」=「勾配の期待値」の性質だったり、「同時分布の期待値」=「条件付き期待値の期待値」の性質だったり、ログトリック(条件付き確率の勾配の変形)を使ったり、...などなどしていい感じに式変形していく...!)
  - この真の方策勾配の式を元に、IPS推定量だったりDR推定量だったりを駆使して、いい感じの推定量を作って、方策のモデルパラメータをいい感じに更新していく！というのが勾配ベースのオフ方策学習の基本的な流れ...!:thinking:
    - よって実務上のパラメータ更新則は以下になる (方策勾配に推定値を表すhatがついてる点が違い!)
    - $\theta_{t+1} <- \theta_{t} + \nu \hat{\nabla_{\theta}} V(\pi_{\theta})$

- 方策勾配の代表的な推定方法1: IPS推定量
  - オフ方策評価でも王道アプローチ的な立ち位置にある「IPS推定量」の考え方を応用して、方策勾配を推定することを考える。
    - あるデータ収集方策 $\pi_{0}$ によって収集されたログデータ $D$ が与えられた時、真の方策勾配に対するIPS推定量は以下のようになる。

$$
\hat{\nabla_{\theta} V_{IPS}(\pi_{\theta};D)} := \frac{1}{n} \sum_{i=1}^{n} w(x_i, a_i) r_i \nabla_{\theta} \log \pi_{\theta}(a_i|x_i)
\tag{5.12}
$$

- なお式中の $w(x, a):= \pi_{\theta}(a|x) / \pi_{0}(a|x)$ は、学習中の方策とデータ収集方策による行動選択確率の比であり、重要度重みと呼ぶ。
- オフ方策評価におけるIPS推定量との対応関係:
  - オフ方策評価のIPS推定量: $\hat{V}_{IPS}(\pi_{\theta};D) := \frac{1}{n} \sum_{i=1}^{n} w(x_i, a_i) r_i$
  - オフ方策学習だと、推定目標が評価方策の性能 $V(\pi)$ から方策勾配 $\nabla_{\theta} V(\pi_{\theta})$ に変わる。
  - -> なので、それに応じて報酬 $r_i$ だけじゃなく、**学習中の方策による行動選択確率の対数の勾配 $\nabla_{\theta} \log \pi_{\theta}(a_i|x_i)$ が推定量の定義に含まれるようになった**のが些細な違い!

- 方策勾配の代表的な推定方法2: DR推定量
  - オフ方策評価と同様、IPS推定量の問題: 重要度重みに起因して発生するバリアンス!
  - IPS推定量の不偏性を維持しつつバリアンスを減少させる方法 -> メジャーなのがDR推定量!!
  - データ収集方策 $\pi_{0}$ によって収集されたログデータ $D$ が与えられた時、真の方策勾配に対するDR推定量は以下のようになる。

$$
\hat{\nabla_{\theta} V_{DR}(\pi_{\theta};D)} := \frac{1}{n} \sum_{i=1}^{n} 
% \left\{
\Big\{
  w(x_i, a_i) (r_i - \hat{q}(x_i, a_i)) \nabla_{\theta} \log \pi_{\theta}(a_i|x_i) 
  \\
  + E_{\pi_{\theta}(a|x)}[\hat{q}(x_i, a) \nabla_{\theta} \log \pi_{\theta}(a|x)]
\Big\}
$$

- なお式中の...
  -  $\hat{q}(x, a)$ は、期待報酬関数 $q(x, a)$ に対する推定モデル!
     - 例えば回帰ベースのアプローチと同様に、ログデータ $D$ を用いて損失関数を最小化して求めたモデルを使ったりする!
  - $w(x, a)$ はIPS推定量と同様の重要度重み!
- オフ方策評価におけるDR推定量との対応関係
  - 基本的には同様のアイデア!
    - 「ログデータを用いて事前に学習しておいた期待報酬関数に対する推定モデル $\hat{q}(x, a)$ をベースラインとして使うことで、**IPS推定量からのバリアンス減少を狙う！**」

## two-towerモデルを勾配ベースのオフ方策学習するための作戦をまとめる

## pytorchでいざ実装!

## open bandit pipelineパッケージの合成データを使って、性能を評価してみる。
