---
marp: true
theme: default
paginate: true
header: 'RecSysの実運用で人気なTwo-Towerモデル! と言ってもいろんなバリエーションがありそうだなぁと思った話'
footer: 'Y-tech AI勉強会 2025/10/07 https://x.com/moritama7431'
---

# RecSysの実運用で人気なTwo-Towerモデル! と言っても、いざ採用しようとするといろんなバリエーションがありそうだなぁと思った話

<!--

章立てのアウトライン

- なぜ本トピックを話したいんだっけ??
- はじめに: そもそもTwo-towerモデルって??なんで人気なんだっけ??
  - RecSysの実運用でTwo-Towerモデルが結構人気
    - Googleさん、Uberさん、リクルートさん、ZOZOさん、日経さん、etc...
  - Two-Towerモデルってざっくり何だっけ??
    - 2つの独立したニューラルネットワーク（タワー）を用いた推薦モデル。
    - NLP(自然言語処理)の分野におけるdual encoderモデルを推薦・検索タスクに応用したもの。
  - なんでRecSysの実運用で人気なんだっけ??
    - NNの恩恵を受けつつ、推論時のスケーラビリティと処理速度を両立させやすい。
    -
- 主張1: Two-towerのネットワーク構造のバリエーションが色々ある話
  - Two-tower型アーキテクチャの基本構成
    - User Tower (検索分野ではQuery Tower)
    - Item Tower
    - Predictionモジュール
  - タワーのバリエーション
    - 基本的には、だんだんと出力層にかけて次元が小さくなっていくMLP的な構造 -> これが「タワー」と呼ばれる所以。
    - 特にnext-item predictionタスクを重視したRecSysの場合は、タワーにAttentionやRNNを組み込むバリエーションもある。
    - また、両タワーの中間部分のパラメータを共通化してる事例もある(Uberさんの事例など)。
  - predictionモジュールのバリエーション
    - おそらく最も一般的なのは、内積(dot-product)でのスコア計算
    - でも内積だけじゃない! 軽量モデルを採用することもあり! (Youtubeにおけるrerankingフェーズでのtwo-towerモデルの活用事例を紹介)
- 主張2: Two-towerの学習方法のバリエーションが色々ある話
  - そもそもRecSysタスクを定式化してみよう
  - タスクの性質ゆえに代理学習問題(surrogate learning problem)も結構採用されがち!
  - 回帰ベースのアプローチ: 予測タスクとして学習する!
  - contrastive learningベースのアプローチ(回帰ベースの一種?)
  - 勾配ベースのアプローチ: 累積報酬最大化タスクとして学習する!
- おわりに: まとめ
  - なぜTwo-TowerモデルがRecSysの実運用で人気なのか -> NNの恩恵を受けつつ、スケーラビリティと推論速度を両立させやすいから!
  - Two-Towerモデルの構造のバリエーションが色々ある話 -> 色々あるが、とりあえずまずはMLP的なタワー、内積ベースのpredictionモジュールから始めて良さそう!
  - Two-Towerモデルの学習方法のバリエーションが色々ある話 -> 自社のユースケース(フェーズの違い、トラフィック量、過去に収集した観測データの性質, RecSysで改善したい指標)などに応じて、試行錯誤しつつ有効なアプローチを選びたい。
  - 思ったこと: Two-tower (に限らずNN系のアプローチ) の自由度の高さは、デメリットともメリットとも言える。
      - -> 「色々選択しないといけなくて大変」とも取れるが、「自社のユースケースに合わせて自由に設計できる」とも取れる。特にモデルアーキテクチャや目的関数の設計は、DS・ML・MLOpsエンジニアの腕の見せ所とも言えそう...!!
      - (そもそも多様なNNアーキテクチャの中でTwo-towerを選ぶ意思決定をすること、その理由をきちんと言語化して納得できる説明ができること、もMLOpsを司るエンジニアとして腕の見せどころのはず...!:thinking:)
-->

---

## なんで今回このトピックを話したいんだっけ??

- ここ数年、RecSysの実運用でTwo-Towerモデルが結構人気!
- 実は我々もまさにTwo-Towerモデルを採用し運用に移ろうとしてるフェーズ。
  - ただ当然「単に人気だからTwo-Towerモデルを採用しよう!」ではなく、「全てのアーキテクチャはトレードオフ」であることを念頭に置いた上で、なぜ実運用で人気なのか、自社ドメインで採用するべきなのかをきちんと言語化・説明できるようにしたい。
- また、実際に運用しようとすると、**Two-Towerモデルにも色々なバリエーションがあって、DS・MLエンジニア・MLOpsエンジニアが意思決定しないといけないことが結構ある**なぁと感じた。

---



## 今日のアウトライン

1. はじめに: そもそもTwo-towerモデルって??なんで人気なんだっけ??
2. 主張1: Two-towerのネットワーク構造のバリエーションが色々ある!
3. 主張2: Two-towerの学習方法のバリエーションが色々ある!
4. まとめ

---

# はじめに

---

## RecSysの実運用でTwo-Towerモデルが結構人気

- Googleさんの論文(Youtube, Google Playの事例)
  - [Deep Neural Networks for YouTube Recommendations](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/45530.pdf)
  - [Mixed Negative Sampling for Learning Two-tower Neural Networks in Recommendations](https://storage.googleapis.com/gweb-research2023-media/pubtools/6090.pdf)
- Uberさんの事例: [Innovative Recommendation Applications Using Two Tower Embeddings at Uber](https://www.uber.com/en-JP/blog/innovative-recommendation-applications-using-two-tower-embeddings/)
- リクルートさんの事例: [Two-Towerモデルと近似最近傍探索による候補生成ロジックの導入](https://blog.recruit.co.jp/data/articles/two-tower-model/#%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E5%89%8D%E5%87%A6%E7%90%86)
- ZOZOさんの事例: [ZOZOTOWNホーム画面のパーソナライズ - Two-Towerモデルで実現するモジュールの並び順最適化](https://techblog.zozo.com/entry/zozotown-home-module-personalization-v1)
- 日経さんの事例: [日経電子版のアプリトップ「おすすめ」をTwo Towerモデルでリプレースしました](https://hack.nikkei.com/blog/replace_ai_rec_by_two_tower/)
- etc...

---

## Two-Towerモデルってざっくり何だっけ??　(1/2)

![alt text](image.png)


---

## Two-Towerモデルってざっくり何だっけ?? (2/2)

- 2つの独立したニューラルネットワーク（タワー）を用いた推薦モデル。
  - NLP分野のdual encoderを推薦・検索タスクに応用したもの。
- two-towerモデルの構成要素
  - User Tower: ユーザ特徴を埋め込みベクトルに変換。
  - Item Tower: アイテム特徴を埋め込みベクトルに変換。
  - predictionモジュール: 両タワーの出力を元に(ユーザ, アイテム)ペアに関する何らかのスコアを算出。スコアを元に何を推薦するかの意思決定を行う。

---

## なんでRecSysの実運用で人気なんだっけ?? (1/2)

### 結論: **DNN系モデルの恩恵は受けつつ、推論時のスケーラビリティと処理速度をコスト効率良く実現できるから!**

- まず非DNN系モデルと比較して...
  - 様々な種類・形状の特徴量を柔軟に考慮することができる表現力の高さ!
- 他のDNN系モデルと比較して...
  - **推論コストが低く実現できる!**
    - ユーザとアイテムの情報を独立した2つのタワーで処理できる構造により、**計算コストの重い処理を事前計算しておける!**
    - **ANN検索との相性 ◎** なので、更にスケーラビリティ up も!

---

## なんでRecSysの実運用で人気なんだっけ?? (2/3)

### Two-Tower型アーキテクチャは、**大量のレコードに対してリアルタイムで軽量にコスト効率よく推論できる稀有な選択肢**である。

- 一般的に、 DNNモデルは推論コストが高い。
  - 大量ユーザに対してリアルタイム推論でレイテンシ要件を満たすためには、GPU付きの高価なサーバを複数台、常時稼働させる必要があったり...!
- 一方で、**Two-Tower型アーキテクチャは、ユーザとアイテムの情報を独立した2つのタワーでそれぞれ処理できる**のが最大の強み。
  - この構造により、**計算コストの重い処理を事前に計算しておける**ので、リアルタイムに推薦結果を作るときは超軽量&シンプルな計算だけで済む！
  - ex. ユーザタワーは日次バッチ, アイテムタワーはストリーミングorマイクロバッチ、最後の内積計算のみリアルタイム演算, etc.

---

## なんでRecSysの実運用で人気なんだっけ?? (3/3)

RecSysの実運用で**Two-Towerモデルが活躍してるシーンは大きく3種類な印象**...! :thinking:

1. レイテンシ要件の厳しい**リアルタイム推薦**をコスト効率よく実現したいケース
   - ex. GoogleさんのYoutube, Google Playの事例,　Uberさんの事例, 日経さんの事例, etc.
2. **膨大な推薦アイテム候補に対する推薦**をコスト効率よく実現したいケース
   - ex. GoogleさんのYoutube, Google Playの事例, Uberさんの事例, リクルートさんの事例, ZOZOさんの事例, etc.
3. ユーザやアイテムのrichな**特徴量としての埋め込み表現**を学習する用途。
   - ex. Netflixさんの事例, Uberさんの事例, etc.



---

<!-- _class: lead -->

# 主張1

ネットワーク構造のバリエーション

---

## Two-tower型アーキテクチャの基本構成

3つの主要コンポーネント:

1. User Tower (Query Tower)
2. Item Tower
3. Predictionモジュール

---

## タワーのバリエーション (1/3): 基本はMLP

だんだんと出力層にかけて次元が小さくなっていく構造 = これが「タワー」と呼ばれる所以。

```
各種特徴量の入力層
     ↓
各種特徴量の前処理層 (Entity Embedding, Normalization, etc.)
     ↓
全結合層 (ex. 1024次元)
     ↓
全結合層 (ex. 512次元)
     ↓
出力層 (ex. 256次元)
```

Googleさんの論文達(Youtube, Google Playの事例)やUberさんの事例では、基本的にはMLP的な構造を採用してた!

---

## タワーのバリエーション (2/3): Attention・RNN

特に「**ユーザがアイテムを消費する順序**」を重視する(=Sequential Recommendationと呼ばれる分野)ようなRecSysの場合は、タワー(特にユーザタワー)にAttentionやRNNを組み込むバリエーションもある。

![]()

(SASRecモデル, BERT4Recモデルなど。でも実運用でどれだけ使われてるかは不明...!:thinking:)

---

## タワーのバリエーション (3/3): タワー間でのレイヤー共有(Layer sharing between two towers)

両タワーの一部分のレイヤーを共通化してる事例もあった(Uberさんの事例!)

- 具体的には、両タワーが同じUUID Embedding Layerを共有(ユーザidとお店idのentity embedding層を共有してるって話っぽい...!:thinking:)

---

ブログで紹介されてたレイヤー共有の採用理由:

- メリット1: モデルサイズの削減
  - eater id (ユーザid?) やstore_id(お店id?) のような**high-cardinality(高カーディナリティ)を持つカテゴリ特徴量**を扱う必要があった。
  - これらのembedding層を両タワーで別々に持つと、モデルサイズが爆発的に大きくなる
  - レイヤー共有によりモデルサイズを20倍削減できたとのこと。
- メリット2: モデルの推薦性能の向上
  - Uberさんのホームフィードの事例では、このレイヤー共有が推薦性能の向上に非常に重要だった。(オフライン, オンライン評価でのablation studyで性能向上を実証できたとのこと)。

まあこれは直感的にもレイヤー共有した方が良い気はする。**同じ意味のカテゴリ特徴量を両タワーで扱うなら、そのカテゴリ値の埋め込みは同じものを使いたい**よね...!:thinking:

---

## predictionモジュールのバリエーション (1/2): 内積!

たぶん最も一般的: **内積(dot-product)**

- シンプルで高速。
- 埋め込み空間での類似度として解釈しやすい。
- 特にtwo-towerを**膨大な推薦アイテム候補を絞り込む用途(candidate retrieve)**で採用する場合、**ANN(Approximate Nearest Neighbor, 近似最近傍)検索**によって推論を更に高速化できる!

---

## predictionモジュールのバリエーション (2/2): 内積以外!

でも内積だけじゃない! 軽量モデルを採用することもあるっぽい!

- Youtubeの**two-towerモデルを少数アイテムのリランキング(candidate reranking)フェーズで採用**した事例では、ロジスティック回帰モデルをpredictionモジュールに採用してたっぽい!

---

<!-- _class: lead -->

# 主張2

学習方法のバリエーション

---

## そもそもRecSysタスクを定式化してみよう

目的: ユーザーuに対して、最も関連性の高いアイテムiを推薦する

課題:
- 明示的なラベルが少ない（暗黙的フィードバックが中心）
- データ収集バイアス
- スケーラビリティ

---

## 代理学習問題(surrogate learning problem)

真の目的を直接最適化するのは困難 → 代わりに関連する別のタスクを学習

代理タスクの例:
- クリック率予測
- 購入確率予測
- next-item予測

---

## 学習アプローチ 1: 回帰ベースアプローチ (予測誤差を最小化!)

予測タスクとして学習することで、累積報酬がより高くなるようなモデルを目指す!

- ユーザー-アイテムペアのスコアを予測
- 損失関数: MSE, Binary Cross Entropyなど

---

## 学習アプローチ 2: 対照学習(contrastive learning) (回帰ベースの一種と言える?)

ユーザ(クエリ)に対するポジティブ・ネガティブアイテムのペアを

- ネガティブサンプリングが重要
- 損失関数: Softmax Cross Entropy, BPRなど

---

## 学習アプローチ 3: 勾配ベースアプローチ　(累積報酬の推定値を最大化!)

推薦システムを強化学習問題として定式化

- 累積報酬最大化
- 長期的なユーザー満足度を最大化

---

<!-- _class: lead -->

# まとめ

- なぜTwo-TowerモデルがRecSysで人気なのか??
  - NNの恩恵を受けつつ、スケーラビリティと推論速度を両立させやすいから!
- Two-Towerモデルの構造のバリエーションが色々ある話
  - 色々あるが、とりあえずまずはMLP的なタワー、内積ベースのpredictionモジュールから始めて良さそう!
- Two-Towerモデルの学習方法のバリエーションが色々ある話。
  - 推薦タスクの性質上、いろんな代理学習問題(surrogate learning problem)が考えられる!
  - 自社のユースケース(フェーズの違い、トラフィック量、過去に収集した観測データの性質, RecSysで改善したい指標)などに応じて、試行錯誤しつつ有効なアプローチを選びたい!

---

## 思ったこと

- Two-tower (に限らずNN系のアプローチ) の自由度の高さは、「色々選択しないといけなくて大変」とも取れるが、「自社のユースケースに合わせて自由に設計できる」とも取れる。
- 特にモデルアーキテクチャや目的関数の設計は、DS・ML・MLOpsエンジニアの腕の見せ所とも言えそう...!!
  - そもそも多様なNNアーキテクチャの中でTwo-towerを選ぶ意思決定をすること、その理由をきちんと言語化して納得できる説明ができること自体が、MLOpsを司るエンジニアとして腕の見せどころのはず...!:thinking:

---

## (再掲) 参考文献

1. Google Play事例 (2020)
2. Uber事例 (2023)

---

<!-- _class: lead -->

# ご清聴ありがとうございました
