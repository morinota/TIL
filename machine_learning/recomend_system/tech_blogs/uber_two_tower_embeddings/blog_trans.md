## link リンク

https://www.uber.com/en-JP/blog/innovative-recommendation-applications-using-two-tower-embeddings/
https://www.uber.com/en-JP/blog/innovative-recommendation-applications-using-two-tower-embeddings/

# Innovative Recommendation Applications Using Two Tower Embeddings at Uber Uber における2つのタワー埋め込みを用いた革新的なレコメンデーション・アプリケーション

## Introduction

The Machine Learning (ML) team at Uber is consistently developing new and innovative components to strengthen our ML Platform (Michelangelo).
Uberの機械学習（ML）チームは、MLプラットフォーム（Michelangelo）を強化するために、常に新しく革新的なコンポーネントを開発しています。
In 2022, we took on a new and exciting challenge by making an investment into embeddings₁ , with a focus on Two-Tower Embeddings (TTE)₂.
2022年、私たちは2タワー・エンベッディング（TTE）₂に焦点を当てたエンベッディング₁への投資を行うことで、新たなエキサイティングな挑戦に挑んだ。
These embeddings create a magical experience by helping customers find what they’re looking for faster and easier.
これらのエンベッディングは、顧客が探しているものをより速く簡単に見つけられるようにすることで、魔法のような体験を生み出します。
Specifically, we focused on the modeling and infrastructure to build out Uber’s first TTE model.
具体的には、ウーバー初のTTEモデルを構築するためのモデリングとインフラに注力した。
We then used that to power our recommendation systems by generating and applying embeddings for eaters and stores.
私たちはそれを使って、**食べ手と店に対する埋め込みを生成**し、適用することで、レコメンデーション・システムを構築した。

Easy, right? Just kidding, but here’s a simple example of how it works for you.
簡単でしょう？というのは冗談だが、これが簡単な例だ。
Recommendation systems are critical in helping users like yourself discover and obtain a wide range of goods and services on our platform.
レコメンデーションシステムは、あなたのようなユーザが私たちのプラットフォームで様々な商品やサービスを発見し、手に入れるのを助けるために重要です。
At Uber, our systems are designed to optimize every step, starting from when you first open the app and see your home feed.
Uberのシステムは、最初にアプリを開いてホームフィードを見るところから、すべてのステップを最適化するように設計されています。
The order of restaurants on this feed is fueled by various rankers that generate the most relevant candidates (stores) to show you.
このフィード上のレストランの順序は、最も関連性の高い候補（店舗）を生成する様々なランカーによって供給されます。
The TTE model generates embeddings about both you, as well as the stores, and then feeds that information into the rankers to identify the best matches while simultaneously cutting down on computing time.
TTEモデルは、あなたとお店の両方に関するエンベッディングを生成し、**その情報をランカーに送り込む**。

Of course, new innovations also come with new bugs, challenges, and setbacks, but that is just part of the fun! Our team wasn’t going to give up easily, so nearly a year later our Two-Tower Embeddings approach launched and unlocked major scalability and reusability wins, on top of impressive performance improvements.
もちろん、新しいイノベーションには新しいバグや課題、挫折もつきものですが、それも楽しみのひとつです！私たちのチームは簡単にはあきらめませんでした。ほぼ1年後、私たちのツータワー・エンベッディング・アプローチが発表され、**目覚ましいパフォーマンス向上に加え、スケーラビリティと再利用性の大きな勝利**を手にしました。

This blog will cover what embeddings are, our architecture, challenges, and the thousands city-wide models we developed before we reached our single, global contextual model.
このブログでは、エンベッディングとは何か、私たちのアーキテクチャ、課題、そして私たちが単一のグローバルなcontextualモデルに到達するまでに開発した何千もの都市全体のモデルについて説明します。
Read on to learn the results of these wins, and what’s coming up next (spoiler: we aren’t just going to stop here)!
これらの勝利の結果、そして今後の展開（ネタバレ：私たちはここで終わるつもりはありません）については、続きをお読みください！

₁ An embedding is a rich representation for any entity such as: stores, eaters, items, drivers, locations, etc.
₁ 埋め込みは、店、食べる人、品物、運転手、場所など、あらゆるentityのための豊富な表現である。
It transforms human-friendly features (i.e., store menu, item price, eater preferred cuisine and past orders, etc.) into machine-learning-friendly vectors.
人間にやさしい特徴量（店のメニュー、商品の価格、食べる人の好みの料理、過去の注文など）を機械学習に適したベクトルに変換する。

₂ Two-tower embeddings are the embeddings generated by a special deep learning architecture named two towers [ref].
₂**Two-tower embeddingsは、ツータワー[ref]と名付けられた特別なディープラーニングアーキテクチャによって生成された埋め込み**である。
TTE model architecture usually consists of a query tower and an item tower: query tower encodes search query and user profile to query embeddings, and item tower encodes store, grocery item, geo location to item embeddings.
TTEモデルのアーキテクチャは通常、queryタワーとitemタワーから構成される。クエリタワーは検索クエリとユーザプロファイルをクエリ埋め込みにエンコードし、アイテムタワーは店舗、食料品、地理的位置をアイテム埋め込みにエンコードする。

<!-- ここまで読んだ! -->

## What are Embeddings and Two-Tower Embeddings (TTE)? エンベッディングと2タワーエンベッディング（TTE）とは？

An embedding is a rich representation for any entity via d-dimensional latent variables; these entities include, but are not limited to: stores, eaters, items, drivers, locations, and so on.
エンベッディングは、**$d$ 次元の潜在変数(latent variable)**を介した任意のentityのリッチな表現(i.e. 密ベクトル...!!:thinking:)である。これらのentityには、店舗、食べる人、アイテム、ドライバー、場所などが含まれるが、これらに限定されない。
It generally transforms human-friendly features, such as store menu, store price, store review, item title and description, item price, location’s full address, eater preference cuisine and past orders, rider preferred drop-offs and so on, to machine-learning-friendly dense vectors.
一般的に、店のメニュー、店の価格、店のレビュー、商品のタイトルと説明、商品の価格、場所の完全な住所、食べる人の好みの料理や過去の注文、ライダーの好みのドロップオフなど、人間にやさしい特徴(=**human-friendly features**=疎ベクトル...!:thinking:)を、機械学習に適した(=**machine-learning-friendly**な)密なベクトルに変換する。
These vectors can be directly used in any ML task (such as clustering, nearest neighbor search, classification, and so on) without too much feature engineering.
これらのベクトルは、（クラスタリング、最近傍探索、分類などの）**どのようなMLタスクでも、あまり特徴エンジニアリングをすることなく、直接使用することができる**。

![](https://blog.uber-cdn.com/cdn-cgi/image/width=1398,quality=80,onerror=redirect,format=auto/wp-content/uploads/2023/07/Figure1.png)

Figure 1: Left graph describes how embeddings are generated from regular features; Right graph describe how embeddings can be used to find similar restaurants.
図1：左のグラフは、通常の特徴から埋め込みがどのように生成されるかを説明し、右のグラフは、類似のレストランを見つけるために埋め込みがどのように使用されるかを説明する。

Two-tower embeddings are the embeddings generated by a special DL architecture named two towers.
Two-tower embeddingは、**two towersという特殊なDLアーキテクチャによって生成される埋め込み**である。
TTE model architecture usually consists of a query tower and an item tower: the query tower encodes search query and user profile to query embeddings, and the item tower encodes store, grocery item, and geo-location-to-item embeddings.
**TTEモデル**のアーキテクチャは通常、クエリタワーとアイテムタワーから構成される。クエリタワーは検索クエリとユーザプロファイルをクエリに埋め込み、アイテムタワーは店舗、食料品、ジオロケーションをアイテムに埋め込む。
The probability of engagement between the query and item is computed via the dot product (or cosine similarity, Euclidean distance, Hadamard product) between the embeddings from two towers, and we use whether there is any engagement between query and tower in the past as the label of ground truth to train the query and item tower models jointly.
クエリとアイテムのengagement(=interaction)の確率は、2つのタワーからの埋め込み間のドット積（またはコサイン類似度、ユークリッド距離、ハダマード積）を介して計算され、クエリとタワーの間に過去にengagementがあったかどうかをground-truthのラベルとして、クエリとアイテムのタワーモデルを共同で学習する。(=推薦モデルの分類としては、CFとCBのハイブリッドアプローチなのかな...!:thinking:)
Thus TTE is very natural to study the relations between two or multiple entity types, as we sketched in the below graph:
したがってTTEは、下のグラフにスケッチしたように、**2つまたは複数のentityタイプ間の関係を研究するのに非常に適している**:

![](https://blog.uber-cdn.com/cdn-cgi/image/width=1392,quality=80,onerror=redirect,format=auto/wp-content/uploads/2023/07/Figure2.png)

Figure 2: Left image shows the architecture of eater-item two tower embedding model; Right images shows how we can use the two tower embeddings to find the most relevant restaurants for eaters.
図2：左の画像は、eater-item two tower embeddingモデルのアーキテクチャを示す。右の画像は、eater-item two tower embeddingを使用して、eaterにとって最も関連性の高いレストランを見つける方法を示す。

<!-- ここまで読んだ! -->

## Why are Two-Tower Embeddings so Important? なぜ2タワー埋め込みが重要なのか？

In any modern recommendation system, there are two major steps: one being candidate retrieval (known as FPR at Uber), and the second being candidate ranking (known as SPR at Uber).
多くのモダンな推薦システムには、主に2段階のステップがある: 1つは**candidate retrieval(候補者検索)**(UberではFPRと呼ばれる)、もう1つは**candidate ranking(候補者ランキング)**(UberではSPRと呼ばれる)である。
In FPR, we usually need to retrieve the top hundreds or thousands of candidates among millions or hundreds of millions of candidates in a very short time such as p99 equals hundreds of milli-seconds.
**FPRでは通常、数百万から数億の候補の中から上位数百から数千の候補を、p99＝数百ミリ秒という非常に短い時間で取り出す必要がある**。
For example, at Uber, we need to retrieve the top several hundred most relevant stores to an eater among several million available stores, but computing the relevance score or click/order rate of several million pairs between the eater and all stores using the DL model within a very short period is not realistic, because you will make O(q *M) real-time DL inference, assuming q is the number of queries from the eaters and M is the number of stores.
例えば、Uberでは、数百万店舗の中から、食べ手と関連性の高い上位数百店舗を検索する必要がありますが、**食べ手と全店舗の数百万ペアの関連性スコアやクリック率・注文率をDLモデルを使って短時間で計算することは現実的ではありません**。(うんうん...!)
Instead, for this application, it is very natural and best to use a two-tower architecture, because it can easily generate two DL models (see ref and ref): one is for an eater named Query Tower, and the other is for a store named Item Tower.
その代わり、このアプリケーションでは、2つのタワーアーキテクチャを使用するのが非常に自然で最適である。なぜなら、2つのDLモデルを簡単に生成できるからである([ref](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/6c8a86c981a62b0126a11896b7f6ae0dae4c3566.pdf?uclick_id=516d5009-f5a0-49d5-b5cb-7fcf02e27ab8)および[ref](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/b9f4e78a8830fe5afcf2f0452862fb3c0d6584ea.pdf?uclick_id=516d5009-f5a0-49d5-b5cb-7fcf02e27ab8)を参照) : 一つはeaterの為のQuery Tower, もう一つはお店の為のItem Tower.
We use the item tower to pre-compute all store embeddings offline and then create their inverted index such as SIA₃ index at Uber for Approximate Nearest Neighbor Search (ANN).
我々は、アイテムタワーを使用して、**オフラインですべての店舗の埋め込みを事前に計算**し、Approximate Nearest Neigbor Search(近似最近傍探索, ANN)のためにUberでSIA₃インデックスなどのそれらの**inverted index(転置インデックス?)**(=要はembedding table的な辞書って事か! fig3を観た感じでは、key=embedding, value=item_idの辞書っぽい!:thinking:)を作成する。
During eaters’ first page recommendations (homefeed) requests, we use the query tower to generate eater embeddings realtime, and then perform fast ANN search using the dot products between eater embeddings and store embeddings in SIA inverted index.
食べ手の最初のページ推薦（homefeed）要求時に、クエリタワーを使って**食べ手の埋め込みをリアルタイムに生成**し、SIA転置インデックスで食べ手の埋め込みと店舗の埋め込みとの間のドット積を使って高速なANN検索を行う。
By this way, instead of $O(q *M)$ real-time DL inference, we only need make $O(M)$ offline DL inference, and $O(q)$ realtime DL inference and $O(q*M)$ realtime dot product in ANN.
これにより、**$O(q*M)$ リアルタイムDL推論の代わりに、$O(M)$ のオフラインDL推論と、$O(q)$ のリアルタイムDL推論と $O(q*M)$ リアルタイムドット積をANNで行えばよい**。(これはあくまでcandidate retrievalの話! 加えてcandidate rankingの計算量が追加されるはず...!:thinking:)
Thus the computation cost will be reduced using two-tower embeddings for retrieval in the recommendation system.
このように、推薦システムにおけるretrieveにtwo-tower埋め込みを用いることで、計算コストを削減することができる。(行列分解よりもembeddingの推論が楽なのかな...!)
See below about how we use embedding in the Eats homefeed:
Eatsのホームフィードでの埋め込み方法については、以下をご覧ください：

₃ SIA is Uber’s unified search platform, scaling across Eats, Groceries, Maps, and more.
SIAはUberの統合検索プラットフォームで、Eats、Groceries、Mapsなどにまたがって拡張できる。

![](https://blog.uber-cdn.com/cdn-cgi/image/width=1362,quality=80,onerror=redirect,format=auto/wp-content/uploads/2023/07/Figure3.png)

Figure 3: A simplified graph on how Uber retrieval system SIA uses two tower embedding to fetch most relevant restaurants for Eats homefeed ranking.
図3：ウーバー検索システムSIAが、Eatsのホームフィードランキングに最も関連性の高いレストランを取得するために、2つのタワー埋め込みを使用する方法を簡略化したグラフ。

<!-- ここまで読んだ! -->

## A Closer Look: Problem and Motivation A Closer Look： 問題と動機

As Uber continues to scale, the need to improve and grow our large-scale recommendation systems has increased.
Uberが規模を拡大し続けるにつれ、大規模なレコメンデーション・システムを改善し、成長させる必要性が高まっている。

### Original State ≠ Scalable 元の状態≠スケーラブル

The current city-wise Deep MF model used in FPR was not scalable, given the fact that Uber is serving more and more cities.
FPRで使用されている現在の都市別ディープMFモデルは、ウーバーがより多くの都市にサービスを提供していることを考えると、**スケーラブルではなかった**。
The maintenance cost is also very high, because we need to run thousands of Apache Spark™ jobs on a weekly basis just for creating city-wise deep MF models.
また、都市単位のディープMFモデルを作成するためだけに、週単位で何千ものApache Spark™ジョブを実行する必要があるため、**メンテナンスコストも非常に高い**。
Thus it is urgent to find an alternative solution that could better scale globally.
そのため、世界規模でよりよく拡大できる別の解決策を見つけることが急務となっている。
Specifically, this is broken down into two main problems.
具体的には、大きく2つの問題に分けられる。

#### Problem 1: Lack of efficient recommendation systems

問題1：効率的な推薦システムの欠如

In recommendation systems, we need to provide personalized retrieval from a large pool of stores.
レコメンデーションシステムでは、大量の店舗からパーソナライズされたretrievalを提供する必要がある。
For example, in Eats we need to retrieve the best stores out of a few thousands in about a hundred of milli-seconds.
例えば、『Eats』では、数千店舗の中から最適な店舗を数百ミリ秒でretrieveする必要がある。

#### Problem 2: Existing technology could not scale

問題2：既存の技術では拡張できない

In addition, our existing technology doing this (Deep Matrix Factorization–DeepMF), was a bottleneck to growth as it could not scale as our business grows.
さらに、これを実現する**既存の技術(Deep Matrix Factorization-DeepMF)**は、ビジネスの成長に合わせて拡張できないため、成長のボトルネックとなっていた。

- Required thousands of city models 何千もの都市モデルが必要
- Could not be reused 再利用不可
- Very expensive to maintain 維持費が非常に高い

### TTE = Scalable, Extendable, and Efficient 拡張性、拡張性、効率性

In this case, Two-Tower Embeddings are the long-term scalable solution to this problem.
この場合、ツータワー・エンベッディングがこの問題に対する長期的なスケーラブル・ソリューションとなる。
Two-Tower embeddings unlock 3 major advantages that DeepMF lacked.
ツータワー埋め込みは、DeepMFに欠けていた3つの大きな利点を解き放つ。
Specifically:
具体的には

- **Scalable labeling**: The TTE model utilizes users’ engagement such as click and order as the training label, thus it is much easier and less costly compared to classification tasks where we rely on human manual tags. スケーラブルなラベリング： TTEモデルは、クリックや注文といったユーザのエンゲージメントを学習ラベルとして利用するため、人間の手作業によるタグ付けに依存する分類タスクに比べ、はるかに簡単でコストがかかりません。(これはDeepMLも同じでは?)

- **Localization and scalable training and inference**: The TTE model utilizes the simplest and localized relation (0-hop or 1-hop) in the interaction graph, thus we can pre-sample and pre-compute the entity features on disk and then fit them to the TTE model batch by batch during training and inference instead of managing (node_id -> node feature) of the whole graph in memory. The localization property makes training and inference of the TTE model more scalable and less costly compared to GNN. **局所化とスケーラブルな学習と推論**: TTEモデルは、相互作用グラフ内の最も単純で局所的な関係(0-hopまたは1-hop)を利用するため、グラフ全体の(node_id -> node feature)をメモリ上で管理する代わりに、ディスク上のエンティティ特徴量を事前にサンプリングして計算し、学習と推論の際にバッチごとにTTEモデルに当てはめることができる。 ローカライゼーションの特性により、TTEモデルの学習と推論は、GNNと比較してよりスケーラブルでコストがかからない。

- **Feature extensibility**: With Deep MF, we had to generate embeddings for store retrieval, in which user_id and store_id are the only features, and it is difficult to add other important information of the entity such as eater contextual features(query time, order location, etc.), numerical features (store prices, store rating, delivery distance, etc.) and NLP features (store menu, ordered dishes). However, with TTE, adding any feature to any tower is straightforward and simple to extend. (入力する)特徴量の拡張性： Deep MFでは、店舗検索用の埋め込みデータを生成する必要があり、ユーザIDと店舗IDのみが特徴であり、他の重要なエンティティ情報、例えば、食べ手のcontextual特徴量（クエリ時間、注文場所など）、数値特徴（店舗価格、店舗評価、配達距離など）、NLP特徴（店舗メニュー、注文料理）を追加することは困難であった。 しかし、TTEを使えば、どんなタワーにもどんな特徴量でも簡単に追加することができる。(DeepMFは純粋なCFモデルだったけど、TTEはhybridなモデル)

![](https://blog.uber-cdn.com/cdn-cgi/image/width=546,quality=80,onerror=redirect,format=auto/wp-content/uploads/2023/07/Image-7-25-23-at-11.46-PM.jpeg)
Figure 4
図4

We used embeddings to solve low performance, high computing costs, and scaling blockers.
エンベッディングを用いて、低パフォーマンス、高計算コスト、スケーリングのブロッカーを解決した。
We brought embeddings to Uber by building the platform capability to leverage for all use cases, starting with our champion use case, Eats Homefeed.
私たちは、代表的なユースケースであるEats Homefeedを手始めに、あらゆるユースケースに活用できるプラットフォーム機能を構築することで、Uberにエンベッディングをもたらした。

Here, we applied embeddings to the Eats Homefeed model to prove the value and replace DeepMF.
ここでは、Eats Homefeedモデルにエンベッディングを適用して、その価値を証明し、**DeepMFを置き換えた**。
Now we can retrieve personalized stores in about hundred of milliseconds, which was initially impossible to do, and it enables customers to quickly and easily find what they need by selecting the best store for them.
今では、当初は不可能だった、**パーソナライズされた店舗を数百ミリ秒でretrieveできるようになり**、顧客は最適な店舗を選択することで、必要なものを素早く簡単に見つけることができる。
Now, embeddings can be scaled, reused, and transferred beyond our initial use case.
エンベッディングは、当初のユースケースを超えて、拡張、再利用、転送が可能になった。

<!-- ここまで読んだ! -->

## Future Use Cases = Scalability ++ 将来のユースケース＝スケーラビリティ＋α

Besides The retrieval step or FPR of eats home feed described above, we identified 2 other possible use cases for TTE at Uber:
前述したeatsホームフィードのretrieveステップまたはFPRの他に、ウーバーにおけるTTEの使用例として、**2つの可能性があること**を確認した:

- TTE is used as final ranking layer for eats item feed (grocery recommendation)TTEを**最終的なランキング・レイヤー**(=candidate rankingの意味??)として使用し、食品を供給する（食料品の推薦）。

While TTE is typically utilized for retrieval purposes, it can also serve as an effective final ranking layer in cases where other frameworks may be too costly or resource-intensive.
**TTEは通常、retrieve目的(=candidate retrieve!)で利用される**が、**他のフレームワークではコストやリソースがかかりすぎるような場合に、最終的なランキングレイヤー(=candidate ranking)として効果的に機能することもある**。
Prior to the introduction of TTE, the Item Feed Ranking system employed non-personalized static rankings, primarily due to concerns around the potential costs and latency associated with online calls for too many item-level features.
TTEが導入される以前は、アイテムフィードランキングシステムは、主に、**アイテムレベルの特徴をオンライン上で呼び出すことに伴う潜在的なコストとレイテンシーの懸念から、パーソナライズされていない静的なランキングを採用**していた。
TTE as a final ranking layer is a very good solution to reduce infrastructure cost and avoid latency increases.
最終的なランキングレイヤーとしてのTTEは、インフラコストを削減し、レイテンシの増加を避けるための非常に優れたソリューションである。

- TTE is used to generate transferrable features for any downstream task, including rider and place embeddings. In this case, downstream ML models will directly use embeddings generated by the TTE model as input features. Embeddings contain rich information about the entity (eater, store, item, rider, etc.) and including them in the model will greatly improve the model performance. TTEは、騎手や場所の埋め込みを含む、**あらゆる下流タスクのための転送可能な特徴を生成するために使用される**。 この場合、**下流のMLモデルは、TTEモデルによって生成された埋め込みを入力特徴として直接使用する**。 エンベッディングには、entity(食べる人、店、アイテム、乗る人など)に関する豊富な情報が含まれており、それらをモデルに含めることで、モデルのパフォーマンスが大幅に向上する。

In the next sections, we will describe where we specifically used
次のセクションでは、具体的にどこを使ったかを説明する。

<!-- ここまで読んだ! -->

# Deep Dive: The Embeddings Solution ディープダイブ エンベッディング・ソリューション

Designing an embeddings model and the infrastructure to support it was no easy task.
エンベデッドモデルとそれをサポートするインフラを設計するのは簡単なことではなかった。
We needed something that supported our large amount of data, but optimized for computing cost and time.
私たちは、大量のデータをサポートしながら、計算コストと時間を最適化できるものを必要としていました。
This section will cover the successful training strategy we choose, and how we simultaneously upleveled the platform for extensibility, scalability, and reusability.
このセクションでは、私たちが選択した成功したトレーニング戦略と、拡張性、スケーラビリティ、再利用性のためにプラットフォームを同時にレベルアップさせた方法について説明する。

## Model Architecture & Training Strategy モデル・アーキテクチャとトレーニング戦略

The key metrics of retrieval tasks in the recommendation system is recall@large_N where large_N could be hundreds, thousands, or even tens of thousands.
**推薦システムにおけるretrieveタスクの重要な指標は、recall@large_N** である。large_Nは、数百、数千、あるいは数万にもなる。
For example at Uber, we use recall@hundreds as the key metrics for Eats homefeed FPR task.
**例えばUberでは、EatsのホームフィードFPRタスク(=candidate retrieve)の主要メトリクスとしてrecall@hundredsを使用している**。
In order to optimize recall@hundreds, the number of negative samples needs to be much larger than a few hundreds.
recall@hundredsを最適化するためには、負のサンプルの数を数百よりはるかに多くする必要がある。
On the other hand, creating so many negatives during training will blow up the training cost and time.
**その一方で、トレーニング中に否定的な意見を多く出すと、トレーニング費用と時間が吹き飛んでしまう**。
Thus the recommended way in industry is to use “in-batch negatives.’.Nevertheless, we also need to create batches of training data smartly so that the negatives within the same batch are meaningful.
とはいえ、同じバッチ内のネガティブが意味を持つように、賢くトレーニングデータのバッチを作成する必要もある。
So we will first discuss geometric hash and sort.
そこでまず、幾何学的ハッシュとソートについて説明する。

### Spatial Indexing 空間インデックス

Uber Recommendation system has a geospatial aspect to it, as the user would not order from a store far from their location.
Uberのレコメンデーション・システムには地理空間的な側面があり、ユーザーは現在地から遠く離れた店には注文しない。
If we denote the location of eaters’ inputs query q as L(q), and the location of candidate restaurant i as L(i), then we are interested in restaurants that satisfy $dh(L(q), L(i)) < δ(q,i)$, where $dh(l1 ,l2)$ refers to the haversine distance between locations l1 and l2.
食べ手の入力クエリー$q$の位置を $L(q)$、候補レストラン $i$ の位置を $L(i)$ とすると、$dh(L(q), L(i)) < δ(q,i)$ を満たすレストランに興味がある. (距離が閾値以内のcandidateをretrieveしてくる)
ここで$dh(l1 ,l2)$は位置l1とl2の間のハバース距離のことである。

### LogQ Correction for In-batch Negatives バッチ内negativeのLogQ補正

The in-batch negatives technique reuses the positive sample items in a mini batch as negatives for other queries/eaters in the same batch (see research paper for reference).
バッチ内ネガテクニックは、**ミニバッチ内のポジティブサンプルアイテムを、同じバッチ内の他のクエリ/食べる人のnegativeとして再利用する**（参考文献は[研究論文](https://research.google/pubs/pub48840/?uclick_id=516d5009-f5a0-49d5-b5cb-7fcf02e27ab8)を参照）。
It is possible that the positive sample for a query also appears as a positive sample for another query in the same minibatch; this happens more often when we use geo-hash of store locations to order the training data and create mini-batch afterwards.
あるクエリに対する正サンプルが、同じミニバッチ内の別のクエリに対する正サンプルとしても現れる可能性がある。これは、店舗位置のジオハッシュを使って学習データを並べ、その後にミニバッチを作成した場合によく起こる。
The in-batch negatives technique attempts to use negatives in the mini-batch as an approximation of the softmax over the full vocabulary, but it is clearly biased to popular items.
in-batch negativesテクニックは、全語彙に対するソフトマックスの近似としてミニバッチのネガティヴを使おうとするものだが、明らかに人気アイテムに偏っている。
Therefore we need to correct this bias using logQ correction [Adaptive Importance Sampling paper, research paper], and the sampling probability in the batch is computed via multinomial distribution as Q =[1-(1-w)ᴮ] where B is the batch size and w is the item weights in the whole data.
したがって、logQ補正を使ってこのバイアスを補正する必要があり [[Adaptive Importance Sampling 論文](https://www.iro.umontreal.ca/~lisa/pointeurs/importance_samplingIEEEtnn.pdf?uclick_id=516d5009-f5a0-49d5-b5cb-7fcf02e27ab8) & [研究論文](https://research.google/pubs/pub48840/?uclick_id=516d5009-f5a0-49d5-b5cb-7fcf02e27ab8)]、バッチでのサンプリング確率は多項分布を使ってQ =[1-(1-w)↪Lm_1D2E] として計算されます。
Accordingly, we shift our prediction score es/TQ = es/T – log(Q) for all pairs in M = Qitems Iᵀitems Our experience with our Global TTE Model shows that logQ correction improves recall@500 from 89% to 93%.
したがって、M = Qitems Iᵀitems のすべてのペアについて、予測スコア es/TQ = es/T - log(Q) をシフトする。**我々のグローバル TTE モデルの経験から、logQ 補正によって recall@500 が 89% から 93% に改善されることがわかる**。

# Modeling モデリング

### Eater past activities as eater feature: eater特徴量としての過去の活動：

One of the most important innovations of TTE model we developed at Uber MA is that we invent a new type of activity feature, now named BOW features, in which we collect and time-decent-sort several months of previous ordered store_ids for each eater on a specific date as a proxy of eater_uuid (eater_id -> [ordered_store_1, ordered_store_2, ordered_store_3, …]).
私たちがUber MAで開発したTTEモデルの最も重要なイノベーションのひとつは、**BOW特徴量と名付けられた新しいタイプのアクティビティ特徴量を考案したこと**です。この特徴量では、eater_uuid(`eater_id→[ordered_store_1, ordered_store_2, ordered_store_3, ...]`)のプロキシとして、**特定の日付に各イーターが過去に注文したstore_idを数ヶ月分収集し、時間順に並べ替えます**。(i.e. 要するにsequenceデータってこと??)
This reduces the model size by 20 times because we only have millions of stores, and this also partially solves the eater cold start problem
また、イータのコールドスタート問題も部分的に解決される。

### Layer sharing between two towers 2つのタワー間でレイヤーを共有

Another very important innovation of the TTE model we developed is that we enforce neural module layer sharing between the two towers, and this is not typical in most textbook TTE models, but it turns out to be very important for the TTE model for Eats homefeed.
私たちが開発したTTEモデルのもう一つの非常に重要な革新点は、**2つのタワー間でニューラルモジュールのレイヤーを共有することを強制すること**である。(??)

Layer sharing is not new, and it originates from pattern recognition models as early as 1989’s Zip Code Recognition Paper and popularized by AlexNet.
レイヤーの共有は新しいものではなく、1989年のZip Code Recognition PaperやAlexNetによって一般化されたパターン認識モデルに端を発している。
It is also widely used in transformer models.
また、トランスモデルにも広く使用されている。
Nevertheless, the layer sharing in the above examples are used inside each module, while in TTE we enforce layer sharing between two relatively independent tower modules.
とはいえ、上記の例のレイヤーシェアリングは各モジュール内部で使用されているのに対し、**TTEでは比較的独立した2つのタワーモジュール間でレイヤーシェアリングを実施している。**
We will discuss one type of layer sharing we developed as an example: where the query tower and item tower share the same UUID Embedding Layer.
クエリ・タワーとアイテム・タワーが同じUUIDエンベッディング・レイヤーを共有する場合である.

![](https://blog.uber-cdn.com/cdn-cgi/image/width=886,quality=80,onerror=redirect,format=auto/wp-content/uploads/2023/07/Figure5.png)

Figure 5: A sketch graph to show how the layer sharing plays role in two tower model
図5：2タワーモデルにおけるレイヤー共有の役割を示すスケッチグラフ

## System Integration and Improvements システムの統合と改善

In addition to modeling improvements, our team took the opportunity to uplevel the platform for extensibility, scalability, and reuse.
モデリングの改良に加え、私たちのチームは、**拡張性、スケーラビリティ、再利用のためのプラットフォームのレベルアップ**を図った。
Previously the system was just a collection of custom Apache PySpark™ jobs and data processing pipelines, but now we’ve fully integrated the use case into Michelangelo.
以前は、システムはカスタムApache PySpark™ジョブとデータ処理パイプラインのコレクションにすぎませんでしたが、今ではユースケースを**Michelangelo(なにそれ?)**に完全に統合しています。
This means two- tower embeddings gain the benefits of Michelangelo across the end-to-end ML lifecycle: including data preparation, training, evaluation, deployment, and serving.
つまり、2つのタワーエンベッディングは、データの準備、トレーニング、評価、デプロイメント、そしてサービングを含む、エンドツーエンドのMLライフサイクルにわたってMichelangeloの利点を得ることができる。

To simplify data preparation and serving, Michelangelo’s Feature Store, Palette, was enhanced with a tighter integration with Uber’s search platform.
データの準備と提供を簡素化するため、ミケランジェロのフィーチャーストアであるパレットは、ウーバーの検索プラットフォームとの統合を強化した。
This integration means that embeddings registered in the feature store can be served both online and offline using a config-driven approach.
この統合は、フィーチャーストアに登録されたエンベッディングを、コンフィグ主導のアプローチでオンラインでもオフラインでも提供できることを意味する。
Previously, for online serving Palette features were only accessible through Michelangelo’s prediction service and key/value store pairing.
以前は、パレットのオンラインサービス機能は、ミケランジェロの予測サービスとキー／バリューストアのペアリングを通じてのみアクセス可能だった。

For training and evaluation, we have leveraged and extended Michelangelo’s Canvas framework to support two-tower embeddings.
訓練と評価のために、我々はMichelangeloのCanvasフレームワークを活用し、2タワー埋め込みをサポートするように拡張した。
With the addition of new training modules and feature encoders, other teams at Uber can now use a spec-driven approach to train and evaluate models on new use cases.
新しいトレーニング・モジュールとフィーチャー・エンコーダーが追加されたことで、Uberの他のチームは、新しいユースケースについてモデルをトレーニングし評価するために、仕様主導型のアプローチを使用できるようになった。
By creating proper Michelangelo models, we gain the benefits of standardized production retraining workflows, deployments, and the online prediction service.
適切なMichelangeloモデルを作成することで、標準化されたプロダクション再トレーニングのワークフロー、デプロイメント、オンライン予測サービスの利点を得ることができる。

![](https://blog.uber-cdn.com/cdn-cgi/image/width=1218,quality=80,onerror=redirect,format=auto/wp-content/uploads/2023/07/Figure6.png)

Figure 6: E2E pipeline flow that shows how the two embeddings are used in uber eats recommender system
図6：ユーバーイーツ・レコメンダー・システムで2つのエンベッディングがどのように使用されるかを示すE2Eパイプライン・フロー

# Challenges チャレンジ

## Model Size モデルサイズ

After conducting ablation studies, we have found that utilizing the eater_id and store_id is crucial for improving the performance of the model.
アブレーション研究を行った結果、eater_idとstore_idを活用することがモデルのパフォーマンスを向上させるために重要であることがわかりました。
These features possess high cardinality and incorporating them has led to a larger model size.
**これらの特徴は高いカーディナリティを持ち、これらを組み込むことでモデルサイズが大きくなっている**。
However, training and deploying this large model size poses a significant challenge to our current infrastructure, demanding substantial computational resources.
しかし、このような大きなサイズのモデルをトレーニングし、展開することは、現在のインフラにとって大きな挑戦であり、かなりの計算資源を必要とする。

## Training Time トレーニング時間

Training a large model requires a considerable amount of time–it could take several days or weeks.
大規模なモデルのトレーニングにはかなりの時間が必要で、数日から数週間かかることもある。

## Evaluation Metrics 評価指標

The performance evaluation of contextual TTE models presents unique challenges, as traditional metrics like AUC or NDCG do not consider the current context of the user.
AUCやNDCGのような従来の評価基準では、ユーザーの現在のコンテキストを考慮しないため、コンテキストTTEモデルの性能評価には独自の課題がある。
To address this challenge, we have developed a comprehensive evaluation framework that assesses TTE recommendations at the session level and incorporates location and context to generate a Factorized Top K list.
この課題に対処するため、TTEの推奨をセッションレベルで評価し、場所とコンテキストを組み込んでファクトライズされたトップKリストを生成する包括的な評価フレームワークを開発した。
This list is used to generate all relevant metrics, including recall@k and business metrics, providing a robust evaluation of the TTE model’s performance.
このリストは、リコール@kやビジネスメトリクスを含む、関連するすべてのメトリクスの生成に使用され、TTEモデルのパフォーマンスの確実な評価を提供する。
By incorporating context-specific factors, our evaluation framework ensures that the TTE model’s performance is assessed in a way that is relevant and meaningful for real-world scenarios.
コンテキスト固有の要因を組み込むことで、我々の評価フレームワークは、TTEモデルの性能が実世界のシナリオに関連し、意味のある方法で評価されることを保証する。

# Result and Takeaways 結果と教訓

Two-Tower Embeddings is the first DL platform at Uber for FPR and Embeddings as a feature.
ツータワー・エンベッディングは、FPR(=candidate retrieve)と特徴量としてのエンベッディングのためのウーバー初のDLプラットフォームである。
To get here, we delivered 3 models, with significant improvements between each version.
ここまで来るのに、私たちは3つのモデルを提供し、それぞれのバージョン間で大幅な改良を加えてきた。

## Models and Iterations モデルと反復

### Reusability, Scalability, and Infra Wins 再利用性、スケーラビリティ、インフラ勝利

By replacing the DeepMF model with our TTE model, we were able to unlock major wins on our infrastructure side.
DeepMFモデルを私たちのTTEモデルに置き換えることで、インフラストラクチャー面で大きな成果を上げることができました。
Specifically:
具体的には

- Our single global model replaces thousands of city DeepMF models 私たちの単一のグローバルモデルが、何千もの都市のDeepMFモデルを置き換える

- It is now scaleable to hundreds of millions eaters, millions of stores, hundreds of millions of grocery items 現在では、数億人の消費者、数百万の店舗、数億の食料品に拡張可能である。

- We decreased model training from hundred of thousands of core-hours to thousands of core-hours per week モデルのトレーニングは、週あたり数十万コア時間から数千コア時間に減少した。

However, we didn’t stop there.
しかし、私たちはそれだけにとどまらなかった。
TTE is now used in 3 production use cases, with more to come:
TTEは現在、3つのプロダクションユースケースで使用されており、今後もさらに増える予定だ：

- FPR of Eats home feed (store recommendation) イーツのhome feed（店舗推薦）のFPR(candidate retrieve)
- Final ranking layer for Eats item feed (grocery recommendation) Eatsアイテムフィードの最終ランキングレイヤー（食料品の推薦） (candidate ranking)
- To generate transferrable features for any downstream task including Eats SPR and risk. Eats SPRとリスクを含む、あらゆる**下流タスクに転用可能な特徴量**を生成する。

## How is Uber Standing out in this Competitive Space? Uberはこの競争空間でどのように際立っているのか？

The innovation is very specific to Uber’s business.
このイノベーションは、ウーバーのビジネスに非常に特化したものだ。
For example, we focused on addressing the challenge of handling high-cardinality features, such as UUID, in building recommendation systems.
例えば、推薦システムを構築する上で、**UUIDのようなhigh-cardinalityを持つ特徴量を扱うという課題**に焦点を当てた。
The proposed bag-of-words approach and layer-sharing technique were introduced as innovative solutions to this problem, resulting in a significant reduction in model size and improved performance, as demonstrated by offline and online evaluations.
この問題に対する革新的な解決策として、提案されたBag-of-Wordsアプローチとレイヤー共有技術が導入され、オフラインおよびオンライン評価により、モデルサイズの大幅な縮小と性能の向上が実証された。

Our work contributes to the development of more efficient and effective recommendation systems, particularly in the retrieval phase.
我々の研究は、**特にretrieve段階において、より効率的で効果的な推薦システムの開発に貢献する**。
By building on existing state-of-the-art techniques, the proposed methods have the potential to inspire further research in this area, with the ultimate goal of enabling more efficient and effective recommendation systems in the future.
既存の最先端技術を基礎とすることで、提案された手法は、将来、より効率的で効果的な推薦システムを実現するという究極の目標に向けて、この分野におけるさらなる研究を促す可能性を秘めている。

# What’s Next? 次は何だ？

## TTE as a Platform: Adoption プラットフォームとしてのTTE： 採用

One of our primary goals is to enable the widespread adoption of the TTE platform beyond just the Eats team.
私たちの主な目標のひとつは、イーツ・チームだけでなく、TTEプラットフォームの普及を可能にすることです。
With this in mind, we are making significant efforts to ensure that the model is scalable and adaptable to various use cases within the company, including Map and Risk.
このことを念頭に置いて、私たちはこのモデルが拡張性を持ち、マップやリスクなど社内のさまざまなユースケースに適応できるよう、多大な努力をしています。
Users can leverage the platform’s capabilities to customize and train their own TTE models, tailored to their specific needs and requirements, without the need to invest significant resources in developing the model from scratch.
ユーザは、このプラットフォームの機能を活用することで、一からモデルを開発するために多大なリソースを投資することなく、特定のニーズや要件に合わせた独自のTTEモデルをカスタマイズし、トレーニングすることができる。

By expanding the use of TTE across the organization, we can unlock significant value and enhance the performance of various processes and workflows.
TTEの利用を組織全体に広げることで、大きな価値を引き出し、さまざまなプロセスやワークフローのパフォーマンスを向上させることができる。
With its ability to process large volumes of data quickly and accurately, TTE can provide valuable insights and inform decision-making across a wide range of teams and departments.
大量のデータを迅速かつ正確に処理する能力により、TTEは幅広いチームや部門にわたって貴重な洞察を提供し、意思決定に役立てることができる。

## Embeddings as a Feature 特徴としての埋め込み

Our team is actively working to generate embeddings that are general enough to be utilized across a diverse range of applications and use cases.
私たちのチームは、**多様なアプリケーションやユースケースで活用できるような一般的なエンベッディングの生成**に積極的に取り組んでいます。
By developing more versatile and adaptable embeddings, we believe that we can enhance the performance of different machine learning models and unlock new opportunities for innovation and growth.
より汎用的で適応性の高いエンベッディングを開発することで、さまざまな機械学習モデルのパフォーマンスを向上させ、イノベーションと成長のための新たな機会を引き出すことができると信じています。
Users can benefit from the embeddings we have generated by directly using them as input features for their models, thereby streamlining the development process and improving overall performance.
ユーザは、我々が生成した埋め込みをモデルの入力特徴として直接使用することで、開発プロセスを合理化し、全体的なパフォーマンスを向上させることができます。
