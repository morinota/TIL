<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.168">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>chen_et_al_2022_trans2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="chen_et_al_2022_trans2_files/libs/clipboard/clipboard.min.js"></script>
<script src="chen_et_al_2022_trans2_files/libs/quarto-html/quarto.js"></script>
<script src="chen_et_al_2022_trans2_files/libs/quarto-html/popper.min.js"></script>
<script src="chen_et_al_2022_trans2_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="chen_et_al_2022_trans2_files/libs/quarto-html/anchor.min.js"></script>
<link href="chen_et_al_2022_trans2_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="chen_et_al_2022_trans2_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="chen_et_al_2022_trans2_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="chen_et_al_2022_trans2_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="chen_et_al_2022_trans2_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<section id="link-0.1.-リンク" class="level2">
<h2 class="anchored" data-anchor-id="link-0.1.-リンク">0.1. link 0.1. リンク</h2>
<ul>
<li><p>https://dl.acm.org/doi/abs/10.1145/3523227.3546788 httpsを使用しています。</p></li>
<li><p>https://arxiv.org/pdf/2212.04120.pdf httpsを使用しています。</p></li>
</ul>
</section>
<section id="title-0.2.-タイトル" class="level2">
<h2 class="anchored" data-anchor-id="title-0.2.-タイトル">0.2. title 0.2. タイトル</h2>
<p>Denoising Self-Attentive Sequential Recommendation デノイジング自己調整型逐次推薦法</p>
</section>
<section id="abstract-0.3.-抽象的" class="level2">
<h2 class="anchored" data-anchor-id="abstract-0.3.-抽象的">0.3. abstract 0.3. 抽象的</h2>
<p>Transformer-based sequential recommenders are very powerful for capturing both short-term and long-term sequential item dependencies. トランスフォーマーに基づくsequential推薦器は、短期的および長期的な<strong>sequentialなアイテム間の依存関係</strong>を捉えるのに非常に強力である. This is mainly attributed to their unique self-attention networks to exploit pairwise item-item interactions within the sequence. これは主に、sequence内のペアワイズアイテム-アイテム相互作用を利用するための、独自のself-attention ネットワークに起因している. However, real-world item sequences are often noisy, which is particularly true for implicit feedback. しかし、<strong>実世界のitem sequencesはしばしばノイズが多く、特に暗黙的なフィードバックにはそれが当てはまる</strong>。 For example, a large portion of clicks do not align well with user preferences, and many products end up with negative reviews or being returned. 例えば、<strong>クリックの大部分はユーザの嗜好に合わないし</strong>、多くの購入した製品は否定的なレビューを受けたり、返品されることになる. As such, the current user action only depends on a subset of items, not on the entire sequences. このように、現在のユーザーアクションはシーケンス全体ではなく、アイテムのsubset(部分集合)にのみ依存する。 Many existing Transformer-based models use full attention distributions, which inevitably assign certain credits to irrelevant items. 既存のTransformerベースのモデルの多くは、完全なattention分布を使用しており、必然的に無関係なアイテムに一定のクレジットを割り当てることになる。 This may lead to sub-optimal performance if Transformers are not regularized properly. これはTransformerが適切に正則化されない場合、最適でない性能につながる可能性がある.</p>
<p>Here we propose the Rec-denoiser model for better training of self-attentive recommender systems. 本論文では、自己調整型推薦システムのより良い学習のために、<strong>Rec-denoiserモデルを提案</strong>する。 In Rec-denoiser, we aim to adaptively prune noisy items that are unrelated to the next item prediction. Rec-denoiserでは、<strong>next item prediction</strong>に関係のないノイズの多いアイテムを適応的に刈り取ることを目的とする.(そっか、NLPのnext-token predictionみたいな感じ…!) To achieve this, we simply attach each self-attention layer with a trainable binary mask to prune noisy attentions, resulting in sparse and clean attention distributions. そのため、<strong>各self-attention層に学習可能なbinary maskを付加</strong>し、ノイズの多いattentionを除去することで、スパースでクリーンなattention分布が得られる。 This largely purifies item-item dependencies and provides better model interpretability. これにより、item-itemの依存性(の多く??)がほぼ除去され、モデルの解釈可能性が向上する。 In addition, the self-attention network is typically not Lipschitz continuous and is vulnerable to small perturbations. さらに、self-attentionネットワークは一般的にリプシッツ連続ではなく、小さな摂動に弱い.(このへんよくわからん…!) Jacobian regularization is further applied to the Transformer blocks to improve the robustness of Transformers for noisy sequences. <strong>さらにヤコビアン正則化をTransformerブロックに適用し、ノイズの多いシーケンスに対するTransformerの頑健性を向上させる</strong>。 Our Rec-denoiser is a general plugin that is compatible to many Transformers. <strong>我々のRec-denoiserは多くのTransformerに対応する汎用プラグインである</strong>. (general plugin、素晴らしい…!!) Quantitative results on real-world datasets show that our Rec-denoiser outperforms the state-of-the-art baselines. 実世界のデータセットにおける定量的な結果は、我々のRec-denoiserが最先端のベースラインを凌駕することを示している.</p>
</section>
<section id="introduction-1.はじめに" class="level1">
<h1>1. introduction 1.はじめに</h1>
<p>Sequential recommendation aims to recommend the next item based on a user’s historical actions [20, 35, 39, 44, 47], e.g., to recommend a bluetooth headphone after a user purchases a smart phone. <strong>Sequential recommendationの目的は，ユーザの過去の行動に基づいて次のアイテムを推薦すること(next-item-prediction)</strong>である[20, 35, 39, 44, 47]．例えば，<strong>ユーザがスマートフォンを購入した後にBluetoothヘッドホンを推薦するような場合</strong>である. Learning sequential user behaviors is, however, challenging since a user’s choices on items generally depend on both long-term and short-term preferences. しかし，一般にユーザのアイテム選択は<strong>長期的嗜好(long-term preference)</strong>と<strong>短期的嗜好(short-term preference)</strong>の両方に依存するため，逐次的(sequential)なユーザ行動の学習は困難である. Early Markov Chain models [19, 39] have been proposed to capture short-term item transitions by assuming that a user’s next decision is derived from a few preceding actions, while neglecting long-term preferences. 初期のマルコフ連鎖モデル[19, 39]は、ユーザの次の決定がいくつかの先行行動から導かれると仮定することによって、短期的なアイテムの遷移を捉えるために提案されたが、長期的な嗜好は無視されたままであった. To alleviate this limitation, many deep neural networks have been proposed to model the entire users’ sequences and achieve great success, including recurrent neural networks [20, 53] and convolutional neural networks [42, 54, 57]. この限界を緩和するために、リカレントニューラルネットワーク[20, 53]や畳み込みニューラルネットワーク[42, 54, 57]など、<strong>ユーザのsequence全体をモデル化する多くのディープニューラルネットワークが提案され、大きな成功を収めている</strong>.</p>
<p>Recently, Transformers have shown promising results in various tasks, such as machine translation [43]. 最近、Transformersは機械翻訳のような様々なタスクで有望な結果を示している[43]. One key component of Transformers is the self-attention network, which is capable of learning long-range dependencies <strong>by computing attention weights between each pair of objects in a sequence</strong>. Transformersの主要な構成要素の1つはself-attention networkであり、シーケンス中の各object ペアのattention weightを計算することで<strong>長距離依存関係を学習することができる</strong>. Inspired by the success of Transformers, several self-attentive sequential recommenders have been proposed and achieve the state-of-the-art performance [26, 41, 49, 50]. Transformersの成功に触発され、いくつかのself-attentive sequential recommendersが提案され、最新の性能を達成している[26, 41, 49, 50]. For example, SASRec [26] is the pioneering framework to adopt self-attention network to learn the importance of items at different positions. 例えば、SASRec [26]は、異なる位置にあるitemの重要度を学習するために、self-attention networkを採用した先駆的なフレームワークである. BERT4Rec [41] further models the correlations of items from both left-to-right and right-to-left directions. BERT4Rec [41]は、さらに左から右、右から左の両方向のitemの相関をモデル化する. SSE-PT [50] is a personalized Transformer model that provides better interpretability of engagement patterns by introducing user embeddings. SSE-PT [50]は、user embeddingsを導入することにより、エンゲージメントパターンの解釈可能性を向上させるパーソナライズドトランスフォーマーモデルである. LSAN [31] adopts a novel twin-attention sequential framework, which can capture both long-term and short-term user preference signals. LSAN [31]は新しいtwin-attention sequential frameworkを採用し、長期と短期の両方のユーザ嗜好シグナルを捉えることができる. Recently, Transformers4Rec [14] performs an empirical analysis with broad experiments of various Transformer architectures for the task of sequential recommendation. 最近、<strong>Transformers4Rec</strong> [14]は、sequential推薦のタスクのために、様々なTransformerアーキテクチャの幅広い実験による実証分析を行っている.</p>
<p>Although encouraging performance has been achieved, the robustness of sequential recommenders is far less studied in the literature. しかし、<strong>sequential推薦器のrobustnessについてはあまり研究されていない</strong>. Many real-world item sequences are naturally noisy, containing both true-positive and false-positive interactions [6, 45, 46]. 実世界の多くのアイテム列は自然にノイズが多く、真陽性(true-positive)と偽陽性(false-positive. ex. <strong>好きじゃないけどクリックしてしまった. 購入してみたが嫌いだった…??</strong>)の両方の相互作用を含んでいる [6, 45, 46]. For example, a large portion of clicks do not align well with user preferences, and many products end up with negative reviews or being returned. 例えば、クリックの大部分はユーザの嗜好に合わず、多くの製品は否定的なレビューで終わったり、返品されたりする. In addition, there is no any prior knowledge about how a user’s historical actions should be generated in online systems. また、オンラインシステムでは、ユーザの過去の行動をどのように生成すべきかという事前知識は存在しない. Therefore, developing robust algorithms to defend noise is of great significance for sequential recommendation. そのため、<strong>ノイズに強いアルゴリズムを開発することは、逐次推薦において大きな意義がある</strong>.</p>
<p>Clearly, not every item in a sequence is aligned well with user preferences, especially for implicit feedbacks (e.g., clicks, views, etc.) [8]. 特に、暗黙のフィードバック（クリック、ビューなど）の場合、<strong>sequence内のすべてのitemがユーザーの嗜好とうまく整合しているわけではないことは明らか</strong>である [8]. Unfortunately, the vanilla self-attention network is not Lipschitz continuous1 , and is vulnerable to the quality of input sequences [28]. 残念ながら、バニラな(デフォルトの) self-attention networkは<strong>Lipschitz連続ではなく(?)</strong>、<strong>入力シーケンスの質に弱いという問題</strong>がある[28]。 Recently, in the tasks of language modeling, people found that a large amount of BERT’s attentions focus on less meaningful tokens, like “[SEP]” and “.”, which leads to a misleading explanation [11]. 最近、言語モデリングのタスクにおいて、BERT の注意の多くが、“[SEP]” や “.”のようなあまり意味のないトークンに集中し、誤解を招く説明になっていることが判明している[11]。 It is thus likely to obtain sub-optimal performance if self-attention networks are not well regularized for noisy sequences. このように、self-attentionネットワークがノイズの多いsequenceに対してうまく正則化されていない場合、最適とは言えない性能が得られる可能性がある. We use the following example to further explain above concerns. 以下の例を用いて、上記の懸念についてさらに説明する.</p>
<p>Figure 1 illustrates an example of left-to-right sequential recommendation where a user’s sequence contains some noisy or irrelevant items. 図1は、左から右へのsequential recommendationの一例である. For example, a father may interchangeably purchase (phone, headphone, laptop) for his son, and (bag, pant) for his daughter, resulting in a sequence: (phone, bag, headphone, pant, laptop). 例えば、ある父親が息子に（携帯電話、ヘッドフォン、ノートパソコン）、娘に（カバン、ズボン）を購入する場合、（携帯電話、カバン、ヘッドフォン、ズボン、ノートパソコン）という順序になる. In the setting of sequential recommendation, we intend to infer the next item, e.g., laptop, based on the user’s previous actions, e.g., (phone, bag, headphone, pant). 逐次推薦の設定では、<strong>ユーザーの以前の行動、例えば（電話、カバン、ヘッドホン、ズボン）から、次のアイテム、例えばノートパソコンを推論すること</strong>を意図している. However, the correlations among items are unclear, and intuitively pant and laptop are neither complementary nor compatible to each other, which makes the prediction untrustworthy. しかし、アイテム間の相関が不明確であり、<strong>直感的にpantとlaptopは補完関係にも相容れない</strong>ため、この予測は信頼できない. A trustworthy model should be able to only capture correlated items while ignoring these irrelevant items within sequences. <strong>信頼できるモデルは、シーケンス内のこれらの無関係なアイテムを無視し、相関のあるアイテムのみを捉えることができるはず</strong>である. Existing self-attentive sequential models (e.g., SASRec [26] and BERT4Rec [41]) are insufficient to address noisy items within sequences. 既存のself-attentive sequential model（例えば、SASRec [26]やBERT4Rec [41]）は、シーケンス内のノイズの多いアイテムに対処するには不十分である. The reason is that their full attention distributions are dense and would assign certain credits to all items, including irrelevant items. その理由は、それらのfull attention distributionsが密であり、無関係なitemを含むすべてのitemに一定のcreditを割り当ててしまうからである. This causes a lack of focus and makes models less interpretable [10, 58]. このため、注目度(focus)が不足し、モデルの解釈性が低下する[10, 58].</p>
<p>To address the above issues, one straightforward strategy is to design sparse Transformer architectures that sparsify the connections in the attention layers, which have been actively investigated in language modeling tasks [10, 58]. 上記の問題を解決するために、一つの簡単な戦略は、言語モデリングタスクで活発に研究されている<strong>attention layersの接続をsparseにした</strong>Transformerアーキテクチャを設計することである[10, 58]. Several representative models are Star Transformer [18], Sparse Transformer [10], Longformer [2], and BigBird [58]. いくつかの代表的なモデルはStar Transformer [18], Sparse Transformer [10], Longformer [2], そしてBigBird [58]である. These sparse attention patterns could mitigate noisy issues and avoid allocating credits to unrelated contents for the query of interest. これらのsparse attentionパターンは、ノイズの問題を軽減し、関心のあるクエリに無関係なコンテンツにクレジットを割り当てることを回避することができる. However, these models largely rely on pre-defined attention schemas, which lacks flexibility and adaptability in practice. しかし、これらのモデルは事前に定義されたattention schemas(??)に大きく依存しており、実際のところ柔軟性や適応性に欠けている. Unlike end-to-end training approaches, whether these sparse patterns could generalize well to sequential recommendation remains unknown and is still an open research question. また、エンドツーエンドの学習アプローチとは異なり、言語モデルタスクのsparseパターンがsequential推薦にうまく一般化できるかどうかは不明であり、まだ未解決の研究課題である.</p>
<section id="contributions." class="level2">
<h2 class="anchored" data-anchor-id="contributions.">1.1. Contributions.</h2>
<p>貢献度 In this work, we propose to design a denoising strategy, Rec-Denoiser, for better training of selfattentive sequential recommenders. 本研究では、自己注意型逐次推薦器をより良く学習させるためのノイズ除去戦略、Rec-Denoiserを提案する. Our idea stems from the recent findings that not all attentions are necessary and simply pruning redundant attentions could further improve the performance [10, 12, 40, 55, 58]. 我々のアイデアは、全てのattentionsは必要ではなく、冗長なattentionsを刈り取ることでさらに性能が向上するという最近の知見に由来する[10, 12, 40, 55, 58]. Rather than randomly dropping out attentions, we introduce differentiable masks to drop task-irrelevant attentions in the self-attention layers, which can yield exactly zero attention scores for noisy items. 我々は、ランダムにattentionsを削除するのではなく、<strong>微分可能なマスクを導入し、タスクと無関係なattentionsをself-attention layersで削除する</strong>ことで、ノイズの多いitemに対してattentionスコアを正確にゼロにすることができる. The introduced sparsity in the self-attention layers has several benefits: self-attention layersに導入されたスパース性には、いくつかの利点がある.</p>
<ul>
<li><ol type="1">
<li>Irrelevant attentions with parameterized masks can be learned to be dropped in a data-driven way. パラメータ化されたマスクを持つ無関係なattentionは、データ駆動型の方法で削除されるように学習させることができる.</li>
</ol>
<ul>
<li>Taking Figure 1 as an example, our Rec-denoiser would prune the sequence (phone, bag, headphone) for pant, and (phone, bag, headphone, pant) for laptop in the attention maps.図1を例にとると、Rec-denoiserは、アテンションマップにおいて、ズボンには(phone, bag, headphone)、ノートパソコンには(phone, bag, headphone, pant)という順序を切り捨てることになる.</li>
<li>Namely, we seek next item prediction explicitly based on a subset of more informative items. つまり、より情報量の多いアイテムの部分集合(subset)に基づき、明示的に次のアイテム予測を行うのです.</li>
</ul></li>
<li><ol start="2" type="1">
<li>Our Rec-Denoiser still takes full advantage of Transformers as it does not change their architectures, but only the attention distributions.我々のRec-DenoiserはTransformerのアーキテクチャを変更せず、<strong>アテンション分布のみを変更する</strong>ため、Transformerを最大限に活用することができる.</li>
</ol>
<ul>
<li>As such, Rec-Denoiser is easy to implement and is compatible to any Transformers, making them less complicated as well as improving their interpretability. そのため、Rec-Denoiserは実装が容易で、あらゆるTransformerと互換性があり、Transformerの複雑さを軽減し、その解釈可能性を向上させることができる.</li>
</ul></li>
</ul>
<p>In our proposed Rec-Denoiser, there are two major challenges. 我々が提案するRec-Denoiserでは、2つの大きな課題がある. First, the discreteness of binary masks (i.e., 0 is dropped while 1 is kept) is, however, intractable in the back-propagation. まず、2値マスクの離散性（すなわち、0は削除され、1は保持される）は、しかし、バックプロパゲーションでは実行不可能である. To remedy this issue, we relax the discrete variables with a continuous approximation through probabilistic reparameterization [25]. この問題を解決するために、我々は確率的再パラメータ化[25]により、離散変数を連続的な近似値で緩和する. As such, our differentiable masks can be trained jointly with original Transformers in an end-to-end fashion. このように、我々の微分可能なマスクは、オリジナルのTransformerとエンドツーエンドで共同して学習することができる. In addition, the scaled dot-product attention is not Lipschitz continuous and is thus vulnerable to input perturbations [28]. また、scaled dot-product attentionはLipschitz連続(?)ではないため、入力摂動に対して脆弱である[28]. In this work, Jacobian regularization [21, 24] is further applied to the entire Transformer blocks, to improve the robustness of Transformers for noisy sequences. この研究では、<strong>ノイズの多いシーケンスに対するTransformerのrobustnessを向上させるため</strong>に、Transformerブロック全体にヤコビアン正則化[21, 24]をさらに適用している. Experimental results on real-world benchmark datasets demonstrate the effectiveness and robustness of the proposed Rec-Denoiser. 実世界のベンチマークデータセットに対する実験結果から、提案するRec-Denoiserの有効性と頑健性を実証する. In summary, our contributions are: まとめると、我々の貢献は以下の通りである.</p>
<ul>
<li><p>We introduce the idea of denoising item sequences for better of training self-attentive sequential recommenders, which greatly reduces the negative impacts of noisy items. 本論文では、自己認識型逐次推薦器を学習するために、item列のノイズ除去のアイデアを紹介し、ノイズの多いitemによる悪影響を大幅に軽減する.</p></li>
<li><p>We present a general Rec-Denoiser framework with differentiable masks that can achieve sparse attentions by dynamically pruning irrelevant information, leading to better model performance. 我々は、微分可能なマスクを持つ一般的なRec-Denoiserフレームワークを提示し、無関係な情報を動的に刈り取ることで疎な注意を達成し、より良いモデル性能を導くことが可能である.</p></li>
<li><p>We propose an unbiased gradient estimator to optimize the binary masks, and apply Jacobian regularization on the gradients of Transformer blocks to further improve its robustness. バイナリマスクの最適化のために不偏勾配推定器を提案し、Transformerブロックの勾配にヤコビアン正則化を適用して、さらに頑健性を向上させる.</p></li>
<li><p>The experimental results demonstrate significant improvements that Rec-Denoiser brings to self-attentive recommenders (5.05% ∼ 19.55% performance gains), as well as its robustness against input perturbations. 実験結果は、Rec-Denoiserが自己注意型推薦器にもたらす大きな改善（5.05% ∼ 19.55%の性能向上）と、入力の摂動に対する頑健性を示している.</p></li>
</ul>
</section>
</section>
<section id="related-work-2.-関連作品" class="level1">
<h1>2. Related Work 2. 関連作品</h1>
<p>In this section, we briefly review the related work on sequential recommendation and sparse Transformers. 本節では、逐次推薦とスパーストランスフォーマーに関する関連研究を簡単にレビューする。 We also highlight the differences between the existing efforts and ours. また、既存の取り組みと我々の取り組みとの相違点を強調する。</p>
<section id="sequential-recommendation" class="level2">
<h2 class="anchored" data-anchor-id="sequential-recommendation">2.1. Sequential Recommendation</h2>
<p>Leveraging sequences of user-item interactions is crucial for sequential recommendation. 逐次推薦では、ユーザとアイテムのinteractionのsequenceを活用することが重要である。 User dynamics can be caught by Markov Chains for inferring the conditional probability of an item based on the previous items [19, 39]. ユーザダイナミクスはマルコフ連鎖によって捕捉され、前のアイテムに基づくアイテムの条件付き確率を推論することができる[19, 39]. More recently, growing efforts have been dedicated to deploying deep neural networks for sequential recommendation such as recurrent neural networks [20, 53], convolutional neural networks [42, 54, 57], memory networks [9, 22], and graph neural networks [4, 7, 51]. 最近では、リカレントニューラルネットワーク [20, 53]、畳み込みニューラルネットワーク [42, 54, 57]、メモリネットワーク [9, 22]、グラフニューラルネットワーク [4, 7, 51] などの<strong>深いニューラルネットワークを逐次推薦に利用する取り組みが盛んになっている</strong>。 For example, GRU4Rec [20] employs a gated recurrent unit to study temporal behaviors of users. 例えば、GRU4Rec[20]はユーザの時間的行動を研究するためにゲーテッドリカレントユニットを採用している。 Caser [42] learns sequential patterns by using convolutional filters on local sequences. Caser [42]は局所的な配列に対して畳み込みフィルタを用いて連続的なパターンを学習する。 MANN [9] adopts memory-augmented neural networks to model user historical records. MANN [9]はユーザの履歴記録をモデル化するためにメモリ補強型ニューラルネットワークを採用する。 SR-GNN [51] converts session sequences into graphs and uses graph neural networks to capture complex item-item transitions. SR-GNN [51]はセッションシーケンスをグラフに変換し、グラフニューラルネットワークを使用して複雑なアイテム-アイテム遷移を捉える。</p>
<p>Transformer-based models have shown promising potential in sequential recommendation [5, 26, 30, 32, 33, 41, 49, 50], due to their ability of modeling arbitrary dependencies in a sequence. Transformerに基づくモデルは、sequence中の任意の依存関係をモデル化できるため、逐次推薦において有望な可能性を示している[5, 26, 30, 32, 33, 41, 49, 50]． For example, SASRec [26] first adopts self-attention network to learn the importance of items at different positions. 例えば、SASRec [26]では、まず、異なる位置にあるアイテムの重要度を学習するために、self-attentionネットワークを採用している. In the follow-up studies, several Transformer variants have been designed for different scenarios by adding bidirectional attentions [41], time intervals [30], personalization [50], importance sampling [32], and sequence augmentation [33]. その後、bidirectional attentions[41]、time intervals[30]、personalization[50]、importance sampling[32]、sequence augmentation[33]を追加し、異なるシナリオのためにいくつかの変種が設計されてきた. However, very few studies pay attention to the robustness of self-attentive recommender models. しかし、self-attention型推薦モデルの頑健性に注目した研究は非常に少ない. Typically, users’ sequences contain lots of irrelevant items since they may subsequently purchase a series of products with different purposes [45]. 一般に、ユーザのシーケンスには無関係なアイテムが多く含まれる. As such, the current user action only depends on a subset of items, not on the entire sequences. このような場合、現在のユーザの行動は、シーケンス全体ではなく、アイテムのサブセットにのみ依存する。 However, the self-attention module is known to be sensitive to noisy sequences [28], which may lead to sub-optimal generalization performance. しかし、<strong>self-attentionモジュールはノイズの多いシーケンスに敏感であることが知られており</strong>[28]、これは最適でない汎化性能につながる可能性がある。 In this paper, we aim to reap the benefits of Transformers while denoising the noisy item sequences by using learnable masks in an end-to-end fashion. 本論文では、学習可能なマスクをend-to-endで用いる(i.e.&nbsp;Transformerの学習と一緒にmaskのパラメータも学習できる)ことにより、ノイズの多いアイテムsequenceをdenoiseしつつ、Transformersの利点を享受することを目指す.</p>
</section>
<section id="sparce-transformer" class="level2">
<h2 class="anchored" data-anchor-id="sparce-transformer">2.2. Sparce Transformer</h2>
<p>Recently, many lightweight Transformers seek to achieve sparse attention maps since not all attentions carry important information in the self-attention layers [2, 10, 18, 29, 58]. 最近、多くの軽量トランスフォーマーが、全てのattentionがself-attention層の重要な情報を持っているわけではないので、sparseなアテンションマップ(=attention分布?)を実現することを追求している[2, 10, 18, 29, 58]. (既存研究において、sparseなattention分布を採用する目的は、noiseへのrobust性の向上というよりも、軽量化やスケーラビリティ向上だったりするんだろうか…??:thinking:) For instance, Reformer [29] computes attentions based on locality-sensitive hashing, leading to lower memory consumption. 例えば、Reformer [29]は局所性を考慮したハッシュに基づいてアテンションを計算し、メモリ消費の低減につながる. Star Transformer [18] replaces the fully-connected structure of self-attention with a star-shape topology. Star Transformer [18]は、self-attentionの完全連結構造を星形のトポロジーに置き換えたものである。 Sparse Transformer [10] and Longformer [2] achieve sparsity by using various sparse patterns, such as diagonal sliding windows, dilated sliding windows, local and global sliding windows. Sparse Transformer [10] と Longformer [2] は、斜めスライド窓、拡張スライド窓、ローカルスライド窓、グローバルスライド窓など、様々なスパースパターンを用いてスパース性を実現する。 BigBird [58] uses random and several fixed patterns to build sparse blocks. BigBird [58]では，<strong>ランダムなパターンといくつかの固定パターン</strong>を用いて，疎なブロックを構築している． It has been shown that these sparse attentions can obtain the state-of-the-art performance and greatly reduce computational complexity. これらのsparse attentionは、最先端の性能を得ることができ、計算量を大幅に削減できることが示されている。 However, many of them rely on fixed attention schemas that lack flexibility and require tremendous engineering efforts. しかし、これらの多くは、柔軟性に欠け、膨<strong>大な工学的努力を必要とする固定的なattention schemaに依存</strong>している.</p>
<p>Another line of work is to use learnable attention distributions [12, 36, 38, 40]. また、<strong>学習可能なattention分布</strong>[12, 36, 38, 40]を使用することもある。 Mostly, they calculate attention weights with variants of sparsemax that replaces the softmax normalization in the self-attention networks. ほとんどの場合、self-attentionネットワークにおけるソフトマックス正規化を置き換える<strong>sparsemax</strong>(最大値のみを残す、みたいなイメージ??:thinking:)の変種を用いてattention weightを計算する. This allows to produce both sparse and bounded attentions, yielding a compact and interpretable set of alignments. これにより、疎でboundedな(境界のある?)attentionを生成することができ、コンパクトで解釈可能なアラインメントの集合を得ることができる. Our Rec-denoiser is related to this line of work. 我々のRec-denoiserは、この研究に関連している. Instead of using sparsemax, we design a trainable binary mask for the self-attention network. sparsemax(? 最大値のみを残す、みたいなイメージ??)を用いる代わりに、我々は<strong>self-attentionネットワークに対して学習可能なbinaryマスクを設計</strong>する。 As a result, our proposed Rec-denoiser can automatically determine which self-attention connections should be deleted or kept in a data-driven way. その結果、我々の提案するRec-denoiserは、データ駆動型の方法で、どのself-attetionの接続を削除すべきか、あるいは保持すべきかを自動的に決定することができる.</p>
</section>
</section>
<section id="problem-and-background-3.-問題点と背景" class="level1">
<h1>3. Problem and Background 3. 問題点と背景</h1>
<p>In this section, we first formulate the problem of sequential recommendation, and then revisit several self-attentive models. 本節では、まず逐次推薦の問題を定式化し、次にいくつかのself-attetnionモデルを再検討する。 We further discuss the limitations of the existing work. さらに、既存の研究の限界について議論する。</p>
<section id="problem-setup-3.1.-問題の設定" class="level2">
<h2 class="anchored" data-anchor-id="problem-setup-3.1.-問題の設定">3.1. Problem Setup 3.1. 問題の設定</h2>
<p>In sequential recommendation, let <span class="math inline">\(U\)</span> be a set of users, <span class="math inline">\(I\)</span> a set of items, and <span class="math inline">\(S = {S^1, S^2,\cdots, S^{|U|}}\)</span> a set of users’ actions. 逐次推薦において、<span class="math inline">\(U\)</span> をユーザの集合、<span class="math inline">\(I\)</span> をアイテムの集合、<span class="math inline">\(S={S^1,S^2, \cdots, S^{|U|}\)</span> をユーザの行動の集合とする. We user <span class="math inline">\(S^u = (S^u_1, S^u_2, \cdots, S^u_{|S^u|})\)</span> to denote a sequence of items for user <span class="math inline">\(u \in U\)</span> in a chronological order, where <span class="math inline">\(S^u_t \in I\)</span> is the item that user <span class="math inline">\(u\)</span> has interacted with at time <span class="math inline">\(t\)</span>, and <span class="math inline">\(|S^u|\)</span> is the length of sequence. <span class="math inline">\(S^u = (S^{u}_{1}, S^{u}_{2}, \cdots, S^{u}_{|S^u|})\)</span> は、時系列に並んだユーザ <span class="math inline">\(u \in U\)</span> のitem sequence を表し、<span class="math inline">\(S^{u}_{t} in I\)</span> は時刻(=sequence内の要素の識別子的な意味合い) <span class="math inline">\(t\)</span> にユーザ <span class="math inline">\(u\)</span> がinteractしたアイテム、<span class="math inline">\(|S^u|\)</span> はsequenceの長さを表す.</p>
<p>Given the interaction history <span class="math inline">\(S^u\)</span>, sequential recommendation seeks to predict the next item <span class="math inline">\(S^u_{|S^{u}+1|}\)</span> at time step <span class="math inline">\(|S^{u}+1|\)</span> interaction history <span class="math inline">\(S^u\)</span> が与えられると、sequential推薦では time step <span class="math inline">\(|S^{u}+1|\)</span> において next item <span class="math inline">\(S^u_{|S^{u}+1|}\)</span> を予測しようとする. During the training process [26, 41], it will be convenient to regard the model’s input as <span class="math inline">\((S^{u}_{1}, S^{u}_{2}, \cdots, S^{u}_{|S^u - 1|})\)</span> and its expected output is a shifted version of the input sequence: <span class="math inline">\((S^{u}_{2}, S^{u}_{3}, \cdtos, S^{u}_{|S^{u}|})\)</span> モデルの学習プロセス[26, 41]では、モデルの入力を <span class="math inline">\((S^{u}_{1}, S^{u}_{2}, ˶cdots, S^{u}_{|S^u - 1|})\)</span> とみなし、その expected output(i.e.&nbsp;教師ラベル!) を入力sequence の シフトバージョン <span class="math inline">\((S^{u}_{2}, S^{u}_{3}, ˶cdtos, S^{u}_{|S^{u}|})\)</span> とみなすと便利である.</p>
</section>
<section id="self-attentive-recommenders" class="level2">
<h2 class="anchored" data-anchor-id="self-attentive-recommenders">3.2. Self-attentive Recommenders</h2>
<p>Owing to the ability of learning long sequences, Transformer architectures [43] have been widely used in sequential recommendation, like <strong>SASRec</strong> [26], BERT4Rec [41], and TiSASRec [30]. Transformerアーキテクチャ[43]は長いシーケンスを学習することができるため、逐次推薦において<strong>SASRec</strong> [26], BERT4Rec [41], TiSASRec [30] など広く利用されている。 Here we briefly introduce the design of SASRec and discuss its limitations. ここでは、SASRecの設計を簡単に紹介し、その限界について考察する.</p>
<section id="embedding-layer-3.2.1.-エンベデッドレイヤー" class="level3">
<h3 class="anchored" data-anchor-id="embedding-layer-3.2.1.-エンベデッドレイヤー">3.2.1. Embedding Layer 3.2.1. エンベデッドレイヤー</h3>
<p>Transformer-based recommenders maintain an item embedding table <span class="math inline">\(T \in R^{|I| \times d}\)</span> , where 𝑑 is the size of the embedding. Transformerベースのレコメンダーはアイテムの <strong>embedding table</strong> <span class="math inline">\(T \in R^{|I| \times d}\)</span> を保持する. ここで、<span class="math inline">\(d\)</span> は embeddingのサイズ. (embedding tableは、アイテムid -&gt; embedding vector のmapみたいなイメージ:thinking:). For each sequence <span class="math inline">\((S^u_1, S^u_2, \cdots, S^u_{|S^u - 1|})\)</span>, it can be converted into a fixed-length sequence <span class="math inline">\((s_1, s_2, \cdots s_n)\)</span>, where <span class="math inline">\(n\)</span> is the maximum length (e.g., keeping the most recent 𝑛 items by truncating or padding items). 各sequence <span class="math inline">\((S^u_1, S^u_2, \cdots, S^u_{|S^u - 1|})\)</span> に対して、<strong>fixed-length(固定長)のsequence <span class="math inline">\((s_1, s_2, \cdots s_n)\)</span> に変換</strong>することができる. ここで、<span class="math inline">\(n\)</span>は、sequenceの最大長である. (ex. アイテムを truncating したり、padding したりして、最新の<span class="math inline">\(n\)</span>個のアイテムを残す) The embedding for <span class="math inline">\((s_1, s_2, \cdots s_n)\)</span> is denoted as <span class="math inline">\(E \in R^{n \times d}\)</span> , which can be retrieved from the table <span class="math inline">\(T\)</span>. <span class="math inline">\((s_1, s_2, \cdots s_n)\)</span> の埋め込みを <span class="math inline">\(E \in R^{n \times d}\)</span> と表し(= <span class="math inline">\(E\)</span> は埋め込みベクトル行列 …!)、embedding table <span class="math inline">\(T\)</span> から取り出すことができる. To capture the impacts of different positions, one can inject a learnable positional embedding <span class="math inline">\(P \in R^{n \times d}\)</span> into the input embedding as: sequence内の異なるpositionの影響(=time step間の距離やsequenceの順序)を捉えるために、学習可能なpositonal embedding <span class="math inline">\(P \in R^{n ౪times d}\)</span> を入力embeddng <span class="math inline">\(E\)</span> にinject(注入)することができる:</p>
<p>(各tokenの特徴量ベクトルに、positonal encoding vectorを追加する式!)</p>
<p><span class="math display">\[
\hat{E} = E + P \tag{1}
\]</span></p>
<p>where <span class="math inline">\(\hat{E} \in R^{n\times d}\)</span> is an order-aware embedding, which can be directly fed to any Transformer-based models. ここで、<span class="math inline">\(hat{E}は \in R^{ntimes d}\)</span> は order-awareな(=sequence内の順序を考慮した) 埋め込みベクトル行列 で、Transformerベースのモデルに直接与えることができる.</p>
</section>
<section id="transformer-block" class="level3">
<h3 class="anchored" data-anchor-id="transformer-block">3.2.2. Transformer Block</h3>
<p>A Transformer block consists of a self-attention layer and a point-wise feed-forward layer. トランスフォーマー・ブロックは、self-attention layer と point-wise feed-forward layer (=全結合層) で構成される.</p>
<p><strong>Self-attention Layer</strong>: The key component of Transformer block is the self-attention layer that is highly efficient to uncover sequential dependencies in a sequence [43]. <strong>self-attention層</strong>： Transformerブロックの重要なコンポーネントは、シーケンスの逐次的な依存関係を明らかにするために非常に効率的であるself-attention層である[43]。 The scaled dot-product attention is a popular attention kernel: scaled dot-product attentionは一般的なattention kernel(=かなり一般的なattention関数の一つ、という認識:thinking:)である：</p>
<p>(scaled-dot-product attentionの関数式)</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d}}) V
\tag{2}
\]</span></p>
<p>where <span class="math inline">\(\text{Attention}(Q, K, V) \in R^{n \times d}\)</span> is the output item representations; <span class="math inline">\(Q = \hat{E}W^Q\)</span>, <span class="math inline">\(K =\hat{E}W^K\)</span>, and <span class="math inline">\(V = \hat{E}W^V\)</span> are the queries, keys and values, respectively; <span class="math inline">\({W^Q, W^K, W^V} \in R^{d \times d}\)</span> are three projection matrices; and <span class="math inline">\(\sqrt{d}\)</span> is the scale factor to produce a softer attention distribution. ここで、</p>
<ul>
<li><span class="math inline">\(\text{Attention}(Q, K, V) \in R^{n \times d}\)</span> は <strong>output item representations</strong> である</li>
<li><span class="math inline">\(Q = \hat{E} W^Q\)</span>, <span class="math inline">\(K =\hat{E} W^K\)</span>, and <span class="math inline">\(V = \hat{E} W^V\)</span> はそれぞれ Query, Key, Valueである. (QもKもVも、埋め込みベクトル行列Eを元にしてるので、attention関数のself-attention的な使い方…!)</li>
<li><span class="math inline">\({W^Q, W^K, W^V} \in R^{d \times d}\)</span> は3つのprojection行列</li>
<li><span class="math inline">\(sqrt{d}\)</span> はより柔らかい(?)attention分布を生成するためのscale-factorである.(正規化的なイメージ!)</li>
</ul>
<p>In sequential recommendation, one can utilize either left-to-right unidirectional attentions (e.g., SASRec [26] and TiSASRec [30]) or bidirectional attentions (e.g., BERT4Rec [41]) to predict the next item. 逐次推薦では、<strong>left-to-right uni-directional attentions(左から右への一方向のattention)</strong>(ex. SASRec [26]やTiSASRec [30])や、もしくは<strong>bi-directional attention(双方向のattention)</strong> (ex. BERT4Rec [41])を利用して、次のアイテムを予測することができる. Moreover, one can apply <span class="math inline">\(H\)</span> attention functions in parallel to enhance expressiveness: <span class="math inline">\(H &lt;- \text{MultiHead}(\hat{E})\)</span> [43]. さらに、<span class="math inline">\(H\)</span> 個のattention関数を並列に適用することで、表現力を高めることができる：<span class="math inline">\(\mathbf{H} &lt;- \text{MultiHead}(\hat{E})\)</span> [43]. (元のtransformerでも採用してる、Multi-head attentionね:thinking:)</p>
<p><strong>Point-wise Feed-forward Layer</strong>: As the self attention layer is built on linear projections, we can endow the nonlinearity by introducing a point-wise feed-forward layers: <strong>ポイント・ワイズ・フィードフォワード層</strong>： self-attention層はlinear projectionで構築されているので、<strong>Point-wise Feed-forward層を導入することで、非線形性を付与する(モデルの表現力を高める為の非線形変換!)</strong>ことができる(あ、そういうモチベーションなのか…!中間層１つのやつ! :thinking:)：</p>
<p>(補足: “point-wise” -&gt; <strong>要素毎に独立して操作を行う</strong>、という意味. Transformerの場合はSequenceデータを入力に取るが、この層は、sequenceデータの各要素に対して個別に処理が行われる. つまり、sequenceデータの各要素に対して同じ操作が行われる.:thinking:) (補足: “feed-forward” -&gt; データの入出力の流れが1方向=前方方向にのみ進む事を意味する. 逆に、“feed-forward”ではない層はRNNとか! CNNは”point-wise”ではないが”feed-forward”である気はする:thinking:)</p>
<p><span class="math display">\[
F_i = FFN(H_i) = \text{ReLU}(H_i W^{(1)} + b^{(1)})W^{(2)} + b^{(2)}
\tag{3}
\]</span></p>
<p>where <span class="math inline">\(W^{(\ast)} \in R^{d \times d}\)</span>, <span class="math inline">\(b^{(*)} \in R^d\)</span> are the learnable weights and bias. ここで、<span class="math inline">\(W^{(*)} \in R^{d }\)</span>, <span class="math inline">\(b^{(*)} \in R^d\)</span> は学習可能な重みとバイアスである。</p>
<p>In practice, it is usually beneficial to learn hierarchical item dependencies by stacking more Transformer blocks. 実際には、より多くのTransformerブロックを積み重ねることによって、階層的なアイテムの依存関係を学習することが通常有益である。 Also, one can adopt the tricks of residual connection, dropout, and layer normalization for stabilizing and accelerating the training. また、残差接続、ドロップアウト、レイヤー正規化などのトリックを採用することで、学習の安定化と高速化を図ることができる。 Here we simply summarize the output of <span class="math inline">\(L\)</span> Transformer blocks as: <span class="math inline">\(F^{(L)} &lt;- \text{Transformer}(\hat{E})\)</span>. ここでは、<span class="math inline">\(L\)</span>Transformerブロックの出力を単純に次のようにまとめる： F^{(L)} &lt;- (E})$.</p>
</section>
<section id="learning-objective-3.2.3.-学習目標" class="level3">
<h3 class="anchored" data-anchor-id="learning-objective-3.2.3.-学習目標">3.2.3. Learning Objective 3.2.3. 学習目標</h3>
<p>After stacked <span class="math inline">\(L\)</span> Transformer blocks, one can predict the next item (given the first <span class="math inline">\(t\)</span> items) based on <span class="math inline">\(F_t^{(L)}\)</span>. L<span class="math inline">\(個のTransformerブロックを積み重ねた後、\)</span>F_t^{(L)}<span class="math inline">\(に基づいて（最初の\)</span>t$個の項目があれば）次の項目を予測できる。 In this work, we use inner product to predict the relevance of item <span class="math inline">\(i\)</span> as: この研究では、内積を使って項目<span class="math inline">\(i\)</span>の関連性を次のように予測する：</p>
<p><span class="math display">\[
r_{i, t} = &lt;F_{t}^{(L)}, T_i&gt;,
\]</span></p>
<p>where <span class="math inline">\(T_i \in R^d\)</span> is the embedding of item <span class="math inline">\(i\)</span>. ここで <span class="math inline">\(T_i \in R^d\)</span> は項目 <span class="math inline">\(i\)</span> の埋め込みである。 Recall that the model inputs a sequence <span class="math inline">\(s = (s_1, s_2, \cdots, s_n)\)</span> and its desired output is a shifted version of the same sequence <span class="math inline">\(o = (o_1, o_2, \cdots, o_n)\)</span>, we can adopt the binary cross-entropy loss as: モデルはシーケンス<span class="math inline">\(s = (s_1, s_2, ˶cdots, s_n)\)</span>を入力し、その出力は同じシーケンス<span class="math inline">\(o = (o_1, o_2, ˶cdots, o_n)\)</span>をシフトしたものである：</p>
<p><span class="math display">\[
L_{BCE} = - \sum_{S^u \in S} \sum_{t=1}^{n}{[\log{\sigma(r_{o_t, t})} + \log{1 - \sigma (r_{o_t', t})}]} + a \cdot ||\theta||^2_F
\tag{4}
\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the mode parameters, <span class="math inline">\(a\)</span> is the reqularizer to prevent over-fitting, <span class="math inline">\(o'_t \notin S^u\)</span> is a negative sample corresponding to <span class="math inline">\(o_t\)</span>, and <span class="math inline">\(\sigma(\cdot)\)</span> is the sigmoid function. ここで、<span class="math inline">\(theta\)</span>はモードパラメータ、<span class="math inline">\(a\)</span>はオーバーフィットを防ぐためのreqularizer、<span class="math inline">\(o'_t \notin S^u\)</span>は<span class="math inline">\(o_t\)</span>に対応する負のサンプル、<span class="math inline">\(sigma( \cdot)\)</span>はシグモイド関数である。</p>
<p>More details can be found in SASRec [26] and BERT4Rec [41]. 詳細は、SASRec[26]およびBERT4Rec[41]に記載されている。</p>
</section>
</section>
<section id="the-noisy-attentions-problem" class="level2">
<h2 class="anchored" data-anchor-id="the-noisy-attentions-problem">3.3. The Noisy Attentions Problem</h2>
<p>Despite the success of SASRec and its variants, we argue that they are insufficient to address the noisy items in sequences. SASRecとその亜種の成功にもかかわらず、シーケンス中のノイズの多い項目に対処するには不十分であると我々は主張する。 The reason is that the full attention distributions (e.g., Eq.(2)) are dense and would assign certain credits to irrelevant items. その理由は、完全な注目度分布（例えば式(2)）は密度が高く、無関係な項目に一定のクレジットを割り当ててしまうからである。 This complicates the item-item dependencies, increases the training difficulty, and even degrades the model performance. これは項目-項目の依存関係を複雑にし、トレーニングの難易度を上げ、さらにはモデルの性能を低下させる。 To address this issue, several attempts have been proposed to manually define sparse attention schemas in language modeling tasks [2, 10, 18, 58]. この問題に対処するため、言語モデリングタスクでスパース注意スキーマを手動で定義する試みがいくつか提案されている[2, 10, 18, 58]。 However, these fixed sparse attentions cannot adapt to the input data [12], leading to sub-optimal performance. しかし、このような固定的なスパース・アテンションは、入力データに適応することができず[12]、最適なパフォーマンスとは言えない。</p>
<p>On the other hand, several dropout techniques are specifically designed for Transformers to keep only a small portion of attentions, including LayerDrop [17], DropHead [60], and UniDrop [52]. 一方、LayerDrop[17]、DropHead[60]、UniDrop[52]など、トランスフォーマーのために特別に設計された、注目のごく一部だけを残すドロップアウト技術がいくつかある。 Nevertheless, these randomly dropout approaches are susceptible to bias: the fact that attentions can be dropped randomly does not mean that the model allows them to be dropped, which may lead to over-aggressive pruning issues. とはいえ、このようなランダムに脱落させるアプローチは、バイアスの影響を受けやすい。注意がランダムに脱落させられるという事実は、モデルが脱落を許容していることを意味しないので、過度に攻撃的な刈り込みの問題につながる可能性がある。 In contrast, we propose a simple yet effective data-driven method to mask out irrelevant attentions by using differentiable masks. これに対して我々は、微分可能なマスクを用いて無関係な注意をマスクする、シンプルかつ効果的なデータ駆動法を提案する。</p>
</section>
</section>
<section id="rec-denoiser-4.-レクデノイザー" class="level1">
<h1>4. Rec-Denoiser 4. レク・デノイザー</h1>
<p>In this section, we present our Rec-Denoiser that consists of two parts: differentiable masks for self-attention layers and Jacobian regularization for Transformer blocks. このセクションでは、Rec-Denoiserを紹介する。Rec-Denoiserは、自己注意層のための微分可能なマスクと、Transformerブロックのためのヤコビアン正則化の2つの部分から構成される。</p>
<section id="differentiable-masks" class="level2">
<h2 class="anchored" data-anchor-id="differentiable-masks">4.1. Differentiable Masks</h2>
<p>The self-attention layer is the cornerstone of Transformers to capture long-range dependencies. セルフ・アテンション・レイヤーは、長距離の依存関係を捉えるトランスフォーマーの要である。 As shown in Eq.(2), the softmax operator assigns a non-zero weight to every item. 式(2)に示すように、ソフトマックス演算子はすべての項目に0でない重みを割り当てる。 However, full attention distributions may not always be advantageous since they may cause irrelevant dependencies, unnecessary computation, and unexpected explanation. しかし、完全な注目度分布は、無関係な依存関係、不必要な計算、予期せぬ説明を引き起こす可能性があるため、必ずしも有利とは限らない。 We next put forward differentiable masks to address this concern. 次に、この懸念に対処するために、微分可能なマスクを提案する。</p>
<section id="learnable-sparse-attentions-4.1.1.-学習可能なスパースアテンション" class="level3">
<h3 class="anchored" data-anchor-id="learnable-sparse-attentions-4.1.1.-学習可能なスパースアテンション">4.1.1. Learnable Sparse Attentions 4.1.1. 学習可能なスパース・アテンション</h3>
<p>Not every item in a sequence is aligned well with user preferences in the same sense that not all attentions are strictly needed in self-attention layers. セルフ・アテンション・レイヤーにおいて、すべてのアテンションが厳密に必要とされるわけではないのと同じ意味で、シーケンス内のすべてのアイテムがユーザーの嗜好にうまく合致しているわけではない。 Therefore, we attach each self-attention layer with a trainable binary mask to prune noisy or task-irrelevant attentions. そこで、各自己注意層に学習可能なバイナリ・マスクを付加し、ノイズの多い注意やタスクと無関係な注意を除去する。 Formally, for the 𝑙-th self-attention layer in Eq.(2), we introduce a binary matrix <span class="math inline">\(Z^{(l)} \in {0, 1}^{n\times n}\)</span>, where <span class="math inline">\(Z^{(l)}_{u,v}\)</span> denotes whether the connection between query <span class="math inline">\(u\)</span> and key <span class="math inline">\(v\)</span> is present. ここで、<span class="math inline">\(Z^{(l)}_{u,v}\)</span>はクエリ<span class="math inline">\(u\)</span>とキー<span class="math inline">\(v\)</span>の接続の有無を表す。 As such, the <span class="math inline">\(l\)</span>-th self-attention layer becomes: このように、<span class="math inline">\(l\)</span>番目の自己注意層は次のようになる：</p>
<p><span class="math display">\[
A^{(l)} = \text{softmax}(\frac{Q^{(l)} K^{(l)T}}{\sqrt{d}}),
\\
M^{(l)} = A^{(l)} \odot Z^{(l)},
\\
\text{Attention}(Q^{(l)}, K^{(l)}, V^{(l)}) = M^{(l)} V^{(l)},
\tag{5}
\]</span></p>
<p>where <span class="math inline">\(A^{(l)}\)</span> is the original full attentions, <span class="math inline">\(M^{(l)}\)</span> denotes the sparse attentions, and <span class="math inline">\(\odot\)</span> is the element-wise product. ここで、<span class="math inline">\(A^{(l)}\)</span>は元の完全注目度、<span class="math inline">\(M^{(l)}\)</span>は疎注目度、<span class="math inline">\(modot\)</span>は要素ごとの積である。 Intuitively, the mask <span class="math inline">\(Z^{(l)}\)</span> (e.g., 1 is kept and 0 is dropped) requires minimal changes to the original self-attention layer. 直感的には、マスク<span class="math inline">\(Z^{(l)}\)</span>（例えば、1を残して0を落とす）は、元の自己注意層に最小限の変更を加えるだけで済む。 More importantly, they are capable of yielding exactly zero attention scores for irrelevant dependencies, resulting in better interpretability. さらに重要なのは、無関係な依存関係に対して注意スコアを正確にゼロにすることができるため、解釈しやすくなるということだ。 The idea of differentiable masks is not new. 微分可能なマスクというアイデアは新しいものではない。 In the language modeling, differentiable masks have been shown to be very powerful to extract short yet sufficient sentences, which achieves better performance [1, 13]. 言語モデリングにおいて、微分可能なマスクは、短くても十分なセンテンスを抽出するのに非常に強力であり、より良いパフォーマンスを達成することが示されている[1, 13]。</p>
<p>One way to encourage sparsity of <span class="math inline">\(M^{(l)}\)</span> is to explicitly penalize the number of non-zero entries of <span class="math inline">\(Z^{(l)}\)</span>, for <span class="math inline">\(1 \leq l \leq L\)</span>, by minimizing: M<sup>{(l)}<span class="math inline">\(のスパース性を奨励する一つの方法は、\)</span>Z</sup>{(l)}$のゼロでないエントリーの数を、<span class="math inline">\(1 ￢l ￢L\)</span>に対して、最小化することで明示的にペナルティを課すことである：</p>
<p><span class="math display">\[
R_M = \sum_{l=1}^{L}||Z^{l}||_{0}
= \sum_{l=1}^{L} \sum_{u=1}^{n} \sum_{v=1}^{n} I[Z_{u,v}^{(l)} \neq 0],
\tag{6}
\]</span></p>
<p>where <span class="math inline">\(I[c]\)</span> is an indicator that is equal to 1 if the condition <span class="math inline">\(c\)</span> holds and 0 otherwise; and <span class="math inline">\(||\cdot||_{0}\)</span> denotes the <span class="math inline">\(L_0\)</span> norm that is able to drive irrelevant attentions to be exact zeros. |</p>
<p>However, there are two challenges for optimizing <span class="math inline">\(Z^{(l)}\)</span>: non-differentiability and large variance. しかし、<span class="math inline">\(Z^{(l)}\)</span>の最適化には、微分不可能性と分散の大きさという2つの課題がある。 <span class="math inline">\(L_0\)</span> is discontinuous and has zero derivatives almost everywhere. L_0$は不連続であり、ほとんどどこでもゼロ導関数を持つ。 Additionally, there are <span class="math inline">\(2^{n^2}\)</span> possible states for the binary mask <span class="math inline">\(Z^{(l)}\)</span> with large variance. さらに、2値マスク<span class="math inline">\(Z^{(l)}\)</span>には大きな分散を持つ<span class="math inline">\(2^{n^2}\)</span>個の可能な状態がある。 Next, we propose an efficient estimator to solve this stochastic binary optimization problem. 次に、この確率的二元最適化問題を解くための効率的な推定器を提案する。</p>
</section>
<section id="efficient-gradient-computation-4.1.2.-効率的な勾配計算" class="level3">
<h3 class="anchored" data-anchor-id="efficient-gradient-computation-4.1.2.-効率的な勾配計算">4.1.2. Efficient Gradient Computation 4.1.2. 効率的な勾配計算</h3>
<p>Since Z (𝑙) is jointly optimized with the original Transformer-based models, we combine Eq.(4) and Eq.(6) into one unified objective: Z (↪Ll_1459) はオリジナルのTransformerベースのモデルと共同で最適化されるので、式(4)と式(6)を1つの統一された目的にまとめる：</p>
<p><span class="math display">\[
L(Z, \Theta) = L_{BCE}({A^{(l)} \odot Z^{(l)}}, \Theta) + \beta \cdot \sum_{l=1}^{L} \sum_{u=1}^{n} \sum_{v=1}^{n} I[Z_{u,v}^{(l)} \neq 0]
\tag{7}
\]</span></p>
<p>where 𝛽 controls the sparsity of masks and we denote Z as Z := {Z (1) , · · · , Z (𝐿) }. ここで ↪L_1FD はマスクのスパース性を制御し、Z を Z := {Z (1) , - - , Z (↪Lu_1D43F) } とする。 We further consider each Z (𝑙) 𝑢,𝑣 is drawn from a Bernoulli distribution parameterized by Π (𝑙) 𝑢,𝑣 such that Z (𝑙) 𝑢,𝑣 ∼ Bern(Π (𝑙) 𝑢,𝑣 ) [34]. さらに、Z (𝑙) ∼ Bern(Π (𝑙) 𝑢,↪Ll_1D463 ) のようなΠ (𝑙) ∼ Bern(Π (𝑢),↪Ll_1D463 ) でパラメータ化されたBernoulli分布からZ (𝑙) ∼ 𝑢,↪Ll_1D463) が引かれると考える。[34]. As the parameter Π (𝑙) 𝑢,𝑣 is jointly trained with the downstream tasks, a small value of Π (𝑙) 𝑢,𝑣 suggests that the attention A (𝑙) 𝑢,𝑣 is more likely to be irrelevant, and could be removed without side effects. パラメータΠ (𝑙) 𝑢,𝑣は下流タスクと共同で学習されるため、Π (𝑙) 𝑢,𝑣の値が小さいと、注目A (𝑙) 𝑢,𝑣は無関係である可能性が高く、副作用なく削除できる。 By doing this, Eq.(7) becomes: こうすることで、式(7)は次のようになる：</p>
<p><span class="math display">\[
\tag{8}
\]</span></p>
<p>where E(·) is the expectation. ここでE(-)は期待値である。 The regularization term is now continuous, but the first term L𝐵𝐶𝐸 (Z, Θ) still involves the discrete variables Z (𝑙) . 正則化項は連続になったが、第一項L𝐵𝐸 (Z, Θ)はまだ離散変数Z (↪Ll_1D459) を含んでいる。 One can address this issue by using existing gradient estimators, such as REINFORCE [48] and Straight Through Estimator [3], etc. REINFORCE [48]やStraight Through Estimator [3]などの既存の勾配推定器を使用することで、この問題に対処することができる。 These approaches, however, suffer from either biased gradients or high variance. しかし、これらのアプローチは、偏った勾配や高い分散に悩まされている。 Alternatively, we directly optimize discrete variables using the recently proposed augment-REINFORCEmerge (ARM) [15, 16, 56], which is unbiased and has low variance. あるいは、最近提案されたaugment-REINFORCEmerge（ARM）[15, 16, 56]を使って直接離散変数を最適化する。 In particular, we adopt the reparameterization trick [25], which reparameterizes Π (𝑙) 𝑢,𝑣 ∈ [0, 1] to a deterministic function 𝑔(·) with parameter Φ (𝑙) 𝑢,𝑣 , such that: 特に、Π (𝑙) 𝑢,↪Ll_1D463 ∈ [0, 1]をパラメータΦ (𝑙) ↪Ll_1D463 を持つ決定論的関数𝑔(-)に再パラメータ化する再パラメータ化トリック[25]を採用する：</p>
<p><span class="math display">\[
\tag{9}
\]</span></p>
<p>since the deterministic function 𝑔(·) should be bounded within [0, 1], we simply choose the standard sigmoid function as our deterministic function: 𝑔(𝑥) = 1/(1 + 𝑒 −𝑥 ). 決定論的関数 ᑔ(-) は[0, 1]内で有界であるべきなので、決定論的関数として標準シグモイド関数を選ぶ： ᑔ = 1/(1 + 𝑒 -↪Ll_1D465 ). As such, the second term in Eq.(8) becomes differentiable with the continuous function 𝑔(·). そのため、式(8)の第2項は連続関数ᑔ(-)で微分可能になる。 We next present the ARM estimator for computing the gradients of binary variables in the first term of Eq.(8) [15, 16, 56]. 次に、式(8)の第1項のバイナリ変数の勾配を計算するためのARM推定器を示す[15, 16, 56]。 According to Theorem 1 in ARM [56], we can compute the gradients for Eq.(8) as: ARM [56]の定理1によれば、式(8)の勾配は次のように計算できる：</p>
<p><span class="math display">\[
\tag{10}
\]</span></p>
<p>where Uni(0, 1) denotes the Uniform distribution within [0, 1], and L𝐵𝐶𝐸 (I[U &gt; 𝑔(−Φ)], Θ) is the cross-entropy loss obtained by setting the binary masks Z (𝑙) to 1 if U (𝑙) &gt; 𝑔(−Φ (𝑙) ) in the forward pass, and 0 otherwise. ここでUni(0, 1)は[0, 1]内の一様分布を表し、L𝐵𝐸 (I[U &gt; 𝑔(-Φ)])、 Θ) は、フォワードパスにおいて、U (↪Ll_1D459) &gt; Φ (-Φ (↪Ll_1D459) の場合にバイナリマスクZ (↪Ll_1D459) を1に設定し、それ以外の場合に0に設定することで得られるクロスエントロピー損失。 The same strategy is applied to L𝐵𝐶𝐸 (I[U &lt; 𝑔(Φ)], Θ). 同じ戦略をL𝐵𝐸 (I[U &lt; ǔ], Θ)にも適用する。 Moreover, ARM is an unbiased estimator due to the linearity of expectations. さらに、ARMは期待値の線形性により不偏推定量となる。 Note that we need to evaluate L𝐵𝐶𝐸 (·) twice to compute gradients in Eq.(10). 式(10)の勾配を計算するために、L𝐵𝐸 (-)を2回評価する必要があることに注意。 Given the fact that 𝑢 ∼ Uni(0, 1) implies (1 − 𝑢) ∼ Uni(0, 1), we can replace U with (1 − U) in the indicator function inside L𝐵𝐶𝐸 (I[U &gt; 𝑔(−Φ)], Θ): ↪Ll_1D462 ∼ Uni(0, 1)が(1 - ↪Ll_1D462) ∼ Uni(0, 1)を意味することから、L𝐶𝐸 (I[U &gt; 𝑔(-Φ)], Θ)内の指標関数において、Uを(1 - U)に置き換えることができる：</p>
<p><span class="math display">\[
\tag{}
\]</span></p>
<p>To this end, we can further reduce the complexity by considering the variants of ARM – Augment-Reinforce (AR) [56]: このため、ARMの変形であるAugment-Reinforce（AR）[56]を考慮することで、さらに複雑さを軽減することができる：</p>
<p><span class="math display">\[
\tag{11}
\]</span></p>
<p>where only requires one-forward pass. ここで必要なのはワンフォワードパスだけだ。 The gradient estimator ∇ 𝐴𝑅 Φ L (Φ, Θ) is still unbiased but may pose higher variance, comparing to ∇ 𝐴𝑅𝑀 Φ L (Φ, Θ). 勾配推定量∇ Φ 𝐴 L (Φ, Θ)は依然として不偏であるが、∇ Φ 𝐴 Φ L (Φ, Θ)に比べて分散が大きくなる可能性がある。 In the experiments, we can trade off the variance of the estimator with complexity. 実験では、推定量の分散と複雑さをトレードオフにすることができる。</p>
<p>In the training stage, we update ∇ΦL (Φ, Θ) (either Eq.(10) or Eq.(11)) and ∇ΘL (Φ, Θ) 3 during the back propagation. 学習段階では、逆伝播中に∇ΦL（Φ，Θ）（式（10）または式（11））と∇ΘL（Φ，Θ）3を更新する。 In the inference stage, we can use the expectation of Z (𝑙) 𝑢,𝑣 ∼ Bern(Π (𝑙) 𝑢,𝑣 ) as the mask in Eq.(5), i.e., E(Z (𝑙) 𝑢,𝑣 ) = Π (𝑙) 𝑢,𝑣 = 𝑔(𝚽 (𝑙) 𝑢,𝑣 ). 推論段階では、Z (𝑙) 𝑢,𝑣 ∼ Bern(Π (𝑢),𝑣 )の期待値を式(5)のマスクとして使うことができる、すなわち、E(Z (𝑙) 𝑢,𝑣 = Π(𝑙) 𝑢,𝑣 = 𝑔(𝑢 (𝑙) 𝑣 )。 Nevertheless, this will not yield a sparse attention M(𝑙) since the sigmoid function is smooth unless the hard sigmoid function is used in Eq.(9). とはいえ、式(9)でハードシグモイド関数を使わない限り、シグモイド関数は平滑なので、これではスパース注意M(↪Ll459)は得られない。 Here we simply clip those values 𝑔(𝚽 (𝑙) 𝑢,𝑣 ) ≤ 0.5 to zeros such that a sparse attention matrix is guaranteed and the corresponding noisy attentions are eventually eliminated. ここでは、疎な注意行列が保証され、対応するノイジーな 注意が最終的に排除されるように、単に値𝑔(𝚽) ≤ 0.5 をゼロに切り取る。</p>
</section>
</section>
<section id="jacobian-regularization" class="level2">
<h2 class="anchored" data-anchor-id="jacobian-regularization">4.2. Jacobian Regularization</h2>
<p>As recently proved by [28], the standard dot-product self-attention is not Lipschitz continuous and is vulnerable to the quality of input sequences. 最近[28]によって証明されたように、標準的なドット積自己アテンションはリプシッツ連続ではなく、入力シーケンスの品質に弱い。 Let 𝑓 (𝑙) be the 𝑙-th Transformer block (Sec 3.2.2) that contains both a self-attention layer and a point-wise feed-forward layer, and x be the input. 𝑓を𝑙番目のTransformerブロック(第3.2.2節)とし、自己注意層とポイント単位のフィードフォワード層を含むとする。 We can measure the robustness of the Transformer block using the residual error: 𝑓 (𝑙) (x + 𝝐) − 𝑓 (𝑙) (x), where 𝝐 is a small perturbation vector and the norm of 𝝐 is bounded by a small scalar 𝛿, i.e., ∥𝝐 ∥2 ≤ 𝛿. ここで、𝝐は小さな摂動ベクトルであり、𝝐のノルムは小さなスカラー𝝐で境界付けられている、 ∥𝝐 ∥2 ≤ 𝛿。 Following the Taylor expansion, we have: テイラー展開に従うと、次のようになる：</p>
<p><span class="math display">\[
\tag{}
\]</span></p>
<p>Let J (𝑙) (x) represent the Jacobian matrix at x where J (𝑙) 𝑖𝑗 = 𝜕 𝑓 (𝑙) 𝑖 (x) 𝜕𝑥𝑗 . J (𝑙) (x)はxにおけるヤコビアン行列を表し、J (𝑙) 𝑖 (𝑗) = 𝜕 𝑓 (𝑙) (x) 𝜕とする。 Then, we set J (𝑙) 𝑖 (x) = 𝜕 𝑓 (𝑙) 𝑖 (x) 𝜕x to denote the 𝑖-th row of J (𝑙) (x). そして、J (↪Ll_1D459) (x) の𝑖番目の行を表すために、J (↪Ll_1D459) (x) = 𝑖 (𝜕) 𝜕x とする。 According to Hölder’s inequality4 , we have: Hölderの不等式4 によれば、次のようになる：</p>
<p><span class="math display">\[
\tag{}
\]</span></p>
<p>Above inequality indicates that regularizing the 𝐿2 norm on the Jacobians enforces a Lipschitz constraint at least locally, and the residual error is strictly bounded. 上記の不等式は、ヤコビアンのᵃ2ノルムを正則化することで、少なくとも局所的にはリプシッツ制約が強制され、残差は厳密に有界であることを示している。 Thus, we propose to regularize Jacobians with Frobenius norm for each Transformer block as: そこで、各トランスフォーマーブロックのヤコビアンをフロベニウスノルムで正則化することを提案する：</p>
<p><span class="math display">\[
\tag{12}
\]</span></p>
<p>Importantly, ∥J (𝑙) ∥ 2 𝐹 can be approximated via various Monte-Carlo estimators [23, 37]. 重要なことは、∥J (↪Ll459) ∥ 2 ǔは様々なモンテカルロ推定量[23, 37]によって近似できることです。 In this work, we adopt the classical Hutchinson estimator [23]. 本研究では、古典的なHutchinson推定量[23]を採用する。 For each Jocobian matrix J (𝑙) ∈ R 𝑛×𝑛 , we have: 各ヨコビア行列J (↪Ll_1D459) ∈ R 𝑛×𝑛 に対して、次のようになる：</p>
<p><span class="math display">\[
\tag{}
\]</span></p>
<p>where 𝜼 ∈ N (0, I𝑛) is the normal distribution vector. ここで、↪Ll_1∈N (0, I𝑛)は正規分布ベクトルである。 We further make use of random projections to compute the norm of Jacobians R𝐽 and its gradient ∇ΘR𝐽 (Θ) [21], which significantly reduces the running time in practice. さらに、ヤコビアンのノルムR𝐽とその勾配∇ΘR𝐽 (Θ)[21]を計算するためにランダム射影を利用する。</p>
</section>
<section id="optimization" class="level2">
<h2 class="anchored" data-anchor-id="optimization">4.3. Optimization</h2>
<section id="joint-training-4.3.1.-合同トレーニング" class="level3">
<h3 class="anchored" data-anchor-id="joint-training-4.3.1.-合同トレーニング">4.3.1. Joint Training 4.3.1. 合同トレーニング</h3>
<p>Putting together loss in Eq.(4), Eq.(6), and Eq.(12), the overall objective of Rec-Denoiser is: 式(4)、式(6)、式(12)の損失をまとめると、Rec-Denoiserの全体的な目的は次のようになる：</p>
<p><span class="math display">\[
\tag{13}
\]</span></p>
<p>where 𝛽 and 𝛾 are regularizers to control the sparsity and robustness of self-attention networks, respectively. ここで↪Ll_1D6FD と↪L_1D6FE↩︎は、それぞれ自己注意ネットワークのスパース性とロバスト性を制御する正則化子である。 Algorithm 1 summarizes the overall training of Rec-Denoiser with the AR estimator. アルゴリズム1は、AR推定器を用いたRec-Denoiserの全体的なトレーニングをまとめたものである。</p>
<p>Lastly, it is worth mentioning that our Rec-Denoiser is compatible to many Transformer-based sequential recommender models since our differentiable masks and gradient regularizations will not change their main architectures. 最後に、我々のRec-Denoiserは、微分可能なマスクと勾配正則化は、それらの主要なアーキテクチャを変更しないので、多くのTransformerベースの逐次推薦モデルと互換性があることを言及する価値がある。 If we simply set all masks Z (𝑙) to be all-ones matrix and 𝛽 = 𝛾 = 0, our model boils down to their original designs. 単純にすべてのマスクZ (↪Ll_1D459) をオール1の行列とし、𝛽 = ↪Ll_1D6FE = 0とすると、モデルは元の設計に帰着する。 If we randomly set subset of masks Z (𝑙) to be zeros, it is equivalent to structured Dropout like LayerDrop [17], DropHead [60]. マスクのサブセットZ (↪Ll45↩︎)をランダムにゼロに設定すると、LayerDrop [17]やDropHead [60]のような構造化Dropoutと等価になる。 In addition, our Rec-Denoiser can work together with linearized self-attention networks [27, 59] to further reduce the complexity of attentions. さらに、私たちのRec-Denoiserは、線形化された自己注意ネットワーク[27, 59]と連携することができ、注意の複雑さをさらに軽減することができる。 We leave this extension in the future. 私たちはこの延長を将来に残す。</p>
</section>
<section id="model-complexity-4.3.2.-モデルの複雑さ" class="level3">
<h3 class="anchored" data-anchor-id="model-complexity-4.3.2.-モデルの複雑さ">4.3.2. Model Complexity 4.3.2. モデルの複雑さ</h3>
<p>The complexity of Rec-Denoiser comes from three parts: a basic Transformer, differentiable masks, and Jacobian regularization. Rec-Denoiserの複雑さは、基本的な変換器、微分可能なマスク、ヤコビアンの正則化という3つの部分から来ている。 The complexity of basic Transformer keeps the same as SASRec [26] or BERT4Rec [41]. 基本的なTransformerの複雑さは、SASRec [26]やBERT4Rec [41]と同じである。 The complexity of differentiable masks requires either one-forward pass (e.g., AR with high variance) or two-forward pass (e.g., ARM with low variance) of the model. 微分可能なマスクの複雑さは、モデルのワン・フォワード・パス（高分散のARなど）またはツー・フォワード・パス（低分散のARMなど）を必要とする。 In sequential recommenders, the number of Transformer blocks is often very small (e.g., 𝐿 = 2 in SASRec [26] and BERT4Rec [41] ). 逐次レコメンダーでは、Transformerブロックの数は非常に少ないことが多い（例えば、SASRec [26]とBERT4Rec [41] では 𝐿 = 2 ）。 It is thus reasonable to use the ARM estimator without heavy computations. 従って、重い計算をせずにARM推定量を使用することは合理的である。 Besides, we compare the performance of AR and ARM estimators in Sec 5.3.Moreover, the random project techniques are surprisingly efficient to compute the norms of Jacobians [21]. さらに、ランダム・プロジェクト技法はヤコビアンのノルムを計算するのに驚くほど効率的である[21]。 As a result, the overall computational complexity remains the same order as the original Transformers during the training. その結果、全体的な計算量は、トレーニング中のオリジナルのTransformersと同じオーダーのままである。 However, during the inference, our attention maps are very sparse, which enables much faster feed-forward computations. しかし、推論中の注意マップは非常に疎なため、フィードフォワード計算をより高速に行うことができる。</p>
</section>
</section>
</section>
<section id="experiments-5.-実験" class="level1">
<h1>5. Experiments 5. 実験</h1>
<p>Here we present our empirical results. ここでは実証的な結果を紹介する。 Our experiments are designed to answer the following research questions: 我々の実験は、以下の研究課題に答えるためにデザインされた：</p>
<ul>
<li><p>RQ1: How effective is the proposed Rec-Denoiser compared to the state-of-the-art sequential recommenders? RQ1: 提案するRec-Denoiserは、最新の逐次推薦器と比較してどの程度有効か？</p></li>
<li><p>RQ2: How can Rec-Denoiser reduce the negative impacts of noisy items in a sequence? RQ2：Rec-Denoiserは、シーケンス内のノイズの多いアイテムの悪影響をどのように軽減できますか？</p></li>
<li><p>RQ3: How do different components (e.g., differentiable masks and Jacobian regularization) affect the overall performance of Rec-Denoiser? RQ3：異なる構成要素（微分可能マスクやヤコビアン正則化など）は、Rec-Denoiserの全体的な性能にどのような影響を与えるか？</p></li>
</ul>
<section id="experimental-setting-5.1.experimental-setting" class="level2">
<h2 class="anchored" data-anchor-id="experimental-setting-5.1.experimental-setting">5.1. Experimental Setting # 5.1.Experimental Setting</h2>
<section id="dataset-5.1.1.-データ集合" class="level3">
<h3 class="anchored" data-anchor-id="dataset-5.1.1.-データ集合">5.1.1. Dataset 5.1.1. データ集合</h3>
<p>We evaluate our models on five benchmark datasets: Movielens5 , Amazon6 (we choose the three commonly used categories: Beauty, Games, and Movies&amp;TV), and Steam7 [30]. 我々は5つのベンチマークデータセットでモデルを評価する： Movielens5、Amazon6（美容、ゲーム、映画＆TVの3つのよく使われるカテゴリーを選択）、Steam7 [30]である。 Their statistics are shown in Table 1. 統計は表1の通りである。 Among different datasets, MovieLens is the most dense one while Beauty has the fewest actions per user. 異なるデータセットの中で、MovieLensは最も密度が高く、Beautyはユーザーあたりのアクション数が最も少ない。 We use the same procedure as [26, 30, 39] to perform preprocessing and split data into train/valid/test sets, i.e., the last item of each user’s sequence for testing, the second-to-last for validation, and the remaining items for training. 26,30,39]と同じ手順で前処理を行い、データをtrain/valid/testセットに分割する。つまり、各ユーザーのシーケンスの最後のアイテムをtestingに、最後から2番目のアイテムをvalidationに、残りのアイテムをtrainingに使用する。</p>
</section>
<section id="baselines-5.1.2.-ベースライン" class="level3">
<h3 class="anchored" data-anchor-id="baselines-5.1.2.-ベースライン">5.1.2. Baselines 5.1.2. ベースライン</h3>
<p>Here we include two groups of baselines. ここでは、2つのベースライン・グループが含まれている。 The first group includes general sequential methods (Sec 5.2): 1) FPMC [39]: a mixture of matrix factorization and first-order Markov chains model; 2) GRU4Rec [20]: a RNN-based method that models user action sequences; 3) Caser [42]: a CNN-based framework that captures high-order relationships via convolutional operations; 4) SASRec [26]: a Transformer-based method that uses left-to-right selfattention layers; 5) BERT4Rec [41]: an architecture that is similar to SASRec, but using bidirectional self-attention layers; 6) TiSASRec [30]: a time-aware self-attention model that further considers the relative time intervals between any two items; 7) SSE-PT [50]: a framework that introduces personalization into self-attention layers; 8) Rec-Denoiser: our proposed Rec-Denoiser that can choose any self-attentive models as its backbone. 最初のグループには、一般的な逐次的手法が含まれる（Sec.5.2）： 1) FPMC [39]：行列分解と一次マルコフ連鎖モデルの混合、2) GRU4Rec [20]：ユーザーの行動シーケンスをモデル化するRNNベースの手法、3) Caser [42]：畳み込み演算によって高次の関係を捉えるCNNベースのフレームワーク、4) SASRec [26]：左から右への自己注意層を使用するTransformerベースの手法、5) BERT4Rec [41]： 6) TiSASRec [30]：任意の2つのアイテムの間の相対的な時間間隔をさらに考慮する、時間を考慮した自己注意モデル、7) SSE-PT [50]：自己注意層にパーソナライズを導入するフレームワーク、8) Rec-Denoiser：バックボーンとして任意の自己注意モデルを選択できる、我々の提案するRec-Denoiser。 The second group contains sparse Transformers (Sec 5.3): 1) Sparse Transformer [10]: it uses a fixed attention pattern, where only specific cells summarize previous locations in the attention layers; 2) 𝛼-entmax sparse attention [12]: it simply replaces softmax with 𝛼-entmax to achieve sparsity. 番目のグループにはスパース変換器（Sec.5.3）が含まれる： 2) 𝛼-entmax sparse attention [12]: スパース性を達成するためにソフトマックスを 𝛼-entmax に置き換えたもの。 Note that we do not compare with other popular sparse Transformers like Star Transformer [18], Longformer [2], and BigBird [58]. なお、Star Transformer [18]、Longformer [2]、BigBird [58]のような他の有名なスパース変換器とは比較していない。 These Transformers are specifically designed for thousands of tokens or longer in the language modeling tasks. これらのトランスフォーマーは、言語モデリングタスクにおける数千以上のトークン用に特別に設計されている。 We leave their explorations for recommendations in the future. 彼らの探求は今後の提言に委ねたい。 We also do not compare with LayerDrop [17] and DropHead [60] since the number of Transformer blocks and heads are often very small (e.g., 𝐿 = 2 in SARRec) in sequential recommendation. また、LayerDrop[17]やDropHead[60]との比較は行わない。なぜなら、逐次推薦では、Transformerブロックやヘッドの数が非常に少ない（例えば、SARRecでは𝐿 = 2）ことが多いからである。 Other sequential architectures like memory networks [9, 22] and graph neural networks [4, 51] have been outperformed by the above baselines, we simply omit these baselines and focus on Transformer-based models. メモリ・ネットワーク[9, 22]やグラフ・ニューラル・ネットワーク[4, 51]のような他の逐次アーキテクチャは、上記のベースラインよりも優れている。 The goal of experiments is to see whether the proposed differentiable mask techniques can reduce the negative impacts of noisy items in the self-attention layers. 実験の目的は、提案された微分可能なマスク技術が、自己アテンション層におけるノイズアイテムの悪影響を軽減できるかどうかを確認することである。</p>
</section>
<section id="evaluation-metrics-5.1.3.-評価指標" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-metrics-5.1.3.-評価指標">5.1.3. Evaluation metrics 5.1.3. 評価指標</h3>
<p>For easy comparison, we adopt two common Top-N metrics, Hit@𝑁 and NDCG@𝑁 (with default value 𝑁 = 10), to evaluate the performance of sequential models [26, 30, 41]. 比較を容易にするために、逐次モデルのパフォーマンスを評価するために、2つの一般的なTop-Nメトリクス、Hit@_141とNDCG@_441（デフォルト値ǔ = 10）を採用する[26, 30, 41]。 Typically, Hit@𝑁 counts the rates of the ground-truth items among top-𝑁 items, while NDCG@𝑁 considers the position and assigns higher weights to higher positions. 一般的に、Hit@_141は、トップ_1アイテムの中でグランドトゥルースアイテムの割合をカウントし、NDCG@_441は位置を考慮し、高い位置に高い重みを割り当てる。 Following the work [26, 30], for each user, we randomly sample 100 negative items, and rank these items with the ground-truth item. 26,30]に従い、各ユーザーについて100個のネガティブアイテムをランダムにサンプリングし、これらのアイテムをグランドトゥルースアイテムと順位付けする。 We calculate Hit@10 and NDCG@10 based on the rankings of these 101 items. この101項目のランキングをもとにHit@10とNDCG<span class="citation" data-cites="10を算出した">@10を算出した</span>。</p>
</section>
<section id="parameter-settings-5.1.4.-パラメータ設定" class="level3">
<h3 class="anchored" data-anchor-id="parameter-settings-5.1.4.-パラメータ設定">5.1.4. Parameter settings 5.1.4. パラメータ設定</h3>
<p>For all baselines, we initialize the hyper-parameters as the ones suggested by their original work. すべてのベースラインについて、ハイパーパラメータを彼らのオリジナル研究で提案されたものとして初期化した。 They are then well tuned on the validation set to achieve optimal performance. そして、最適なパフォーマンスを達成するために、検証セット上で十分に調整される。 The final results are conducted on the test set. 最終的な結果はテストセットで実施される。 We search the dimension size of items within {10, 20, 30, 40, 50}. 項目の次元サイズを{10, 20, 30, 40, 50}の範囲で検索する。 As our Rec-Denoiser is a general plugin, we use the same hyper-parameters as the basic Transformers, e.g., number of Transformer blocks, batch size, learning rate in Adam optimizer, etc. 我々のRec-Denoiserは一般的なプラグインであるため、基本的なTransformerと同じハイパーパラメーターを使用する。例えば、Transformerブロックの数、バッチサイズ、Adamオプティマイザーの学習率などである。 According to Table 1, we set the maximum length of item sequence 𝑛 = 50 for dense datasets MovieLens and Movies&amp;TV, and 𝑛 = 25 for sparse datasets Beauty, Games, and Steam. 表1によると、密なデータセットであるMovieLensとMovies&amp;TVにはアイテム列の最大長𝑛 = 50を、疎なデータセットであるBeauty、Games、Steamには𝑛 = 25を設定した。 In addition, we set the number of Transformer blocks 𝐿 = 2, and the number of heads 𝐻 = 2 for self-attentive models. さらに、トランスフォーマーブロックの数𝐿 = 2、自己注意モデルのヘッド数𝐻 = 2とした。 For Rec-Denoiser, two extra regularizers 𝛽 and 𝛾 are both searched within {10−1 , 10−2 , . Rec-Denoiserでは、2つの正則化子ǽと𝛾が{10-1 , 10-2 , . .. .. , 10−5 }. , 10-5 }. We choose ARM estimator due to the shallow structures of self-attentive recommenders. 我々は、自己アテンション型推薦者の浅い構造からARM推定器を選択した。</p>
</section>
</section>
<section id="overall-performancerq1-5.2.総合成績rq1" class="level2">
<h2 class="anchored" data-anchor-id="overall-performancerq1-5.2.総合成績rq1">5.2. Overall Performance(RQ1) 5.2.総合成績（RQ1）</h2>
<p>Table 2 presents the overall recommendation performance of all methods on the five datasets. 表2は、5つのデータセットにおけるすべての手法の総合的な推薦性能を示している。 Our proposed Recdenoisers consistently obtain the best performance for all datasets. 我々の提案するRecdenoisersは、すべてのデータセットで一貫して最高の性能を得た。 Additionally, we have the following observations: さらに、次のような見解もある：</p>
<ul>
<li><p>The self-attentive sequential models (e.g., SASRec, BERT4Rec, TiSASRec, and SSE-PT) generally outperform FPMC, GRU4Rec, and Caser with a large margin, verifying that the self-attention networks have good ability of capture long-range item dependencies for the task of sequential recommendation. 自己注意型逐次推薦モデル（SASRec、BERT4Rec、TiSASRec、SSE-PTなど）は、一般にFPMC、GRU4Rec、Caserを大きなマージンをもって上回り、自己注意型ネットワークが逐次推薦のタスクに対して長距離項目依存性を捕捉する優れた能力を持つことが検証された。</p></li>
<li><p>Comparing the original SASRec and its variants BERT4Rec, TiSASRec and SSE-PT, we find that the self-attentive models can gets benefit from incorporating additional information such as bi-directional attentions, time intervals, and user personalization. Such auxiliary information is important to interpret the dynamic behaviors of users. オリジナルのSASRecとその変種であるBERT4Rec、TiSASRec、SSE-PTを比較すると、自己注意モデルは、双方向の注意、時間間隔、ユーザーのパーソナライゼーションなどの追加情報を取り入れることで利益を得ることができることがわかる。 このような補助情報は、ユーザーのダイナミックな行動を解釈するために重要である。</p></li>
<li><p>The relative improvements of Rec-denoisers over their backbones are significant for all cases. For example, SASRec+Denoiser has on average 8.04% improvement with respect to Hit@10 and over 12.42% improvements with respect to NDCG@10. Analogously, BERT4Rec+Denoiser outperforms the vanilla BERT4Rec by average 7.47% in Hit@10 and 11.64% in NDCG@10. We also conduct the significant test between Rec-denoisers and their backbones, where all 𝑝-values&lt; 1𝑒 −6 , showing that the improvements of Rec-denoisers are statistically significant in all cases. Rec-denoisersのバックボーンに対する相対的な向上は、すべてのケースで顕著である。 例えば、SASRec+Denoiserは、Hit@10に対して平均8.04%、NDCG@10に対して平均12.42%以上の改善が見られる。 同様に、BERT4Rec+Denoiserは、Hit@10では平均7.47%、NDCG@10では平均11.64%で、バニラBERT4Recを上回っている。 また、Rec-denoiserとそのバックボーンとの間の有意差検定も行った。ここでは、すべての ↪L_1D45↩︎値&lt; 1𝑒 -6であり、Rec-denoiserの改善がすべてのケースで統計的に有意であることが示された。</p></li>
</ul>
<p>These improvements of our proposed models are mainly attributed to the following reasons: 1) Rec-denoisers inherit full advantages of the self-attention networks as in SASRec, BERT4Rec, TiSASRec, and SSE-PT; 2) Through differentiable masks, irrelevant item-item dependencies are removed, which could largely reduce the negative impacts of noisy data; 3) Jacobian regularization enforces the smoothness of gradients, limiting quick changes of the output against input perturbations. 提案モデルのこれらの改善は主に以下の理由による： 1）Rec-denoisersは、SASRec、BERT4Rec、TiSASRec、SSE-PTのような自己注意ネットワークの利点を完全に受け継いでいる、2）微分可能なマスクを通して、無関係な項目-項目依存性が除去され、ノイズの多いデータの悪影響を大幅に軽減できる、3）ヤコビアの正則化は勾配の滑らかさを強制し、入力摂動に対する出力の素早い変化を制限する。 In general, smoothness improves the generalization of sequential recommendation. 一般的に、滑らかさは逐次推薦の一般性を向上させる。 Overall, the experimental results demonstrate the superiority of our Rec-Denoisers. 全体として、実験結果は我々のRec-Denoisersの優位性を示している。</p>
</section>
<section id="robustness-to-noisesrq2" class="level2">
<h2 class="anchored" data-anchor-id="robustness-to-noisesrq2">5.3. Robustness to Noises(RQ2)</h2>
<p>As discussed before, the observed item sequences often contain some noisy items that are uncorrelated to each other. 前述したように、観測された項目列には、しばしば互いに無相関なノイズ項目が含まれる。 Generally, the performance of self-attention networks is sensitive to noisy input. 一般に、自己注意ネットワークの性能はノイズの多い入力に敏感である。 Here we analyze how robust our training strategy is for noisy sequences. ここでは、ノイズの多いシーケンスに対して、我々のトレーニング戦略がどの程度ロバストであるかを分析する。 To achieve this, we follow the strategy [35] that corrupts the training data by randomly replacing a portion of the observed items in the training set with uniformly sampled items that are not in the validation or test set. これを達成するために、我々は、トレーニングセットで観測された項目の一部を、検証セットやテストセットにはない一様にサンプリングされた項目でランダムに置き換えることによって、トレーニングデータを破損させる戦略[35]に従う。 We range the ratio of the corrupted training data from 0% to 25%. 破損したトレーニングデータの比率は、0%から25%の範囲である。 We only report the results of SASRec and SASRec-Denoiser in terms of Hit@10. SASRecとSASRec-Denoiserの結果は、Hit@10でのみ報告する。 The performance of other self-attentive models is the same and omitted here due to page limitations. 他の自己アテンションモデルのパフォーマンスも同様で、ここではページの都合上省略した。 In addition, we compare with two recent sparse Transformers: Sparse Transformer [10] and 𝛼-entmax sparse attention [12]. さらに、最近の2つのスパース変換器と比較する： Sparse Transformer [10]と ↪L_1D6FC↩︎-entmax sparse attention [12]である。 All the simulated experiments are repeated five times and the average results are shown in Figure 2. すべての模擬実験を5回繰り返し、その平均結果を図2に示す。 Clearly, the performance of all models degrades with the increasing noise ratio. 明らかに、すべてのモデルの性能は、ノイズ比率の増加とともに低下する。 We observe that our Rec-denoiser (use either ARM or AR estimators) consistently outperforms 𝛼-entmax and Sparse Transformer under different ratios of noise on all datasets. 我々は、我々のRec-denoiser（ARMまたはAR推定量を使用）が、全てのデータセットにおいて、異なるノイズ比率の下で一貫して↪L_1D6FC↩︎-entmaxとSparse Transformerを上回ることを観察した。 𝛼-entmax heavily relies on one trainable parameter 𝛼 to filter out the noise, which may be over tuned during the training, while Sparse Transformer adopts a fixed attention pattern, which may lead to uncertain results, especially for short item sequences like Beauty and Games. 𝛼-entmax は、ノイズをフィルタリングするための1つの訓練可能なパラメータ𝛼に大きく依存しており、訓練中に過剰に調整される可能性があります。一方、Sparse Transformerは固定された注意パターンを採用しており、特にBeautyやGamesのような短いアイテムシーケンスでは、不確実な結果につながる可能性があります。 In contrast, our differentaible masks have much more flexibility to adapt to noisy sequences. 対照的に、我々の異種マスクは、ノイズの多いシーケンスに適応する柔軟性がはるかに高い。 The Jacobian regularization further encourages the smoothness of our gradients, leading to better generalization. ヤコビアン正則化は、勾配の滑らかさをさらに促進し、より良い汎化をもたらす。 From the results, the AR estimator performs better than 𝛼-entmax but worse than ARM. その結果、AR推定器は𝛼-entmaxよりは良いが、ARMよりは悪い。 This result is expected since ARM has much low variance. ARMは分散が少ないので、この結果は予想通りである。 In summary, both ARM and AR estimators are able to reduce the negative impacts of noisy sequences, which could improve the robustness of self-attentive models. まとめると、ARMとARの両推定器は、ノイズの多いシーケンスの悪影響を軽減することができ、自己注意モデルの頑健性を向上させることができる。</p>
</section>
<section id="study-of-rec-denoiserrq3" class="level2">
<h2 class="anchored" data-anchor-id="study-of-rec-denoiserrq3">5.4. Study of Rec-Denoiser(RQ3)</h2>
<p>We further investigate the parameter sensitivity of Rec-Denoiser. さらにRec-Denoiserのパラメータ感度を調べた。 For the number of blocks 𝐿 and the number of heads 𝐻, we find that self-attentive models typically benefit from small values (e.g., 𝐻, 𝐿 ≤ 4), which is similar to [31, 41]. ブロック数ᵃとヘッド数ᵃについては、自己注意モデルは一般的に小さな値（例えば、ᵃ≦4）が有効であることがわかり、これは[31, 41]と同様である。 In this section, we mainly study the following hyper-parameters: 1) the maximum length 𝑛, 2) the regularizers 𝛽 and 𝛾 to control the sparsity and smoothness. 本節では、主に以下のハイパーパラメータを研究する： 1)最大長𝑛、2)スパース性と平滑性を制御する正則化子ǽと↪L_1FE↩︎。 Here we only study the SASRec and SASRec-Denoiser due to page limitations. ここでは、ページの都合上、SASRecとSASRec-Denoiserについてのみ述べる。</p>
<p>Fig.3.Effect of maximum length 𝑛 on ranking performance (Hit@10). 図3.最大長𝑛がランキング性能に与える影響（Hit@10）。</p>
<p>Fig.4.Effect of regularizers 𝛽 and 𝛾 on ranking performance (Hit@10). 図4.正則化量ǽとǖがランキング性能に与える影響(Hit@10FE6)。</p>
<section id="maximum-length-n-5.4.1.-最大長-n" class="level3">
<h3 class="anchored" data-anchor-id="maximum-length-n-5.4.1.-最大長-n">5.4.1. Maximum Length <span class="math inline">\(n\)</span> 5.4.1. 最大長 <span class="math inline">\(n\)</span></h3>
<p>Figure 3 shows the Hit@10 for maximum length 𝑛 from 20 to 80 while keeping other optimal hyper-parameters unchanged. 図3は、他の最適ハイパーパラメータを変えずに、最大長𝑛を20から80まで変化させた場合のHit@10を示している。 We only test on the densest and sparsest datasets: MovieLeans and Beauty. 最も高密度で疎なデータセットでのみテストする： MovieLeansとBeautyである。 Intuitively, the larger sequence we have, the larger probability that the sequence contains noisy items. 直観的には、シーケンスが大きければ大きいほど、そのシーケンスにノイズのある項目が含まれる確率が高くなる。 FWe observed that our SASRec-Denoiser improves the performance dramatically with longer sequences. F我々のSASRec-Denoiserは、長いシーケンスで劇的に性能が向上することが確認された。 This demonstrates that our design is more suitable for longer inputs, without worrying about the quality of sequences. これは、我々の設計がシーケンスの質を気にすることなく、より長い入力に適していることを示している。</p>
</section>
<section id="the-regularizers-beta-and-gamma-5.4.2.-正則化記号betaとgamma" class="level3">
<h3 class="anchored" data-anchor-id="the-regularizers-beta-and-gamma-5.4.2.-正則化記号betaとgamma">5.4.2. The regularizers <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\gamma\)</span> 5.4.2. 正則化記号<span class="math inline">\(beta\)</span>と<span class="math inline">\(gamma\)</span>。</h3>
<p>There are two major regularization parameters 𝛽 and 𝛾 for sparsity and gradient smoothness, respectively. 正則化パラメータǽと𝛾は、それぞれスパース性と勾配平滑性を表す。 Figure 4 shows the performance by changing one parameter while fixing the other as 0.01. 図4は、一方のパラメーターを0.01に固定し、もう一方のパラメーターを変更した場合のパフォーマンスを示している。 As can be seen, our performance is relatively stable with respect to different settings. 見てわかるように、我々のパフォーマンスは異なる設定に対して比較的安定している。 In the experiments, the best performance can be achieved at 𝛽 = 0.01 and 𝛾 = 0.001 for the MovieLens dataset. 実験では、MovieLensデータセットに対して、↪Ll_1D6FD = 0.01とǖ = 0.001で最高の性能を達成することができた。</p>
</section>
</section>
</section>
<section id="conclusion-and-fture-work-6.-結論と今後の課題" class="level1">
<h1>6. Conclusion and Fture Work 6. 結論と今後の課題</h1>
<p>In this work, we propose Rec-Denoiser to adaptively eliminate the negative impacts of the noisy items for self-attentive recommender systems. 本研究では、Rec-Denoiserを提案し、自己アテンション型レコメンダーシステムにおけるノイズアイテムの悪影響を適応的に除去する。 The proposed Rec-Denoiser employs differentiable masks for the self-attention layers, which can dynamically prune irrelevant information. 提案するRec-Denoiserは、自己注意層に微分可能なマスクを採用し、無関係な情報を動的に除去することができる。 To further tackle the vulnerability of self-attention networks to small perturbations, Jacobian regularization is applied to the Transformer blocks to improve the robustness. 小さな摂動に対する自己アテンション・ネットワークの脆弱性にさらに取り組むため、ロバスト性を向上させるヤコビアン正則化がトランスフォーマー・ブロックに適用される。 Our experimental results on multiple real-world sequential recommendation tasks illustrate the effectiveness of our design. 実世界の複数の逐次推薦タスクに関する実験結果は、我々の設計の有効性を示している。</p>
<p>Our proposed Rec-Denoiser framework (e.g., differentiable masks and Jacobian regularization) can be easily applied to any Transformer-based models in many tasks besides sequential recommendation. 我々の提案するRec-Denoiserフレームワーク（微分可能マスクやヤコビアン正則化など）は、逐次推薦以外の多くのタスクにおいて、あらゆるTransformerベースのモデルに容易に適用できる。 In the future, we will continue to demonstrate the contributions of our design in many real-world applications. 将来的には、多くの実世界のアプリケーションにおいて、我々の設計の貢献を実証していくつもりである。</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>