<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.168">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>chen_et_al_2022_trans2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="chen_et_al_2022_trans2_files/libs/clipboard/clipboard.min.js"></script>
<script src="chen_et_al_2022_trans2_files/libs/quarto-html/quarto.js"></script>
<script src="chen_et_al_2022_trans2_files/libs/quarto-html/popper.min.js"></script>
<script src="chen_et_al_2022_trans2_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="chen_et_al_2022_trans2_files/libs/quarto-html/anchor.min.js"></script>
<link href="chen_et_al_2022_trans2_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="chen_et_al_2022_trans2_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="chen_et_al_2022_trans2_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="chen_et_al_2022_trans2_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="chen_et_al_2022_trans2_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<section id="link-0.1.-ãƒªãƒ³ã‚¯" class="level2">
<h2 class="anchored" data-anchor-id="link-0.1.-ãƒªãƒ³ã‚¯">0.1. link 0.1. ãƒªãƒ³ã‚¯</h2>
<ul>
<li><p>https://dl.acm.org/doi/abs/10.1145/3523227.3546788 httpsã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚</p></li>
<li><p>https://arxiv.org/pdf/2212.04120.pdf httpsã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚</p></li>
</ul>
</section>
<section id="title-0.2.-ã‚¿ã‚¤ãƒˆãƒ«" class="level2">
<h2 class="anchored" data-anchor-id="title-0.2.-ã‚¿ã‚¤ãƒˆãƒ«">0.2. title 0.2. ã‚¿ã‚¤ãƒˆãƒ«</h2>
<p>Denoising Self-Attentive Sequential Recommendation ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°è‡ªå·±èª¿æ•´å‹é€æ¬¡æ¨è–¦æ³•</p>
</section>
<section id="abstract-0.3.-æŠ½è±¡çš„" class="level2">
<h2 class="anchored" data-anchor-id="abstract-0.3.-æŠ½è±¡çš„">0.3. abstract 0.3. æŠ½è±¡çš„</h2>
<p>Transformer-based sequential recommenders are very powerful for capturing both short-term and long-term sequential item dependencies. ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã«åŸºã¥ãsequentialæ¨è–¦å™¨ã¯ã€çŸ­æœŸçš„ãŠã‚ˆã³é•·æœŸçš„ãª<strong>sequentialãªã‚¢ã‚¤ãƒ†ãƒ é–“ã®ä¾å­˜é–¢ä¿‚</strong>ã‚’æ‰ãˆã‚‹ã®ã«éå¸¸ã«å¼·åŠ›ã§ã‚ã‚‹. This is mainly attributed to their unique self-attention networks to exploit pairwise item-item interactions within the sequence. ã“ã‚Œã¯ä¸»ã«ã€sequenceå†…ã®ãƒšã‚¢ãƒ¯ã‚¤ã‚ºã‚¢ã‚¤ãƒ†ãƒ -ã‚¢ã‚¤ãƒ†ãƒ ç›¸äº’ä½œç”¨ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã®ã€ç‹¬è‡ªã®self-attention ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«èµ·å› ã—ã¦ã„ã‚‹. However, real-world item sequences are often noisy, which is particularly true for implicit feedback. ã—ã‹ã—ã€<strong>å®Ÿä¸–ç•Œã®item sequencesã¯ã—ã°ã—ã°ãƒã‚¤ã‚ºãŒå¤šãã€ç‰¹ã«æš—é»™çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«ã¯ãã‚ŒãŒå½“ã¦ã¯ã¾ã‚‹</strong>ã€‚ For example, a large portion of clicks do not align well with user preferences, and many products end up with negative reviews or being returned. ä¾‹ãˆã°ã€<strong>ã‚¯ãƒªãƒƒã‚¯ã®å¤§éƒ¨åˆ†ã¯ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã«åˆã‚ãªã„ã—</strong>ã€å¤šãã®è³¼å…¥ã—ãŸè£½å“ã¯å¦å®šçš„ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ãŸã‚Šã€è¿”å“ã•ã‚Œã‚‹ã“ã¨ã«ãªã‚‹. As such, the current user action only depends on a subset of items, not on the entire sequences. ã“ã®ã‚ˆã†ã«ã€ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã§ã¯ãªãã€ã‚¢ã‚¤ãƒ†ãƒ ã®subset(éƒ¨åˆ†é›†åˆ)ã«ã®ã¿ä¾å­˜ã™ã‚‹ã€‚ Many existing Transformer-based models use full attention distributions, which inevitably assign certain credits to irrelevant items. æ—¢å­˜ã®Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã®å¤šãã¯ã€å®Œå…¨ãªattentionåˆ†å¸ƒã‚’ä½¿ç”¨ã—ã¦ãŠã‚Šã€å¿…ç„¶çš„ã«ç„¡é–¢ä¿‚ãªã‚¢ã‚¤ãƒ†ãƒ ã«ä¸€å®šã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã«ãªã‚‹ã€‚ This may lead to sub-optimal performance if Transformers are not regularized properly. ã“ã‚Œã¯TransformerãŒé©åˆ‡ã«æ­£å‰‡åŒ–ã•ã‚Œãªã„å ´åˆã€æœ€é©ã§ãªã„æ€§èƒ½ã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹.</p>
<p>Here we propose the Rec-denoiser model for better training of self-attentive recommender systems. æœ¬è«–æ–‡ã§ã¯ã€è‡ªå·±èª¿æ•´å‹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ã‚ˆã‚Šè‰¯ã„å­¦ç¿’ã®ãŸã‚ã«ã€<strong>Rec-denoiserãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆ</strong>ã™ã‚‹ã€‚ In Rec-denoiser, we aim to adaptively prune noisy items that are unrelated to the next item prediction. Rec-denoiserã§ã¯ã€<strong>next item prediction</strong>ã«é–¢ä¿‚ã®ãªã„ãƒã‚¤ã‚ºã®å¤šã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’é©å¿œçš„ã«åˆˆã‚Šå–ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã™ã‚‹.(ãã£ã‹ã€NLPã®next-token predictionã¿ãŸã„ãªæ„Ÿã˜â€¦!) To achieve this, we simply attach each self-attention layer with a trainable binary mask to prune noisy attentions, resulting in sparse and clean attention distributions. ãã®ãŸã‚ã€<strong>å„self-attentionå±¤ã«å­¦ç¿’å¯èƒ½ãªbinary maskã‚’ä»˜åŠ </strong>ã—ã€ãƒã‚¤ã‚ºã®å¤šã„attentionã‚’é™¤å»ã™ã‚‹ã“ã¨ã§ã€ã‚¹ãƒ‘ãƒ¼ã‚¹ã§ã‚¯ãƒªãƒ¼ãƒ³ãªattentionåˆ†å¸ƒãŒå¾—ã‚‰ã‚Œã‚‹ã€‚ This largely purifies item-item dependencies and provides better model interpretability. ã“ã‚Œã«ã‚ˆã‚Šã€item-itemã®ä¾å­˜æ€§(ã®å¤šã??)ãŒã»ã¼é™¤å»ã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆå¯èƒ½æ€§ãŒå‘ä¸Šã™ã‚‹ã€‚ In addition, the self-attention network is typically not Lipschitz continuous and is vulnerable to small perturbations. ã•ã‚‰ã«ã€self-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ä¸€èˆ¬çš„ã«ãƒªãƒ—ã‚·ãƒƒãƒ„é€£ç¶šã§ã¯ãªãã€å°ã•ãªæ‘‚å‹•ã«å¼±ã„.(ã“ã®ã¸ã‚“ã‚ˆãã‚ã‹ã‚‰ã‚“â€¦!) Jacobian regularization is further applied to the Transformer blocks to improve the robustness of Transformers for noisy sequences. <strong>ã•ã‚‰ã«ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–ã‚’Transformerãƒ–ãƒ­ãƒƒã‚¯ã«é©ç”¨ã—ã€ãƒã‚¤ã‚ºã®å¤šã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã™ã‚‹Transformerã®é ‘å¥æ€§ã‚’å‘ä¸Šã•ã›ã‚‹</strong>ã€‚ Our Rec-denoiser is a general plugin that is compatible to many Transformers. <strong>æˆ‘ã€…ã®Rec-denoiserã¯å¤šãã®Transformerã«å¯¾å¿œã™ã‚‹æ±ç”¨ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã§ã‚ã‚‹</strong>. (general pluginã€ç´ æ™´ã‚‰ã—ã„â€¦!!) Quantitative results on real-world datasets show that our Rec-denoiser outperforms the state-of-the-art baselines. å®Ÿä¸–ç•Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã‘ã‚‹å®šé‡çš„ãªçµæœã¯ã€æˆ‘ã€…ã®Rec-denoiserãŒæœ€å…ˆç«¯ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’å‡Œé§•ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹.</p>
</section>
<section id="introduction-1.ã¯ã˜ã‚ã«" class="level1">
<h1>1. introduction 1.ã¯ã˜ã‚ã«</h1>
<p>Sequential recommendation aims to recommend the next item based on a userâ€™s historical actions [20, 35, 39, 44, 47], e.g., to recommend a bluetooth headphone after a user purchases a smart phone. <strong>Sequential recommendationã®ç›®çš„ã¯ï¼Œãƒ¦ãƒ¼ã‚¶ã®éå»ã®è¡Œå‹•ã«åŸºã¥ã„ã¦æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ¨è–¦ã™ã‚‹ã“ã¨(next-item-prediction)</strong>ã§ã‚ã‚‹[20, 35, 39, 44, 47]ï¼ä¾‹ãˆã°ï¼Œ<strong>ãƒ¦ãƒ¼ã‚¶ãŒã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã‚’è³¼å…¥ã—ãŸå¾Œã«Bluetoothãƒ˜ãƒƒãƒ‰ãƒ›ãƒ³ã‚’æ¨è–¦ã™ã‚‹ã‚ˆã†ãªå ´åˆ</strong>ã§ã‚ã‚‹. Learning sequential user behaviors is, however, challenging since a userâ€™s choices on items generally depend on both long-term and short-term preferences. ã—ã‹ã—ï¼Œä¸€èˆ¬ã«ãƒ¦ãƒ¼ã‚¶ã®ã‚¢ã‚¤ãƒ†ãƒ é¸æŠã¯<strong>é•·æœŸçš„å—œå¥½(long-term preference)</strong>ã¨<strong>çŸ­æœŸçš„å—œå¥½(short-term preference)</strong>ã®ä¸¡æ–¹ã«ä¾å­˜ã™ã‚‹ãŸã‚ï¼Œé€æ¬¡çš„(sequential)ãªãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã®å­¦ç¿’ã¯å›°é›£ã§ã‚ã‚‹. Early Markov Chain models [19, 39] have been proposed to capture short-term item transitions by assuming that a userâ€™s next decision is derived from a few preceding actions, while neglecting long-term preferences. åˆæœŸã®ãƒãƒ«ã‚³ãƒ•é€£é–ãƒ¢ãƒ‡ãƒ«[19, 39]ã¯ã€ãƒ¦ãƒ¼ã‚¶ã®æ¬¡ã®æ±ºå®šãŒã„ãã¤ã‹ã®å…ˆè¡Œè¡Œå‹•ã‹ã‚‰å°ã‹ã‚Œã‚‹ã¨ä»®å®šã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€çŸ­æœŸçš„ãªã‚¢ã‚¤ãƒ†ãƒ ã®é·ç§»ã‚’æ‰ãˆã‚‹ãŸã‚ã«ææ¡ˆã•ã‚ŒãŸãŒã€é•·æœŸçš„ãªå—œå¥½ã¯ç„¡è¦–ã•ã‚ŒãŸã¾ã¾ã§ã‚ã£ãŸ. To alleviate this limitation, many deep neural networks have been proposed to model the entire usersâ€™ sequences and achieve great success, including recurrent neural networks [20, 53] and convolutional neural networks [42, 54, 57]. ã“ã®é™ç•Œã‚’ç·©å’Œã™ã‚‹ãŸã‚ã«ã€ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯[20, 53]ã‚„ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯[42, 54, 57]ãªã©ã€<strong>ãƒ¦ãƒ¼ã‚¶ã®sequenceå…¨ä½“ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹å¤šãã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒææ¡ˆã•ã‚Œã€å¤§ããªæˆåŠŸã‚’åã‚ã¦ã„ã‚‹</strong>.</p>
<p>Recently, Transformers have shown promising results in various tasks, such as machine translation [43]. æœ€è¿‘ã€Transformersã¯æ©Ÿæ¢°ç¿»è¨³ã®ã‚ˆã†ãªæ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã§æœ‰æœ›ãªçµæœã‚’ç¤ºã—ã¦ã„ã‚‹[43]. One key component of Transformers is the self-attention network, which is capable of learning long-range dependencies <strong>by computing attention weights between each pair of objects in a sequence</strong>. Transformersã®ä¸»è¦ãªæ§‹æˆè¦ç´ ã®1ã¤ã¯self-attention networkã§ã‚ã‚Šã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸­ã®å„object ãƒšã‚¢ã®attention weightã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã§<strong>é•·è·é›¢ä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã‚‹</strong>. Inspired by the success of Transformers, several self-attentive sequential recommenders have been proposed and achieve the state-of-the-art performance [26, 41, 49, 50]. Transformersã®æˆåŠŸã«è§¦ç™ºã•ã‚Œã€ã„ãã¤ã‹ã®self-attentive sequential recommendersãŒææ¡ˆã•ã‚Œã€æœ€æ–°ã®æ€§èƒ½ã‚’é”æˆã—ã¦ã„ã‚‹[26, 41, 49, 50]. For example, SASRec [26] is the pioneering framework to adopt self-attention network to learn the importance of items at different positions. ä¾‹ãˆã°ã€SASRec [26]ã¯ã€ç•°ãªã‚‹ä½ç½®ã«ã‚ã‚‹itemã®é‡è¦åº¦ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã€self-attention networkã‚’æ¡ç”¨ã—ãŸå…ˆé§†çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚‹. BERT4Rec [41] further models the correlations of items from both left-to-right and right-to-left directions. BERT4Rec [41]ã¯ã€ã•ã‚‰ã«å·¦ã‹ã‚‰å³ã€å³ã‹ã‚‰å·¦ã®ä¸¡æ–¹å‘ã®itemã®ç›¸é–¢ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹. SSE-PT [50] is a personalized Transformer model that provides better interpretability of engagement patterns by introducing user embeddings. SSE-PT [50]ã¯ã€user embeddingsã‚’å°å…¥ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®è§£é‡ˆå¯èƒ½æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºãƒ‰ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹. LSAN [31] adopts a novel twin-attention sequential framework, which can capture both long-term and short-term user preference signals. LSAN [31]ã¯æ–°ã—ã„twin-attention sequential frameworkã‚’æ¡ç”¨ã—ã€é•·æœŸã¨çŸ­æœŸã®ä¸¡æ–¹ã®ãƒ¦ãƒ¼ã‚¶å—œå¥½ã‚·ã‚°ãƒŠãƒ«ã‚’æ‰ãˆã‚‹ã“ã¨ãŒã§ãã‚‹. Recently, Transformers4Rec [14] performs an empirical analysis with broad experiments of various Transformer architectures for the task of sequential recommendation. æœ€è¿‘ã€<strong>Transformers4Rec</strong> [14]ã¯ã€sequentialæ¨è–¦ã®ã‚¿ã‚¹ã‚¯ã®ãŸã‚ã«ã€æ§˜ã€…ãªTransformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å¹…åºƒã„å®Ÿé¨“ã«ã‚ˆã‚‹å®Ÿè¨¼åˆ†æã‚’è¡Œã£ã¦ã„ã‚‹.</p>
<p>Although encouraging performance has been achieved, the robustness of sequential recommenders is far less studied in the literature. ã—ã‹ã—ã€<strong>sequentialæ¨è–¦å™¨ã®robustnessã«ã¤ã„ã¦ã¯ã‚ã¾ã‚Šç ”ç©¶ã•ã‚Œã¦ã„ãªã„</strong>. Many real-world item sequences are naturally noisy, containing both true-positive and false-positive interactions [6, 45, 46]. å®Ÿä¸–ç•Œã®å¤šãã®ã‚¢ã‚¤ãƒ†ãƒ åˆ—ã¯è‡ªç„¶ã«ãƒã‚¤ã‚ºãŒå¤šãã€çœŸé™½æ€§(true-positive)ã¨å½é™½æ€§(false-positive. ex. <strong>å¥½ãã˜ã‚ƒãªã„ã‘ã©ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã—ã¾ã£ãŸ. è³¼å…¥ã—ã¦ã¿ãŸãŒå«Œã„ã ã£ãŸâ€¦??</strong>)ã®ä¸¡æ–¹ã®ç›¸äº’ä½œç”¨ã‚’å«ã‚“ã§ã„ã‚‹ [6, 45, 46]. For example, a large portion of clicks do not align well with user preferences, and many products end up with negative reviews or being returned. ä¾‹ãˆã°ã€ã‚¯ãƒªãƒƒã‚¯ã®å¤§éƒ¨åˆ†ã¯ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã«åˆã‚ãšã€å¤šãã®è£½å“ã¯å¦å®šçš„ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã§çµ‚ã‚ã£ãŸã‚Šã€è¿”å“ã•ã‚ŒãŸã‚Šã™ã‚‹. In addition, there is no any prior knowledge about how a userâ€™s historical actions should be generated in online systems. ã¾ãŸã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ã®éå»ã®è¡Œå‹•ã‚’ã©ã®ã‚ˆã†ã«ç”Ÿæˆã™ã¹ãã‹ã¨ã„ã†äº‹å‰çŸ¥è­˜ã¯å­˜åœ¨ã—ãªã„. Therefore, developing robust algorithms to defend noise is of great significance for sequential recommendation. ãã®ãŸã‚ã€<strong>ãƒã‚¤ã‚ºã«å¼·ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é–‹ç™ºã™ã‚‹ã“ã¨ã¯ã€é€æ¬¡æ¨è–¦ã«ãŠã„ã¦å¤§ããªæ„ç¾©ãŒã‚ã‚‹</strong>.</p>
<p>Clearly, not every item in a sequence is aligned well with user preferences, especially for implicit feedbacks (e.g., clicks, views, etc.) [8]. ç‰¹ã«ã€æš—é»™ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼ˆã‚¯ãƒªãƒƒã‚¯ã€ãƒ“ãƒ¥ãƒ¼ãªã©ï¼‰ã®å ´åˆã€<strong>sequenceå†…ã®ã™ã¹ã¦ã®itemãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å—œå¥½ã¨ã†ã¾ãæ•´åˆã—ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªã„ã“ã¨ã¯æ˜ã‚‰ã‹</strong>ã§ã‚ã‚‹ [8]. Unfortunately, the vanilla self-attention network is not Lipschitz continuous1 , and is vulnerable to the quality of input sequences [28]. æ®‹å¿µãªãŒã‚‰ã€ãƒãƒ‹ãƒ©ãª(ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®) self-attention networkã¯<strong>Lipschitzé€£ç¶šã§ã¯ãªã(?)</strong>ã€<strong>å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®è³ªã«å¼±ã„ã¨ã„ã†å•é¡Œ</strong>ãŒã‚ã‚‹[28]ã€‚ Recently, in the tasks of language modeling, people found that a large amount of BERTâ€™s attentions focus on less meaningful tokens, like â€œ[SEP]â€ and â€œ.â€, which leads to a misleading explanation [11]. æœ€è¿‘ã€è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€BERT ã®æ³¨æ„ã®å¤šããŒã€â€œ[SEP]â€ ã‚„ â€œ.â€ã®ã‚ˆã†ãªã‚ã¾ã‚Šæ„å‘³ã®ãªã„ãƒˆãƒ¼ã‚¯ãƒ³ã«é›†ä¸­ã—ã€èª¤è§£ã‚’æ‹›ãèª¬æ˜ã«ãªã£ã¦ã„ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ã¦ã„ã‚‹[11]ã€‚ It is thus likely to obtain sub-optimal performance if self-attention networks are not well regularized for noisy sequences. ã“ã®ã‚ˆã†ã«ã€self-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒãƒã‚¤ã‚ºã®å¤šã„sequenceã«å¯¾ã—ã¦ã†ã¾ãæ­£å‰‡åŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã€æœ€é©ã¨ã¯è¨€ãˆãªã„æ€§èƒ½ãŒå¾—ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹. We use the following example to further explain above concerns. ä»¥ä¸‹ã®ä¾‹ã‚’ç”¨ã„ã¦ã€ä¸Šè¨˜ã®æ‡¸å¿µã«ã¤ã„ã¦ã•ã‚‰ã«èª¬æ˜ã™ã‚‹.</p>
<p>Figure 1 illustrates an example of left-to-right sequential recommendation where a userâ€™s sequence contains some noisy or irrelevant items. å›³1ã¯ã€å·¦ã‹ã‚‰å³ã¸ã®sequential recommendationã®ä¸€ä¾‹ã§ã‚ã‚‹. For example, a father may interchangeably purchase (phone, headphone, laptop) for his son, and (bag, pant) for his daughter, resulting in a sequence: (phone, bag, headphone, pant, laptop). ä¾‹ãˆã°ã€ã‚ã‚‹çˆ¶è¦ªãŒæ¯å­ã«ï¼ˆæºå¸¯é›»è©±ã€ãƒ˜ãƒƒãƒ‰ãƒ•ã‚©ãƒ³ã€ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ï¼‰ã€å¨˜ã«ï¼ˆã‚«ãƒãƒ³ã€ã‚ºãƒœãƒ³ï¼‰ã‚’è³¼å…¥ã™ã‚‹å ´åˆã€ï¼ˆæºå¸¯é›»è©±ã€ã‚«ãƒãƒ³ã€ãƒ˜ãƒƒãƒ‰ãƒ•ã‚©ãƒ³ã€ã‚ºãƒœãƒ³ã€ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ï¼‰ã¨ã„ã†é †åºã«ãªã‚‹. In the setting of sequential recommendation, we intend to infer the next item, e.g., laptop, based on the userâ€™s previous actions, e.g., (phone, bag, headphone, pant). é€æ¬¡æ¨è–¦ã®è¨­å®šã§ã¯ã€<strong>ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä»¥å‰ã®è¡Œå‹•ã€ä¾‹ãˆã°ï¼ˆé›»è©±ã€ã‚«ãƒãƒ³ã€ãƒ˜ãƒƒãƒ‰ãƒ›ãƒ³ã€ã‚ºãƒœãƒ³ï¼‰ã‹ã‚‰ã€æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã€ä¾‹ãˆã°ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ã‚’æ¨è«–ã™ã‚‹ã“ã¨</strong>ã‚’æ„å›³ã—ã¦ã„ã‚‹. However, the correlations among items are unclear, and intuitively pant and laptop are neither complementary nor compatible to each other, which makes the prediction untrustworthy. ã—ã‹ã—ã€ã‚¢ã‚¤ãƒ†ãƒ é–“ã®ç›¸é–¢ãŒä¸æ˜ç¢ºã§ã‚ã‚Šã€<strong>ç›´æ„Ÿçš„ã«pantã¨laptopã¯è£œå®Œé–¢ä¿‚ã«ã‚‚ç›¸å®¹ã‚Œãªã„</strong>ãŸã‚ã€ã“ã®äºˆæ¸¬ã¯ä¿¡é ¼ã§ããªã„. A trustworthy model should be able to only capture correlated items while ignoring these irrelevant items within sequences. <strong>ä¿¡é ¼ã§ãã‚‹ãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ã“ã‚Œã‚‰ã®ç„¡é–¢ä¿‚ãªã‚¢ã‚¤ãƒ†ãƒ ã‚’ç„¡è¦–ã—ã€ç›¸é–¢ã®ã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®ã¿ã‚’æ‰ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã¯ãš</strong>ã§ã‚ã‚‹. Existing self-attentive sequential models (e.g., SASRec [26] and BERT4Rec [41]) are insufficient to address noisy items within sequences. æ—¢å­˜ã®self-attentive sequential modelï¼ˆä¾‹ãˆã°ã€SASRec [26]ã‚„BERT4Rec [41]ï¼‰ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ãƒã‚¤ã‚ºã®å¤šã„ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾å‡¦ã™ã‚‹ã«ã¯ä¸ååˆ†ã§ã‚ã‚‹. The reason is that their full attention distributions are dense and would assign certain credits to all items, including irrelevant items. ãã®ç†ç”±ã¯ã€ãã‚Œã‚‰ã®full attention distributionsãŒå¯†ã§ã‚ã‚Šã€ç„¡é–¢ä¿‚ãªitemã‚’å«ã‚€ã™ã¹ã¦ã®itemã«ä¸€å®šã®creditã‚’å‰²ã‚Šå½“ã¦ã¦ã—ã¾ã†ã‹ã‚‰ã§ã‚ã‚‹. This causes a lack of focus and makes models less interpretable [10, 58]. ã“ã®ãŸã‚ã€æ³¨ç›®åº¦(focus)ãŒä¸è¶³ã—ã€ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆæ€§ãŒä½ä¸‹ã™ã‚‹[10, 58].</p>
<p>To address the above issues, one straightforward strategy is to design sparse Transformer architectures that sparsify the connections in the attention layers, which have been actively investigated in language modeling tasks [10, 58]. ä¸Šè¨˜ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€ä¸€ã¤ã®ç°¡å˜ãªæˆ¦ç•¥ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã§æ´»ç™ºã«ç ”ç©¶ã•ã‚Œã¦ã„ã‚‹<strong>attention layersã®æ¥ç¶šã‚’sparseã«ã—ãŸ</strong>Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ã§ã‚ã‚‹[10, 58]. Several representative models are Star Transformer [18], Sparse Transformer [10], Longformer [2], and BigBird [58]. ã„ãã¤ã‹ã®ä»£è¡¨çš„ãªãƒ¢ãƒ‡ãƒ«ã¯Star Transformer [18], Sparse Transformer [10], Longformer [2], ãã—ã¦BigBird [58]ã§ã‚ã‚‹. These sparse attention patterns could mitigate noisy issues and avoid allocating credits to unrelated contents for the query of interest. ã“ã‚Œã‚‰ã®sparse attentionãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã€ãƒã‚¤ã‚ºã®å•é¡Œã‚’è»½æ¸›ã—ã€é–¢å¿ƒã®ã‚ã‚‹ã‚¯ã‚¨ãƒªã«ç„¡é–¢ä¿‚ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã‚’å›é¿ã™ã‚‹ã“ã¨ãŒã§ãã‚‹. However, these models largely rely on pre-defined attention schemas, which lacks flexibility and adaptability in practice. ã—ã‹ã—ã€ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯äº‹å‰ã«å®šç¾©ã•ã‚ŒãŸattention schemas(??)ã«å¤§ããä¾å­˜ã—ã¦ãŠã‚Šã€å®Ÿéš›ã®ã¨ã“ã‚æŸ”è»Ÿæ€§ã‚„é©å¿œæ€§ã«æ¬ ã‘ã¦ã„ã‚‹. Unlike end-to-end training approaches, whether these sparse patterns could generalize well to sequential recommendation remains unknown and is still an open research question. ã¾ãŸã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®å­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã¯ç•°ãªã‚Šã€è¨€èªãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¹ã‚¯ã®sparseãƒ‘ã‚¿ãƒ¼ãƒ³ãŒsequentialæ¨è–¦ã«ã†ã¾ãä¸€èˆ¬åŒ–ã§ãã‚‹ã‹ã©ã†ã‹ã¯ä¸æ˜ã§ã‚ã‚Šã€ã¾ã æœªè§£æ±ºã®ç ”ç©¶èª²é¡Œã§ã‚ã‚‹.</p>
<section id="contributions." class="level2">
<h2 class="anchored" data-anchor-id="contributions.">1.1. Contributions.</h2>
<p>è²¢çŒ®åº¦ In this work, we propose to design a denoising strategy, Rec-Denoiser, for better training of selfattentive sequential recommenders. æœ¬ç ”ç©¶ã§ã¯ã€è‡ªå·±æ³¨æ„å‹é€æ¬¡æ¨è–¦å™¨ã‚’ã‚ˆã‚Šè‰¯ãå­¦ç¿’ã•ã›ã‚‹ãŸã‚ã®ãƒã‚¤ã‚ºé™¤å»æˆ¦ç•¥ã€Rec-Denoiserã‚’ææ¡ˆã™ã‚‹. Our idea stems from the recent findings that not all attentions are necessary and simply pruning redundant attentions could further improve the performance [10, 12, 40, 55, 58]. æˆ‘ã€…ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã€å…¨ã¦ã®attentionsã¯å¿…è¦ã§ã¯ãªãã€å†—é•·ãªattentionsã‚’åˆˆã‚Šå–ã‚‹ã“ã¨ã§ã•ã‚‰ã«æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã¨ã„ã†æœ€è¿‘ã®çŸ¥è¦‹ã«ç”±æ¥ã™ã‚‹[10, 12, 40, 55, 58]. Rather than randomly dropping out attentions, we introduce differentiable masks to drop task-irrelevant attentions in the self-attention layers, which can yield exactly zero attention scores for noisy items. æˆ‘ã€…ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ã«attentionsã‚’å‰Šé™¤ã™ã‚‹ã®ã§ã¯ãªãã€<strong>å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’å°å…¥ã—ã€ã‚¿ã‚¹ã‚¯ã¨ç„¡é–¢ä¿‚ãªattentionsã‚’self-attention layersã§å‰Šé™¤ã™ã‚‹</strong>ã“ã¨ã§ã€ãƒã‚¤ã‚ºã®å¤šã„itemã«å¯¾ã—ã¦attentionã‚¹ã‚³ã‚¢ã‚’æ­£ç¢ºã«ã‚¼ãƒ­ã«ã™ã‚‹ã“ã¨ãŒã§ãã‚‹. The introduced sparsity in the self-attention layers has several benefits: self-attention layersã«å°å…¥ã•ã‚ŒãŸã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã«ã¯ã€ã„ãã¤ã‹ã®åˆ©ç‚¹ãŒã‚ã‚‹.</p>
<ul>
<li><ol type="1">
<li>Irrelevant attentions with parameterized masks can be learned to be dropped in a data-driven way. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚ŒãŸãƒã‚¹ã‚¯ã‚’æŒã¤ç„¡é–¢ä¿‚ãªattentionã¯ã€ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ã®æ–¹æ³•ã§å‰Šé™¤ã•ã‚Œã‚‹ã‚ˆã†ã«å­¦ç¿’ã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹.</li>
</ol>
<ul>
<li>Taking Figure 1 as an example, our Rec-denoiser would prune the sequence (phone, bag, headphone) for pant, and (phone, bag, headphone, pant) for laptop in the attention maps.å›³1ã‚’ä¾‹ã«ã¨ã‚‹ã¨ã€Rec-denoiserã¯ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ—ã«ãŠã„ã¦ã€ã‚ºãƒœãƒ³ã«ã¯(phone, bag, headphone)ã€ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ã«ã¯(phone, bag, headphone, pant)ã¨ã„ã†é †åºã‚’åˆ‡ã‚Šæ¨ã¦ã‚‹ã“ã¨ã«ãªã‚‹.</li>
<li>Namely, we seek next item prediction explicitly based on a subset of more informative items. ã¤ã¾ã‚Šã€ã‚ˆã‚Šæƒ…å ±é‡ã®å¤šã„ã‚¢ã‚¤ãƒ†ãƒ ã®éƒ¨åˆ†é›†åˆ(subset)ã«åŸºã¥ãã€æ˜ç¤ºçš„ã«æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ äºˆæ¸¬ã‚’è¡Œã†ã®ã§ã™.</li>
</ul></li>
<li><ol start="2" type="1">
<li>Our Rec-Denoiser still takes full advantage of Transformers as it does not change their architectures, but only the attention distributions.æˆ‘ã€…ã®Rec-Denoiserã¯Transformerã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å¤‰æ›´ã›ãšã€<strong>ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³åˆ†å¸ƒã®ã¿ã‚’å¤‰æ›´ã™ã‚‹</strong>ãŸã‚ã€Transformerã‚’æœ€å¤§é™ã«æ´»ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã‚‹.</li>
</ol>
<ul>
<li>As such, Rec-Denoiser is easy to implement and is compatible to any Transformers, making them less complicated as well as improving their interpretability. ãã®ãŸã‚ã€Rec-Denoiserã¯å®Ÿè£…ãŒå®¹æ˜“ã§ã€ã‚ã‚‰ã‚†ã‚‹Transformerã¨äº’æ›æ€§ãŒã‚ã‚Šã€Transformerã®è¤‡é›‘ã•ã‚’è»½æ¸›ã—ã€ãã®è§£é‡ˆå¯èƒ½æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹.</li>
</ul></li>
</ul>
<p>In our proposed Rec-Denoiser, there are two major challenges. æˆ‘ã€…ãŒææ¡ˆã™ã‚‹Rec-Denoiserã§ã¯ã€2ã¤ã®å¤§ããªèª²é¡ŒãŒã‚ã‚‹. First, the discreteness of binary masks (i.e., 0 is dropped while 1 is kept) is, however, intractable in the back-propagation. ã¾ãšã€2å€¤ãƒã‚¹ã‚¯ã®é›¢æ•£æ€§ï¼ˆã™ãªã‚ã¡ã€0ã¯å‰Šé™¤ã•ã‚Œã€1ã¯ä¿æŒã•ã‚Œã‚‹ï¼‰ã¯ã€ã—ã‹ã—ã€ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯å®Ÿè¡Œä¸å¯èƒ½ã§ã‚ã‚‹. To remedy this issue, we relax the discrete variables with a continuous approximation through probabilistic reparameterization [25]. ã“ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€æˆ‘ã€…ã¯ç¢ºç‡çš„å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–[25]ã«ã‚ˆã‚Šã€é›¢æ•£å¤‰æ•°ã‚’é€£ç¶šçš„ãªè¿‘ä¼¼å€¤ã§ç·©å’Œã™ã‚‹. As such, our differentiable masks can be trained jointly with original Transformers in an end-to-end fashion. ã“ã®ã‚ˆã†ã«ã€æˆ‘ã€…ã®å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã¯ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã®Transformerã¨ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§å…±åŒã—ã¦å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã‚‹. In addition, the scaled dot-product attention is not Lipschitz continuous and is thus vulnerable to input perturbations [28]. ã¾ãŸã€scaled dot-product attentionã¯Lipschitzé€£ç¶š(?)ã§ã¯ãªã„ãŸã‚ã€å…¥åŠ›æ‘‚å‹•ã«å¯¾ã—ã¦è„†å¼±ã§ã‚ã‚‹[28]. In this work, Jacobian regularization [21, 24] is further applied to the entire Transformer blocks, to improve the robustness of Transformers for noisy sequences. ã“ã®ç ”ç©¶ã§ã¯ã€<strong>ãƒã‚¤ã‚ºã®å¤šã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã™ã‚‹Transformerã®robustnessã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚</strong>ã«ã€Transformerãƒ–ãƒ­ãƒƒã‚¯å…¨ä½“ã«ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–[21, 24]ã‚’ã•ã‚‰ã«é©ç”¨ã—ã¦ã„ã‚‹. Experimental results on real-world benchmark datasets demonstrate the effectiveness and robustness of the proposed Rec-Denoiser. å®Ÿä¸–ç•Œã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹å®Ÿé¨“çµæœã‹ã‚‰ã€ææ¡ˆã™ã‚‹Rec-Denoiserã®æœ‰åŠ¹æ€§ã¨é ‘å¥æ€§ã‚’å®Ÿè¨¼ã™ã‚‹. In summary, our contributions are: ã¾ã¨ã‚ã‚‹ã¨ã€æˆ‘ã€…ã®è²¢çŒ®ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã‚ã‚‹.</p>
<ul>
<li><p>We introduce the idea of denoising item sequences for better of training self-attentive sequential recommenders, which greatly reduces the negative impacts of noisy items. æœ¬è«–æ–‡ã§ã¯ã€è‡ªå·±èªè­˜å‹é€æ¬¡æ¨è–¦å™¨ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã€itemåˆ—ã®ãƒã‚¤ã‚ºé™¤å»ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ç´¹ä»‹ã—ã€ãƒã‚¤ã‚ºã®å¤šã„itemã«ã‚ˆã‚‹æ‚ªå½±éŸ¿ã‚’å¤§å¹…ã«è»½æ¸›ã™ã‚‹.</p></li>
<li><p>We present a general Rec-Denoiser framework with differentiable masks that can achieve sparse attentions by dynamically pruning irrelevant information, leading to better model performance. æˆ‘ã€…ã¯ã€å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’æŒã¤ä¸€èˆ¬çš„ãªRec-Denoiserãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æç¤ºã—ã€ç„¡é–¢ä¿‚ãªæƒ…å ±ã‚’å‹•çš„ã«åˆˆã‚Šå–ã‚‹ã“ã¨ã§ç–ãªæ³¨æ„ã‚’é”æˆã—ã€ã‚ˆã‚Šè‰¯ã„ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’å°ãã“ã¨ãŒå¯èƒ½ã§ã‚ã‚‹.</p></li>
<li><p>We propose an unbiased gradient estimator to optimize the binary masks, and apply Jacobian regularization on the gradients of Transformer blocks to further improve its robustness. ãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯ã®æœ€é©åŒ–ã®ãŸã‚ã«ä¸åå‹¾é…æ¨å®šå™¨ã‚’ææ¡ˆã—ã€Transformerãƒ–ãƒ­ãƒƒã‚¯ã®å‹¾é…ã«ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–ã‚’é©ç”¨ã—ã¦ã€ã•ã‚‰ã«é ‘å¥æ€§ã‚’å‘ä¸Šã•ã›ã‚‹.</p></li>
<li><p>The experimental results demonstrate significant improvements that Rec-Denoiser brings to self-attentive recommenders (5.05% âˆ¼ 19.55% performance gains), as well as its robustness against input perturbations. å®Ÿé¨“çµæœã¯ã€Rec-DenoiserãŒè‡ªå·±æ³¨æ„å‹æ¨è–¦å™¨ã«ã‚‚ãŸã‚‰ã™å¤§ããªæ”¹å–„ï¼ˆ5.05% âˆ¼ 19.55%ã®æ€§èƒ½å‘ä¸Šï¼‰ã¨ã€å…¥åŠ›ã®æ‘‚å‹•ã«å¯¾ã™ã‚‹é ‘å¥æ€§ã‚’ç¤ºã—ã¦ã„ã‚‹.</p></li>
</ul>
</section>
</section>
<section id="related-work-2.-é–¢é€£ä½œå“" class="level1">
<h1>2. Related Work 2. é–¢é€£ä½œå“</h1>
<p>In this section, we briefly review the related work on sequential recommendation and sparse Transformers. æœ¬ç¯€ã§ã¯ã€é€æ¬¡æ¨è–¦ã¨ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã«é–¢ã™ã‚‹é–¢é€£ç ”ç©¶ã‚’ç°¡å˜ã«ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹ã€‚ We also highlight the differences between the existing efforts and ours. ã¾ãŸã€æ—¢å­˜ã®å–ã‚Šçµ„ã¿ã¨æˆ‘ã€…ã®å–ã‚Šçµ„ã¿ã¨ã®ç›¸é•ç‚¹ã‚’å¼·èª¿ã™ã‚‹ã€‚</p>
<section id="sequential-recommendation" class="level2">
<h2 class="anchored" data-anchor-id="sequential-recommendation">2.1. Sequential Recommendation</h2>
<p>Leveraging sequences of user-item interactions is crucial for sequential recommendation. é€æ¬¡æ¨è–¦ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®interactionã®sequenceã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã‚ã‚‹ã€‚ User dynamics can be caught by Markov Chains for inferring the conditional probability of an item based on the previous items [19, 39]. ãƒ¦ãƒ¼ã‚¶ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã¯ãƒãƒ«ã‚³ãƒ•é€£é–ã«ã‚ˆã£ã¦æ•æ‰ã•ã‚Œã€å‰ã®ã‚¢ã‚¤ãƒ†ãƒ ã«åŸºã¥ãã‚¢ã‚¤ãƒ†ãƒ ã®æ¡ä»¶ä»˜ãç¢ºç‡ã‚’æ¨è«–ã™ã‚‹ã“ã¨ãŒã§ãã‚‹[19, 39]. More recently, growing efforts have been dedicated to deploying deep neural networks for sequential recommendation such as recurrent neural networks [20, 53], convolutional neural networks [42, 54, 57], memory networks [9, 22], and graph neural networks [4, 7, 51]. æœ€è¿‘ã§ã¯ã€ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ [20, 53]ã€ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ [42, 54, 57]ã€ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ [9, 22]ã€ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ [4, 7, 51] ãªã©ã®<strong>æ·±ã„ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’é€æ¬¡æ¨è–¦ã«åˆ©ç”¨ã™ã‚‹å–ã‚Šçµ„ã¿ãŒç››ã‚“ã«ãªã£ã¦ã„ã‚‹</strong>ã€‚ For example, GRU4Rec [20] employs a gated recurrent unit to study temporal behaviors of users. ä¾‹ãˆã°ã€GRU4Rec[20]ã¯ãƒ¦ãƒ¼ã‚¶ã®æ™‚é–“çš„è¡Œå‹•ã‚’ç ”ç©¶ã™ã‚‹ãŸã‚ã«ã‚²ãƒ¼ãƒ†ãƒƒãƒ‰ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ¦ãƒ‹ãƒƒãƒˆã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã€‚ Caser [42] learns sequential patterns by using convolutional filters on local sequences. Caser [42]ã¯å±€æ‰€çš„ãªé…åˆ—ã«å¯¾ã—ã¦ç•³ã¿è¾¼ã¿ãƒ•ã‚£ãƒ«ã‚¿ã‚’ç”¨ã„ã¦é€£ç¶šçš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹ã€‚ MANN [9] adopts memory-augmented neural networks to model user historical records. MANN [9]ã¯ãƒ¦ãƒ¼ã‚¶ã®å±¥æ­´è¨˜éŒ²ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ãŸã‚ã«ãƒ¡ãƒ¢ãƒªè£œå¼·å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ¡ç”¨ã™ã‚‹ã€‚ SR-GNN [51] converts session sequences into graphs and uses graph neural networks to capture complex item-item transitions. SR-GNN [51]ã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ã‚°ãƒ©ãƒ•ã«å¤‰æ›ã—ã€ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦è¤‡é›‘ãªã‚¢ã‚¤ãƒ†ãƒ -ã‚¢ã‚¤ãƒ†ãƒ é·ç§»ã‚’æ‰ãˆã‚‹ã€‚</p>
<p>Transformer-based models have shown promising potential in sequential recommendation [5, 26, 30, 32, 33, 41, 49, 50], due to their ability of modeling arbitrary dependencies in a sequence. Transformerã«åŸºã¥ããƒ¢ãƒ‡ãƒ«ã¯ã€sequenceä¸­ã®ä»»æ„ã®ä¾å­˜é–¢ä¿‚ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã§ãã‚‹ãŸã‚ã€é€æ¬¡æ¨è–¦ã«ãŠã„ã¦æœ‰æœ›ãªå¯èƒ½æ€§ã‚’ç¤ºã—ã¦ã„ã‚‹[5, 26, 30, 32, 33, 41, 49, 50]ï¼ For example, SASRec [26] first adopts self-attention network to learn the importance of items at different positions. ä¾‹ãˆã°ã€SASRec [26]ã§ã¯ã€ã¾ãšã€ç•°ãªã‚‹ä½ç½®ã«ã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®é‡è¦åº¦ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã€self-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ¡ç”¨ã—ã¦ã„ã‚‹. In the follow-up studies, several Transformer variants have been designed for different scenarios by adding bidirectional attentions [41], time intervals [30], personalization [50], importance sampling [32], and sequence augmentation [33]. ãã®å¾Œã€bidirectional attentions[41]ã€time intervals[30]ã€personalization[50]ã€importance sampling[32]ã€sequence augmentation[33]ã‚’è¿½åŠ ã—ã€ç•°ãªã‚‹ã‚·ãƒŠãƒªã‚ªã®ãŸã‚ã«ã„ãã¤ã‹ã®å¤‰ç¨®ãŒè¨­è¨ˆã•ã‚Œã¦ããŸ. However, very few studies pay attention to the robustness of self-attentive recommender models. ã—ã‹ã—ã€self-attentionå‹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®é ‘å¥æ€§ã«æ³¨ç›®ã—ãŸç ”ç©¶ã¯éå¸¸ã«å°‘ãªã„. Typically, usersâ€™ sequences contain lots of irrelevant items since they may subsequently purchase a series of products with different purposes [45]. ä¸€èˆ¬ã«ã€ãƒ¦ãƒ¼ã‚¶ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ã¯ç„¡é–¢ä¿‚ãªã‚¢ã‚¤ãƒ†ãƒ ãŒå¤šãå«ã¾ã‚Œã‚‹. As such, the current user action only depends on a subset of items, not on the entire sequences. ã“ã®ã‚ˆã†ãªå ´åˆã€ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ã®è¡Œå‹•ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã§ã¯ãªãã€ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã«ã®ã¿ä¾å­˜ã™ã‚‹ã€‚ However, the self-attention module is known to be sensitive to noisy sequences [28], which may lead to sub-optimal generalization performance. ã—ã‹ã—ã€<strong>self-attentionãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ãƒã‚¤ã‚ºã®å¤šã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«æ•æ„Ÿã§ã‚ã‚‹ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ãŠã‚Š</strong>[28]ã€ã“ã‚Œã¯æœ€é©ã§ãªã„æ±åŒ–æ€§èƒ½ã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚ In this paper, we aim to reap the benefits of Transformers while denoising the noisy item sequences by using learnable masks in an end-to-end fashion. æœ¬è«–æ–‡ã§ã¯ã€å­¦ç¿’å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’end-to-endã§ç”¨ã„ã‚‹(i.e.&nbsp;Transformerã®å­¦ç¿’ã¨ä¸€ç·’ã«maskã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚å­¦ç¿’ã§ãã‚‹)ã“ã¨ã«ã‚ˆã‚Šã€ãƒã‚¤ã‚ºã®å¤šã„ã‚¢ã‚¤ãƒ†ãƒ sequenceã‚’denoiseã—ã¤ã¤ã€Transformersã®åˆ©ç‚¹ã‚’äº«å—ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™.</p>
</section>
<section id="sparce-transformer" class="level2">
<h2 class="anchored" data-anchor-id="sparce-transformer">2.2. Sparce Transformer</h2>
<p>Recently, many lightweight Transformers seek to achieve sparse attention maps since not all attentions carry important information in the self-attention layers [2, 10, 18, 29, 58]. æœ€è¿‘ã€å¤šãã®è»½é‡ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãŒã€å…¨ã¦ã®attentionãŒself-attentionå±¤ã®é‡è¦ãªæƒ…å ±ã‚’æŒã£ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªã„ã®ã§ã€sparseãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ—(=attentionåˆ†å¸ƒ?)ã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã‚’è¿½æ±‚ã—ã¦ã„ã‚‹[2, 10, 18, 29, 58]. (æ—¢å­˜ç ”ç©¶ã«ãŠã„ã¦ã€sparseãªattentionåˆ†å¸ƒã‚’æ¡ç”¨ã™ã‚‹ç›®çš„ã¯ã€noiseã¸ã®robustæ€§ã®å‘ä¸Šã¨ã„ã†ã‚ˆã‚Šã‚‚ã€è»½é‡åŒ–ã‚„ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£å‘ä¸Šã ã£ãŸã‚Šã™ã‚‹ã‚“ã ã‚ã†ã‹â€¦??:thinking:) For instance, Reformer [29] computes attentions based on locality-sensitive hashing, leading to lower memory consumption. ä¾‹ãˆã°ã€Reformer [29]ã¯å±€æ‰€æ€§ã‚’è€ƒæ…®ã—ãŸãƒãƒƒã‚·ãƒ¥ã«åŸºã¥ã„ã¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—ã—ã€ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã®ä½æ¸›ã«ã¤ãªãŒã‚‹. Star Transformer [18] replaces the fully-connected structure of self-attention with a star-shape topology. Star Transformer [18]ã¯ã€self-attentionã®å®Œå…¨é€£çµæ§‹é€ ã‚’æ˜Ÿå½¢ã®ãƒˆãƒãƒ­ã‚¸ãƒ¼ã«ç½®ãæ›ãˆãŸã‚‚ã®ã§ã‚ã‚‹ã€‚ Sparse Transformer [10] and Longformer [2] achieve sparsity by using various sparse patterns, such as diagonal sliding windows, dilated sliding windows, local and global sliding windows. Sparse Transformer [10] ã¨ Longformer [2] ã¯ã€æ–œã‚ã‚¹ãƒ©ã‚¤ãƒ‰çª“ã€æ‹¡å¼µã‚¹ãƒ©ã‚¤ãƒ‰çª“ã€ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒ©ã‚¤ãƒ‰çª“ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ãƒ©ã‚¤ãƒ‰çª“ãªã©ã€æ§˜ã€…ãªã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”¨ã„ã¦ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã‚’å®Ÿç¾ã™ã‚‹ã€‚ BigBird [58] uses random and several fixed patterns to build sparse blocks. BigBird [58]ã§ã¯ï¼Œ<strong>ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã„ãã¤ã‹ã®å›ºå®šãƒ‘ã‚¿ãƒ¼ãƒ³</strong>ã‚’ç”¨ã„ã¦ï¼Œç–ãªãƒ–ãƒ­ãƒƒã‚¯ã‚’æ§‹ç¯‰ã—ã¦ã„ã‚‹ï¼ It has been shown that these sparse attentions can obtain the state-of-the-art performance and greatly reduce computational complexity. ã“ã‚Œã‚‰ã®sparse attentionã¯ã€æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã€è¨ˆç®—é‡ã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚ However, many of them rely on fixed attention schemas that lack flexibility and require tremendous engineering efforts. ã—ã‹ã—ã€ã“ã‚Œã‚‰ã®å¤šãã¯ã€æŸ”è»Ÿæ€§ã«æ¬ ã‘ã€è†¨<strong>å¤§ãªå·¥å­¦çš„åŠªåŠ›ã‚’å¿…è¦ã¨ã™ã‚‹å›ºå®šçš„ãªattention schemaã«ä¾å­˜</strong>ã—ã¦ã„ã‚‹.</p>
<p>Another line of work is to use learnable attention distributions [12, 36, 38, 40]. ã¾ãŸã€<strong>å­¦ç¿’å¯èƒ½ãªattentionåˆ†å¸ƒ</strong>[12, 36, 38, 40]ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€‚ Mostly, they calculate attention weights with variants of sparsemax that replaces the softmax normalization in the self-attention networks. ã»ã¨ã‚“ã©ã®å ´åˆã€self-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã‘ã‚‹ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹æ­£è¦åŒ–ã‚’ç½®ãæ›ãˆã‚‹<strong>sparsemax</strong>(æœ€å¤§å€¤ã®ã¿ã‚’æ®‹ã™ã€ã¿ãŸã„ãªã‚¤ãƒ¡ãƒ¼ã‚¸??:thinking:)ã®å¤‰ç¨®ã‚’ç”¨ã„ã¦attention weightã‚’è¨ˆç®—ã™ã‚‹. This allows to produce both sparse and bounded attentions, yielding a compact and interpretable set of alignments. ã“ã‚Œã«ã‚ˆã‚Šã€ç–ã§boundedãª(å¢ƒç•Œã®ã‚ã‚‹?)attentionã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã€ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã§è§£é‡ˆå¯èƒ½ãªã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆã®é›†åˆã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹. Our Rec-denoiser is related to this line of work. æˆ‘ã€…ã®Rec-denoiserã¯ã€ã“ã®ç ”ç©¶ã«é–¢é€£ã—ã¦ã„ã‚‹. Instead of using sparsemax, we design a trainable binary mask for the self-attention network. sparsemax(? æœ€å¤§å€¤ã®ã¿ã‚’æ®‹ã™ã€ã¿ãŸã„ãªã‚¤ãƒ¡ãƒ¼ã‚¸??)ã‚’ç”¨ã„ã‚‹ä»£ã‚ã‚Šã«ã€æˆ‘ã€…ã¯<strong>self-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«å¯¾ã—ã¦å­¦ç¿’å¯èƒ½ãªbinaryãƒã‚¹ã‚¯ã‚’è¨­è¨ˆ</strong>ã™ã‚‹ã€‚ As a result, our proposed Rec-denoiser can automatically determine which self-attention connections should be deleted or kept in a data-driven way. ãã®çµæœã€æˆ‘ã€…ã®ææ¡ˆã™ã‚‹Rec-denoiserã¯ã€ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ã®æ–¹æ³•ã§ã€ã©ã®self-attetionã®æ¥ç¶šã‚’å‰Šé™¤ã™ã¹ãã‹ã€ã‚ã‚‹ã„ã¯ä¿æŒã™ã¹ãã‹ã‚’è‡ªå‹•çš„ã«æ±ºå®šã™ã‚‹ã“ã¨ãŒã§ãã‚‹.</p>
</section>
</section>
<section id="problem-and-background-3.-å•é¡Œç‚¹ã¨èƒŒæ™¯" class="level1">
<h1>3. Problem and Background 3. å•é¡Œç‚¹ã¨èƒŒæ™¯</h1>
<p>In this section, we first formulate the problem of sequential recommendation, and then revisit several self-attentive models. æœ¬ç¯€ã§ã¯ã€ã¾ãšé€æ¬¡æ¨è–¦ã®å•é¡Œã‚’å®šå¼åŒ–ã—ã€æ¬¡ã«ã„ãã¤ã‹ã®self-attetnionãƒ¢ãƒ‡ãƒ«ã‚’å†æ¤œè¨ã™ã‚‹ã€‚ We further discuss the limitations of the existing work. ã•ã‚‰ã«ã€æ—¢å­˜ã®ç ”ç©¶ã®é™ç•Œã«ã¤ã„ã¦è­°è«–ã™ã‚‹ã€‚</p>
<section id="problem-setup-3.1.-å•é¡Œã®è¨­å®š" class="level2">
<h2 class="anchored" data-anchor-id="problem-setup-3.1.-å•é¡Œã®è¨­å®š">3.1. Problem Setup 3.1. å•é¡Œã®è¨­å®š</h2>
<p>In sequential recommendation, let <span class="math inline">\(U\)</span> be a set of users, <span class="math inline">\(I\)</span> a set of items, and <span class="math inline">\(S = {S^1, S^2,\cdots, S^{|U|}}\)</span> a set of usersâ€™ actions. é€æ¬¡æ¨è–¦ã«ãŠã„ã¦ã€<span class="math inline">\(U\)</span> ã‚’ãƒ¦ãƒ¼ã‚¶ã®é›†åˆã€<span class="math inline">\(I\)</span> ã‚’ã‚¢ã‚¤ãƒ†ãƒ ã®é›†åˆã€<span class="math inline">\(S={S^1,S^2, \cdots, S^{|U|}\)</span> ã‚’ãƒ¦ãƒ¼ã‚¶ã®è¡Œå‹•ã®é›†åˆã¨ã™ã‚‹. We user <span class="math inline">\(S^u = (S^u_1, S^u_2, \cdots, S^u_{|S^u|})\)</span> to denote a sequence of items for user <span class="math inline">\(u \in U\)</span> in a chronological order, where <span class="math inline">\(S^u_t \in I\)</span> is the item that user <span class="math inline">\(u\)</span> has interacted with at time <span class="math inline">\(t\)</span>, and <span class="math inline">\(|S^u|\)</span> is the length of sequence. <span class="math inline">\(S^u = (S^{u}_{1}, S^{u}_{2}, \cdots, S^{u}_{|S^u|})\)</span> ã¯ã€æ™‚ç³»åˆ—ã«ä¸¦ã‚“ã ãƒ¦ãƒ¼ã‚¶ <span class="math inline">\(u \in U\)</span> ã®item sequence ã‚’è¡¨ã—ã€<span class="math inline">\(S^{u}_{t} in I\)</span> ã¯æ™‚åˆ»(=sequenceå†…ã®è¦ç´ ã®è­˜åˆ¥å­çš„ãªæ„å‘³åˆã„) <span class="math inline">\(t\)</span> ã«ãƒ¦ãƒ¼ã‚¶ <span class="math inline">\(u\)</span> ãŒinteractã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã€<span class="math inline">\(|S^u|\)</span> ã¯sequenceã®é•·ã•ã‚’è¡¨ã™.</p>
<p>Given the interaction history <span class="math inline">\(S^u\)</span>, sequential recommendation seeks to predict the next item <span class="math inline">\(S^u_{|S^{u}+1|}\)</span> at time step <span class="math inline">\(|S^{u}+1|\)</span> interaction history <span class="math inline">\(S^u\)</span> ãŒä¸ãˆã‚‰ã‚Œã‚‹ã¨ã€sequentialæ¨è–¦ã§ã¯ time step <span class="math inline">\(|S^{u}+1|\)</span> ã«ãŠã„ã¦ next item <span class="math inline">\(S^u_{|S^{u}+1|}\)</span> ã‚’äºˆæ¸¬ã—ã‚ˆã†ã¨ã™ã‚‹. During the training process [26, 41], it will be convenient to regard the modelâ€™s input as <span class="math inline">\((S^{u}_{1}, S^{u}_{2}, \cdots, S^{u}_{|S^u - 1|})\)</span> and its expected output is a shifted version of the input sequence: <span class="math inline">\((S^{u}_{2}, S^{u}_{3}, \cdtos, S^{u}_{|S^{u}|})\)</span> ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹[26, 41]ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã‚’ <span class="math inline">\((S^{u}_{1}, S^{u}_{2}, Ë¶cdots, S^{u}_{|S^u - 1|})\)</span> ã¨ã¿ãªã—ã€ãã® expected output(i.e.&nbsp;æ•™å¸«ãƒ©ãƒ™ãƒ«!) ã‚’å…¥åŠ›sequence ã® ã‚·ãƒ•ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ <span class="math inline">\((S^{u}_{2}, S^{u}_{3}, Ë¶cdtos, S^{u}_{|S^{u}|})\)</span> ã¨ã¿ãªã™ã¨ä¾¿åˆ©ã§ã‚ã‚‹.</p>
</section>
<section id="self-attentive-recommenders" class="level2">
<h2 class="anchored" data-anchor-id="self-attentive-recommenders">3.2. Self-attentive Recommenders</h2>
<p>Owing to the ability of learning long sequences, Transformer architectures [43] have been widely used in sequential recommendation, like <strong>SASRec</strong> [26], BERT4Rec [41], and TiSASRec [30]. Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£[43]ã¯é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ãŸã‚ã€é€æ¬¡æ¨è–¦ã«ãŠã„ã¦<strong>SASRec</strong> [26], BERT4Rec [41], TiSASRec [30] ãªã©åºƒãåˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚ Here we briefly introduce the design of SASRec and discuss its limitations. ã“ã“ã§ã¯ã€SASRecã®è¨­è¨ˆã‚’ç°¡å˜ã«ç´¹ä»‹ã—ã€ãã®é™ç•Œã«ã¤ã„ã¦è€ƒå¯Ÿã™ã‚‹.</p>
<section id="embedding-layer-3.2.1.-ã‚¨ãƒ³ãƒ™ãƒ‡ãƒƒãƒ‰ãƒ¬ã‚¤ãƒ¤ãƒ¼" class="level3">
<h3 class="anchored" data-anchor-id="embedding-layer-3.2.1.-ã‚¨ãƒ³ãƒ™ãƒ‡ãƒƒãƒ‰ãƒ¬ã‚¤ãƒ¤ãƒ¼">3.2.1. Embedding Layer 3.2.1. ã‚¨ãƒ³ãƒ™ãƒ‡ãƒƒãƒ‰ãƒ¬ã‚¤ãƒ¤ãƒ¼</h3>
<p>Transformer-based recommenders maintain an item embedding table <span class="math inline">\(T \in R^{|I| \times d}\)</span> , where ğ‘‘ is the size of the embedding. Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã¯ã‚¢ã‚¤ãƒ†ãƒ ã® <strong>embedding table</strong> <span class="math inline">\(T \in R^{|I| \times d}\)</span> ã‚’ä¿æŒã™ã‚‹. ã“ã“ã§ã€<span class="math inline">\(d\)</span> ã¯ embeddingã®ã‚µã‚¤ã‚º. (embedding tableã¯ã€ã‚¢ã‚¤ãƒ†ãƒ id -&gt; embedding vector ã®mapã¿ãŸã„ãªã‚¤ãƒ¡ãƒ¼ã‚¸:thinking:). For each sequence <span class="math inline">\((S^u_1, S^u_2, \cdots, S^u_{|S^u - 1|})\)</span>, it can be converted into a fixed-length sequence <span class="math inline">\((s_1, s_2, \cdots s_n)\)</span>, where <span class="math inline">\(n\)</span> is the maximum length (e.g., keeping the most recent ğ‘› items by truncating or padding items). å„sequence <span class="math inline">\((S^u_1, S^u_2, \cdots, S^u_{|S^u - 1|})\)</span> ã«å¯¾ã—ã¦ã€<strong>fixed-length(å›ºå®šé•·)ã®sequence <span class="math inline">\((s_1, s_2, \cdots s_n)\)</span> ã«å¤‰æ›</strong>ã™ã‚‹ã“ã¨ãŒã§ãã‚‹. ã“ã“ã§ã€<span class="math inline">\(n\)</span>ã¯ã€sequenceã®æœ€å¤§é•·ã§ã‚ã‚‹. (ex. ã‚¢ã‚¤ãƒ†ãƒ ã‚’ truncating ã—ãŸã‚Šã€padding ã—ãŸã‚Šã—ã¦ã€æœ€æ–°ã®<span class="math inline">\(n\)</span>å€‹ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ®‹ã™) The embedding for <span class="math inline">\((s_1, s_2, \cdots s_n)\)</span> is denoted as <span class="math inline">\(E \in R^{n \times d}\)</span> , which can be retrieved from the table <span class="math inline">\(T\)</span>. <span class="math inline">\((s_1, s_2, \cdots s_n)\)</span> ã®åŸ‹ã‚è¾¼ã¿ã‚’ <span class="math inline">\(E \in R^{n \times d}\)</span> ã¨è¡¨ã—(= <span class="math inline">\(E\)</span> ã¯åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ— â€¦!)ã€embedding table <span class="math inline">\(T\)</span> ã‹ã‚‰å–ã‚Šå‡ºã™ã“ã¨ãŒã§ãã‚‹. To capture the impacts of different positions, one can inject a learnable positional embedding <span class="math inline">\(P \in R^{n \times d}\)</span> into the input embedding as: sequenceå†…ã®ç•°ãªã‚‹positionã®å½±éŸ¿(=time stepé–“ã®è·é›¢ã‚„sequenceã®é †åº)ã‚’æ‰ãˆã‚‹ãŸã‚ã«ã€å­¦ç¿’å¯èƒ½ãªpositonal embedding <span class="math inline">\(P \in R^{n à±ªtimes d}\)</span> ã‚’å…¥åŠ›embeddng <span class="math inline">\(E\)</span> ã«inject(æ³¨å…¥)ã™ã‚‹ã“ã¨ãŒã§ãã‚‹:</p>
<p>(å„tokenã®ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã«ã€positonal encoding vectorã‚’è¿½åŠ ã™ã‚‹å¼!)</p>
<p><span class="math display">\[
\hat{E} = E + P \tag{1}
\]</span></p>
<p>where <span class="math inline">\(\hat{E} \in R^{n\times d}\)</span> is an order-aware embedding, which can be directly fed to any Transformer-based models. ã“ã“ã§ã€<span class="math inline">\(hat{E}ã¯ \in R^{ntimes d}\)</span> ã¯ order-awareãª(=sequenceå†…ã®é †åºã‚’è€ƒæ…®ã—ãŸ) åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ— ã§ã€Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã«ç›´æ¥ä¸ãˆã‚‹ã“ã¨ãŒã§ãã‚‹.</p>
</section>
<section id="transformer-block" class="level3">
<h3 class="anchored" data-anchor-id="transformer-block">3.2.2. Transformer Block</h3>
<p>A Transformer block consists of a self-attention layer and a point-wise feed-forward layer. ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ»ãƒ–ãƒ­ãƒƒã‚¯ã¯ã€self-attention layer ã¨ point-wise feed-forward layer (=å…¨çµåˆå±¤) ã§æ§‹æˆã•ã‚Œã‚‹.</p>
<p><strong>Self-attention Layer</strong>: The key component of Transformer block is the self-attention layer that is highly efficient to uncover sequential dependencies in a sequence [43]. <strong>self-attentionå±¤</strong>ï¼š Transformerãƒ–ãƒ­ãƒƒã‚¯ã®é‡è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é€æ¬¡çš„ãªä¾å­˜é–¢ä¿‚ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ãŸã‚ã«éå¸¸ã«åŠ¹ç‡çš„ã§ã‚ã‚‹self-attentionå±¤ã§ã‚ã‚‹[43]ã€‚ The scaled dot-product attention is a popular attention kernel: scaled dot-product attentionã¯ä¸€èˆ¬çš„ãªattention kernel(=ã‹ãªã‚Šä¸€èˆ¬çš„ãªattentioné–¢æ•°ã®ä¸€ã¤ã€ã¨ã„ã†èªè­˜:thinking:)ã§ã‚ã‚‹ï¼š</p>
<p>(scaled-dot-product attentionã®é–¢æ•°å¼)</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d}}) V
\tag{2}
\]</span></p>
<p>where <span class="math inline">\(\text{Attention}(Q, K, V) \in R^{n \times d}\)</span> is the output item representations; <span class="math inline">\(Q = \hat{E}W^Q\)</span>, <span class="math inline">\(K =\hat{E}W^K\)</span>, and <span class="math inline">\(V = \hat{E}W^V\)</span> are the queries, keys and values, respectively; <span class="math inline">\({W^Q, W^K, W^V} \in R^{d \times d}\)</span> are three projection matrices; and <span class="math inline">\(\sqrt{d}\)</span> is the scale factor to produce a softer attention distribution. ã“ã“ã§ã€</p>
<ul>
<li><span class="math inline">\(\text{Attention}(Q, K, V) \in R^{n \times d}\)</span> ã¯ <strong>output item representations</strong> ã§ã‚ã‚‹</li>
<li><span class="math inline">\(Q = \hat{E} W^Q\)</span>, <span class="math inline">\(K =\hat{E} W^K\)</span>, and <span class="math inline">\(V = \hat{E} W^V\)</span> ã¯ãã‚Œãã‚Œ Query, Key, Valueã§ã‚ã‚‹. (Qã‚‚Kã‚‚Vã‚‚ã€åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ—Eã‚’å…ƒã«ã—ã¦ã‚‹ã®ã§ã€attentioné–¢æ•°ã®self-attentionçš„ãªä½¿ã„æ–¹â€¦!)</li>
<li><span class="math inline">\({W^Q, W^K, W^V} \in R^{d \times d}\)</span> ã¯3ã¤ã®projectionè¡Œåˆ—</li>
<li><span class="math inline">\(sqrt{d}\)</span> ã¯ã‚ˆã‚ŠæŸ”ã‚‰ã‹ã„(?)attentionåˆ†å¸ƒã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®scale-factorã§ã‚ã‚‹.(æ­£è¦åŒ–çš„ãªã‚¤ãƒ¡ãƒ¼ã‚¸!)</li>
</ul>
<p>In sequential recommendation, one can utilize either left-to-right unidirectional attentions (e.g., SASRec [26] and TiSASRec [30]) or bidirectional attentions (e.g., BERT4Rec [41]) to predict the next item. é€æ¬¡æ¨è–¦ã§ã¯ã€<strong>left-to-right uni-directional attentions(å·¦ã‹ã‚‰å³ã¸ã®ä¸€æ–¹å‘ã®attention)</strong>(ex. SASRec [26]ã‚„TiSASRec [30])ã‚„ã€ã‚‚ã—ãã¯<strong>bi-directional attention(åŒæ–¹å‘ã®attention)</strong> (ex. BERT4Rec [41])ã‚’åˆ©ç”¨ã—ã¦ã€æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ãŒã§ãã‚‹. Moreover, one can apply <span class="math inline">\(H\)</span> attention functions in parallel to enhance expressiveness: <span class="math inline">\(H &lt;- \text{MultiHead}(\hat{E})\)</span> [43]. ã•ã‚‰ã«ã€<span class="math inline">\(H\)</span> å€‹ã®attentioné–¢æ•°ã‚’ä¸¦åˆ—ã«é©ç”¨ã™ã‚‹ã“ã¨ã§ã€è¡¨ç¾åŠ›ã‚’é«˜ã‚ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼š<span class="math inline">\(\mathbf{H} &lt;- \text{MultiHead}(\hat{E})\)</span> [43]. (å…ƒã®transformerã§ã‚‚æ¡ç”¨ã—ã¦ã‚‹ã€Multi-head attentionã­:thinking:)</p>
<p><strong>Point-wise Feed-forward Layer</strong>: As the self attention layer is built on linear projections, we can endow the nonlinearity by introducing a point-wise feed-forward layers: <strong>ãƒã‚¤ãƒ³ãƒˆãƒ»ãƒ¯ã‚¤ã‚ºãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤</strong>ï¼š self-attentionå±¤ã¯linear projectionã§æ§‹ç¯‰ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€<strong>Point-wise Feed-forwardå±¤ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€éç·šå½¢æ€§ã‚’ä»˜ä¸ã™ã‚‹(ãƒ¢ãƒ‡ãƒ«ã®è¡¨ç¾åŠ›ã‚’é«˜ã‚ã‚‹ç‚ºã®éç·šå½¢å¤‰æ›!)</strong>ã“ã¨ãŒã§ãã‚‹(ã‚ã€ãã†ã„ã†ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãªã®ã‹â€¦!ä¸­é–“å±¤ï¼‘ã¤ã®ã‚„ã¤! :thinking:)ï¼š</p>
<p>(è£œè¶³: â€œpoint-wiseâ€ -&gt; <strong>è¦ç´ æ¯ã«ç‹¬ç«‹ã—ã¦æ“ä½œã‚’è¡Œã†</strong>ã€ã¨ã„ã†æ„å‘³. Transformerã®å ´åˆã¯Sequenceãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›ã«å–ã‚‹ãŒã€ã“ã®å±¤ã¯ã€sequenceãƒ‡ãƒ¼ã‚¿ã®å„è¦ç´ ã«å¯¾ã—ã¦å€‹åˆ¥ã«å‡¦ç†ãŒè¡Œã‚ã‚Œã‚‹. ã¤ã¾ã‚Šã€sequenceãƒ‡ãƒ¼ã‚¿ã®å„è¦ç´ ã«å¯¾ã—ã¦åŒã˜æ“ä½œãŒè¡Œã‚ã‚Œã‚‹.:thinking:) (è£œè¶³: â€œfeed-forwardâ€ -&gt; ãƒ‡ãƒ¼ã‚¿ã®å…¥å‡ºåŠ›ã®æµã‚ŒãŒ1æ–¹å‘=å‰æ–¹æ–¹å‘ã«ã®ã¿é€²ã‚€äº‹ã‚’æ„å‘³ã™ã‚‹. é€†ã«ã€â€œfeed-forwardâ€ã§ã¯ãªã„å±¤ã¯RNNã¨ã‹! CNNã¯â€point-wiseâ€ã§ã¯ãªã„ãŒâ€feed-forwardâ€ã§ã‚ã‚‹æ°—ã¯ã™ã‚‹:thinking:)</p>
<p><span class="math display">\[
F_i = FFN(H_i) = \text{ReLU}(H_i W^{(1)} + b^{(1)})W^{(2)} + b^{(2)}
\tag{3}
\]</span></p>
<p>where <span class="math inline">\(W^{(\ast)} \in R^{d \times d}\)</span>, <span class="math inline">\(b^{(*)} \in R^d\)</span> are the learnable weights and bias. ã“ã“ã§ã€<span class="math inline">\(W^{(*)} \in R^{d }\)</span>, <span class="math inline">\(b^{(*)} \in R^d\)</span> ã¯å­¦ç¿’å¯èƒ½ãªé‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã§ã‚ã‚‹ã€‚</p>
<p>In practice, it is usually beneficial to learn hierarchical item dependencies by stacking more Transformer blocks. å®Ÿéš›ã«ã¯ã€ã‚ˆã‚Šå¤šãã®Transformerãƒ–ãƒ­ãƒƒã‚¯ã‚’ç©ã¿é‡ã­ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€éšå±¤çš„ãªã‚¢ã‚¤ãƒ†ãƒ ã®ä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒé€šå¸¸æœ‰ç›Šã§ã‚ã‚‹ã€‚ Also, one can adopt the tricks of residual connection, dropout, and layer normalization for stabilizing and accelerating the training. ã¾ãŸã€æ®‹å·®æ¥ç¶šã€ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã€ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ãªã©ã®ãƒˆãƒªãƒƒã‚¯ã‚’æ¡ç”¨ã™ã‚‹ã“ã¨ã§ã€å­¦ç¿’ã®å®‰å®šåŒ–ã¨é«˜é€ŸåŒ–ã‚’å›³ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ Here we simply summarize the output of <span class="math inline">\(L\)</span> Transformer blocks as: <span class="math inline">\(F^{(L)} &lt;- \text{Transformer}(\hat{E})\)</span>. ã“ã“ã§ã¯ã€<span class="math inline">\(L\)</span>Transformerãƒ–ãƒ­ãƒƒã‚¯ã®å‡ºåŠ›ã‚’å˜ç´”ã«æ¬¡ã®ã‚ˆã†ã«ã¾ã¨ã‚ã‚‹ï¼š F^{(L)} &lt;- (E})$.</p>
</section>
<section id="learning-objective-3.2.3.-å­¦ç¿’ç›®æ¨™" class="level3">
<h3 class="anchored" data-anchor-id="learning-objective-3.2.3.-å­¦ç¿’ç›®æ¨™">3.2.3. Learning Objective 3.2.3. å­¦ç¿’ç›®æ¨™</h3>
<p>After stacked <span class="math inline">\(L\)</span> Transformer blocks, one can predict the next item (given the first <span class="math inline">\(t\)</span> items) based on <span class="math inline">\(F_t^{(L)}\)</span>. L<span class="math inline">\(å€‹ã®Transformerãƒ–ãƒ­ãƒƒã‚¯ã‚’ç©ã¿é‡ã­ãŸå¾Œã€\)</span>F_t^{(L)}<span class="math inline">\(ã«åŸºã¥ã„ã¦ï¼ˆæœ€åˆã®\)</span>t$å€‹ã®é …ç›®ãŒã‚ã‚Œã°ï¼‰æ¬¡ã®é …ç›®ã‚’äºˆæ¸¬ã§ãã‚‹ã€‚ In this work, we use inner product to predict the relevance of item <span class="math inline">\(i\)</span> as: ã“ã®ç ”ç©¶ã§ã¯ã€å†…ç©ã‚’ä½¿ã£ã¦é …ç›®<span class="math inline">\(i\)</span>ã®é–¢é€£æ€§ã‚’æ¬¡ã®ã‚ˆã†ã«äºˆæ¸¬ã™ã‚‹ï¼š</p>
<p><span class="math display">\[
r_{i, t} = &lt;F_{t}^{(L)}, T_i&gt;,
\]</span></p>
<p>where <span class="math inline">\(T_i \in R^d\)</span> is the embedding of item <span class="math inline">\(i\)</span>. ã“ã“ã§ <span class="math inline">\(T_i \in R^d\)</span> ã¯é …ç›® <span class="math inline">\(i\)</span> ã®åŸ‹ã‚è¾¼ã¿ã§ã‚ã‚‹ã€‚ Recall that the model inputs a sequence <span class="math inline">\(s = (s_1, s_2, \cdots, s_n)\)</span> and its desired output is a shifted version of the same sequence <span class="math inline">\(o = (o_1, o_2, \cdots, o_n)\)</span>, we can adopt the binary cross-entropy loss as: ãƒ¢ãƒ‡ãƒ«ã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹<span class="math inline">\(s = (s_1, s_2, Ë¶cdots, s_n)\)</span>ã‚’å…¥åŠ›ã—ã€ãã®å‡ºåŠ›ã¯åŒã˜ã‚·ãƒ¼ã‚±ãƒ³ã‚¹<span class="math inline">\(o = (o_1, o_2, Ë¶cdots, o_n)\)</span>ã‚’ã‚·ãƒ•ãƒˆã—ãŸã‚‚ã®ã§ã‚ã‚‹ï¼š</p>
<p><span class="math display">\[
L_{BCE} = - \sum_{S^u \in S} \sum_{t=1}^{n}{[\log{\sigma(r_{o_t, t})} + \log{1 - \sigma (r_{o_t', t})}]} + a \cdot ||\theta||^2_F
\tag{4}
\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the mode parameters, <span class="math inline">\(a\)</span> is the reqularizer to prevent over-fitting, <span class="math inline">\(o'_t \notin S^u\)</span> is a negative sample corresponding to <span class="math inline">\(o_t\)</span>, and <span class="math inline">\(\sigma(\cdot)\)</span> is the sigmoid function. ã“ã“ã§ã€<span class="math inline">\(theta\)</span>ã¯ãƒ¢ãƒ¼ãƒ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€<span class="math inline">\(a\)</span>ã¯ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒˆã‚’é˜²ããŸã‚ã®reqularizerã€<span class="math inline">\(o'_t \notin S^u\)</span>ã¯<span class="math inline">\(o_t\)</span>ã«å¯¾å¿œã™ã‚‹è² ã®ã‚µãƒ³ãƒ—ãƒ«ã€<span class="math inline">\(sigma( \cdot)\)</span>ã¯ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã§ã‚ã‚‹ã€‚</p>
<p>More details can be found in SASRec [26] and BERT4Rec [41]. è©³ç´°ã¯ã€SASRec[26]ãŠã‚ˆã³BERT4Rec[41]ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã€‚</p>
</section>
</section>
<section id="the-noisy-attentions-problem" class="level2">
<h2 class="anchored" data-anchor-id="the-noisy-attentions-problem">3.3. The Noisy Attentions Problem</h2>
<p>Despite the success of SASRec and its variants, we argue that they are insufficient to address the noisy items in sequences. SASRecã¨ãã®äºœç¨®ã®æˆåŠŸã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸­ã®ãƒã‚¤ã‚ºã®å¤šã„é …ç›®ã«å¯¾å‡¦ã™ã‚‹ã«ã¯ä¸ååˆ†ã§ã‚ã‚‹ã¨æˆ‘ã€…ã¯ä¸»å¼µã™ã‚‹ã€‚ The reason is that the full attention distributions (e.g., Eq.(2)) are dense and would assign certain credits to irrelevant items. ãã®ç†ç”±ã¯ã€å®Œå…¨ãªæ³¨ç›®åº¦åˆ†å¸ƒï¼ˆä¾‹ãˆã°å¼(2)ï¼‰ã¯å¯†åº¦ãŒé«˜ãã€ç„¡é–¢ä¿‚ãªé …ç›®ã«ä¸€å®šã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å‰²ã‚Šå½“ã¦ã¦ã—ã¾ã†ã‹ã‚‰ã§ã‚ã‚‹ã€‚ This complicates the item-item dependencies, increases the training difficulty, and even degrades the model performance. ã“ã‚Œã¯é …ç›®-é …ç›®ã®ä¾å­˜é–¢ä¿‚ã‚’è¤‡é›‘ã«ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®é›£æ˜“åº¦ã‚’ä¸Šã’ã€ã•ã‚‰ã«ã¯ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ä½ä¸‹ã•ã›ã‚‹ã€‚ To address this issue, several attempts have been proposed to manually define sparse attention schemas in language modeling tasks [2, 10, 18, 58]. ã“ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã§ã‚¹ãƒ‘ãƒ¼ã‚¹æ³¨æ„ã‚¹ã‚­ãƒ¼ãƒã‚’æ‰‹å‹•ã§å®šç¾©ã™ã‚‹è©¦ã¿ãŒã„ãã¤ã‹ææ¡ˆã•ã‚Œã¦ã„ã‚‹[2, 10, 18, 58]ã€‚ However, these fixed sparse attentions cannot adapt to the input data [12], leading to sub-optimal performance. ã—ã‹ã—ã€ã“ã®ã‚ˆã†ãªå›ºå®šçš„ãªã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¯ã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«é©å¿œã™ã‚‹ã“ã¨ãŒã§ããš[12]ã€æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ã¯è¨€ãˆãªã„ã€‚</p>
<p>On the other hand, several dropout techniques are specifically designed for Transformers to keep only a small portion of attentions, including LayerDrop [17], DropHead [60], and UniDrop [52]. ä¸€æ–¹ã€LayerDrop[17]ã€DropHead[60]ã€UniDrop[52]ãªã©ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ãŸã‚ã«ç‰¹åˆ¥ã«è¨­è¨ˆã•ã‚ŒãŸã€æ³¨ç›®ã®ã”ãä¸€éƒ¨ã ã‘ã‚’æ®‹ã™ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆæŠ€è¡“ãŒã„ãã¤ã‹ã‚ã‚‹ã€‚ Nevertheless, these randomly dropout approaches are susceptible to bias: the fact that attentions can be dropped randomly does not mean that the model allows them to be dropped, which may lead to over-aggressive pruning issues. ã¨ã¯ã„ãˆã€ã“ã®ã‚ˆã†ãªãƒ©ãƒ³ãƒ€ãƒ ã«è„±è½ã•ã›ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ãƒã‚¤ã‚¢ã‚¹ã®å½±éŸ¿ã‚’å—ã‘ã‚„ã™ã„ã€‚æ³¨æ„ãŒãƒ©ãƒ³ãƒ€ãƒ ã«è„±è½ã•ã›ã‚‰ã‚Œã‚‹ã¨ã„ã†äº‹å®Ÿã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒè„±è½ã‚’è¨±å®¹ã—ã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ãªã„ã®ã§ã€éåº¦ã«æ”»æ’ƒçš„ãªåˆˆã‚Šè¾¼ã¿ã®å•é¡Œã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚ In contrast, we propose a simple yet effective data-driven method to mask out irrelevant attentions by using differentiable masks. ã“ã‚Œã«å¯¾ã—ã¦æˆ‘ã€…ã¯ã€å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’ç”¨ã„ã¦ç„¡é–¢ä¿‚ãªæ³¨æ„ã‚’ãƒã‚¹ã‚¯ã™ã‚‹ã€ã‚·ãƒ³ãƒ—ãƒ«ã‹ã¤åŠ¹æœçš„ãªãƒ‡ãƒ¼ã‚¿é§†å‹•æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚</p>
</section>
</section>
<section id="rec-denoiser-4.-ãƒ¬ã‚¯ãƒ‡ãƒã‚¤ã‚¶ãƒ¼" class="level1">
<h1>4. Rec-Denoiser 4. ãƒ¬ã‚¯ãƒ»ãƒ‡ãƒã‚¤ã‚¶ãƒ¼</h1>
<p>In this section, we present our Rec-Denoiser that consists of two parts: differentiable masks for self-attention layers and Jacobian regularization for Transformer blocks. ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€Rec-Denoiserã‚’ç´¹ä»‹ã™ã‚‹ã€‚Rec-Denoiserã¯ã€è‡ªå·±æ³¨æ„å±¤ã®ãŸã‚ã®å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã¨ã€Transformerãƒ–ãƒ­ãƒƒã‚¯ã®ãŸã‚ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–ã®2ã¤ã®éƒ¨åˆ†ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ã€‚</p>
<section id="differentiable-masks" class="level2">
<h2 class="anchored" data-anchor-id="differentiable-masks">4.1. Differentiable Masks</h2>
<p>The self-attention layer is the cornerstone of Transformers to capture long-range dependencies. ã‚»ãƒ«ãƒ•ãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯ã€é•·è·é›¢ã®ä¾å­˜é–¢ä¿‚ã‚’æ‰ãˆã‚‹ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®è¦ã§ã‚ã‚‹ã€‚ As shown in Eq.(2), the softmax operator assigns a non-zero weight to every item. å¼(2)ã«ç¤ºã™ã‚ˆã†ã«ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹æ¼”ç®—å­ã¯ã™ã¹ã¦ã®é …ç›®ã«0ã§ãªã„é‡ã¿ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã€‚ However, full attention distributions may not always be advantageous since they may cause irrelevant dependencies, unnecessary computation, and unexpected explanation. ã—ã‹ã—ã€å®Œå…¨ãªæ³¨ç›®åº¦åˆ†å¸ƒã¯ã€ç„¡é–¢ä¿‚ãªä¾å­˜é–¢ä¿‚ã€ä¸å¿…è¦ãªè¨ˆç®—ã€äºˆæœŸã›ã¬èª¬æ˜ã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€å¿…ãšã—ã‚‚æœ‰åˆ©ã¨ã¯é™ã‚‰ãªã„ã€‚ We next put forward differentiable masks to address this concern. æ¬¡ã«ã€ã“ã®æ‡¸å¿µã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’ææ¡ˆã™ã‚‹ã€‚</p>
<section id="learnable-sparse-attentions-4.1.1.-å­¦ç¿’å¯èƒ½ãªã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³" class="level3">
<h3 class="anchored" data-anchor-id="learnable-sparse-attentions-4.1.1.-å­¦ç¿’å¯èƒ½ãªã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³">4.1.1. Learnable Sparse Attentions 4.1.1. å­¦ç¿’å¯èƒ½ãªã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³</h3>
<p>Not every item in a sequence is aligned well with user preferences in the same sense that not all attentions are strictly needed in self-attention layers. ã‚»ãƒ«ãƒ•ãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«ãŠã„ã¦ã€ã™ã¹ã¦ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãŒå³å¯†ã«å¿…è¦ã¨ã•ã‚Œã‚‹ã‚ã‘ã§ã¯ãªã„ã®ã¨åŒã˜æ„å‘³ã§ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ã™ã¹ã¦ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å—œå¥½ã«ã†ã¾ãåˆè‡´ã—ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªã„ã€‚ Therefore, we attach each self-attention layer with a trainable binary mask to prune noisy or task-irrelevant attentions. ãã“ã§ã€å„è‡ªå·±æ³¨æ„å±¤ã«å­¦ç¿’å¯èƒ½ãªãƒã‚¤ãƒŠãƒªãƒ»ãƒã‚¹ã‚¯ã‚’ä»˜åŠ ã—ã€ãƒã‚¤ã‚ºã®å¤šã„æ³¨æ„ã‚„ã‚¿ã‚¹ã‚¯ã¨ç„¡é–¢ä¿‚ãªæ³¨æ„ã‚’é™¤å»ã™ã‚‹ã€‚ Formally, for the ğ‘™-th self-attention layer in Eq.(2), we introduce a binary matrix <span class="math inline">\(Z^{(l)} \in {0, 1}^{n\times n}\)</span>, where <span class="math inline">\(Z^{(l)}_{u,v}\)</span> denotes whether the connection between query <span class="math inline">\(u\)</span> and key <span class="math inline">\(v\)</span> is present. ã“ã“ã§ã€<span class="math inline">\(Z^{(l)}_{u,v}\)</span>ã¯ã‚¯ã‚¨ãƒª<span class="math inline">\(u\)</span>ã¨ã‚­ãƒ¼<span class="math inline">\(v\)</span>ã®æ¥ç¶šã®æœ‰ç„¡ã‚’è¡¨ã™ã€‚ As such, the <span class="math inline">\(l\)</span>-th self-attention layer becomes: ã“ã®ã‚ˆã†ã«ã€<span class="math inline">\(l\)</span>ç•ªç›®ã®è‡ªå·±æ³¨æ„å±¤ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚‹ï¼š</p>
<p><span class="math display">\[
A^{(l)} = \text{softmax}(\frac{Q^{(l)} K^{(l)T}}{\sqrt{d}}),
\\
M^{(l)} = A^{(l)} \odot Z^{(l)},
\\
\text{Attention}(Q^{(l)}, K^{(l)}, V^{(l)}) = M^{(l)} V^{(l)},
\tag{5}
\]</span></p>
<p>where <span class="math inline">\(A^{(l)}\)</span> is the original full attentions, <span class="math inline">\(M^{(l)}\)</span> denotes the sparse attentions, and <span class="math inline">\(\odot\)</span> is the element-wise product. ã“ã“ã§ã€<span class="math inline">\(A^{(l)}\)</span>ã¯å…ƒã®å®Œå…¨æ³¨ç›®åº¦ã€<span class="math inline">\(M^{(l)}\)</span>ã¯ç–æ³¨ç›®åº¦ã€<span class="math inline">\(modot\)</span>ã¯è¦ç´ ã”ã¨ã®ç©ã§ã‚ã‚‹ã€‚ Intuitively, the mask <span class="math inline">\(Z^{(l)}\)</span> (e.g., 1 is kept and 0 is dropped) requires minimal changes to the original self-attention layer. ç›´æ„Ÿçš„ã«ã¯ã€ãƒã‚¹ã‚¯<span class="math inline">\(Z^{(l)}\)</span>ï¼ˆä¾‹ãˆã°ã€1ã‚’æ®‹ã—ã¦0ã‚’è½ã¨ã™ï¼‰ã¯ã€å…ƒã®è‡ªå·±æ³¨æ„å±¤ã«æœ€å°é™ã®å¤‰æ›´ã‚’åŠ ãˆã‚‹ã ã‘ã§æ¸ˆã‚€ã€‚ More importantly, they are capable of yielding exactly zero attention scores for irrelevant dependencies, resulting in better interpretability. ã•ã‚‰ã«é‡è¦ãªã®ã¯ã€ç„¡é–¢ä¿‚ãªä¾å­˜é–¢ä¿‚ã«å¯¾ã—ã¦æ³¨æ„ã‚¹ã‚³ã‚¢ã‚’æ­£ç¢ºã«ã‚¼ãƒ­ã«ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ãŸã‚ã€è§£é‡ˆã—ã‚„ã™ããªã‚‹ã¨ã„ã†ã“ã¨ã ã€‚ The idea of differentiable masks is not new. å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã¨ã„ã†ã‚¢ã‚¤ãƒ‡ã‚¢ã¯æ–°ã—ã„ã‚‚ã®ã§ã¯ãªã„ã€‚ In the language modeling, differentiable masks have been shown to be very powerful to extract short yet sufficient sentences, which achieves better performance [1, 13]. è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ãŠã„ã¦ã€å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã¯ã€çŸ­ãã¦ã‚‚ååˆ†ãªã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã‚’æŠ½å‡ºã™ã‚‹ã®ã«éå¸¸ã«å¼·åŠ›ã§ã‚ã‚Šã€ã‚ˆã‚Šè‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹[1, 13]ã€‚</p>
<p>One way to encourage sparsity of <span class="math inline">\(M^{(l)}\)</span> is to explicitly penalize the number of non-zero entries of <span class="math inline">\(Z^{(l)}\)</span>, for <span class="math inline">\(1 \leq l \leq L\)</span>, by minimizing: M<sup>{(l)}<span class="math inline">\(ã®ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã‚’å¥¨åŠ±ã™ã‚‹ä¸€ã¤ã®æ–¹æ³•ã¯ã€\)</span>Z</sup>{(l)}$ã®ã‚¼ãƒ­ã§ãªã„ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã®æ•°ã‚’ã€<span class="math inline">\(1 ï¿¢l ï¿¢L\)</span>ã«å¯¾ã—ã¦ã€æœ€å°åŒ–ã™ã‚‹ã“ã¨ã§æ˜ç¤ºçš„ã«ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’èª²ã™ã“ã¨ã§ã‚ã‚‹ï¼š</p>
<p><span class="math display">\[
R_M = \sum_{l=1}^{L}||Z^{l}||_{0}
= \sum_{l=1}^{L} \sum_{u=1}^{n} \sum_{v=1}^{n} I[Z_{u,v}^{(l)} \neq 0],
\tag{6}
\]</span></p>
<p>where <span class="math inline">\(I[c]\)</span> is an indicator that is equal to 1 if the condition <span class="math inline">\(c\)</span> holds and 0 otherwise; and <span class="math inline">\(||\cdot||_{0}\)</span> denotes the <span class="math inline">\(L_0\)</span> norm that is able to drive irrelevant attentions to be exact zeros. |</p>
<p>However, there are two challenges for optimizing <span class="math inline">\(Z^{(l)}\)</span>: non-differentiability and large variance. ã—ã‹ã—ã€<span class="math inline">\(Z^{(l)}\)</span>ã®æœ€é©åŒ–ã«ã¯ã€å¾®åˆ†ä¸å¯èƒ½æ€§ã¨åˆ†æ•£ã®å¤§ãã•ã¨ã„ã†2ã¤ã®èª²é¡ŒãŒã‚ã‚‹ã€‚ <span class="math inline">\(L_0\)</span> is discontinuous and has zero derivatives almost everywhere. L_0$ã¯ä¸é€£ç¶šã§ã‚ã‚Šã€ã»ã¨ã‚“ã©ã©ã“ã§ã‚‚ã‚¼ãƒ­å°é–¢æ•°ã‚’æŒã¤ã€‚ Additionally, there are <span class="math inline">\(2^{n^2}\)</span> possible states for the binary mask <span class="math inline">\(Z^{(l)}\)</span> with large variance. ã•ã‚‰ã«ã€2å€¤ãƒã‚¹ã‚¯<span class="math inline">\(Z^{(l)}\)</span>ã«ã¯å¤§ããªåˆ†æ•£ã‚’æŒã¤<span class="math inline">\(2^{n^2}\)</span>å€‹ã®å¯èƒ½ãªçŠ¶æ…‹ãŒã‚ã‚‹ã€‚ Next, we propose an efficient estimator to solve this stochastic binary optimization problem. æ¬¡ã«ã€ã“ã®ç¢ºç‡çš„äºŒå…ƒæœ€é©åŒ–å•é¡Œã‚’è§£ããŸã‚ã®åŠ¹ç‡çš„ãªæ¨å®šå™¨ã‚’ææ¡ˆã™ã‚‹ã€‚</p>
</section>
<section id="efficient-gradient-computation-4.1.2.-åŠ¹ç‡çš„ãªå‹¾é…è¨ˆç®—" class="level3">
<h3 class="anchored" data-anchor-id="efficient-gradient-computation-4.1.2.-åŠ¹ç‡çš„ãªå‹¾é…è¨ˆç®—">4.1.2. Efficient Gradient Computation 4.1.2. åŠ¹ç‡çš„ãªå‹¾é…è¨ˆç®—</h3>
<p>Since Z (ğ‘™) is jointly optimized with the original Transformer-based models, we combine Eq.(4) and Eq.(6) into one unified objective: Z (â†ªLl_1459) ã¯ã‚ªãƒªã‚¸ãƒŠãƒ«ã®Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã¨å…±åŒã§æœ€é©åŒ–ã•ã‚Œã‚‹ã®ã§ã€å¼(4)ã¨å¼(6)ã‚’1ã¤ã®çµ±ä¸€ã•ã‚ŒãŸç›®çš„ã«ã¾ã¨ã‚ã‚‹ï¼š</p>
<p><span class="math display">\[
L(Z, \Theta) = L_{BCE}({A^{(l)} \odot Z^{(l)}}, \Theta) + \beta \cdot \sum_{l=1}^{L} \sum_{u=1}^{n} \sum_{v=1}^{n} I[Z_{u,v}^{(l)} \neq 0]
\tag{7}
\]</span></p>
<p>where ğ›½ controls the sparsity of masks and we denote Z as Z := {Z (1) , Â· Â· Â· , Z (ğ¿) }. ã“ã“ã§ â†ªL_1FD ã¯ãƒã‚¹ã‚¯ã®ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã‚’åˆ¶å¾¡ã—ã€Z ã‚’ Z := {Z (1) , - - , Z (â†ªLu_1D43F) } ã¨ã™ã‚‹ã€‚ We further consider each Z (ğ‘™) ğ‘¢,ğ‘£ is drawn from a Bernoulli distribution parameterized by Î  (ğ‘™) ğ‘¢,ğ‘£ such that Z (ğ‘™) ğ‘¢,ğ‘£ âˆ¼ Bern(Î  (ğ‘™) ğ‘¢,ğ‘£ ) [34]. ã•ã‚‰ã«ã€Z (ğ‘™) âˆ¼ Bern(Î  (ğ‘™) ğ‘¢,â†ªLl_1D463 ) ã®ã‚ˆã†ãªÎ  (ğ‘™) âˆ¼ Bern(Î  (ğ‘¢),â†ªLl_1D463 ) ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚ŒãŸBernoulliåˆ†å¸ƒã‹ã‚‰Z (ğ‘™) âˆ¼ ğ‘¢,â†ªLl_1D463) ãŒå¼•ã‹ã‚Œã‚‹ã¨è€ƒãˆã‚‹ã€‚[34]. As the parameter Î  (ğ‘™) ğ‘¢,ğ‘£ is jointly trained with the downstream tasks, a small value of Î  (ğ‘™) ğ‘¢,ğ‘£ suggests that the attention A (ğ‘™) ğ‘¢,ğ‘£ is more likely to be irrelevant, and could be removed without side effects. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Î  (ğ‘™) ğ‘¢,ğ‘£ã¯ä¸‹æµã‚¿ã‚¹ã‚¯ã¨å…±åŒã§å­¦ç¿’ã•ã‚Œã‚‹ãŸã‚ã€Î  (ğ‘™) ğ‘¢,ğ‘£ã®å€¤ãŒå°ã•ã„ã¨ã€æ³¨ç›®A (ğ‘™) ğ‘¢,ğ‘£ã¯ç„¡é–¢ä¿‚ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ãã€å‰¯ä½œç”¨ãªãå‰Šé™¤ã§ãã‚‹ã€‚ By doing this, Eq.(7) becomes: ã“ã†ã™ã‚‹ã“ã¨ã§ã€å¼(7)ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚‹ï¼š</p>
<p><span class="math display">\[
\tag{8}
\]</span></p>
<p>where E(Â·) is the expectation. ã“ã“ã§E(-)ã¯æœŸå¾…å€¤ã§ã‚ã‚‹ã€‚ The regularization term is now continuous, but the first term Lğµğ¶ğ¸ (Z, Î˜) still involves the discrete variables Z (ğ‘™) . æ­£å‰‡åŒ–é …ã¯é€£ç¶šã«ãªã£ãŸãŒã€ç¬¬ä¸€é …Lğµğ¸ (Z, Î˜)ã¯ã¾ã é›¢æ•£å¤‰æ•°Z (â†ªLl_1D459) ã‚’å«ã‚“ã§ã„ã‚‹ã€‚ One can address this issue by using existing gradient estimators, such as REINFORCE [48] and Straight Through Estimator [3], etc. REINFORCE [48]ã‚„Straight Through Estimator [3]ãªã©ã®æ—¢å­˜ã®å‹¾é…æ¨å®šå™¨ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ã“ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ These approaches, however, suffer from either biased gradients or high variance. ã—ã‹ã—ã€ã“ã‚Œã‚‰ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€åã£ãŸå‹¾é…ã‚„é«˜ã„åˆ†æ•£ã«æ‚©ã¾ã•ã‚Œã¦ã„ã‚‹ã€‚ Alternatively, we directly optimize discrete variables using the recently proposed augment-REINFORCEmerge (ARM) [15, 16, 56], which is unbiased and has low variance. ã‚ã‚‹ã„ã¯ã€æœ€è¿‘ææ¡ˆã•ã‚ŒãŸaugment-REINFORCEmergeï¼ˆARMï¼‰[15, 16, 56]ã‚’ä½¿ã£ã¦ç›´æ¥é›¢æ•£å¤‰æ•°ã‚’æœ€é©åŒ–ã™ã‚‹ã€‚ In particular, we adopt the reparameterization trick [25], which reparameterizes Î  (ğ‘™) ğ‘¢,ğ‘£ âˆˆ [0, 1] to a deterministic function ğ‘”(Â·) with parameter Î¦ (ğ‘™) ğ‘¢,ğ‘£ , such that: ç‰¹ã«ã€Î  (ğ‘™) ğ‘¢,â†ªLl_1D463 âˆˆ [0, 1]ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Î¦ (ğ‘™) â†ªLl_1D463 ã‚’æŒã¤æ±ºå®šè«–çš„é–¢æ•°ğ‘”(-)ã«å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã™ã‚‹å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯[25]ã‚’æ¡ç”¨ã™ã‚‹ï¼š</p>
<p><span class="math display">\[
\tag{9}
\]</span></p>
<p>since the deterministic function ğ‘”(Â·) should be bounded within [0, 1], we simply choose the standard sigmoid function as our deterministic function: ğ‘”(ğ‘¥) = 1/(1 + ğ‘’ âˆ’ğ‘¥ ). æ±ºå®šè«–çš„é–¢æ•° á‘”(-) ã¯[0, 1]å†…ã§æœ‰ç•Œã§ã‚ã‚‹ã¹ããªã®ã§ã€æ±ºå®šè«–çš„é–¢æ•°ã¨ã—ã¦æ¨™æº–ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã‚’é¸ã¶ï¼š á‘” = 1/(1 + ğ‘’ -â†ªLl_1D465 ). As such, the second term in Eq.(8) becomes differentiable with the continuous function ğ‘”(Â·). ãã®ãŸã‚ã€å¼(8)ã®ç¬¬2é …ã¯é€£ç¶šé–¢æ•°á‘”(-)ã§å¾®åˆ†å¯èƒ½ã«ãªã‚‹ã€‚ We next present the ARM estimator for computing the gradients of binary variables in the first term of Eq.(8) [15, 16, 56]. æ¬¡ã«ã€å¼(8)ã®ç¬¬1é …ã®ãƒã‚¤ãƒŠãƒªå¤‰æ•°ã®å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã®ARMæ¨å®šå™¨ã‚’ç¤ºã™[15, 16, 56]ã€‚ According to Theorem 1 in ARM [56], we can compute the gradients for Eq.(8) as: ARM [56]ã®å®šç†1ã«ã‚ˆã‚Œã°ã€å¼(8)ã®å‹¾é…ã¯æ¬¡ã®ã‚ˆã†ã«è¨ˆç®—ã§ãã‚‹ï¼š</p>
<p><span class="math display">\[
\tag{10}
\]</span></p>
<p>where Uni(0, 1) denotes the Uniform distribution within [0, 1], and Lğµğ¶ğ¸ (I[U &gt; ğ‘”(âˆ’Î¦)], Î˜) is the cross-entropy loss obtained by setting the binary masks Z (ğ‘™) to 1 if U (ğ‘™) &gt; ğ‘”(âˆ’Î¦ (ğ‘™) ) in the forward pass, and 0 otherwise. ã“ã“ã§Uni(0, 1)ã¯[0, 1]å†…ã®ä¸€æ§˜åˆ†å¸ƒã‚’è¡¨ã—ã€Lğµğ¸ (I[U &gt; ğ‘”(-Î¦)])ã€ Î˜) ã¯ã€ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã«ãŠã„ã¦ã€U (â†ªLl_1D459) &gt; Î¦ (-Î¦ (â†ªLl_1D459) ã®å ´åˆã«ãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯Z (â†ªLl_1D459) ã‚’1ã«è¨­å®šã—ã€ãã‚Œä»¥å¤–ã®å ´åˆã«0ã«è¨­å®šã™ã‚‹ã“ã¨ã§å¾—ã‚‰ã‚Œã‚‹ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã€‚ The same strategy is applied to Lğµğ¶ğ¸ (I[U &lt; ğ‘”(Î¦)], Î˜). åŒã˜æˆ¦ç•¥ã‚’Lğµğ¸ (I[U &lt; Ç”], Î˜)ã«ã‚‚é©ç”¨ã™ã‚‹ã€‚ Moreover, ARM is an unbiased estimator due to the linearity of expectations. ã•ã‚‰ã«ã€ARMã¯æœŸå¾…å€¤ã®ç·šå½¢æ€§ã«ã‚ˆã‚Šä¸åæ¨å®šé‡ã¨ãªã‚‹ã€‚ Note that we need to evaluate Lğµğ¶ğ¸ (Â·) twice to compute gradients in Eq.(10). å¼(10)ã®å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã«ã€Lğµğ¸ (-)ã‚’2å›è©•ä¾¡ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã€‚ Given the fact that ğ‘¢ âˆ¼ Uni(0, 1) implies (1 âˆ’ ğ‘¢) âˆ¼ Uni(0, 1), we can replace U with (1 âˆ’ U) in the indicator function inside Lğµğ¶ğ¸ (I[U &gt; ğ‘”(âˆ’Î¦)], Î˜): â†ªLl_1D462 âˆ¼ Uni(0, 1)ãŒ(1 - â†ªLl_1D462) âˆ¼ Uni(0, 1)ã‚’æ„å‘³ã™ã‚‹ã“ã¨ã‹ã‚‰ã€Lğ¶ğ¸ (I[U &gt; ğ‘”(-Î¦)], Î˜)å†…ã®æŒ‡æ¨™é–¢æ•°ã«ãŠã„ã¦ã€Uã‚’(1 - U)ã«ç½®ãæ›ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ï¼š</p>
<p><span class="math display">\[
\tag{}
\]</span></p>
<p>To this end, we can further reduce the complexity by considering the variants of ARM â€“ Augment-Reinforce (AR) [56]: ã“ã®ãŸã‚ã€ARMã®å¤‰å½¢ã§ã‚ã‚‹Augment-Reinforceï¼ˆARï¼‰[56]ã‚’è€ƒæ…®ã™ã‚‹ã“ã¨ã§ã€ã•ã‚‰ã«è¤‡é›‘ã•ã‚’è»½æ¸›ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼š</p>
<p><span class="math display">\[
\tag{11}
\]</span></p>
<p>where only requires one-forward pass. ã“ã“ã§å¿…è¦ãªã®ã¯ãƒ¯ãƒ³ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã ã‘ã ã€‚ The gradient estimator âˆ‡ ğ´ğ‘… Î¦ L (Î¦, Î˜) is still unbiased but may pose higher variance, comparing to âˆ‡ ğ´ğ‘…ğ‘€ Î¦ L (Î¦, Î˜). å‹¾é…æ¨å®šé‡âˆ‡ Î¦ ğ´ L (Î¦, Î˜)ã¯ä¾ç„¶ã¨ã—ã¦ä¸åã§ã‚ã‚‹ãŒã€âˆ‡ Î¦ ğ´ Î¦ L (Î¦, Î˜)ã«æ¯”ã¹ã¦åˆ†æ•£ãŒå¤§ãããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚ In the experiments, we can trade off the variance of the estimator with complexity. å®Ÿé¨“ã§ã¯ã€æ¨å®šé‡ã®åˆ†æ•£ã¨è¤‡é›‘ã•ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã«ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚</p>
<p>In the training stage, we update âˆ‡Î¦L (Î¦, Î˜) (either Eq.(10) or Eq.(11)) and âˆ‡Î˜L (Î¦, Î˜) 3 during the back propagation. å­¦ç¿’æ®µéšã§ã¯ã€é€†ä¼æ’­ä¸­ã«âˆ‡Î¦Lï¼ˆÎ¦ï¼ŒÎ˜ï¼‰ï¼ˆå¼ï¼ˆ10ï¼‰ã¾ãŸã¯å¼ï¼ˆ11ï¼‰ï¼‰ã¨âˆ‡Î˜Lï¼ˆÎ¦ï¼ŒÎ˜ï¼‰3ã‚’æ›´æ–°ã™ã‚‹ã€‚ In the inference stage, we can use the expectation of Z (ğ‘™) ğ‘¢,ğ‘£ âˆ¼ Bern(Î  (ğ‘™) ğ‘¢,ğ‘£ ) as the mask in Eq.(5), i.e., E(Z (ğ‘™) ğ‘¢,ğ‘£ ) = Î  (ğ‘™) ğ‘¢,ğ‘£ = ğ‘”(ğš½ (ğ‘™) ğ‘¢,ğ‘£ ). æ¨è«–æ®µéšã§ã¯ã€Z (ğ‘™) ğ‘¢,ğ‘£ âˆ¼ Bern(Î  (ğ‘¢),ğ‘£ )ã®æœŸå¾…å€¤ã‚’å¼(5)ã®ãƒã‚¹ã‚¯ã¨ã—ã¦ä½¿ã†ã“ã¨ãŒã§ãã‚‹ã€ã™ãªã‚ã¡ã€E(Z (ğ‘™) ğ‘¢,ğ‘£ = Î (ğ‘™) ğ‘¢,ğ‘£ = ğ‘”(ğ‘¢ (ğ‘™) ğ‘£ )ã€‚ Nevertheless, this will not yield a sparse attention M(ğ‘™) since the sigmoid function is smooth unless the hard sigmoid function is used in Eq.(9). ã¨ã¯ã„ãˆã€å¼(9)ã§ãƒãƒ¼ãƒ‰ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã‚’ä½¿ã‚ãªã„é™ã‚Šã€ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã¯å¹³æ»‘ãªã®ã§ã€ã“ã‚Œã§ã¯ã‚¹ãƒ‘ãƒ¼ã‚¹æ³¨æ„M(â†ªLl459)ã¯å¾—ã‚‰ã‚Œãªã„ã€‚ Here we simply clip those values ğ‘”(ğš½ (ğ‘™) ğ‘¢,ğ‘£ ) â‰¤ 0.5 to zeros such that a sparse attention matrix is guaranteed and the corresponding noisy attentions are eventually eliminated. ã“ã“ã§ã¯ã€ç–ãªæ³¨æ„è¡Œåˆ—ãŒä¿è¨¼ã•ã‚Œã€å¯¾å¿œã™ã‚‹ãƒã‚¤ã‚¸ãƒ¼ãª æ³¨æ„ãŒæœ€çµ‚çš„ã«æ’é™¤ã•ã‚Œã‚‹ã‚ˆã†ã«ã€å˜ã«å€¤ğ‘”(ğš½) â‰¤ 0.5 ã‚’ã‚¼ãƒ­ã«åˆ‡ã‚Šå–ã‚‹ã€‚</p>
</section>
</section>
<section id="jacobian-regularization" class="level2">
<h2 class="anchored" data-anchor-id="jacobian-regularization">4.2. Jacobian Regularization</h2>
<p>As recently proved by [28], the standard dot-product self-attention is not Lipschitz continuous and is vulnerable to the quality of input sequences. æœ€è¿‘[28]ã«ã‚ˆã£ã¦è¨¼æ˜ã•ã‚ŒãŸã‚ˆã†ã«ã€æ¨™æº–çš„ãªãƒ‰ãƒƒãƒˆç©è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¯ãƒªãƒ—ã‚·ãƒƒãƒ„é€£ç¶šã§ã¯ãªãã€å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å“è³ªã«å¼±ã„ã€‚ Let ğ‘“ (ğ‘™) be the ğ‘™-th Transformer block (Sec 3.2.2) that contains both a self-attention layer and a point-wise feed-forward layer, and x be the input. ğ‘“ã‚’ğ‘™ç•ªç›®ã®Transformerãƒ–ãƒ­ãƒƒã‚¯(ç¬¬3.2.2ç¯€)ã¨ã—ã€è‡ªå·±æ³¨æ„å±¤ã¨ãƒã‚¤ãƒ³ãƒˆå˜ä½ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã‚’å«ã‚€ã¨ã™ã‚‹ã€‚ We can measure the robustness of the Transformer block using the residual error: ğ‘“ (ğ‘™) (x + ğ) âˆ’ ğ‘“ (ğ‘™) (x), where ğ is a small perturbation vector and the norm of ğ is bounded by a small scalar ğ›¿, i.e., âˆ¥ğ âˆ¥2 â‰¤ ğ›¿. ã“ã“ã§ã€ğã¯å°ã•ãªæ‘‚å‹•ãƒ™ã‚¯ãƒˆãƒ«ã§ã‚ã‚Šã€ğã®ãƒãƒ«ãƒ ã¯å°ã•ãªã‚¹ã‚«ãƒ©ãƒ¼ğã§å¢ƒç•Œä»˜ã‘ã‚‰ã‚Œã¦ã„ã‚‹ã€ âˆ¥ğ âˆ¥2 â‰¤ ğ›¿ã€‚ Following the Taylor expansion, we have: ãƒ†ã‚¤ãƒ©ãƒ¼å±•é–‹ã«å¾“ã†ã¨ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚‹ï¼š</p>
<p><span class="math display">\[
\tag{}
\]</span></p>
<p>Let J (ğ‘™) (x) represent the Jacobian matrix at x where J (ğ‘™) ğ‘–ğ‘— = ğœ• ğ‘“ (ğ‘™) ğ‘– (x) ğœ•ğ‘¥ğ‘— . J (ğ‘™) (x)ã¯xã«ãŠã‘ã‚‹ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—ã‚’è¡¨ã—ã€J (ğ‘™) ğ‘– (ğ‘—) = ğœ• ğ‘“ (ğ‘™) (x) ğœ•ã¨ã™ã‚‹ã€‚ Then, we set J (ğ‘™) ğ‘– (x) = ğœ• ğ‘“ (ğ‘™) ğ‘– (x) ğœ•x to denote the ğ‘–-th row of J (ğ‘™) (x). ãã—ã¦ã€J (â†ªLl_1D459) (x) ã®ğ‘–ç•ªç›®ã®è¡Œã‚’è¡¨ã™ãŸã‚ã«ã€J (â†ªLl_1D459) (x) = ğ‘– (ğœ•) ğœ•x ã¨ã™ã‚‹ã€‚ According to HÃ¶lderâ€™s inequality4 , we have: HÃ¶lderã®ä¸ç­‰å¼4 ã«ã‚ˆã‚Œã°ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚‹ï¼š</p>
<p><span class="math display">\[
\tag{}
\]</span></p>
<p>Above inequality indicates that regularizing the ğ¿2 norm on the Jacobians enforces a Lipschitz constraint at least locally, and the residual error is strictly bounded. ä¸Šè¨˜ã®ä¸ç­‰å¼ã¯ã€ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®áµƒ2ãƒãƒ«ãƒ ã‚’æ­£å‰‡åŒ–ã™ã‚‹ã“ã¨ã§ã€å°‘ãªãã¨ã‚‚å±€æ‰€çš„ã«ã¯ãƒªãƒ—ã‚·ãƒƒãƒ„åˆ¶ç´„ãŒå¼·åˆ¶ã•ã‚Œã€æ®‹å·®ã¯å³å¯†ã«æœ‰ç•Œã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚ Thus, we propose to regularize Jacobians with Frobenius norm for each Transformer block as: ãã“ã§ã€å„ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã‚’ãƒ•ãƒ­ãƒ™ãƒ‹ã‚¦ã‚¹ãƒãƒ«ãƒ ã§æ­£å‰‡åŒ–ã™ã‚‹ã“ã¨ã‚’ææ¡ˆã™ã‚‹ï¼š</p>
<p><span class="math display">\[
\tag{12}
\]</span></p>
<p>Importantly, âˆ¥J (ğ‘™) âˆ¥ 2 ğ¹ can be approximated via various Monte-Carlo estimators [23, 37]. é‡è¦ãªã“ã¨ã¯ã€âˆ¥J (â†ªLl459) âˆ¥ 2 Ç”ã¯æ§˜ã€…ãªãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ¨å®šé‡[23, 37]ã«ã‚ˆã£ã¦è¿‘ä¼¼ã§ãã‚‹ã“ã¨ã§ã™ã€‚ In this work, we adopt the classical Hutchinson estimator [23]. æœ¬ç ”ç©¶ã§ã¯ã€å¤å…¸çš„ãªHutchinsonæ¨å®šé‡[23]ã‚’æ¡ç”¨ã™ã‚‹ã€‚ For each Jocobian matrix J (ğ‘™) âˆˆ R ğ‘›Ã—ğ‘› , we have: å„ãƒ¨ã‚³ãƒ“ã‚¢è¡Œåˆ—J (â†ªLl_1D459) âˆˆ R ğ‘›Ã—ğ‘› ã«å¯¾ã—ã¦ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚‹ï¼š</p>
<p><span class="math display">\[
\tag{}
\]</span></p>
<p>where ğœ¼ âˆˆ N (0, Iğ‘›) is the normal distribution vector. ã“ã“ã§ã€â†ªLl_1âˆˆN (0, Iğ‘›)ã¯æ­£è¦åˆ†å¸ƒãƒ™ã‚¯ãƒˆãƒ«ã§ã‚ã‚‹ã€‚ We further make use of random projections to compute the norm of Jacobians Rğ½ and its gradient âˆ‡Î˜Rğ½ (Î˜) [21], which significantly reduces the running time in practice. ã•ã‚‰ã«ã€ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®ãƒãƒ«ãƒ Rğ½ã¨ãã®å‹¾é…âˆ‡Î˜Rğ½ (Î˜)[21]ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã«ãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã‚’åˆ©ç”¨ã™ã‚‹ã€‚</p>
</section>
<section id="optimization" class="level2">
<h2 class="anchored" data-anchor-id="optimization">4.3. Optimization</h2>
<section id="joint-training-4.3.1.-åˆåŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°" class="level3">
<h3 class="anchored" data-anchor-id="joint-training-4.3.1.-åˆåŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°">4.3.1. Joint Training 4.3.1. åˆåŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°</h3>
<p>Putting together loss in Eq.(4), Eq.(6), and Eq.(12), the overall objective of Rec-Denoiser is: å¼(4)ã€å¼(6)ã€å¼(12)ã®æå¤±ã‚’ã¾ã¨ã‚ã‚‹ã¨ã€Rec-Denoiserã®å…¨ä½“çš„ãªç›®çš„ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚‹ï¼š</p>
<p><span class="math display">\[
\tag{13}
\]</span></p>
<p>where ğ›½ and ğ›¾ are regularizers to control the sparsity and robustness of self-attention networks, respectively. ã“ã“ã§â†ªLl_1D6FD ã¨â†ªL_1D6FEâ†©ï¸ã¯ã€ãã‚Œãã‚Œè‡ªå·±æ³¨æ„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã¨ãƒ­ãƒã‚¹ãƒˆæ€§ã‚’åˆ¶å¾¡ã™ã‚‹æ­£å‰‡åŒ–å­ã§ã‚ã‚‹ã€‚ Algorithm 1 summarizes the overall training of Rec-Denoiser with the AR estimator. ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 1ã¯ã€ARæ¨å®šå™¨ã‚’ç”¨ã„ãŸRec-Denoiserã®å…¨ä½“çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã¾ã¨ã‚ãŸã‚‚ã®ã§ã‚ã‚‹ã€‚</p>
<p>Lastly, it is worth mentioning that our Rec-Denoiser is compatible to many Transformer-based sequential recommender models since our differentiable masks and gradient regularizations will not change their main architectures. æœ€å¾Œã«ã€æˆ‘ã€…ã®Rec-Denoiserã¯ã€å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã¨å‹¾é…æ­£å‰‡åŒ–ã¯ã€ãã‚Œã‚‰ã®ä¸»è¦ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å¤‰æ›´ã—ãªã„ã®ã§ã€å¤šãã®Transformerãƒ™ãƒ¼ã‚¹ã®é€æ¬¡æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã¨äº’æ›æ€§ãŒã‚ã‚‹ã“ã¨ã‚’è¨€åŠã™ã‚‹ä¾¡å€¤ãŒã‚ã‚‹ã€‚ If we simply set all masks Z (ğ‘™) to be all-ones matrix and ğ›½ = ğ›¾ = 0, our model boils down to their original designs. å˜ç´”ã«ã™ã¹ã¦ã®ãƒã‚¹ã‚¯Z (â†ªLl_1D459) ã‚’ã‚ªãƒ¼ãƒ«1ã®è¡Œåˆ—ã¨ã—ã€ğ›½ = â†ªLl_1D6FE = 0ã¨ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã¯å…ƒã®è¨­è¨ˆã«å¸°ç€ã™ã‚‹ã€‚ If we randomly set subset of masks Z (ğ‘™) to be zeros, it is equivalent to structured Dropout like LayerDrop [17], DropHead [60]. ãƒã‚¹ã‚¯ã®ã‚µãƒ–ã‚»ãƒƒãƒˆZ (â†ªLl45â†©ï¸)ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚¼ãƒ­ã«è¨­å®šã™ã‚‹ã¨ã€LayerDrop [17]ã‚„DropHead [60]ã®ã‚ˆã†ãªæ§‹é€ åŒ–Dropoutã¨ç­‰ä¾¡ã«ãªã‚‹ã€‚ In addition, our Rec-Denoiser can work together with linearized self-attention networks [27, 59] to further reduce the complexity of attentions. ã•ã‚‰ã«ã€ç§ãŸã¡ã®Rec-Denoiserã¯ã€ç·šå½¢åŒ–ã•ã‚ŒãŸè‡ªå·±æ³¨æ„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯[27, 59]ã¨é€£æºã™ã‚‹ã“ã¨ãŒã§ãã€æ³¨æ„ã®è¤‡é›‘ã•ã‚’ã•ã‚‰ã«è»½æ¸›ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ We leave this extension in the future. ç§ãŸã¡ã¯ã“ã®å»¶é•·ã‚’å°†æ¥ã«æ®‹ã™ã€‚</p>
</section>
<section id="model-complexity-4.3.2.-ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•" class="level3">
<h3 class="anchored" data-anchor-id="model-complexity-4.3.2.-ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•">4.3.2. Model Complexity 4.3.2. ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•</h3>
<p>The complexity of Rec-Denoiser comes from three parts: a basic Transformer, differentiable masks, and Jacobian regularization. Rec-Denoiserã®è¤‡é›‘ã•ã¯ã€åŸºæœ¬çš„ãªå¤‰æ›å™¨ã€å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã€ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®æ­£å‰‡åŒ–ã¨ã„ã†3ã¤ã®éƒ¨åˆ†ã‹ã‚‰æ¥ã¦ã„ã‚‹ã€‚ The complexity of basic Transformer keeps the same as SASRec [26] or BERT4Rec [41]. åŸºæœ¬çš„ãªTransformerã®è¤‡é›‘ã•ã¯ã€SASRec [26]ã‚„BERT4Rec [41]ã¨åŒã˜ã§ã‚ã‚‹ã€‚ The complexity of differentiable masks requires either one-forward pass (e.g., AR with high variance) or two-forward pass (e.g., ARM with low variance) of the model. å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã®è¤‡é›‘ã•ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ¯ãƒ³ãƒ»ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒ‘ã‚¹ï¼ˆé«˜åˆ†æ•£ã®ARãªã©ï¼‰ã¾ãŸã¯ãƒ„ãƒ¼ãƒ»ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒ‘ã‚¹ï¼ˆä½åˆ†æ•£ã®ARMãªã©ï¼‰ã‚’å¿…è¦ã¨ã™ã‚‹ã€‚ In sequential recommenders, the number of Transformer blocks is often very small (e.g., ğ¿ = 2 in SASRec [26] and BERT4Rec [41] ). é€æ¬¡ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã§ã¯ã€Transformerãƒ–ãƒ­ãƒƒã‚¯ã®æ•°ã¯éå¸¸ã«å°‘ãªã„ã“ã¨ãŒå¤šã„ï¼ˆä¾‹ãˆã°ã€SASRec [26]ã¨BERT4Rec [41] ã§ã¯ ğ¿ = 2 ï¼‰ã€‚ It is thus reasonable to use the ARM estimator without heavy computations. å¾“ã£ã¦ã€é‡ã„è¨ˆç®—ã‚’ã›ãšã«ARMæ¨å®šé‡ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯åˆç†çš„ã§ã‚ã‚‹ã€‚ Besides, we compare the performance of AR and ARM estimators in Sec 5.3.Moreover, the random project techniques are surprisingly efficient to compute the norms of Jacobians [21]. ã•ã‚‰ã«ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ»ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæŠ€æ³•ã¯ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®ãƒãƒ«ãƒ ã‚’è¨ˆç®—ã™ã‚‹ã®ã«é©šãã»ã©åŠ¹ç‡çš„ã§ã‚ã‚‹[21]ã€‚ As a result, the overall computational complexity remains the same order as the original Transformers during the training. ãã®çµæœã€å…¨ä½“çš„ãªè¨ˆç®—é‡ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã®ã‚ªãƒªã‚¸ãƒŠãƒ«ã®Transformersã¨åŒã˜ã‚ªãƒ¼ãƒ€ãƒ¼ã®ã¾ã¾ã§ã‚ã‚‹ã€‚ However, during the inference, our attention maps are very sparse, which enables much faster feed-forward computations. ã—ã‹ã—ã€æ¨è«–ä¸­ã®æ³¨æ„ãƒãƒƒãƒ—ã¯éå¸¸ã«ç–ãªãŸã‚ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰è¨ˆç®—ã‚’ã‚ˆã‚Šé«˜é€Ÿã«è¡Œã†ã“ã¨ãŒã§ãã‚‹ã€‚</p>
</section>
</section>
</section>
<section id="experiments-5.-å®Ÿé¨“" class="level1">
<h1>5. Experiments 5. å®Ÿé¨“</h1>
<p>Here we present our empirical results. ã“ã“ã§ã¯å®Ÿè¨¼çš„ãªçµæœã‚’ç´¹ä»‹ã™ã‚‹ã€‚ Our experiments are designed to answer the following research questions: æˆ‘ã€…ã®å®Ÿé¨“ã¯ã€ä»¥ä¸‹ã®ç ”ç©¶èª²é¡Œã«ç­”ãˆã‚‹ãŸã‚ã«ãƒ‡ã‚¶ã‚¤ãƒ³ã•ã‚ŒãŸï¼š</p>
<ul>
<li><p>RQ1: How effective is the proposed Rec-Denoiser compared to the state-of-the-art sequential recommenders? RQ1: ææ¡ˆã™ã‚‹Rec-Denoiserã¯ã€æœ€æ–°ã®é€æ¬¡æ¨è–¦å™¨ã¨æ¯”è¼ƒã—ã¦ã©ã®ç¨‹åº¦æœ‰åŠ¹ã‹ï¼Ÿ</p></li>
<li><p>RQ2: How can Rec-Denoiser reduce the negative impacts of noisy items in a sequence? RQ2ï¼šRec-Denoiserã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ãƒã‚¤ã‚ºã®å¤šã„ã‚¢ã‚¤ãƒ†ãƒ ã®æ‚ªå½±éŸ¿ã‚’ã©ã®ã‚ˆã†ã«è»½æ¸›ã§ãã¾ã™ã‹ï¼Ÿ</p></li>
<li><p>RQ3: How do different components (e.g., differentiable masks and Jacobian regularization) affect the overall performance of Rec-Denoiser? RQ3ï¼šç•°ãªã‚‹æ§‹æˆè¦ç´ ï¼ˆå¾®åˆ†å¯èƒ½ãƒã‚¹ã‚¯ã‚„ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–ãªã©ï¼‰ã¯ã€Rec-Denoiserã®å…¨ä½“çš„ãªæ€§èƒ½ã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ï¼Ÿ</p></li>
</ul>
<section id="experimental-setting-5.1.experimental-setting" class="level2">
<h2 class="anchored" data-anchor-id="experimental-setting-5.1.experimental-setting">5.1. Experimental Setting # 5.1.Experimental Setting</h2>
<section id="dataset-5.1.1.-ãƒ‡ãƒ¼ã‚¿é›†åˆ" class="level3">
<h3 class="anchored" data-anchor-id="dataset-5.1.1.-ãƒ‡ãƒ¼ã‚¿é›†åˆ">5.1.1. Dataset 5.1.1. ãƒ‡ãƒ¼ã‚¿é›†åˆ</h3>
<p>We evaluate our models on five benchmark datasets: Movielens5 , Amazon6 (we choose the three commonly used categories: Beauty, Games, and Movies&amp;TV), and Steam7 [30]. æˆ‘ã€…ã¯5ã¤ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ï¼š Movielens5ã€Amazon6ï¼ˆç¾å®¹ã€ã‚²ãƒ¼ãƒ ã€æ˜ ç”»ï¼†TVã®3ã¤ã®ã‚ˆãä½¿ã‚ã‚Œã‚‹ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’é¸æŠï¼‰ã€Steam7 [30]ã§ã‚ã‚‹ã€‚ Their statistics are shown in Table 1. çµ±è¨ˆã¯è¡¨1ã®é€šã‚Šã§ã‚ã‚‹ã€‚ Among different datasets, MovieLens is the most dense one while Beauty has the fewest actions per user. ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸­ã§ã€MovieLensã¯æœ€ã‚‚å¯†åº¦ãŒé«˜ãã€Beautyã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚ãŸã‚Šã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°ãŒæœ€ã‚‚å°‘ãªã„ã€‚ We use the same procedure as [26, 30, 39] to perform preprocessing and split data into train/valid/test sets, i.e., the last item of each userâ€™s sequence for testing, the second-to-last for validation, and the remaining items for training. 26,30,39]ã¨åŒã˜æ‰‹é †ã§å‰å‡¦ç†ã‚’è¡Œã„ã€ãƒ‡ãƒ¼ã‚¿ã‚’train/valid/testã‚»ãƒƒãƒˆã«åˆ†å‰²ã™ã‚‹ã€‚ã¤ã¾ã‚Šã€å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ€å¾Œã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’testingã«ã€æœ€å¾Œã‹ã‚‰2ç•ªç›®ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’validationã«ã€æ®‹ã‚Šã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’trainingã«ä½¿ç”¨ã™ã‚‹ã€‚</p>
</section>
<section id="baselines-5.1.2.-ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³" class="level3">
<h3 class="anchored" data-anchor-id="baselines-5.1.2.-ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³">5.1.2. Baselines 5.1.2. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³</h3>
<p>Here we include two groups of baselines. ã“ã“ã§ã¯ã€2ã¤ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ»ã‚°ãƒ«ãƒ¼ãƒ—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã€‚ The first group includes general sequential methods (Sec 5.2): 1) FPMC [39]: a mixture of matrix factorization and first-order Markov chains model; 2) GRU4Rec [20]: a RNN-based method that models user action sequences; 3) Caser [42]: a CNN-based framework that captures high-order relationships via convolutional operations; 4) SASRec [26]: a Transformer-based method that uses left-to-right selfattention layers; 5) BERT4Rec [41]: an architecture that is similar to SASRec, but using bidirectional self-attention layers; 6) TiSASRec [30]: a time-aware self-attention model that further considers the relative time intervals between any two items; 7) SSE-PT [50]: a framework that introduces personalization into self-attention layers; 8) Rec-Denoiser: our proposed Rec-Denoiser that can choose any self-attentive models as its backbone. æœ€åˆã®ã‚°ãƒ«ãƒ¼ãƒ—ã«ã¯ã€ä¸€èˆ¬çš„ãªé€æ¬¡çš„æ‰‹æ³•ãŒå«ã¾ã‚Œã‚‹ï¼ˆSec.5.2ï¼‰ï¼š 1) FPMC [39]ï¼šè¡Œåˆ—åˆ†è§£ã¨ä¸€æ¬¡ãƒãƒ«ã‚³ãƒ•é€£é–ãƒ¢ãƒ‡ãƒ«ã®æ··åˆã€2) GRU4Rec [20]ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹RNNãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã€3) Caser [42]ï¼šç•³ã¿è¾¼ã¿æ¼”ç®—ã«ã‚ˆã£ã¦é«˜æ¬¡ã®é–¢ä¿‚ã‚’æ‰ãˆã‚‹CNNãƒ™ãƒ¼ã‚¹ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€4) SASRec [26]ï¼šå·¦ã‹ã‚‰å³ã¸ã®è‡ªå·±æ³¨æ„å±¤ã‚’ä½¿ç”¨ã™ã‚‹Transformerãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã€5) BERT4Rec [41]ï¼š 6) TiSASRec [30]ï¼šä»»æ„ã®2ã¤ã®ã‚¢ã‚¤ãƒ†ãƒ ã®é–“ã®ç›¸å¯¾çš„ãªæ™‚é–“é–“éš”ã‚’ã•ã‚‰ã«è€ƒæ…®ã™ã‚‹ã€æ™‚é–“ã‚’è€ƒæ…®ã—ãŸè‡ªå·±æ³¨æ„ãƒ¢ãƒ‡ãƒ«ã€7) SSE-PT [50]ï¼šè‡ªå·±æ³¨æ„å±¤ã«ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã‚’å°å…¥ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€8) Rec-Denoiserï¼šãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦ä»»æ„ã®è‡ªå·±æ³¨æ„ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã§ãã‚‹ã€æˆ‘ã€…ã®ææ¡ˆã™ã‚‹Rec-Denoiserã€‚ The second group contains sparse Transformers (Sec 5.3): 1) Sparse Transformer [10]: it uses a fixed attention pattern, where only specific cells summarize previous locations in the attention layers; 2) ğ›¼-entmax sparse attention [12]: it simply replaces softmax with ğ›¼-entmax to achieve sparsity. ç•ªç›®ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«ã¯ã‚¹ãƒ‘ãƒ¼ã‚¹å¤‰æ›å™¨ï¼ˆSec.5.3ï¼‰ãŒå«ã¾ã‚Œã‚‹ï¼š 2) ğ›¼-entmax sparse attention [12]: ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã‚’é”æˆã™ã‚‹ãŸã‚ã«ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚’ ğ›¼-entmax ã«ç½®ãæ›ãˆãŸã‚‚ã®ã€‚ Note that we do not compare with other popular sparse Transformers like Star Transformer [18], Longformer [2], and BigBird [58]. ãªãŠã€Star Transformer [18]ã€Longformer [2]ã€BigBird [58]ã®ã‚ˆã†ãªä»–ã®æœ‰åãªã‚¹ãƒ‘ãƒ¼ã‚¹å¤‰æ›å™¨ã¨ã¯æ¯”è¼ƒã—ã¦ã„ãªã„ã€‚ These Transformers are specifically designed for thousands of tokens or longer in the language modeling tasks. ã“ã‚Œã‚‰ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹æ•°åƒä»¥ä¸Šã®ãƒˆãƒ¼ã‚¯ãƒ³ç”¨ã«ç‰¹åˆ¥ã«è¨­è¨ˆã•ã‚Œã¦ã„ã‚‹ã€‚ We leave their explorations for recommendations in the future. å½¼ã‚‰ã®æ¢æ±‚ã¯ä»Šå¾Œã®æè¨€ã«å§”ã­ãŸã„ã€‚ We also do not compare with LayerDrop [17] and DropHead [60] since the number of Transformer blocks and heads are often very small (e.g., ğ¿ = 2 in SARRec) in sequential recommendation. ã¾ãŸã€LayerDrop[17]ã‚„DropHead[60]ã¨ã®æ¯”è¼ƒã¯è¡Œã‚ãªã„ã€‚ãªãœãªã‚‰ã€é€æ¬¡æ¨è–¦ã§ã¯ã€Transformerãƒ–ãƒ­ãƒƒã‚¯ã‚„ãƒ˜ãƒƒãƒ‰ã®æ•°ãŒéå¸¸ã«å°‘ãªã„ï¼ˆä¾‹ãˆã°ã€SARRecã§ã¯ğ¿ = 2ï¼‰ã“ã¨ãŒå¤šã„ã‹ã‚‰ã§ã‚ã‚‹ã€‚ Other sequential architectures like memory networks [9, 22] and graph neural networks [4, 51] have been outperformed by the above baselines, we simply omit these baselines and focus on Transformer-based models. ãƒ¡ãƒ¢ãƒªãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯[9, 22]ã‚„ã‚°ãƒ©ãƒ•ãƒ»ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯[4, 51]ã®ã‚ˆã†ãªä»–ã®é€æ¬¡ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€ä¸Šè¨˜ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã‚‹ã€‚ The goal of experiments is to see whether the proposed differentiable mask techniques can reduce the negative impacts of noisy items in the self-attention layers. å®Ÿé¨“ã®ç›®çš„ã¯ã€ææ¡ˆã•ã‚ŒãŸå¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯æŠ€è¡“ãŒã€è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã«ãŠã‘ã‚‹ãƒã‚¤ã‚ºã‚¢ã‚¤ãƒ†ãƒ ã®æ‚ªå½±éŸ¿ã‚’è»½æ¸›ã§ãã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚</p>
</section>
<section id="evaluation-metrics-5.1.3.-è©•ä¾¡æŒ‡æ¨™" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-metrics-5.1.3.-è©•ä¾¡æŒ‡æ¨™">5.1.3. Evaluation metrics 5.1.3. è©•ä¾¡æŒ‡æ¨™</h3>
<p>For easy comparison, we adopt two common Top-N metrics, Hit@ğ‘ and NDCG@ğ‘ (with default value ğ‘ = 10), to evaluate the performance of sequential models [26, 30, 41]. æ¯”è¼ƒã‚’å®¹æ˜“ã«ã™ã‚‹ãŸã‚ã«ã€é€æ¬¡ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€2ã¤ã®ä¸€èˆ¬çš„ãªTop-Nãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€Hit@_141ã¨NDCG@_441ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤Ç” = 10ï¼‰ã‚’æ¡ç”¨ã™ã‚‹[26, 30, 41]ã€‚ Typically, Hit@ğ‘ counts the rates of the ground-truth items among top-ğ‘ items, while NDCG@ğ‘ considers the position and assigns higher weights to higher positions. ä¸€èˆ¬çš„ã«ã€Hit@_141ã¯ã€ãƒˆãƒƒãƒ—_1ã‚¢ã‚¤ãƒ†ãƒ ã®ä¸­ã§ã‚°ãƒ©ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ã®å‰²åˆã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã€NDCG@_441ã¯ä½ç½®ã‚’è€ƒæ…®ã—ã€é«˜ã„ä½ç½®ã«é«˜ã„é‡ã¿ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã€‚ Following the work [26, 30], for each user, we randomly sample 100 negative items, and rank these items with the ground-truth item. 26,30]ã«å¾“ã„ã€å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¤ã„ã¦100å€‹ã®ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ã“ã‚Œã‚‰ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ã‚°ãƒ©ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ã¨é †ä½ä»˜ã‘ã™ã‚‹ã€‚ We calculate Hit@10 and NDCG@10 based on the rankings of these 101 items. ã“ã®101é …ç›®ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ã‚‚ã¨ã«Hit@10ã¨NDCG<span class="citation" data-cites="10ã‚’ç®—å‡ºã—ãŸ">@10ã‚’ç®—å‡ºã—ãŸ</span>ã€‚</p>
</section>
<section id="parameter-settings-5.1.4.-ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š" class="level3">
<h3 class="anchored" data-anchor-id="parameter-settings-5.1.4.-ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š">5.1.4. Parameter settings 5.1.4. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š</h3>
<p>For all baselines, we initialize the hyper-parameters as the ones suggested by their original work. ã™ã¹ã¦ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«ã¤ã„ã¦ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å½¼ã‚‰ã®ã‚ªãƒªã‚¸ãƒŠãƒ«ç ”ç©¶ã§ææ¡ˆã•ã‚ŒãŸã‚‚ã®ã¨ã—ã¦åˆæœŸåŒ–ã—ãŸã€‚ They are then well tuned on the validation set to achieve optimal performance. ãã—ã¦ã€æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã™ã‚‹ãŸã‚ã«ã€æ¤œè¨¼ã‚»ãƒƒãƒˆä¸Šã§ååˆ†ã«èª¿æ•´ã•ã‚Œã‚‹ã€‚ The final results are conducted on the test set. æœ€çµ‚çš„ãªçµæœã¯ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§å®Ÿæ–½ã•ã‚Œã‚‹ã€‚ We search the dimension size of items within {10, 20, 30, 40, 50}. é …ç›®ã®æ¬¡å…ƒã‚µã‚¤ã‚ºã‚’{10, 20, 30, 40, 50}ã®ç¯„å›²ã§æ¤œç´¢ã™ã‚‹ã€‚ As our Rec-Denoiser is a general plugin, we use the same hyper-parameters as the basic Transformers, e.g., number of Transformer blocks, batch size, learning rate in Adam optimizer, etc. æˆ‘ã€…ã®Rec-Denoiserã¯ä¸€èˆ¬çš„ãªãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã§ã‚ã‚‹ãŸã‚ã€åŸºæœ¬çš„ãªTransformerã¨åŒã˜ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã€‚ä¾‹ãˆã°ã€Transformerãƒ–ãƒ­ãƒƒã‚¯ã®æ•°ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã€Adamã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®å­¦ç¿’ç‡ãªã©ã§ã‚ã‚‹ã€‚ According to Table 1, we set the maximum length of item sequence ğ‘› = 50 for dense datasets MovieLens and Movies&amp;TV, and ğ‘› = 25 for sparse datasets Beauty, Games, and Steam. è¡¨1ã«ã‚ˆã‚‹ã¨ã€å¯†ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹MovieLensã¨Movies&amp;TVã«ã¯ã‚¢ã‚¤ãƒ†ãƒ åˆ—ã®æœ€å¤§é•·ğ‘› = 50ã‚’ã€ç–ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Beautyã€Gamesã€Steamã«ã¯ğ‘› = 25ã‚’è¨­å®šã—ãŸã€‚ In addition, we set the number of Transformer blocks ğ¿ = 2, and the number of heads ğ» = 2 for self-attentive models. ã•ã‚‰ã«ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ã®æ•°ğ¿ = 2ã€è‡ªå·±æ³¨æ„ãƒ¢ãƒ‡ãƒ«ã®ãƒ˜ãƒƒãƒ‰æ•°ğ» = 2ã¨ã—ãŸã€‚ For Rec-Denoiser, two extra regularizers ğ›½ and ğ›¾ are both searched within {10âˆ’1 , 10âˆ’2 , . Rec-Denoiserã§ã¯ã€2ã¤ã®æ­£å‰‡åŒ–å­Ç½ã¨ğ›¾ãŒ{10-1 , 10-2 , . .. .. , 10âˆ’5 }. , 10-5 }. We choose ARM estimator due to the shallow structures of self-attentive recommenders. æˆ‘ã€…ã¯ã€è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‹æ¨è–¦è€…ã®æµ…ã„æ§‹é€ ã‹ã‚‰ARMæ¨å®šå™¨ã‚’é¸æŠã—ãŸã€‚</p>
</section>
</section>
<section id="overall-performancerq1-5.2.ç·åˆæˆç¸¾rq1" class="level2">
<h2 class="anchored" data-anchor-id="overall-performancerq1-5.2.ç·åˆæˆç¸¾rq1">5.2. Overall Performance(RQ1) 5.2.ç·åˆæˆç¸¾ï¼ˆRQ1ï¼‰</h2>
<p>Table 2 presents the overall recommendation performance of all methods on the five datasets. è¡¨2ã¯ã€5ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã‘ã‚‹ã™ã¹ã¦ã®æ‰‹æ³•ã®ç·åˆçš„ãªæ¨è–¦æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚ Our proposed Recdenoisers consistently obtain the best performance for all datasets. æˆ‘ã€…ã®ææ¡ˆã™ã‚‹Recdenoisersã¯ã€ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ä¸€è²«ã—ã¦æœ€é«˜ã®æ€§èƒ½ã‚’å¾—ãŸã€‚ Additionally, we have the following observations: ã•ã‚‰ã«ã€æ¬¡ã®ã‚ˆã†ãªè¦‹è§£ã‚‚ã‚ã‚‹ï¼š</p>
<ul>
<li><p>The self-attentive sequential models (e.g., SASRec, BERT4Rec, TiSASRec, and SSE-PT) generally outperform FPMC, GRU4Rec, and Caser with a large margin, verifying that the self-attention networks have good ability of capture long-range item dependencies for the task of sequential recommendation. è‡ªå·±æ³¨æ„å‹é€æ¬¡æ¨è–¦ãƒ¢ãƒ‡ãƒ«ï¼ˆSASRecã€BERT4Recã€TiSASRecã€SSE-PTãªã©ï¼‰ã¯ã€ä¸€èˆ¬ã«FPMCã€GRU4Recã€Caserã‚’å¤§ããªãƒãƒ¼ã‚¸ãƒ³ã‚’ã‚‚ã£ã¦ä¸Šå›ã‚Šã€è‡ªå·±æ³¨æ„å‹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒé€æ¬¡æ¨è–¦ã®ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦é•·è·é›¢é …ç›®ä¾å­˜æ€§ã‚’æ•æ‰ã™ã‚‹å„ªã‚ŒãŸèƒ½åŠ›ã‚’æŒã¤ã“ã¨ãŒæ¤œè¨¼ã•ã‚ŒãŸã€‚</p></li>
<li><p>Comparing the original SASRec and its variants BERT4Rec, TiSASRec and SSE-PT, we find that the self-attentive models can gets benefit from incorporating additional information such as bi-directional attentions, time intervals, and user personalization. Such auxiliary information is important to interpret the dynamic behaviors of users. ã‚ªãƒªã‚¸ãƒŠãƒ«ã®SASRecã¨ãã®å¤‰ç¨®ã§ã‚ã‚‹BERT4Recã€TiSASRecã€SSE-PTã‚’æ¯”è¼ƒã™ã‚‹ã¨ã€è‡ªå·±æ³¨æ„ãƒ¢ãƒ‡ãƒ«ã¯ã€åŒæ–¹å‘ã®æ³¨æ„ã€æ™‚é–“é–“éš”ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ã®è¿½åŠ æƒ…å ±ã‚’å–ã‚Šå…¥ã‚Œã‚‹ã“ã¨ã§åˆ©ç›Šã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚ ã“ã®ã‚ˆã†ãªè£œåŠ©æƒ…å ±ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ãªè¡Œå‹•ã‚’è§£é‡ˆã™ã‚‹ãŸã‚ã«é‡è¦ã§ã‚ã‚‹ã€‚</p></li>
<li><p>The relative improvements of Rec-denoisers over their backbones are significant for all cases. For example, SASRec+Denoiser has on average 8.04% improvement with respect to Hit@10 and over 12.42% improvements with respect to NDCG@10. Analogously, BERT4Rec+Denoiser outperforms the vanilla BERT4Rec by average 7.47% in Hit@10 and 11.64% in NDCG@10. We also conduct the significant test between Rec-denoisers and their backbones, where all ğ‘-values&lt; 1ğ‘’ âˆ’6 , showing that the improvements of Rec-denoisers are statistically significant in all cases. Rec-denoisersã®ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã«å¯¾ã™ã‚‹ç›¸å¯¾çš„ãªå‘ä¸Šã¯ã€ã™ã¹ã¦ã®ã‚±ãƒ¼ã‚¹ã§é¡•è‘—ã§ã‚ã‚‹ã€‚ ä¾‹ãˆã°ã€SASRec+Denoiserã¯ã€Hit@10ã«å¯¾ã—ã¦å¹³å‡8.04%ã€NDCG@10ã«å¯¾ã—ã¦å¹³å‡12.42%ä»¥ä¸Šã®æ”¹å–„ãŒè¦‹ã‚‰ã‚Œã‚‹ã€‚ åŒæ§˜ã«ã€BERT4Rec+Denoiserã¯ã€Hit@10ã§ã¯å¹³å‡7.47%ã€NDCG@10ã§ã¯å¹³å‡11.64%ã§ã€ãƒãƒ‹ãƒ©BERT4Recã‚’ä¸Šå›ã£ã¦ã„ã‚‹ã€‚ ã¾ãŸã€Rec-denoiserã¨ãã®ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã®é–“ã®æœ‰æ„å·®æ¤œå®šã‚‚è¡Œã£ãŸã€‚ã“ã“ã§ã¯ã€ã™ã¹ã¦ã® â†ªL_1D45â†©ï¸å€¤&lt; 1ğ‘’ -6ã§ã‚ã‚Šã€Rec-denoiserã®æ”¹å–„ãŒã™ã¹ã¦ã®ã‚±ãƒ¼ã‚¹ã§çµ±è¨ˆçš„ã«æœ‰æ„ã§ã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</p></li>
</ul>
<p>These improvements of our proposed models are mainly attributed to the following reasons: 1) Rec-denoisers inherit full advantages of the self-attention networks as in SASRec, BERT4Rec, TiSASRec, and SSE-PT; 2) Through differentiable masks, irrelevant item-item dependencies are removed, which could largely reduce the negative impacts of noisy data; 3) Jacobian regularization enforces the smoothness of gradients, limiting quick changes of the output against input perturbations. ææ¡ˆãƒ¢ãƒ‡ãƒ«ã®ã“ã‚Œã‚‰ã®æ”¹å–„ã¯ä¸»ã«ä»¥ä¸‹ã®ç†ç”±ã«ã‚ˆã‚‹ï¼š 1ï¼‰Rec-denoisersã¯ã€SASRecã€BERT4Recã€TiSASRecã€SSE-PTã®ã‚ˆã†ãªè‡ªå·±æ³¨æ„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åˆ©ç‚¹ã‚’å®Œå…¨ã«å—ã‘ç¶™ã„ã§ã„ã‚‹ã€2ï¼‰å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’é€šã—ã¦ã€ç„¡é–¢ä¿‚ãªé …ç›®-é …ç›®ä¾å­˜æ€§ãŒé™¤å»ã•ã‚Œã€ãƒã‚¤ã‚ºã®å¤šã„ãƒ‡ãƒ¼ã‚¿ã®æ‚ªå½±éŸ¿ã‚’å¤§å¹…ã«è»½æ¸›ã§ãã‚‹ã€3ï¼‰ãƒ¤ã‚³ãƒ“ã‚¢ã®æ­£å‰‡åŒ–ã¯å‹¾é…ã®æ»‘ã‚‰ã‹ã•ã‚’å¼·åˆ¶ã—ã€å…¥åŠ›æ‘‚å‹•ã«å¯¾ã™ã‚‹å‡ºåŠ›ã®ç´ æ—©ã„å¤‰åŒ–ã‚’åˆ¶é™ã™ã‚‹ã€‚ In general, smoothness improves the generalization of sequential recommendation. ä¸€èˆ¬çš„ã«ã€æ»‘ã‚‰ã‹ã•ã¯é€æ¬¡æ¨è–¦ã®ä¸€èˆ¬æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ Overall, the experimental results demonstrate the superiority of our Rec-Denoisers. å…¨ä½“ã¨ã—ã¦ã€å®Ÿé¨“çµæœã¯æˆ‘ã€…ã®Rec-Denoisersã®å„ªä½æ€§ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚</p>
</section>
<section id="robustness-to-noisesrq2" class="level2">
<h2 class="anchored" data-anchor-id="robustness-to-noisesrq2">5.3. Robustness to Noises(RQ2)</h2>
<p>As discussed before, the observed item sequences often contain some noisy items that are uncorrelated to each other. å‰è¿°ã—ãŸã‚ˆã†ã«ã€è¦³æ¸¬ã•ã‚ŒãŸé …ç›®åˆ—ã«ã¯ã€ã—ã°ã—ã°äº’ã„ã«ç„¡ç›¸é–¢ãªãƒã‚¤ã‚ºé …ç›®ãŒå«ã¾ã‚Œã‚‹ã€‚ Generally, the performance of self-attention networks is sensitive to noisy input. ä¸€èˆ¬ã«ã€è‡ªå·±æ³¨æ„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ€§èƒ½ã¯ãƒã‚¤ã‚ºã®å¤šã„å…¥åŠ›ã«æ•æ„Ÿã§ã‚ã‚‹ã€‚ Here we analyze how robust our training strategy is for noisy sequences. ã“ã“ã§ã¯ã€ãƒã‚¤ã‚ºã®å¤šã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã—ã¦ã€æˆ‘ã€…ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ãŒã©ã®ç¨‹åº¦ãƒ­ãƒã‚¹ãƒˆã§ã‚ã‚‹ã‹ã‚’åˆ†æã™ã‚‹ã€‚ To achieve this, we follow the strategy [35] that corrupts the training data by randomly replacing a portion of the observed items in the training set with uniformly sampled items that are not in the validation or test set. ã“ã‚Œã‚’é”æˆã™ã‚‹ãŸã‚ã«ã€æˆ‘ã€…ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã§è¦³æ¸¬ã•ã‚ŒãŸé …ç›®ã®ä¸€éƒ¨ã‚’ã€æ¤œè¨¼ã‚»ãƒƒãƒˆã‚„ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«ã¯ãªã„ä¸€æ§˜ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸé …ç›®ã§ãƒ©ãƒ³ãƒ€ãƒ ã«ç½®ãæ›ãˆã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ç ´æã•ã›ã‚‹æˆ¦ç•¥[35]ã«å¾“ã†ã€‚ We range the ratio of the corrupted training data from 0% to 25%. ç ´æã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®æ¯”ç‡ã¯ã€0%ã‹ã‚‰25%ã®ç¯„å›²ã§ã‚ã‚‹ã€‚ We only report the results of SASRec and SASRec-Denoiser in terms of Hit@10. SASRecã¨SASRec-Denoiserã®çµæœã¯ã€Hit@10ã§ã®ã¿å ±å‘Šã™ã‚‹ã€‚ The performance of other self-attentive models is the same and omitted here due to page limitations. ä»–ã®è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚‚åŒæ§˜ã§ã€ã“ã“ã§ã¯ãƒšãƒ¼ã‚¸ã®éƒ½åˆä¸Šçœç•¥ã—ãŸã€‚ In addition, we compare with two recent sparse Transformers: Sparse Transformer [10] and ğ›¼-entmax sparse attention [12]. ã•ã‚‰ã«ã€æœ€è¿‘ã®2ã¤ã®ã‚¹ãƒ‘ãƒ¼ã‚¹å¤‰æ›å™¨ã¨æ¯”è¼ƒã™ã‚‹ï¼š Sparse Transformer [10]ã¨ â†ªL_1D6FCâ†©ï¸-entmax sparse attention [12]ã§ã‚ã‚‹ã€‚ All the simulated experiments are repeated five times and the average results are shown in Figure 2. ã™ã¹ã¦ã®æ¨¡æ“¬å®Ÿé¨“ã‚’5å›ç¹°ã‚Šè¿”ã—ã€ãã®å¹³å‡çµæœã‚’å›³2ã«ç¤ºã™ã€‚ Clearly, the performance of all models degrades with the increasing noise ratio. æ˜ã‚‰ã‹ã«ã€ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯ã€ãƒã‚¤ã‚ºæ¯”ç‡ã®å¢—åŠ ã¨ã¨ã‚‚ã«ä½ä¸‹ã™ã‚‹ã€‚ We observe that our Rec-denoiser (use either ARM or AR estimators) consistently outperforms ğ›¼-entmax and Sparse Transformer under different ratios of noise on all datasets. æˆ‘ã€…ã¯ã€æˆ‘ã€…ã®Rec-denoiserï¼ˆARMã¾ãŸã¯ARæ¨å®šé‡ã‚’ä½¿ç”¨ï¼‰ãŒã€å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã„ã¦ã€ç•°ãªã‚‹ãƒã‚¤ã‚ºæ¯”ç‡ã®ä¸‹ã§ä¸€è²«ã—ã¦â†ªL_1D6FCâ†©ï¸-entmaxã¨Sparse Transformerã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’è¦³å¯Ÿã—ãŸã€‚ ğ›¼-entmax heavily relies on one trainable parameter ğ›¼ to filter out the noise, which may be over tuned during the training, while Sparse Transformer adopts a fixed attention pattern, which may lead to uncertain results, especially for short item sequences like Beauty and Games. ğ›¼-entmax ã¯ã€ãƒã‚¤ã‚ºã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ãŸã‚ã®1ã¤ã®è¨“ç·´å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ğ›¼ã«å¤§ããä¾å­˜ã—ã¦ãŠã‚Šã€è¨“ç·´ä¸­ã«éå‰°ã«èª¿æ•´ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ä¸€æ–¹ã€Sparse Transformerã¯å›ºå®šã•ã‚ŒãŸæ³¨æ„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¡ç”¨ã—ã¦ãŠã‚Šã€ç‰¹ã«Beautyã‚„Gamesã®ã‚ˆã†ãªçŸ­ã„ã‚¢ã‚¤ãƒ†ãƒ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§ã¯ã€ä¸ç¢ºå®Ÿãªçµæœã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ In contrast, our differentaible masks have much more flexibility to adapt to noisy sequences. å¯¾ç…§çš„ã«ã€æˆ‘ã€…ã®ç•°ç¨®ãƒã‚¹ã‚¯ã¯ã€ãƒã‚¤ã‚ºã®å¤šã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«é©å¿œã™ã‚‹æŸ”è»Ÿæ€§ãŒã¯ã‚‹ã‹ã«é«˜ã„ã€‚ The Jacobian regularization further encourages the smoothness of our gradients, leading to better generalization. ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–ã¯ã€å‹¾é…ã®æ»‘ã‚‰ã‹ã•ã‚’ã•ã‚‰ã«ä¿ƒé€²ã—ã€ã‚ˆã‚Šè‰¯ã„æ±åŒ–ã‚’ã‚‚ãŸã‚‰ã™ã€‚ From the results, the AR estimator performs better than ğ›¼-entmax but worse than ARM. ãã®çµæœã€ARæ¨å®šå™¨ã¯ğ›¼-entmaxã‚ˆã‚Šã¯è‰¯ã„ãŒã€ARMã‚ˆã‚Šã¯æ‚ªã„ã€‚ This result is expected since ARM has much low variance. ARMã¯åˆ†æ•£ãŒå°‘ãªã„ã®ã§ã€ã“ã®çµæœã¯äºˆæƒ³é€šã‚Šã§ã‚ã‚‹ã€‚ In summary, both ARM and AR estimators are able to reduce the negative impacts of noisy sequences, which could improve the robustness of self-attentive models. ã¾ã¨ã‚ã‚‹ã¨ã€ARMã¨ARã®ä¸¡æ¨å®šå™¨ã¯ã€ãƒã‚¤ã‚ºã®å¤šã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æ‚ªå½±éŸ¿ã‚’è»½æ¸›ã™ã‚‹ã“ã¨ãŒã§ãã€è‡ªå·±æ³¨æ„ãƒ¢ãƒ‡ãƒ«ã®é ‘å¥æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚</p>
</section>
<section id="study-of-rec-denoiserrq3" class="level2">
<h2 class="anchored" data-anchor-id="study-of-rec-denoiserrq3">5.4. Study of Rec-Denoiser(RQ3)</h2>
<p>We further investigate the parameter sensitivity of Rec-Denoiser. ã•ã‚‰ã«Rec-Denoiserã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ„Ÿåº¦ã‚’èª¿ã¹ãŸã€‚ For the number of blocks ğ¿ and the number of heads ğ», we find that self-attentive models typically benefit from small values (e.g., ğ», ğ¿ â‰¤ 4), which is similar to [31, 41]. ãƒ–ãƒ­ãƒƒã‚¯æ•°áµƒã¨ãƒ˜ãƒƒãƒ‰æ•°áµƒã«ã¤ã„ã¦ã¯ã€è‡ªå·±æ³¨æ„ãƒ¢ãƒ‡ãƒ«ã¯ä¸€èˆ¬çš„ã«å°ã•ãªå€¤ï¼ˆä¾‹ãˆã°ã€áµƒâ‰¦4ï¼‰ãŒæœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã€ã“ã‚Œã¯[31, 41]ã¨åŒæ§˜ã§ã‚ã‚‹ã€‚ In this section, we mainly study the following hyper-parameters: 1) the maximum length ğ‘›, 2) the regularizers ğ›½ and ğ›¾ to control the sparsity and smoothness. æœ¬ç¯€ã§ã¯ã€ä¸»ã«ä»¥ä¸‹ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç ”ç©¶ã™ã‚‹ï¼š 1)æœ€å¤§é•·ğ‘›ã€2)ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã¨å¹³æ»‘æ€§ã‚’åˆ¶å¾¡ã™ã‚‹æ­£å‰‡åŒ–å­Ç½ã¨â†ªL_1FEâ†©ï¸ã€‚ Here we only study the SASRec and SASRec-Denoiser due to page limitations. ã“ã“ã§ã¯ã€ãƒšãƒ¼ã‚¸ã®éƒ½åˆä¸Šã€SASRecã¨SASRec-Denoiserã«ã¤ã„ã¦ã®ã¿è¿°ã¹ã‚‹ã€‚</p>
<p>Fig.3.Effect of maximum length ğ‘› on ranking performance (Hit@10). å›³3.æœ€å¤§é•·ğ‘›ãŒãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿ï¼ˆHit@10ï¼‰ã€‚</p>
<p>Fig.4.Effect of regularizers ğ›½ and ğ›¾ on ranking performance (Hit@10). å›³4.æ­£å‰‡åŒ–é‡Ç½ã¨Ç–ãŒãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿(Hit@10FE6)ã€‚</p>
<section id="maximum-length-n-5.4.1.-æœ€å¤§é•·-n" class="level3">
<h3 class="anchored" data-anchor-id="maximum-length-n-5.4.1.-æœ€å¤§é•·-n">5.4.1. Maximum Length <span class="math inline">\(n\)</span> 5.4.1. æœ€å¤§é•· <span class="math inline">\(n\)</span></h3>
<p>Figure 3 shows the Hit@10 for maximum length ğ‘› from 20 to 80 while keeping other optimal hyper-parameters unchanged. å›³3ã¯ã€ä»–ã®æœ€é©ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰ãˆãšã«ã€æœ€å¤§é•·ğ‘›ã‚’20ã‹ã‚‰80ã¾ã§å¤‰åŒ–ã•ã›ãŸå ´åˆã®Hit@10ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚ We only test on the densest and sparsest datasets: MovieLeans and Beauty. æœ€ã‚‚é«˜å¯†åº¦ã§ç–ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®ã¿ãƒ†ã‚¹ãƒˆã™ã‚‹ï¼š MovieLeansã¨Beautyã§ã‚ã‚‹ã€‚ Intuitively, the larger sequence we have, the larger probability that the sequence contains noisy items. ç›´è¦³çš„ã«ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒå¤§ãã‘ã‚Œã°å¤§ãã„ã»ã©ã€ãã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ãƒã‚¤ã‚ºã®ã‚ã‚‹é …ç›®ãŒå«ã¾ã‚Œã‚‹ç¢ºç‡ãŒé«˜ããªã‚‹ã€‚ FWe observed that our SASRec-Denoiser improves the performance dramatically with longer sequences. Fæˆ‘ã€…ã®SASRec-Denoiserã¯ã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§åŠ‡çš„ã«æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚ This demonstrates that our design is more suitable for longer inputs, without worrying about the quality of sequences. ã“ã‚Œã¯ã€æˆ‘ã€…ã®è¨­è¨ˆãŒã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®è³ªã‚’æ°—ã«ã™ã‚‹ã“ã¨ãªãã€ã‚ˆã‚Šé•·ã„å…¥åŠ›ã«é©ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚</p>
</section>
<section id="the-regularizers-beta-and-gamma-5.4.2.-æ­£å‰‡åŒ–è¨˜å·betaã¨gamma" class="level3">
<h3 class="anchored" data-anchor-id="the-regularizers-beta-and-gamma-5.4.2.-æ­£å‰‡åŒ–è¨˜å·betaã¨gamma">5.4.2. The regularizers <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\gamma\)</span> 5.4.2. æ­£å‰‡åŒ–è¨˜å·<span class="math inline">\(beta\)</span>ã¨<span class="math inline">\(gamma\)</span>ã€‚</h3>
<p>There are two major regularization parameters ğ›½ and ğ›¾ for sparsity and gradient smoothness, respectively. æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Ç½ã¨ğ›¾ã¯ã€ãã‚Œãã‚Œã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã¨å‹¾é…å¹³æ»‘æ€§ã‚’è¡¨ã™ã€‚ Figure 4 shows the performance by changing one parameter while fixing the other as 0.01. å›³4ã¯ã€ä¸€æ–¹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’0.01ã«å›ºå®šã—ã€ã‚‚ã†ä¸€æ–¹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’å¤‰æ›´ã—ãŸå ´åˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚ As can be seen, our performance is relatively stable with respect to different settings. è¦‹ã¦ã‚ã‹ã‚‹ã‚ˆã†ã«ã€æˆ‘ã€…ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ç•°ãªã‚‹è¨­å®šã«å¯¾ã—ã¦æ¯”è¼ƒçš„å®‰å®šã—ã¦ã„ã‚‹ã€‚ In the experiments, the best performance can be achieved at ğ›½ = 0.01 and ğ›¾ = 0.001 for the MovieLens dataset. å®Ÿé¨“ã§ã¯ã€MovieLensãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ã€â†ªLl_1D6FD = 0.01ã¨Ç– = 0.001ã§æœ€é«˜ã®æ€§èƒ½ã‚’é”æˆã™ã‚‹ã“ã¨ãŒã§ããŸã€‚</p>
</section>
</section>
</section>
<section id="conclusion-and-fture-work-6.-çµè«–ã¨ä»Šå¾Œã®èª²é¡Œ" class="level1">
<h1>6. Conclusion and Fture Work 6. çµè«–ã¨ä»Šå¾Œã®èª²é¡Œ</h1>
<p>In this work, we propose Rec-Denoiser to adaptively eliminate the negative impacts of the noisy items for self-attentive recommender systems. æœ¬ç ”ç©¶ã§ã¯ã€Rec-Denoiserã‚’ææ¡ˆã—ã€è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‹ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹ãƒã‚¤ã‚ºã‚¢ã‚¤ãƒ†ãƒ ã®æ‚ªå½±éŸ¿ã‚’é©å¿œçš„ã«é™¤å»ã™ã‚‹ã€‚ The proposed Rec-Denoiser employs differentiable masks for the self-attention layers, which can dynamically prune irrelevant information. ææ¡ˆã™ã‚‹Rec-Denoiserã¯ã€è‡ªå·±æ³¨æ„å±¤ã«å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’æ¡ç”¨ã—ã€ç„¡é–¢ä¿‚ãªæƒ…å ±ã‚’å‹•çš„ã«é™¤å»ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ To further tackle the vulnerability of self-attention networks to small perturbations, Jacobian regularization is applied to the Transformer blocks to improve the robustness. å°ã•ãªæ‘‚å‹•ã«å¯¾ã™ã‚‹è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è„†å¼±æ€§ã«ã•ã‚‰ã«å–ã‚Šçµ„ã‚€ãŸã‚ã€ãƒ­ãƒã‚¹ãƒˆæ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–ãŒãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ»ãƒ–ãƒ­ãƒƒã‚¯ã«é©ç”¨ã•ã‚Œã‚‹ã€‚ Our experimental results on multiple real-world sequential recommendation tasks illustrate the effectiveness of our design. å®Ÿä¸–ç•Œã®è¤‡æ•°ã®é€æ¬¡æ¨è–¦ã‚¿ã‚¹ã‚¯ã«é–¢ã™ã‚‹å®Ÿé¨“çµæœã¯ã€æˆ‘ã€…ã®è¨­è¨ˆã®æœ‰åŠ¹æ€§ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚</p>
<p>Our proposed Rec-Denoiser framework (e.g., differentiable masks and Jacobian regularization) can be easily applied to any Transformer-based models in many tasks besides sequential recommendation. æˆ‘ã€…ã®ææ¡ˆã™ã‚‹Rec-Denoiserãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆå¾®åˆ†å¯èƒ½ãƒã‚¹ã‚¯ã‚„ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–ãªã©ï¼‰ã¯ã€é€æ¬¡æ¨è–¦ä»¥å¤–ã®å¤šãã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€ã‚ã‚‰ã‚†ã‚‹Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã«å®¹æ˜“ã«é©ç”¨ã§ãã‚‹ã€‚ In the future, we will continue to demonstrate the contributions of our design in many real-world applications. å°†æ¥çš„ã«ã¯ã€å¤šãã®å®Ÿä¸–ç•Œã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ãŠã„ã¦ã€æˆ‘ã€…ã®è¨­è¨ˆã®è²¢çŒ®ã‚’å®Ÿè¨¼ã—ã¦ã„ãã¤ã‚‚ã‚Šã§ã‚ã‚‹ã€‚</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>