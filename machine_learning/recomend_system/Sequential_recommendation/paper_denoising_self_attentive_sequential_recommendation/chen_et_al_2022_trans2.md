## 0.1. link 0.1. ãƒªãƒ³ã‚¯

- https://dl.acm.org/doi/abs/10.1145/3523227.3546788 httpsã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

- https://arxiv.org/pdf/2212.04120.pdf httpsã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

## 0.2. title 0.2. ã‚¿ã‚¤ãƒˆãƒ«

Denoising Self-Attentive Sequential Recommendation
ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°è‡ªå·±èª¿æ•´å‹é€æ¬¡æ¨è–¦æ³•

## 0.3. abstract 0.3. æŠ½è±¡çš„

Transformer-based sequential recommenders are very powerful for capturing both short-term and long-term sequential item dependencies.
ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã«åŸºã¥ãsequentialæ¨è–¦å™¨ã¯ã€çŸ­æœŸçš„ãŠã‚ˆã³é•·æœŸçš„ãª**sequentialãªã‚¢ã‚¤ãƒ†ãƒ é–“ã®ä¾å­˜é–¢ä¿‚**ã‚’æ‰ãˆã‚‹ã®ã«éå¸¸ã«å¼·åŠ›ã§ã‚ã‚‹.
This is mainly attributed to their unique self-attention networks to exploit pairwise item-item interactions within the sequence.
ã“ã‚Œã¯ä¸»ã«ã€sequenceå†…ã®ãƒšã‚¢ãƒ¯ã‚¤ã‚ºã‚¢ã‚¤ãƒ†ãƒ -ã‚¢ã‚¤ãƒ†ãƒ ç›¸äº’ä½œç”¨ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã®ã€ç‹¬è‡ªã®self-attention ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«èµ·å› ã—ã¦ã„ã‚‹.
However, real-world item sequences are often noisy, which is particularly true for implicit feedback.
ã—ã‹ã—ã€**å®Ÿä¸–ç•Œã®item sequencesã¯ã—ã°ã—ã°ãƒã‚¤ã‚ºãŒå¤šãã€ç‰¹ã«æš—é»™çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«ã¯ãã‚ŒãŒå½“ã¦ã¯ã¾ã‚‹**ã€‚
For example, a large portion of clicks do not align well with user preferences, and many products end up with negative reviews or being returned.
ä¾‹ãˆã°ã€**ã‚¯ãƒªãƒƒã‚¯ã®å¤§éƒ¨åˆ†ã¯ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã«åˆã‚ãªã„ã—**ã€å¤šãã®è³¼å…¥ã—ãŸè£½å“ã¯å¦å®šçš„ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ãŸã‚Šã€è¿”å“ã•ã‚Œã‚‹ã“ã¨ã«ãªã‚‹.
As such, the current user action only depends on a subset of items, not on the entire sequences.
ã“ã®ã‚ˆã†ã«ã€ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã§ã¯ãªãã€ã‚¢ã‚¤ãƒ†ãƒ ã®subset(éƒ¨åˆ†é›†åˆ)ã«ã®ã¿ä¾å­˜ã™ã‚‹ã€‚
Many existing Transformer-based models use full attention distributions, which inevitably assign certain credits to irrelevant items.
æ—¢å­˜ã®Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã®å¤šãã¯ã€å®Œå…¨ãªattentionåˆ†å¸ƒã‚’ä½¿ç”¨ã—ã¦ãŠã‚Šã€å¿…ç„¶çš„ã«ç„¡é–¢ä¿‚ãªã‚¢ã‚¤ãƒ†ãƒ ã«ä¸€å®šã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã«ãªã‚‹ã€‚
This may lead to sub-optimal performance if Transformers are not regularized properly.
ã“ã‚Œã¯TransformerãŒé©åˆ‡ã«æ­£å‰‡åŒ–ã•ã‚Œãªã„å ´åˆã€æœ€é©ã§ãªã„æ€§èƒ½ã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹.

Here we propose the Rec-denoiser model for better training of self-attentive recommender systems.
æœ¬è«–æ–‡ã§ã¯ã€è‡ªå·±èª¿æ•´å‹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ã‚ˆã‚Šè‰¯ã„å­¦ç¿’ã®ãŸã‚ã«ã€**Rec-denoiserãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆ**ã™ã‚‹ã€‚
In Rec-denoiser, we aim to adaptively prune noisy items that are unrelated to the next item prediction.
Rec-denoiserã§ã¯ã€**next item prediction**ã«é–¢ä¿‚ã®ãªã„ãƒã‚¤ã‚ºã®å¤šã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’é©å¿œçš„ã«åˆˆã‚Šå–ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã™ã‚‹.(ãã£ã‹ã€NLPã®next-token predictionã¿ãŸã„ãªæ„Ÿã˜...!)
To achieve this, we simply attach each self-attention layer with a trainable binary mask to prune noisy attentions, resulting in sparse and clean attention distributions.
ãã®ãŸã‚ã€**å„self-attentionå±¤ã«å­¦ç¿’å¯èƒ½ãªbinary maskã‚’ä»˜åŠ **ã—ã€ãƒã‚¤ã‚ºã®å¤šã„attentionã‚’é™¤å»ã™ã‚‹ã“ã¨ã§ã€ã‚¹ãƒ‘ãƒ¼ã‚¹ã§ã‚¯ãƒªãƒ¼ãƒ³ãªattentionåˆ†å¸ƒãŒå¾—ã‚‰ã‚Œã‚‹ã€‚
This largely purifies item-item dependencies and provides better model interpretability.
ã“ã‚Œã«ã‚ˆã‚Šã€item-itemã®ä¾å­˜æ€§(ã®å¤šã??)ãŒã»ã¼é™¤å»ã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆå¯èƒ½æ€§ãŒå‘ä¸Šã™ã‚‹ã€‚
In addition, the self-attention network is typically not Lipschitz continuous and is vulnerable to small perturbations.
ã•ã‚‰ã«ã€self-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ä¸€èˆ¬çš„ã«ãƒªãƒ—ã‚·ãƒƒãƒ„é€£ç¶šã§ã¯ãªãã€å°ã•ãªæ‘‚å‹•ã«å¼±ã„.(ã“ã®ã¸ã‚“ã‚ˆãã‚ã‹ã‚‰ã‚“...!)
Jacobian regularization is further applied to the Transformer blocks to improve the robustness of Transformers for noisy sequences.
**ã•ã‚‰ã«ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–ã‚’Transformerãƒ–ãƒ­ãƒƒã‚¯ã«é©ç”¨ã—ã€ãƒã‚¤ã‚ºã®å¤šã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã™ã‚‹Transformerã®é ‘å¥æ€§ã‚’å‘ä¸Šã•ã›ã‚‹**ã€‚
Our Rec-denoiser is a general plugin that is compatible to many Transformers.
**æˆ‘ã€…ã®Rec-denoiserã¯å¤šãã®Transformerã«å¯¾å¿œã™ã‚‹æ±ç”¨ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã§ã‚ã‚‹**. (general pluginã€ç´ æ™´ã‚‰ã—ã„...!!)
Quantitative results on real-world datasets show that our Rec-denoiser outperforms the state-of-the-art baselines.
å®Ÿä¸–ç•Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã‘ã‚‹å®šé‡çš„ãªçµæœã¯ã€æˆ‘ã€…ã®Rec-denoiserãŒæœ€å…ˆç«¯ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’å‡Œé§•ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹.

# 1. introduction 1.ã¯ã˜ã‚ã«

Sequential recommendation aims to recommend the next item based on a userâ€™s historical actions [20, 35, 39, 44, 47], e.g., to recommend a bluetooth headphone after a user purchases a smart phone.
**Sequential recommendationã®ç›®çš„ã¯ï¼Œãƒ¦ãƒ¼ã‚¶ã®éå»ã®è¡Œå‹•ã«åŸºã¥ã„ã¦æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ¨è–¦ã™ã‚‹ã“ã¨(next-item-prediction)**ã§ã‚ã‚‹[20, 35, 39, 44, 47]ï¼ä¾‹ãˆã°ï¼Œ**ãƒ¦ãƒ¼ã‚¶ãŒã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã‚’è³¼å…¥ã—ãŸå¾Œã«Bluetoothãƒ˜ãƒƒãƒ‰ãƒ›ãƒ³ã‚’æ¨è–¦ã™ã‚‹ã‚ˆã†ãªå ´åˆ**ã§ã‚ã‚‹.
Learning sequential user behaviors is, however, challenging since a userâ€™s choices on items generally depend on both long-term and short-term preferences.
ã—ã‹ã—ï¼Œä¸€èˆ¬ã«ãƒ¦ãƒ¼ã‚¶ã®ã‚¢ã‚¤ãƒ†ãƒ é¸æŠã¯**é•·æœŸçš„å—œå¥½(long-term preference)**ã¨**çŸ­æœŸçš„å—œå¥½(short-term preference)**ã®ä¸¡æ–¹ã«ä¾å­˜ã™ã‚‹ãŸã‚ï¼Œé€æ¬¡çš„(sequential)ãªãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã®å­¦ç¿’ã¯å›°é›£ã§ã‚ã‚‹.
Early Markov Chain models [19, 39] have been proposed to capture short-term item transitions by assuming that a userâ€™s next decision is derived from a few preceding actions, while neglecting long-term preferences.
åˆæœŸã®ãƒãƒ«ã‚³ãƒ•é€£é–ãƒ¢ãƒ‡ãƒ«[19, 39]ã¯ã€ãƒ¦ãƒ¼ã‚¶ã®æ¬¡ã®æ±ºå®šãŒã„ãã¤ã‹ã®å…ˆè¡Œè¡Œå‹•ã‹ã‚‰å°ã‹ã‚Œã‚‹ã¨ä»®å®šã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€çŸ­æœŸçš„ãªã‚¢ã‚¤ãƒ†ãƒ ã®é·ç§»ã‚’æ‰ãˆã‚‹ãŸã‚ã«ææ¡ˆã•ã‚ŒãŸãŒã€é•·æœŸçš„ãªå—œå¥½ã¯ç„¡è¦–ã•ã‚ŒãŸã¾ã¾ã§ã‚ã£ãŸ.
To alleviate this limitation, many deep neural networks have been proposed to model the entire usersâ€™ sequences and achieve great success, including recurrent neural networks [20, 53] and convolutional neural networks [42, 54, 57].
ã“ã®é™ç•Œã‚’ç·©å’Œã™ã‚‹ãŸã‚ã«ã€ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯[20, 53]ã‚„ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯[42, 54, 57]ãªã©ã€**ãƒ¦ãƒ¼ã‚¶ã®sequenceå…¨ä½“ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹å¤šãã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒææ¡ˆã•ã‚Œã€å¤§ããªæˆåŠŸã‚’åã‚ã¦ã„ã‚‹**.

Recently, Transformers have shown promising results in various tasks, such as machine translation [43].
æœ€è¿‘ã€Transformersã¯æ©Ÿæ¢°ç¿»è¨³ã®ã‚ˆã†ãªæ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã§æœ‰æœ›ãªçµæœã‚’ç¤ºã—ã¦ã„ã‚‹[43].
One key component of Transformers is the self-attention network, which is capable of learning long-range dependencies **by computing attention weights between each pair of objects in a sequence**.
Transformersã®ä¸»è¦ãªæ§‹æˆè¦ç´ ã®1ã¤ã¯self-attention networkã§ã‚ã‚Šã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸­ã®å„object ãƒšã‚¢ã®attention weightã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã§**é•·è·é›¢ä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã‚‹**.
Inspired by the success of Transformers, several self-attentive sequential recommenders have been proposed and achieve the state-of-the-art performance [26, 41, 49, 50].
Transformersã®æˆåŠŸã«è§¦ç™ºã•ã‚Œã€ã„ãã¤ã‹ã®self-attentive sequential recommendersãŒææ¡ˆã•ã‚Œã€æœ€æ–°ã®æ€§èƒ½ã‚’é”æˆã—ã¦ã„ã‚‹[26, 41, 49, 50].
For example, SASRec [26] is the pioneering framework to adopt self-attention network to learn the importance of items at different positions.
ä¾‹ãˆã°ã€SASRec [26]ã¯ã€ç•°ãªã‚‹ä½ç½®ã«ã‚ã‚‹itemã®é‡è¦åº¦ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã€self-attention networkã‚’æ¡ç”¨ã—ãŸå…ˆé§†çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚‹.
BERT4Rec [41] further models the correlations of items from both left-to-right and right-to-left directions.
BERT4Rec [41]ã¯ã€ã•ã‚‰ã«å·¦ã‹ã‚‰å³ã€å³ã‹ã‚‰å·¦ã®ä¸¡æ–¹å‘ã®itemã®ç›¸é–¢ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹.
SSE-PT [50] is a personalized Transformer model that provides better interpretability of engagement patterns by introducing user embeddings.
SSE-PT [50]ã¯ã€user embeddingsã‚’å°å…¥ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®è§£é‡ˆå¯èƒ½æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºãƒ‰ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹.
LSAN [31] adopts a novel twin-attention sequential framework, which can capture both long-term and short-term user preference signals.
LSAN [31]ã¯æ–°ã—ã„twin-attention sequential frameworkã‚’æ¡ç”¨ã—ã€é•·æœŸã¨çŸ­æœŸã®ä¸¡æ–¹ã®ãƒ¦ãƒ¼ã‚¶å—œå¥½ã‚·ã‚°ãƒŠãƒ«ã‚’æ‰ãˆã‚‹ã“ã¨ãŒã§ãã‚‹.
Recently, Transformers4Rec [14] performs an empirical analysis with broad experiments of various Transformer architectures for the task of sequential recommendation.
æœ€è¿‘ã€**Transformers4Rec** [14]ã¯ã€sequentialæ¨è–¦ã®ã‚¿ã‚¹ã‚¯ã®ãŸã‚ã«ã€æ§˜ã€…ãªTransformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å¹…åºƒã„å®Ÿé¨“ã«ã‚ˆã‚‹å®Ÿè¨¼åˆ†æã‚’è¡Œã£ã¦ã„ã‚‹.

Although encouraging performance has been achieved, the robustness of sequential recommenders is far less studied in the literature.
ã—ã‹ã—ã€**sequentialæ¨è–¦å™¨ã®robustnessã«ã¤ã„ã¦ã¯ã‚ã¾ã‚Šç ”ç©¶ã•ã‚Œã¦ã„ãªã„**.
Many real-world item sequences are naturally noisy, containing both true-positive and false-positive interactions [6, 45, 46].
å®Ÿä¸–ç•Œã®å¤šãã®ã‚¢ã‚¤ãƒ†ãƒ åˆ—ã¯è‡ªç„¶ã«ãƒã‚¤ã‚ºãŒå¤šãã€çœŸé™½æ€§(true-positive)ã¨å½é™½æ€§(false-positive. ex. **å¥½ãã˜ã‚ƒãªã„ã‘ã©ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã—ã¾ã£ãŸ. è³¼å…¥ã—ã¦ã¿ãŸãŒå«Œã„ã ã£ãŸ...??**)ã®ä¸¡æ–¹ã®ç›¸äº’ä½œç”¨ã‚’å«ã‚“ã§ã„ã‚‹ [6, 45, 46].
For example, a large portion of clicks do not align well with user preferences, and many products end up with negative reviews or being returned.
ä¾‹ãˆã°ã€ã‚¯ãƒªãƒƒã‚¯ã®å¤§éƒ¨åˆ†ã¯ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã«åˆã‚ãšã€å¤šãã®è£½å“ã¯å¦å®šçš„ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã§çµ‚ã‚ã£ãŸã‚Šã€è¿”å“ã•ã‚ŒãŸã‚Šã™ã‚‹.
In addition, there is no any prior knowledge about how a userâ€™s historical actions should be generated in online systems.
ã¾ãŸã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ã®éå»ã®è¡Œå‹•ã‚’ã©ã®ã‚ˆã†ã«ç”Ÿæˆã™ã¹ãã‹ã¨ã„ã†äº‹å‰çŸ¥è­˜ã¯å­˜åœ¨ã—ãªã„.
Therefore, developing robust algorithms to defend noise is of great significance for sequential recommendation.
ãã®ãŸã‚ã€**ãƒã‚¤ã‚ºã«å¼·ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é–‹ç™ºã™ã‚‹ã“ã¨ã¯ã€é€æ¬¡æ¨è–¦ã«ãŠã„ã¦å¤§ããªæ„ç¾©ãŒã‚ã‚‹**.

Clearly, not every item in a sequence is aligned well with user preferences, especially for implicit feedbacks (e.g., clicks, views, etc.) [8].
ç‰¹ã«ã€æš—é»™ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼ˆã‚¯ãƒªãƒƒã‚¯ã€ãƒ“ãƒ¥ãƒ¼ãªã©ï¼‰ã®å ´åˆã€**sequenceå†…ã®ã™ã¹ã¦ã®itemãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å—œå¥½ã¨ã†ã¾ãæ•´åˆã—ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªã„ã“ã¨ã¯æ˜ã‚‰ã‹**ã§ã‚ã‚‹ [8].
Unfortunately, the vanilla self-attention network is not Lipschitz continuous1 , and is vulnerable to the quality of input sequences [28].
æ®‹å¿µãªãŒã‚‰ã€ãƒãƒ‹ãƒ©ãª(ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®) self-attention networkã¯**Lipschitzé€£ç¶šã§ã¯ãªã(?)**ã€**å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®è³ªã«å¼±ã„ã¨ã„ã†å•é¡Œ**ãŒã‚ã‚‹[28]ã€‚
Recently, in the tasks of language modeling, people found that a large amount of BERTâ€™s attentions focus on less meaningful tokens, like "[SEP]" and ".", which leads to a misleading explanation [11].
æœ€è¿‘ã€è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€BERT ã®æ³¨æ„ã®å¤šããŒã€"[SEP]" ã‚„ "."ã®ã‚ˆã†ãªã‚ã¾ã‚Šæ„å‘³ã®ãªã„ãƒˆãƒ¼ã‚¯ãƒ³ã«é›†ä¸­ã—ã€èª¤è§£ã‚’æ‹›ãèª¬æ˜ã«ãªã£ã¦ã„ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ã¦ã„ã‚‹[11]ã€‚
It is thus likely to obtain sub-optimal performance if self-attention networks are not well regularized for noisy sequences.
ã“ã®ã‚ˆã†ã«ã€self-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒãƒã‚¤ã‚ºã®å¤šã„sequenceã«å¯¾ã—ã¦ã†ã¾ãæ­£å‰‡åŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã€æœ€é©ã¨ã¯è¨€ãˆãªã„æ€§èƒ½ãŒå¾—ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹.
We use the following example to further explain above concerns.
ä»¥ä¸‹ã®ä¾‹ã‚’ç”¨ã„ã¦ã€ä¸Šè¨˜ã®æ‡¸å¿µã«ã¤ã„ã¦ã•ã‚‰ã«èª¬æ˜ã™ã‚‹.

Figure 1 illustrates an example of left-to-right sequential recommendation where a userâ€™s sequence contains some noisy or irrelevant items.
å›³1ã¯ã€å·¦ã‹ã‚‰å³ã¸ã®sequential recommendationã®ä¸€ä¾‹ã§ã‚ã‚‹.
For example, a father may interchangeably purchase (phone, headphone, laptop) for his son, and (bag, pant) for his daughter, resulting in a sequence: (phone, bag, headphone, pant, laptop).
ä¾‹ãˆã°ã€ã‚ã‚‹çˆ¶è¦ªãŒæ¯å­ã«ï¼ˆæºå¸¯é›»è©±ã€ãƒ˜ãƒƒãƒ‰ãƒ•ã‚©ãƒ³ã€ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ï¼‰ã€å¨˜ã«ï¼ˆã‚«ãƒãƒ³ã€ã‚ºãƒœãƒ³ï¼‰ã‚’è³¼å…¥ã™ã‚‹å ´åˆã€ï¼ˆæºå¸¯é›»è©±ã€ã‚«ãƒãƒ³ã€ãƒ˜ãƒƒãƒ‰ãƒ•ã‚©ãƒ³ã€ã‚ºãƒœãƒ³ã€ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ï¼‰ã¨ã„ã†é †åºã«ãªã‚‹.
In the setting of sequential recommendation, we intend to infer the next item, e.g., laptop, based on the userâ€™s previous actions, e.g., (phone, bag, headphone, pant).
é€æ¬¡æ¨è–¦ã®è¨­å®šã§ã¯ã€**ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä»¥å‰ã®è¡Œå‹•ã€ä¾‹ãˆã°ï¼ˆé›»è©±ã€ã‚«ãƒãƒ³ã€ãƒ˜ãƒƒãƒ‰ãƒ›ãƒ³ã€ã‚ºãƒœãƒ³ï¼‰ã‹ã‚‰ã€æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã€ä¾‹ãˆã°ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ã‚’æ¨è«–ã™ã‚‹ã“ã¨**ã‚’æ„å›³ã—ã¦ã„ã‚‹.
However, the correlations among items are unclear, and intuitively pant and laptop are neither complementary nor compatible to each other, which makes the prediction untrustworthy.
ã—ã‹ã—ã€ã‚¢ã‚¤ãƒ†ãƒ é–“ã®ç›¸é–¢ãŒä¸æ˜ç¢ºã§ã‚ã‚Šã€**ç›´æ„Ÿçš„ã«pantã¨laptopã¯è£œå®Œé–¢ä¿‚ã«ã‚‚ç›¸å®¹ã‚Œãªã„**ãŸã‚ã€ã“ã®äºˆæ¸¬ã¯ä¿¡é ¼ã§ããªã„.
A trustworthy model should be able to only capture correlated items while ignoring these irrelevant items within sequences.
**ä¿¡é ¼ã§ãã‚‹ãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ã“ã‚Œã‚‰ã®ç„¡é–¢ä¿‚ãªã‚¢ã‚¤ãƒ†ãƒ ã‚’ç„¡è¦–ã—ã€ç›¸é–¢ã®ã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®ã¿ã‚’æ‰ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã¯ãš**ã§ã‚ã‚‹.
Existing self-attentive sequential models (e.g., SASRec [26] and BERT4Rec [41]) are insufficient to address noisy items within sequences.
æ—¢å­˜ã®self-attentive sequential modelï¼ˆä¾‹ãˆã°ã€SASRec [26]ã‚„BERT4Rec [41]ï¼‰ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ãƒã‚¤ã‚ºã®å¤šã„ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾å‡¦ã™ã‚‹ã«ã¯ä¸ååˆ†ã§ã‚ã‚‹.
The reason is that their full attention distributions are dense and would assign certain credits to all items, including irrelevant items.
ãã®ç†ç”±ã¯ã€ãã‚Œã‚‰ã®full attention distributionsãŒå¯†ã§ã‚ã‚Šã€ç„¡é–¢ä¿‚ãªitemã‚’å«ã‚€ã™ã¹ã¦ã®itemã«ä¸€å®šã®creditã‚’å‰²ã‚Šå½“ã¦ã¦ã—ã¾ã†ã‹ã‚‰ã§ã‚ã‚‹.
This causes a lack of focus and makes models less interpretable [10, 58].
ã“ã®ãŸã‚ã€æ³¨ç›®åº¦(focus)ãŒä¸è¶³ã—ã€ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆæ€§ãŒä½ä¸‹ã™ã‚‹[10, 58].

To address the above issues, one straightforward strategy is to design sparse Transformer architectures that sparsify the connections in the attention layers, which have been actively investigated in language modeling tasks [10, 58].
ä¸Šè¨˜ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€ä¸€ã¤ã®ç°¡å˜ãªæˆ¦ç•¥ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã§æ´»ç™ºã«ç ”ç©¶ã•ã‚Œã¦ã„ã‚‹**attention layersã®æ¥ç¶šã‚’sparseã«ã—ãŸ**Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ã§ã‚ã‚‹[10, 58].
Several representative models are Star Transformer [18], Sparse Transformer [10], Longformer [2], and BigBird [58].
ã„ãã¤ã‹ã®ä»£è¡¨çš„ãªãƒ¢ãƒ‡ãƒ«ã¯Star Transformer [18], Sparse Transformer [10], Longformer [2], ãã—ã¦BigBird [58]ã§ã‚ã‚‹.
These sparse attention patterns could mitigate noisy issues and avoid allocating credits to unrelated contents for the query of interest.
ã“ã‚Œã‚‰ã®sparse attentionãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã€ãƒã‚¤ã‚ºã®å•é¡Œã‚’è»½æ¸›ã—ã€é–¢å¿ƒã®ã‚ã‚‹ã‚¯ã‚¨ãƒªã«ç„¡é–¢ä¿‚ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã‚’å›é¿ã™ã‚‹ã“ã¨ãŒã§ãã‚‹.
However, these models largely rely on pre-defined attention schemas, which lacks flexibility and adaptability in practice.
ã—ã‹ã—ã€ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯äº‹å‰ã«å®šç¾©ã•ã‚ŒãŸattention schemas(??)ã«å¤§ããä¾å­˜ã—ã¦ãŠã‚Šã€å®Ÿéš›ã®ã¨ã“ã‚æŸ”è»Ÿæ€§ã‚„é©å¿œæ€§ã«æ¬ ã‘ã¦ã„ã‚‹.
Unlike end-to-end training approaches, whether these sparse patterns could generalize well to sequential recommendation remains unknown and is still an open research question.
ã¾ãŸã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®å­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã¯ç•°ãªã‚Šã€è¨€èªãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¹ã‚¯ã®sparseãƒ‘ã‚¿ãƒ¼ãƒ³ãŒsequentialæ¨è–¦ã«ã†ã¾ãä¸€èˆ¬åŒ–ã§ãã‚‹ã‹ã©ã†ã‹ã¯ä¸æ˜ã§ã‚ã‚Šã€ã¾ã æœªè§£æ±ºã®ç ”ç©¶èª²é¡Œã§ã‚ã‚‹.

## 1.1. Contributions.

è²¢çŒ®åº¦
In this work, we propose to design a denoising strategy, Rec-Denoiser, for better training of selfattentive sequential recommenders.
æœ¬ç ”ç©¶ã§ã¯ã€è‡ªå·±æ³¨æ„å‹é€æ¬¡æ¨è–¦å™¨ã‚’ã‚ˆã‚Šè‰¯ãå­¦ç¿’ã•ã›ã‚‹ãŸã‚ã®ãƒã‚¤ã‚ºé™¤å»æˆ¦ç•¥ã€Rec-Denoiserã‚’ææ¡ˆã™ã‚‹.
Our idea stems from the recent findings that not all attentions are necessary and simply pruning redundant attentions could further improve the performance [10, 12, 40, 55, 58].
æˆ‘ã€…ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã€å…¨ã¦ã®attentionsã¯å¿…è¦ã§ã¯ãªãã€å†—é•·ãªattentionsã‚’åˆˆã‚Šå–ã‚‹ã“ã¨ã§ã•ã‚‰ã«æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã¨ã„ã†æœ€è¿‘ã®çŸ¥è¦‹ã«ç”±æ¥ã™ã‚‹[10, 12, 40, 55, 58].
Rather than randomly dropping out attentions, we introduce differentiable masks to drop task-irrelevant attentions in the self-attention layers, which can yield exactly zero attention scores for noisy items.
æˆ‘ã€…ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ã«attentionsã‚’å‰Šé™¤ã™ã‚‹ã®ã§ã¯ãªãã€**å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’å°å…¥ã—ã€ã‚¿ã‚¹ã‚¯ã¨ç„¡é–¢ä¿‚ãªattentionsã‚’self-attention layersã§å‰Šé™¤ã™ã‚‹**ã“ã¨ã§ã€ãƒã‚¤ã‚ºã®å¤šã„itemã«å¯¾ã—ã¦attentionã‚¹ã‚³ã‚¢ã‚’æ­£ç¢ºã«ã‚¼ãƒ­ã«ã™ã‚‹ã“ã¨ãŒã§ãã‚‹.
The introduced sparsity in the self-attention layers has several benefits:
self-attention layersã«å°å…¥ã•ã‚ŒãŸã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã«ã¯ã€ã„ãã¤ã‹ã®åˆ©ç‚¹ãŒã‚ã‚‹.

- 1. Irrelevant attentions with parameterized masks can be learned to be dropped in a data-driven way. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚ŒãŸãƒã‚¹ã‚¯ã‚’æŒã¤ç„¡é–¢ä¿‚ãªattentionã¯ã€ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ã®æ–¹æ³•ã§å‰Šé™¤ã•ã‚Œã‚‹ã‚ˆã†ã«å­¦ç¿’ã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹.
  - Taking Figure 1 as an example, our Rec-denoiser would prune the sequence (phone, bag, headphone) for pant, and (phone, bag, headphone, pant) for laptop in the attention maps.å›³1ã‚’ä¾‹ã«ã¨ã‚‹ã¨ã€Rec-denoiserã¯ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ—ã«ãŠã„ã¦ã€ã‚ºãƒœãƒ³ã«ã¯(phone, bag, headphone)ã€ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ã«ã¯(phone, bag, headphone, pant)ã¨ã„ã†é †åºã‚’åˆ‡ã‚Šæ¨ã¦ã‚‹ã“ã¨ã«ãªã‚‹.
  - Namely, we seek next item prediction explicitly based on a subset of more informative items. ã¤ã¾ã‚Šã€ã‚ˆã‚Šæƒ…å ±é‡ã®å¤šã„ã‚¢ã‚¤ãƒ†ãƒ ã®éƒ¨åˆ†é›†åˆ(subset)ã«åŸºã¥ãã€æ˜ç¤ºçš„ã«æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ äºˆæ¸¬ã‚’è¡Œã†ã®ã§ã™.
- 2. Our Rec-Denoiser still takes full advantage of Transformers as it does not change their architectures, but only the attention distributions.æˆ‘ã€…ã®Rec-Denoiserã¯Transformerã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å¤‰æ›´ã›ãšã€**ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³åˆ†å¸ƒã®ã¿ã‚’å¤‰æ›´ã™ã‚‹**ãŸã‚ã€Transformerã‚’æœ€å¤§é™ã«æ´»ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã‚‹.
  - As such, Rec-Denoiser is easy to implement and is compatible to any Transformers, making them less complicated as well as improving their interpretability. ãã®ãŸã‚ã€Rec-Denoiserã¯å®Ÿè£…ãŒå®¹æ˜“ã§ã€ã‚ã‚‰ã‚†ã‚‹Transformerã¨äº’æ›æ€§ãŒã‚ã‚Šã€Transformerã®è¤‡é›‘ã•ã‚’è»½æ¸›ã—ã€ãã®è§£é‡ˆå¯èƒ½æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹.

In our proposed Rec-Denoiser, there are two major challenges.
æˆ‘ã€…ãŒææ¡ˆã™ã‚‹Rec-Denoiserã§ã¯ã€2ã¤ã®å¤§ããªèª²é¡ŒãŒã‚ã‚‹.
First, the discreteness of binary masks (i.e., 0 is dropped while 1 is kept) is, however, intractable in the back-propagation.
ã¾ãšã€2å€¤ãƒã‚¹ã‚¯ã®é›¢æ•£æ€§ï¼ˆã™ãªã‚ã¡ã€0ã¯å‰Šé™¤ã•ã‚Œã€1ã¯ä¿æŒã•ã‚Œã‚‹ï¼‰ã¯ã€ã—ã‹ã—ã€ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯å®Ÿè¡Œä¸å¯èƒ½ã§ã‚ã‚‹.
To remedy this issue, we relax the discrete variables with a continuous approximation through probabilistic reparameterization [25].
ã“ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€æˆ‘ã€…ã¯ç¢ºç‡çš„å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–[25]ã«ã‚ˆã‚Šã€é›¢æ•£å¤‰æ•°ã‚’é€£ç¶šçš„ãªè¿‘ä¼¼å€¤ã§ç·©å’Œã™ã‚‹.
As such, our differentiable masks can be trained jointly with original Transformers in an end-to-end fashion.
ã“ã®ã‚ˆã†ã«ã€æˆ‘ã€…ã®å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã¯ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã®Transformerã¨ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§å…±åŒã—ã¦å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã‚‹.
In addition, the scaled dot-product attention is not Lipschitz continuous and is thus vulnerable to input perturbations [28].
ã¾ãŸã€scaled dot-product attentionã¯Lipschitzé€£ç¶š(?)ã§ã¯ãªã„ãŸã‚ã€å…¥åŠ›æ‘‚å‹•ã«å¯¾ã—ã¦è„†å¼±ã§ã‚ã‚‹[28].
In this work, Jacobian regularization [21, 24] is further applied to the entire Transformer blocks, to improve the robustness of Transformers for noisy sequences.
ã“ã®ç ”ç©¶ã§ã¯ã€**ãƒã‚¤ã‚ºã®å¤šã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã™ã‚‹Transformerã®robustnessã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚**ã«ã€Transformerãƒ–ãƒ­ãƒƒã‚¯å…¨ä½“ã«ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–[21, 24]ã‚’ã•ã‚‰ã«é©ç”¨ã—ã¦ã„ã‚‹.
Experimental results on real-world benchmark datasets demonstrate the effectiveness and robustness of the proposed Rec-Denoiser.
å®Ÿä¸–ç•Œã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹å®Ÿé¨“çµæœã‹ã‚‰ã€ææ¡ˆã™ã‚‹Rec-Denoiserã®æœ‰åŠ¹æ€§ã¨é ‘å¥æ€§ã‚’å®Ÿè¨¼ã™ã‚‹.
In summary, our contributions are:
ã¾ã¨ã‚ã‚‹ã¨ã€æˆ‘ã€…ã®è²¢çŒ®ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã‚ã‚‹.

- We introduce the idea of denoising item sequences for better of training self-attentive sequential recommenders, which greatly reduces the negative impacts of noisy items. æœ¬è«–æ–‡ã§ã¯ã€è‡ªå·±èªè­˜å‹é€æ¬¡æ¨è–¦å™¨ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã€itemåˆ—ã®ãƒã‚¤ã‚ºé™¤å»ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ç´¹ä»‹ã—ã€ãƒã‚¤ã‚ºã®å¤šã„itemã«ã‚ˆã‚‹æ‚ªå½±éŸ¿ã‚’å¤§å¹…ã«è»½æ¸›ã™ã‚‹.

- We present a general Rec-Denoiser framework with differentiable masks that can achieve sparse attentions by dynamically pruning irrelevant information, leading to better model performance. æˆ‘ã€…ã¯ã€å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’æŒã¤ä¸€èˆ¬çš„ãªRec-Denoiserãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æç¤ºã—ã€ç„¡é–¢ä¿‚ãªæƒ…å ±ã‚’å‹•çš„ã«åˆˆã‚Šå–ã‚‹ã“ã¨ã§ç–ãªæ³¨æ„ã‚’é”æˆã—ã€ã‚ˆã‚Šè‰¯ã„ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’å°ãã“ã¨ãŒå¯èƒ½ã§ã‚ã‚‹.

- We propose an unbiased gradient estimator to optimize the binary masks, and apply Jacobian regularization on the gradients of Transformer blocks to further improve its robustness. ãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯ã®æœ€é©åŒ–ã®ãŸã‚ã«ä¸åå‹¾é…æ¨å®šå™¨ã‚’ææ¡ˆã—ã€Transformerãƒ–ãƒ­ãƒƒã‚¯ã®å‹¾é…ã«ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–ã‚’é©ç”¨ã—ã¦ã€ã•ã‚‰ã«é ‘å¥æ€§ã‚’å‘ä¸Šã•ã›ã‚‹.

- The experimental results demonstrate significant improvements that Rec-Denoiser brings to self-attentive recommenders (5.05% âˆ¼ 19.55% performance gains), as well as its robustness against input perturbations. å®Ÿé¨“çµæœã¯ã€Rec-DenoiserãŒè‡ªå·±æ³¨æ„å‹æ¨è–¦å™¨ã«ã‚‚ãŸã‚‰ã™å¤§ããªæ”¹å–„ï¼ˆ5.05% âˆ¼ 19.55%ã®æ€§èƒ½å‘ä¸Šï¼‰ã¨ã€å…¥åŠ›ã®æ‘‚å‹•ã«å¯¾ã™ã‚‹é ‘å¥æ€§ã‚’ç¤ºã—ã¦ã„ã‚‹.

# 2. Related Work 2. é–¢é€£ä½œå“

In this section, we briefly review the related work on sequential recommendation and sparse Transformers.
æœ¬ç¯€ã§ã¯ã€é€æ¬¡æ¨è–¦ã¨ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã«é–¢ã™ã‚‹é–¢é€£ç ”ç©¶ã‚’ç°¡å˜ã«ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹ã€‚
We also highlight the differences between the existing efforts and ours.
ã¾ãŸã€æ—¢å­˜ã®å–ã‚Šçµ„ã¿ã¨æˆ‘ã€…ã®å–ã‚Šçµ„ã¿ã¨ã®ç›¸é•ç‚¹ã‚’å¼·èª¿ã™ã‚‹ã€‚

## 2.1. Sequential Recommendation

Leveraging sequences of user-item interactions is crucial for sequential recommendation.
é€æ¬¡æ¨è–¦ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®interactionã®sequenceã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã‚ã‚‹ã€‚
User dynamics can be caught by Markov Chains for inferring the conditional probability of an item based on the previous items [19, 39].
ãƒ¦ãƒ¼ã‚¶ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã¯ãƒãƒ«ã‚³ãƒ•é€£é–ã«ã‚ˆã£ã¦æ•æ‰ã•ã‚Œã€å‰ã®ã‚¢ã‚¤ãƒ†ãƒ ã«åŸºã¥ãã‚¢ã‚¤ãƒ†ãƒ ã®æ¡ä»¶ä»˜ãç¢ºç‡ã‚’æ¨è«–ã™ã‚‹ã“ã¨ãŒã§ãã‚‹[19, 39].
More recently, growing efforts have been dedicated to deploying deep neural networks for sequential recommendation such as recurrent neural networks [20, 53], convolutional neural networks [42, 54, 57], memory networks [9, 22], and graph neural networks [4, 7, 51].
æœ€è¿‘ã§ã¯ã€ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ [20, 53]ã€ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ [42, 54, 57]ã€ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ [9, 22]ã€ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ [4, 7, 51] ãªã©ã®**æ·±ã„ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’é€æ¬¡æ¨è–¦ã«åˆ©ç”¨ã™ã‚‹å–ã‚Šçµ„ã¿ãŒç››ã‚“ã«ãªã£ã¦ã„ã‚‹**ã€‚
For example, GRU4Rec [20] employs a gated recurrent unit to study temporal behaviors of users.
ä¾‹ãˆã°ã€GRU4Rec[20]ã¯ãƒ¦ãƒ¼ã‚¶ã®æ™‚é–“çš„è¡Œå‹•ã‚’ç ”ç©¶ã™ã‚‹ãŸã‚ã«ã‚²ãƒ¼ãƒ†ãƒƒãƒ‰ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ¦ãƒ‹ãƒƒãƒˆã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã€‚
Caser [42] learns sequential patterns by using convolutional filters on local sequences.
Caser [42]ã¯å±€æ‰€çš„ãªé…åˆ—ã«å¯¾ã—ã¦ç•³ã¿è¾¼ã¿ãƒ•ã‚£ãƒ«ã‚¿ã‚’ç”¨ã„ã¦é€£ç¶šçš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹ã€‚
MANN [9] adopts memory-augmented neural networks to model user historical records.
MANN [9]ã¯ãƒ¦ãƒ¼ã‚¶ã®å±¥æ­´è¨˜éŒ²ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ãŸã‚ã«ãƒ¡ãƒ¢ãƒªè£œå¼·å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ¡ç”¨ã™ã‚‹ã€‚
SR-GNN [51] converts session sequences into graphs and uses graph neural networks to capture complex item-item transitions.
SR-GNN [51]ã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ã‚°ãƒ©ãƒ•ã«å¤‰æ›ã—ã€ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦è¤‡é›‘ãªã‚¢ã‚¤ãƒ†ãƒ -ã‚¢ã‚¤ãƒ†ãƒ é·ç§»ã‚’æ‰ãˆã‚‹ã€‚

Transformer-based models have shown promising potential in sequential recommendation [5, 26, 30, 32, 33, 41, 49, 50], due to their ability of modeling arbitrary dependencies in a sequence.
Transformerã«åŸºã¥ããƒ¢ãƒ‡ãƒ«ã¯ã€sequenceä¸­ã®ä»»æ„ã®ä¾å­˜é–¢ä¿‚ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã§ãã‚‹ãŸã‚ã€é€æ¬¡æ¨è–¦ã«ãŠã„ã¦æœ‰æœ›ãªå¯èƒ½æ€§ã‚’ç¤ºã—ã¦ã„ã‚‹[5, 26, 30, 32, 33, 41, 49, 50]ï¼
For example, SASRec [26] first adopts self-attention network to learn the importance of items at different positions.
ä¾‹ãˆã°ã€SASRec [26]ã§ã¯ã€ã¾ãšã€ç•°ãªã‚‹ä½ç½®ã«ã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®é‡è¦åº¦ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã€self-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ¡ç”¨ã—ã¦ã„ã‚‹.
In the follow-up studies, several Transformer variants have been designed for different scenarios by adding bidirectional attentions [41], time intervals [30], personalization [50], importance sampling [32], and sequence augmentation [33].
ãã®å¾Œã€bidirectional attentions[41]ã€time intervals[30]ã€personalization[50]ã€importance sampling[32]ã€sequence augmentation[33]ã‚’è¿½åŠ ã—ã€ç•°ãªã‚‹ã‚·ãƒŠãƒªã‚ªã®ãŸã‚ã«ã„ãã¤ã‹ã®å¤‰ç¨®ãŒè¨­è¨ˆã•ã‚Œã¦ããŸ.
However, very few studies pay attention to the robustness of self-attentive recommender models.
ã—ã‹ã—ã€self-attentionå‹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®é ‘å¥æ€§ã«æ³¨ç›®ã—ãŸç ”ç©¶ã¯éå¸¸ã«å°‘ãªã„.
Typically, usersâ€™ sequences contain lots of irrelevant items since they may subsequently purchase a series of products with different purposes [45].
ä¸€èˆ¬ã«ã€ãƒ¦ãƒ¼ã‚¶ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ã¯ç„¡é–¢ä¿‚ãªã‚¢ã‚¤ãƒ†ãƒ ãŒå¤šãå«ã¾ã‚Œã‚‹.
As such, the current user action only depends on a subset of items, not on the entire sequences.
ã“ã®ã‚ˆã†ãªå ´åˆã€ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ã®è¡Œå‹•ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã§ã¯ãªãã€ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã«ã®ã¿ä¾å­˜ã™ã‚‹ã€‚
However, the self-attention module is known to be sensitive to noisy sequences [28], which may lead to sub-optimal generalization performance.
ã—ã‹ã—ã€**self-attentionãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ãƒã‚¤ã‚ºã®å¤šã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«æ•æ„Ÿã§ã‚ã‚‹ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ãŠã‚Š**[28]ã€ã“ã‚Œã¯æœ€é©ã§ãªã„æ±åŒ–æ€§èƒ½ã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
In this paper, we aim to reap the benefits of Transformers while denoising the noisy item sequences by using learnable masks in an end-to-end fashion.
æœ¬è«–æ–‡ã§ã¯ã€å­¦ç¿’å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’end-to-endã§ç”¨ã„ã‚‹(i.e. Transformerã®å­¦ç¿’ã¨ä¸€ç·’ã«maskã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚å­¦ç¿’ã§ãã‚‹)ã“ã¨ã«ã‚ˆã‚Šã€ãƒã‚¤ã‚ºã®å¤šã„ã‚¢ã‚¤ãƒ†ãƒ sequenceã‚’denoiseã—ã¤ã¤ã€Transformersã®åˆ©ç‚¹ã‚’äº«å—ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™.

## 2.2. Sparce Transformer

Recently, many lightweight Transformers seek to achieve sparse attention maps since not all attentions carry important information in the self-attention layers [2, 10, 18, 29, 58].
æœ€è¿‘ã€å¤šãã®è»½é‡ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãŒã€å…¨ã¦ã®attentionãŒself-attentionå±¤ã®é‡è¦ãªæƒ…å ±ã‚’æŒã£ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªã„ã®ã§ã€sparseãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ—(=attentionåˆ†å¸ƒ?)ã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã‚’è¿½æ±‚ã—ã¦ã„ã‚‹[2, 10, 18, 29, 58].
(æ—¢å­˜ç ”ç©¶ã«ãŠã„ã¦ã€sparseãªattentionåˆ†å¸ƒã‚’æ¡ç”¨ã™ã‚‹ç›®çš„ã¯ã€noiseã¸ã®robustæ€§ã®å‘ä¸Šã¨ã„ã†ã‚ˆã‚Šã‚‚ã€è»½é‡åŒ–ã‚„ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£å‘ä¸Šã ã£ãŸã‚Šã™ã‚‹ã‚“ã ã‚ã†ã‹...??:thinking:)
For instance, Reformer [29] computes attentions based on locality-sensitive hashing, leading to lower memory consumption.
ä¾‹ãˆã°ã€Reformer [29]ã¯å±€æ‰€æ€§ã‚’è€ƒæ…®ã—ãŸãƒãƒƒã‚·ãƒ¥ã«åŸºã¥ã„ã¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—ã—ã€ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã®ä½æ¸›ã«ã¤ãªãŒã‚‹.
Star Transformer [18] replaces the fully-connected structure of self-attention with a star-shape topology.
Star Transformer [18]ã¯ã€self-attentionã®å®Œå…¨é€£çµæ§‹é€ ã‚’æ˜Ÿå½¢ã®ãƒˆãƒãƒ­ã‚¸ãƒ¼ã«ç½®ãæ›ãˆãŸã‚‚ã®ã§ã‚ã‚‹ã€‚
Sparse Transformer [10] and Longformer [2] achieve sparsity by using various sparse patterns, such as diagonal sliding windows, dilated sliding windows, local and global sliding windows.
Sparse Transformer [10] ã¨ Longformer [2] ã¯ã€æ–œã‚ã‚¹ãƒ©ã‚¤ãƒ‰çª“ã€æ‹¡å¼µã‚¹ãƒ©ã‚¤ãƒ‰çª“ã€ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒ©ã‚¤ãƒ‰çª“ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ãƒ©ã‚¤ãƒ‰çª“ãªã©ã€æ§˜ã€…ãªã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”¨ã„ã¦ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã‚’å®Ÿç¾ã™ã‚‹ã€‚
BigBird [58] uses random and several fixed patterns to build sparse blocks.
BigBird [58]ã§ã¯ï¼Œ**ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã„ãã¤ã‹ã®å›ºå®šãƒ‘ã‚¿ãƒ¼ãƒ³**ã‚’ç”¨ã„ã¦ï¼Œç–ãªãƒ–ãƒ­ãƒƒã‚¯ã‚’æ§‹ç¯‰ã—ã¦ã„ã‚‹ï¼
It has been shown that these sparse attentions can obtain the state-of-the-art performance and greatly reduce computational complexity.
ã“ã‚Œã‚‰ã®sparse attentionã¯ã€æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã€è¨ˆç®—é‡ã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚
However, many of them rely on fixed attention schemas that lack flexibility and require tremendous engineering efforts.
ã—ã‹ã—ã€ã“ã‚Œã‚‰ã®å¤šãã¯ã€æŸ”è»Ÿæ€§ã«æ¬ ã‘ã€è†¨**å¤§ãªå·¥å­¦çš„åŠªåŠ›ã‚’å¿…è¦ã¨ã™ã‚‹å›ºå®šçš„ãªattention schemaã«ä¾å­˜**ã—ã¦ã„ã‚‹.

Another line of work is to use learnable attention distributions [12, 36, 38, 40].
ã¾ãŸã€**å­¦ç¿’å¯èƒ½ãªattentionåˆ†å¸ƒ**[12, 36, 38, 40]ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€‚
Mostly, they calculate attention weights with variants of sparsemax that replaces the softmax normalization in the self-attention networks.
ã»ã¨ã‚“ã©ã®å ´åˆã€self-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã‘ã‚‹ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹æ­£è¦åŒ–ã‚’ç½®ãæ›ãˆã‚‹**sparsemax**(æœ€å¤§å€¤ã®ã¿ã‚’æ®‹ã™ã€ã¿ãŸã„ãªã‚¤ãƒ¡ãƒ¼ã‚¸??:thinking:)ã®å¤‰ç¨®ã‚’ç”¨ã„ã¦attention weightã‚’è¨ˆç®—ã™ã‚‹.
This allows to produce both sparse and bounded attentions, yielding a compact and interpretable set of alignments.
ã“ã‚Œã«ã‚ˆã‚Šã€ç–ã§boundedãª(å¢ƒç•Œã®ã‚ã‚‹?)attentionã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã€ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã§è§£é‡ˆå¯èƒ½ãªã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆã®é›†åˆã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹.
Our Rec-denoiser is related to this line of work.
æˆ‘ã€…ã®Rec-denoiserã¯ã€ã“ã®ç ”ç©¶ã«é–¢é€£ã—ã¦ã„ã‚‹.
Instead of using sparsemax, we design a trainable binary mask for the self-attention network.
sparsemax(? æœ€å¤§å€¤ã®ã¿ã‚’æ®‹ã™ã€ã¿ãŸã„ãªã‚¤ãƒ¡ãƒ¼ã‚¸??)ã‚’ç”¨ã„ã‚‹ä»£ã‚ã‚Šã«ã€æˆ‘ã€…ã¯**self-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«å¯¾ã—ã¦å­¦ç¿’å¯èƒ½ãªbinaryãƒã‚¹ã‚¯ã‚’è¨­è¨ˆ**ã™ã‚‹ã€‚
As a result, our proposed Rec-denoiser can automatically determine which self-attention connections should be deleted or kept in a data-driven way.
ãã®çµæœã€æˆ‘ã€…ã®ææ¡ˆã™ã‚‹Rec-denoiserã¯ã€ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ã®æ–¹æ³•ã§ã€ã©ã®self-attetionã®æ¥ç¶šã‚’å‰Šé™¤ã™ã¹ãã‹ã€ã‚ã‚‹ã„ã¯ä¿æŒã™ã¹ãã‹ã‚’è‡ªå‹•çš„ã«æ±ºå®šã™ã‚‹ã“ã¨ãŒã§ãã‚‹.

# 3. Problem and Background 3. å•é¡Œç‚¹ã¨èƒŒæ™¯

In this section, we first formulate the problem of sequential recommendation, and then revisit several self-attentive models.
æœ¬ç¯€ã§ã¯ã€ã¾ãšé€æ¬¡æ¨è–¦ã®å•é¡Œã‚’å®šå¼åŒ–ã—ã€æ¬¡ã«ã„ãã¤ã‹ã®self-attetnionãƒ¢ãƒ‡ãƒ«ã‚’å†æ¤œè¨ã™ã‚‹ã€‚
We further discuss the limitations of the existing work.
ã•ã‚‰ã«ã€æ—¢å­˜ã®ç ”ç©¶ã®é™ç•Œã«ã¤ã„ã¦è­°è«–ã™ã‚‹ã€‚

## 3.1. Problem Setup 3.1. å•é¡Œã®è¨­å®š

In sequential recommendation, let $U$ be a set of users, $I$ a set of items, and $S = {S^1, S^2,\cdots, S^{|U|}}$ a set of users' actions.
é€æ¬¡æ¨è–¦ã«ãŠã„ã¦ã€$U$ ã‚’ãƒ¦ãƒ¼ã‚¶ã®é›†åˆã€$I$ ã‚’ã‚¢ã‚¤ãƒ†ãƒ ã®é›†åˆã€$S= (S^1,S^2, \cdots, S^{|U|})$ ã‚’ãƒ¦ãƒ¼ã‚¶ã®è¡Œå‹•ã®é›†åˆã¨ã™ã‚‹.
We user $S^u = (S^u_1, S^u_2, \cdots, S^u_{|S^u|})$ to denote a sequence of items for user $u \in U$ in a chronological order, where $S^u_t \in I$ is the item that user $u$ has interacted with at time $t$, and $|S^u|$ is the length of sequence.
$S^u = (S^{u}_{1}, S^{u}_{2}, \cdots, S^{u}_{|S^u|})$ ã¯ã€æ™‚ç³»åˆ—ã«ä¸¦ã‚“ã ãƒ¦ãƒ¼ã‚¶ $u \in U$ ã®item sequence ã‚’è¡¨ã—ã€$S^{u}_{t} in I$ ã¯æ™‚åˆ»(=sequenceå†…ã®è¦ç´ ã®è­˜åˆ¥å­çš„ãªæ„å‘³åˆã„) $t$ ã«ãƒ¦ãƒ¼ã‚¶ $u$ ãŒinteractã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã€$|S^u|$ ã¯sequenceã®é•·ã•ã‚’è¡¨ã™.

Given the interaction history $S^u$, sequential recommendation seeks to predict the next item $S^u_{|S^{u}+1|}$ at time step $|S^{u}+1|$
interaction history $S^u$ ãŒä¸ãˆã‚‰ã‚Œã‚‹ã¨ã€sequentialæ¨è–¦ã§ã¯ time step $|S^{u}+1|$ ã«ãŠã„ã¦ next item $S^u_{|S^{u}+1|}$ ã‚’äºˆæ¸¬ã—ã‚ˆã†ã¨ã™ã‚‹.
During the training process [26, 41], it will be convenient to regard the modelâ€™s input as $(S^{u}_{1}, S^{u}_{2}, \cdots, S^{u}_{|S^u - 1|})$ and its expected output is a shifted version of the input sequence: $(S^{u}_{2}, S^{u}_{3}, \cdtos, S^{u}_{|S^{u}|})$
ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹[26, 41]ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã‚’ $(S^{u}_{1}, S^{u}_{2}, Ë¶cdots, S^{u}_{|S^u - 1|})$ ã¨ã¿ãªã—ã€ãã® expected output(i.e. æ•™å¸«ãƒ©ãƒ™ãƒ«!) ã‚’å…¥åŠ›sequence ã® ã‚·ãƒ•ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ $(S^{u}_{2}, S^{u}_{3}, Ë¶cdtos, S^{u}_{|S^{u}|})$ ã¨ã¿ãªã™ã¨ä¾¿åˆ©ã§ã‚ã‚‹.

## 3.2. Self-attentive Recommenders

Owing to the ability of learning long sequences, Transformer architectures [43] have been widely used in sequential recommendation, like **SASRec** [26], BERT4Rec [41], and TiSASRec [30].
Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£[43]ã¯é•·ã„sequenceã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ãŸã‚ã€é€æ¬¡æ¨è–¦ã«ãŠã„ã¦**SASRec** [26], BERT4Rec [41], TiSASRec [30] ãªã©åºƒãåˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚
Here we briefly introduce the design of SASRec and discuss its limitations.
ã“ã“ã§ã¯ã€SASRecã®è¨­è¨ˆã‚’ç°¡å˜ã«ç´¹ä»‹ã—ã€ãã®é™ç•Œã«ã¤ã„ã¦è€ƒå¯Ÿã™ã‚‹.

### 3.2.1. Embedding Layer 3.2.1. ã‚¨ãƒ³ãƒ™ãƒ‡ãƒƒãƒ‰ãƒ¬ã‚¤ãƒ¤ãƒ¼

Transformer-based recommenders maintain an item embedding table $T \in R^{|I| \times d}$ , where ğ‘‘ is the size of the embedding.
Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã¯ã‚¢ã‚¤ãƒ†ãƒ ã® **embedding table** $T \in R^{|I| \times d}$ ã‚’ä¿æŒã™ã‚‹. ã“ã“ã§ã€$d$ ã¯ embeddingã®ã‚µã‚¤ã‚º. (embedding tableã¯ã€ã‚¢ã‚¤ãƒ†ãƒ id -> embedding vector ã®mapã¿ãŸã„ãªã‚¤ãƒ¡ãƒ¼ã‚¸:thinking:).
For each sequence $(S^u_1, S^u_2, \cdots, S^u_{|S^u - 1|})$, it can be converted into a fixed-length sequence $(s_1, s_2, \cdots s_n)$, where $n$ is the maximum length (e.g., keeping the most recent ğ‘› items by truncating or padding items).
å„sequence $(S^u_1, S^u_2, \cdots, S^u_{|S^u - 1|})$ ã«å¯¾ã—ã¦ã€**fixed-length(å›ºå®šé•·)ã®sequence $(s_1, s_2, \cdots s_n)$ ã«å¤‰æ›**ã™ã‚‹ã“ã¨ãŒã§ãã‚‹. ã“ã“ã§ã€$n$ã¯ã€sequenceã®æœ€å¤§é•·ã§ã‚ã‚‹. (ex. ã‚¢ã‚¤ãƒ†ãƒ ã‚’ truncating ã—ãŸã‚Šã€padding ã—ãŸã‚Šã—ã¦ã€æœ€æ–°ã®$n$å€‹ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ®‹ã™)
The embedding for $(s_1, s_2, \cdots s_n)$ is denoted as $E \in R^{n \times d}$ , which can be retrieved from the table $T$.
$(s_1, s_2, \cdots s_n)$ ã®åŸ‹ã‚è¾¼ã¿ã‚’ $E \in R^{n \times d}$ ã¨è¡¨ã—(= $E$ ã¯åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ— ...!)ã€embedding table $T$ ã‹ã‚‰å–ã‚Šå‡ºã™ã“ã¨ãŒã§ãã‚‹.
To capture the impacts of different positions, one can inject a learnable positional embedding $P \in R^{n \times d}$ into the input embedding as:
sequenceå†…ã®ç•°ãªã‚‹positionã®å½±éŸ¿(=time stepé–“ã®è·é›¢ã‚„sequenceã®é †åº)ã‚’æ‰ãˆã‚‹ãŸã‚ã«ã€å­¦ç¿’å¯èƒ½ãªpositonal embedding $P \in R^{n à±ªtimes d}$ ã‚’å…¥åŠ›embeddng $E$ ã«inject(æ³¨å…¥)ã™ã‚‹ã“ã¨ãŒã§ãã‚‹:

(å„tokenã®ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã«ã€positonal encoding vectorã‚’è¿½åŠ ã™ã‚‹å¼!)

$$
\hat{E} = E + P \tag{1}
$$

where $\hat{E} \in R^{n\times d}$ is an order-aware embedding, which can be directly fed to any Transformer-based models.
ã“ã“ã§ã€$hat{E} \in R^{n \times d}$ ã¯ order-awareãª(=sequenceå†…ã®é †åºã‚’è€ƒæ…®ã—ãŸ) åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ— ã§ã€Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã«ç›´æ¥ä¸ãˆã‚‹ã“ã¨ãŒã§ãã‚‹.

### 3.2.2. Transformer Block

A Transformer block consists of a self-attention layer and a point-wise feed-forward layer.
ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ»ãƒ–ãƒ­ãƒƒã‚¯ã¯ã€self-attention layer ã¨ point-wise feed-forward layer (=å…¨çµåˆå±¤) ã§æ§‹æˆã•ã‚Œã‚‹.

**Self-attention Layer**: The key component of Transformer block is the self-attention layer that is highly efficient to uncover sequential dependencies in a sequence [43].
**self-attentionå±¤**ï¼š Transformerãƒ–ãƒ­ãƒƒã‚¯ã®é‡è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é€æ¬¡çš„ãªä¾å­˜é–¢ä¿‚ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ãŸã‚ã«éå¸¸ã«åŠ¹ç‡çš„ã§ã‚ã‚‹self-attentionå±¤ã§ã‚ã‚‹[43]ã€‚
The scaled dot-product attention is a popular attention kernel:
scaled dot-product attentionã¯ä¸€èˆ¬çš„ãªattention kernel(=ã‹ãªã‚Šä¸€èˆ¬çš„ãªattentioné–¢æ•°ã®ä¸€ã¤ã€ã¨ã„ã†èªè­˜:thinking:)ã§ã‚ã‚‹ï¼š

(scaled-dot-product attentionã®é–¢æ•°å¼)

$$
\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d}}) V
\tag{2}
$$

where $\text{Attention}(Q, K, V) \in R^{n \times d}$ is the output item representations; $Q = \hat{E}W^Q$, $K =\hat{E}W^K$, and $V = \hat{E}W^V$ are the queries, keys and values, respectively; ${W^Q, W^K, W^V} \in R^{d \times d}$ are three projection matrices; and $\sqrt{d}$ is the scale factor to produce a softer attention distribution.
ã“ã“ã§ã€

- $\text{Attention}(Q, K, V) \in R^{n \times d}$ ã¯ **output item representations**.
- $Q = \hat{E} W^Q$, $K =\hat{E} W^K$, and $V = \hat{E} W^V$ ã¯ãã‚Œãã‚Œ Query, Key, Valueã§ã‚ã‚‹. (Qã‚‚Kã‚‚Vã‚‚ã€åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ—Eã‚’å…ƒã«ã—ã¦ã‚‹ã®ã§ã€attentioné–¢æ•°ã®self-attentionçš„ãªä½¿ã„æ–¹...!)
- ${W^Q, W^K, W^V} \in R^{d \times d}$ ã¯3ã¤ã®projectionè¡Œåˆ—
- $sqrt{d}$ ã¯ã‚ˆã‚ŠæŸ”ã‚‰ã‹ã„(?)attentionåˆ†å¸ƒã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®scale-factorã§ã‚ã‚‹.(æ­£è¦åŒ–çš„ãªã‚¤ãƒ¡ãƒ¼ã‚¸!)

In sequential recommendation, one can utilize either left-to-right unidirectional attentions (e.g., SASRec [26] and TiSASRec [30]) or bidirectional attentions (e.g., BERT4Rec [41]) to predict the next item.
é€æ¬¡æ¨è–¦ã§ã¯ã€**left-to-right uni-directional attentions(å·¦ã‹ã‚‰å³ã¸ã®ä¸€æ–¹å‘ã®attention)**(ex. SASRec [26]ã‚„TiSASRec [30])ã‚„ã€ã‚‚ã—ãã¯**bi-directional attention(åŒæ–¹å‘ã®attention)** (ex. BERT4Rec [41])ã‚’åˆ©ç”¨ã—ã¦ã€æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ãŒã§ãã‚‹.
Moreover, one can apply $H$ attention functions in parallel to enhance expressiveness: $H <- \text{MultiHead}(\hat{E})$ [43].
ã•ã‚‰ã«ã€$H$ å€‹ã®attentioné–¢æ•°ã‚’ä¸¦åˆ—ã«é©ç”¨ã™ã‚‹ã“ã¨ã§ã€è¡¨ç¾åŠ›ã‚’é«˜ã‚ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼š$\mathbf{H} <- \text{MultiHead}(\hat{E})$ [43].
(å…ƒã®transformerã§ã‚‚æ¡ç”¨ã—ã¦ã‚‹ã€Multi-head attentionã­:thinking:)

**Point-wise Feed-forward Layer**: As the self attention layer is built on linear projections, we can endow the nonlinearity by introducing a point-wise feed-forward layers:
**ãƒã‚¤ãƒ³ãƒˆãƒ»ãƒ¯ã‚¤ã‚ºãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤**ï¼š self-attentionå±¤ã¯linear projectionã§æ§‹ç¯‰ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€**Point-wise Feed-forwardå±¤ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€éç·šå½¢æ€§ã‚’ä»˜ä¸ã™ã‚‹(ãƒ¢ãƒ‡ãƒ«ã®è¡¨ç¾åŠ›ã‚’é«˜ã‚ã‚‹ç‚ºã®éç·šå½¢å¤‰æ›!)**ã“ã¨ãŒã§ãã‚‹(ã‚ã€ãã†ã„ã†ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãªã®ã‹...!ä¸­é–“å±¤ï¼‘ã¤ã®ã‚„ã¤! :thinking:)ï¼š

(è£œè¶³: "point-wise" -> **è¦ç´ æ¯ã«ç‹¬ç«‹ã—ã¦æ“ä½œã‚’è¡Œã†**ã€ã¨ã„ã†æ„å‘³. Transformerã®å ´åˆã¯Sequenceãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›ã«å–ã‚‹ãŒã€ã“ã®å±¤ã¯ã€sequenceãƒ‡ãƒ¼ã‚¿ã®å„è¦ç´ ã«å¯¾ã—ã¦å€‹åˆ¥ã«å‡¦ç†ãŒè¡Œã‚ã‚Œã‚‹. ã¤ã¾ã‚Šã€sequenceãƒ‡ãƒ¼ã‚¿ã®å„è¦ç´ ã«å¯¾ã—ã¦åŒã˜æ“ä½œãŒè¡Œã‚ã‚Œã‚‹.:thinking:)
(è£œè¶³: "feed-forward" -> ãƒ‡ãƒ¼ã‚¿ã®å…¥å‡ºåŠ›ã®æµã‚ŒãŒ1æ–¹å‘=å‰æ–¹æ–¹å‘ã«ã®ã¿é€²ã‚€äº‹ã‚’æ„å‘³ã™ã‚‹. é€†ã«ã€"feed-forward"ã§ã¯ãªã„å±¤ã¯RNNã¨ã‹! CNNã¯"point-wise"ã§ã¯ãªã„ãŒ"feed-forward"ã§ã‚ã‚‹æ°—ã¯ã™ã‚‹:thinking:)

$$
F_i = FFN(H_i) = \text{ReLU}(H_i W^{(1)} + b^{(1)})W^{(2)} + b^{(2)}
\tag{3}
$$

where $W^{(\ast)} \in R^{d \times d}$, $b^{(\ast)} \in R^d$ are the learnable weights and bias.
ã“ã“ã§ã€$W^{(\ast)} \in R^{d \times d}$, $b^{(\ast)} \in R^d$ ã¯ FFNå±¤å†…ã®å­¦ç¿’å¯èƒ½ãªparameters(é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹).

In practice, it is usually beneficial to learn hierarchical item dependencies by stacking more Transformer blocks.
å®Ÿéš›ã«ã¯ã€ã‚ˆã‚Šå¤šãã®Transformerãƒ–ãƒ­ãƒƒã‚¯ã‚’ç©ã¿é‡ã­ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€éšå±¤çš„ãªã‚¢ã‚¤ãƒ†ãƒ ã®ä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒé€šå¸¸æœ‰ç›Šã§ã‚ã‚‹.(å…ƒç¥–Transformerã§ã‚‚ã€$N$å€‹ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’é‡ã­ã‚‹ã‚ˆã­...:thinking:)
Also, one can adopt the tricks of residual connection, dropout, and layer normalization for stabilizing and accelerating the training.
ã¾ãŸã€**residual connection(æ®‹å·®æ¥ç¶š), dropout, layer normalization(ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£å‰‡åŒ–) ãªã©ã®ãƒˆãƒªãƒƒã‚¯ã‚’æ¡ç”¨ã™ã‚‹ã“ã¨ã§ã€å­¦ç¿’ã®å®‰å®šåŒ–ã¨é«˜é€ŸåŒ–ã‚’å›³ã‚‹ã“ã¨ãŒã§ãã‚‹**.(ãã†ã„ã†ç›®çš„ãªã‚“ã ...!:thinking:)
Here we simply summarize the output of $L$ Transformer blocks as: $F^{(L)} <- \text{Transformer}(\hat{E})$.
ã“ã“ã§ã¯ã€$L$ å€‹ã®Transformerãƒ–ãƒ­ãƒƒã‚¯ã®å‡ºåŠ›(=Lå€‹ã®ãƒ–ãƒ­ãƒƒã‚¯ãŒé€£çµã—ãŸæœ€çµ‚çš„ãªå‡ºåŠ›ã®æ„å‘³??)ã‚’å˜ç´”ã«æ¬¡ã®ã‚ˆã†ã«ã¾ã¨ã‚ã‚‹: $F^{(L)} <- \text{Transformer}(\hat{E})$.

### 3.2.3. Learning Objective 3.2.3. å­¦ç¿’ç›®æ¨™

After stacked $L$ Transformer blocks, one can predict the next item (given the first $t$ items) based on $F_t^{(L)}$.
$L$ å€‹ã®Transformerãƒ–ãƒ­ãƒƒã‚¯ã‚’ç©ã¿é‡ã­ãŸå¾Œã€$F_t^{(L)}$ ã«åŸºã¥ã„ã¦(æœ€åˆã®$t$å€‹ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒä¸ãˆã‚‰ã‚Œã‚Œã°)æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’äºˆæ¸¬ã§ãã‚‹.
In this work, we use inner product to predict the relevance of item $i$ as:
ã“ã®ç ”ç©¶ã§ã¯ã€å†…ç©ã‚’ä½¿ã£ã¦ã‚¢ã‚¤ãƒ†ãƒ  $i$ ã® relevance ã‚’æ¬¡ã®ã‚ˆã†ã«äºˆæ¸¬ã™ã‚‹:

$$
r_{i, t} = <F_{t}^{(L)}, T_i>,
$$

where $T_i \in R^d$ is the embedding of item $i$.
ã“ã“ã§ $T_i \in R^d$ ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ  $i$ ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã§ã‚ã‚‹.
Recall that the model inputs a sequence $s = (s_1, s_2, \cdots, s_n)$ and its desired output is a shifted version of the same sequence $o = (o_1, o_2, \cdots, o_n)$, we can adopt the binary cross-entropy loss as:
ãƒ¢ãƒ‡ãƒ«ã¯sequence $s = (s_{1}, s_{2}, \cdots, s_{n})$ ã‚’å…¥åŠ›ã—ã€ãã®å‡ºåŠ›ã¯åŒã˜ sequence ã‚’ã‚·ãƒ•ãƒˆã—ãŸã‚‚ã® $o = (o_{1}, o_{2}, \cdots, o_{n})$ ã§ã‚ã‚‹. ãªã®ã§ã€binary cross-entropy lossã‚’é©ç”¨ã§ãã‚‹:

(= $o_2$ ã¯ $s = (s_1, s_2)$ ãŒä¸ãˆã‚‰ã‚ŒãŸæ™‚ã®next-item predictionã®æ­£è§£ãƒ©ãƒ™ãƒ«ã€ã¨ã„ã†èªè­˜. ~~$r_{i,t}$ãŒæœ€ã‚‚é«˜ã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’$o_2$ã¨ã—ã¦æ¡ç”¨ã™ã‚‹ã€ã¿ãŸã„ãªã‚¤ãƒ¡ãƒ¼ã‚¸??~~ ã“ã‚Œã¯æ¨è«–æ™‚ã§ã¯ãªãå­¦ç¿’æ™‚ã®è©±ãªã®ã§ã€$o_2 = s_3$ ã£ã¦äº‹ã‹ãª.:thinking:)

$$
L_{BCE} = - \sum_{S^{u} \in S} \sum_{t=1}^{n}{[\log{\sigma(r_{o_t, t})} + \log{1 - \sigma (r_{o_t', t})}]} + \alpha \cdot ||\theta||^2_F
\tag{4}
$$

where $\theta$ is the model parameters, $a$ is the reqularizer to prevent over-fitting, $o'_t \notin S^u$ is a negative sample corresponding to $o_t$, and $\sigma(\cdot)$ is the sigmoid function.
ã“ã“ã§ã€

- $\theta$ã¯model parameters
- $\alpha$ã¯ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒˆã‚’é˜²ããŸã‚ã®reqularizer(æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
- $o_{t}' \notin S^{u}$ ã¯ $o_t$ ã«å¯¾å¿œã™ã‚‹negative sample()
- $sigma(\cdot)$ ã¯ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã§ã‚ã‚‹.

More details can be found in SASRec [26] and BERT4Rec [41].
è©³ç´°ã¯ã€SASRec[26]ãŠã‚ˆã³BERT4Rec[41]ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹.

## 3.3. The Noisy Attentions Problem

Despite the success of SASRec and its variants, we argue that they are insufficient to address the noisy items in sequences.
SASRecã¨ãã®äºœç¨®ã®æˆåŠŸã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€sequenceä¸­ã®ãƒã‚¤ã‚ºã®å¤šã„ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾å‡¦ã™ã‚‹ã«ã¯ä¸ååˆ†ã§ã‚ã‚‹ã¨æˆ‘ã€…ã¯ä¸»å¼µã™ã‚‹.
The reason is that the full attention distributions (e.g., Eq.(2)) are dense and would assign certain credits to irrelevant items.
ãã®ç†ç”±ã¯ã€**full attetnionåˆ†å¸ƒï¼ˆä¾‹ãˆã°å¼(2)ï¼‰ã¯å¯†åº¦ãŒé«˜ãã€ç„¡é–¢ä¿‚ãªé …ç›®ã«ä¸€å®šã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆ(=attention weight)ã‚’å‰²ã‚Šå½“ã¦ã¦ã—ã¾ã†ã‹ã‚‰**ã§ã‚ã‚‹.
This complicates the item-item dependencies, increases the training difficulty, and even degrades the model performance.
ã“ã‚Œã¯item-itemã®ä¾å­˜é–¢ä¿‚ã‚’è¤‡é›‘ã«ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®é›£æ˜“åº¦ã‚’ä¸Šã’ã€ã•ã‚‰ã«ã¯ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ä½ä¸‹ã•ã›ã‚‹.
To address this issue, several attempts have been proposed to manually define sparse attention schemas in language modeling tasks [2, 10, 18, 58].
ã“ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã§ sparse attention schemas ã‚’æ‰‹å‹•ã§å®šç¾©ã™ã‚‹è©¦ã¿ãŒã„ãã¤ã‹ææ¡ˆã•ã‚Œã¦ã„ã‚‹[2, 10, 18, 58]ã€‚
However, these fixed sparse attentions cannot adapt to the input data [12], leading to sub-optimal performance.
ã—ã‹ã—ã€ã“ã®ã‚ˆã†ãªfixed(å›ºå®šçš„ãª) sparse attentions ã¯ã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«é©å¿œã™ã‚‹ã“ã¨ãŒã§ããš[12]ã€æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ã¯è¨€ãˆãªã„.

On the other hand, several dropout techniques are specifically designed for Transformers to keep only a small portion of attentions, including LayerDrop [17], DropHead [60], and UniDrop [52].
ä¸€æ–¹ã€LayerDrop[17]ã€DropHead[60]ã€UniDrop[52]ãªã©ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ãŸã‚ã«ç‰¹åˆ¥ã«è¨­è¨ˆã•ã‚ŒãŸã€**attentionã®ã”ãä¸€éƒ¨ã ã‘ã‚’æ®‹ã™dropoutæŠ€è¡“**ãŒã„ãã¤ã‹ã‚ã‚‹.
Nevertheless, these randomly dropout approaches are susceptible to bias: the fact that attentions can be dropped randomly does not mean that the model allows them to be dropped, which may lead to over-aggressive pruning issues.
ã¨ã¯ã„ãˆã€ã“ã®ã‚ˆã†ãªãƒ©ãƒ³ãƒ€ãƒ ã«dropoutã•ã›ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ãƒã‚¤ã‚¢ã‚¹ã®å½±éŸ¿ã‚’å—ã‘ã‚„ã™ã„ã€‚attentionãŒãƒ©ãƒ³ãƒ€ãƒ ã«dropoutã•ã›ã‚‰ã‚Œã‚‹ã¨ã„ã†äº‹å®Ÿã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒè„±è½ã‚’è¨±å®¹ã—ã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ãªã„ã®ã§ã€**éåº¦ã«æ”»æ’ƒçš„ãªåˆˆã‚Šè¾¼ã¿ã®å•é¡Œ**ã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
In contrast, we propose a simple yet effective data-driven method to mask out irrelevant attentions by using differentiable masks.
ã“ã‚Œã«å¯¾ã—ã¦æˆ‘ã€…ã¯ã€å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’ç”¨ã„ã¦ç„¡é–¢ä¿‚ãªattentionã‚’ãƒã‚¹ã‚¯ã™ã‚‹ã€ã‚·ãƒ³ãƒ—ãƒ«ã‹ã¤åŠ¹æœçš„ãªdata-driven method(=ç¢ºã‹ã«ã€å›ºå®šçš„ã§ã‚‚ãƒ©ãƒ³ãƒ€ãƒ ã§ã‚‚ãªã„:thinking:)ã‚’ææ¡ˆã™ã‚‹.

# 4. Rec-Denoiser

In this section, we present our Rec-Denoiser that consists of two parts: differentiable masks for self-attention layers and Jacobian regularization for Transformer blocks.
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€Rec-Denoiserã‚’ç´¹ä»‹ã™ã‚‹ã€‚Rec-Denoiserã¯ã€**self-attentionå±¤ã®ãŸã‚ã®å¾®åˆ†å¯èƒ½ãªmask**ã¨ã€**Transformerãƒ–ãƒ­ãƒƒã‚¯ã®ãŸã‚ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–**ã®2ã¤ã®éƒ¨åˆ†ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹.

## 4.1. Differentiable Masks

The self-attention layer is the cornerstone of Transformers to capture long-range dependencies.
ã‚»ãƒ«ãƒ•ãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯ã€é•·è·é›¢ã®ä¾å­˜é–¢ä¿‚ã‚’æ‰ãˆã‚‹ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®è¦ã§ã‚ã‚‹.
As shown in Eq.(2), the softmax operator assigns a non-zero weight to every item.
**å¼(2)ã«ç¤ºã™ã‚ˆã†ã«ã€softmaxæ¼”ç®—å­ã¯ã™ã¹ã¦ã®itemã«non-zeroã®é‡ã¿ã‚’å‰²ã‚Šå½“ã¦ã‚‹**. (ãªã‚‹ã»ã©. ã“ã‚ŒãŒfull attentionåˆ†å¸ƒã‹...!:thinking:)
However, full attention distributions may not always be advantageous since they may cause irrelevant dependencies, unnecessary computation, and unexpected explanation.
ã—ã‹ã—ã€full attentionåˆ†å¸ƒã¯ã€ç„¡é–¢ä¿‚ãªä¾å­˜é–¢ä¿‚ã€ä¸å¿…è¦ãªè¨ˆç®—ã€äºˆæœŸã›ã¬(è§£é‡ˆå›°é›£ãª)èª¬æ˜ã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€å¿…ãšã—ã‚‚æœ‰åˆ©ã¨ã¯é™ã‚‰ãªã„.
We next put forward differentiable masks to address this concern.
æ¬¡ã«ã€ã“ã®æ‡¸å¿µã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’ææ¡ˆã™ã‚‹.

### 4.1.1. Learnable Sparse Attentions 4.1.1. å­¦ç¿’å¯èƒ½ãªã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³

Not every item in a sequence is aligned well with user preferences in the same sense that not all attentions are strictly needed in self-attention layers.
**self-attentionå±¤ã«ãŠã„ã¦å…¨ã¦ã®attentionãŒå³å¯†ã«å¿…è¦ã¨ã•ã‚Œã‚‹ã‚ã‘ã§ã¯ãªã„ã®ã¨åŒã˜æ„å‘³ã§ã€sequenceå†…ã®ã™ã¹ã¦ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã«ã†ã¾ãåˆè‡´ã—ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªã„**.
Therefore, we attach each self-attention layer with a trainable binary mask to prune noisy or task-irrelevant attentions.
ãã“ã§ã€å„self-attentionå±¤ã«å­¦ç¿’å¯èƒ½ãª binary mask ã‚’ä»˜åŠ ã—ã€ãƒã‚¤ã‚ºã®å¤šã„ attention ã‚„ã‚¿ã‚¹ã‚¯ã¨ç„¡é–¢ä¿‚ãª attention ã‚’é™¤å»ã™ã‚‹.
Formally, for the $l$-th self-attention layer in Eq.(2), we introduce a binary matrix $Z^{(l)} \in {0, 1}^{n \times n}$, where $Z^{(l)}_{u,v}$ denotes whether the connection between query $u$ and key $v$ is present.
å½¢å¼çš„ã«ã¯ã€å¼(2)ã® $l$ ç•ªç›®ã®self-attentionå±¤ã«å¯¾ã—ã¦ã€$Z^{(l)} \in {0, 1}^{n \times n}$ ã®**binaryè¡Œåˆ— $Z^{(l)}$ ã‚’å°å…¥**ã™ã‚‹. ã“ã“ã§ã€$Z^{(l)}_{u,v}$ ã¯ query $u$ ã¨ key $v$ ã®connectionã®æœ‰ç„¡(??:thinking:)ã‚’è¡¨ã™.
As such, the $l$ -th self-attention layer becomes:
ã‚ˆã£ã¦ã€$l$ ç•ªç›®ã®self-attentionå±¤ã¯æ¬¡ã®ã‚ˆã†ã«æ”¹è‰¯ã•ã‚Œã‚‹:

$$
A^{(l)} = \text{softmax}(\frac{Q^{(l)} K^{(l)T}}{\sqrt{d}}),
\\
M^{(l)} = A^{(l)} \odot Z^{(l)},
\\
\text{Attention}(Q^{(l)}, K^{(l)}, V^{(l)}) = M^{(l)} V^{(l)},
\tag{5}
$$

where $A^{(l)}$ is the original full attentions, $M^{(l)}$ denotes the sparse attentions, and $\odot$ is the element-wise product.
ã“ã“ã§ã€$A^{(l)}$ã¯å…ƒã®full attentionã€$M^{(l)}$ã¯ sparse attention, $\odot$ ã¯ è¦ç´ ã”ã¨ã®ç©.(=ç¢ºã‹"ã‚¢ãƒ€ãƒãƒ¼ãƒ«ç©"ã ã£ã‘??:thinking:)
Intuitively, the mask $Z^{(l)}$ (e.g., 1 is kept and 0 is dropped) requires minimal changes to the original self-attention layer.
ç›´æ„Ÿçš„ã«ã¯ã€mask $Z^{(l)}$ (ex. 1ã‚’æ®‹ã—ã¦0ã‚’è½ã¨ã™)ã¯ã€å…ƒã®self-attentionå±¤ã«æœ€å°é™ã®å¤‰æ›´ã‚’åŠ ãˆã‚‹ã ã‘ã§æ¸ˆã‚€.
More importantly, they are capable of yielding exactly zero attention scores for irrelevant dependencies, resulting in better interpretability.
ã•ã‚‰ã«é‡è¦ãªã®ã¯ã€**ç„¡é–¢ä¿‚ãªä¾å­˜é–¢ä¿‚ã«å¯¾ã—ã¦attentionã‚¹ã‚³ã‚¢ã‚’æ­£ç¢ºã«ã‚¼ãƒ­ã«ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ãŸã‚ã€è§£é‡ˆã—ã‚„ã™ããªã‚‹**ã¨ã„ã†ã“ã¨ã .
The idea of differentiable masks is not new.
å¾®åˆ†å¯èƒ½ãªmaskã¨ã„ã†ã‚¢ã‚¤ãƒ‡ã‚¢ã¯æ–°ã—ã„ã‚‚ã®ã§ã¯ãªã„.(ãªã‚‹ã»ã©. LMã®ä¸–ç•Œã§æ—¢å­˜ç ”ç©¶ãŒã‚ã‚‹ã‚“ã ...!:thinking:)
In the language modeling, differentiable masks have been shown to be very powerful to extract short yet sufficient sentences, which achieves better performance [1, 13].
è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ãŠã„ã¦ã€å¾®åˆ†å¯èƒ½ãªmaskã¯ã€çŸ­ãã¦ã‚‚ååˆ†ãªsentencesã‚’æŠ½å‡ºã™ã‚‹ã®ã«éå¸¸ã«å¼·åŠ›ã§ã‚ã‚Šã€ã‚ˆã‚Šè‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹[1, 13].

One way to encourage sparsity of $M^{(l)}$ is to explicitly penalize the number of non-zero entries of $Z^{(l)}$, for $1 \leq l \leq L$, by minimizing:
$M^{(l)}$ ã®sparsity(sparseæ€§)ã‚’å¥¨åŠ±ã™ã‚‹ä¸€ã¤ã®æ–¹æ³•ã¯ã€$Z^{(l)}$ å†…ã®non-zero entry(=è¦ç´ )ã®æ•°ã«å¯¾ã—ã¦æ˜ç¤ºçš„ã«ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’èª²ã™ã“ã¨ã§ã‚ã‚‹.
ãã®ãŸã‚ã«ã€ä»¥ä¸‹ã® $R_M$ ã‚’ $1 \leq l \leq L$ ã®é–“ã§æœ€å°åŒ–ã™ã‚‹:

$$
R_M = \sum_{l=1}^{L}||Z^{l}||_{0}
= \sum_{l=1}^{L} \sum_{u=1}^{n} \sum_{v=1}^{n} I[Z_{u,v}^{(l)} \neq 0],
\tag{6}
$$

(è¦ã¯ã€Lå€‹ã®mask binaryè¡Œåˆ—å†…ã®éã‚¼ãƒ­è¦ç´ ã®æ•°ã‚’æœ€å°ã™ã‚‹é …ã‚’ã€å­¦ç¿’æ™‚ã®æå¤±é–¢æ•°ã«çµ„ã¿è¾¼ã‚€ã£ã¦äº‹...??)

where $I[c]$ is an indicator that is equal to 1 if the condition $c$ holds and 0 otherwise; and $||\cdot||_{0}$ denotes the $L_0$ norm that is able to drive irrelevant attentions to be exact zeros.
ã“ã“ã§ã€

- $I[c]$ ã¯æ¡ä»¶ $c$ ãŒæˆç«‹ã™ã‚Œã°1ã€æˆç«‹ã—ãªã‘ã‚Œã°0ã«ç­‰ã—ã„ indicator function ã§ã‚ã‚‹.
- $||\cdot||_{0}$ ã¯ã€ç„¡é–¢ä¿‚ãª attention ã‚’æ­£ç¢ºãª 0 ã«è¿½ã„è¾¼ã‚€ã“ã¨ãŒã§ãã‚‹ $L_0$ ãƒãƒ«ãƒ ã‚’è¡¨ã™.

However, there are two challenges for optimizing $Z^{(l)}$: non-differentiability and large variance.
ã—ã‹ã—ã€$Z^{(l)}$ ã®æœ€é©åŒ–ã«ã¯ã€**å¾®åˆ†ä¸å¯èƒ½æ€§ã¨åˆ†æ•£ã®å¤§ãã•ã¨ã„ã†2ã¤ã®èª²é¡Œ**ãŒã‚ã‚‹.
$L_0$ is discontinuous and has zero derivatives almost everywhere.
$L_0$ ã¯ä¸é€£ç¶š(binaryã ã‹ã‚‰?)ã§ã‚ã‚Šã€ã»ã¨ã‚“ã©ã©ã“ã§ã‚‚ã‚¼ãƒ­å°é–¢æ•°ã‚’æŒã¤.
Additionally, there are $2^{n^2}$ possible states for the binary mask $Z^{(l)}$ with large variance.
ã•ã‚‰ã«ã€binary maskè¡Œåˆ— $Z^{(l)}$ ã«ã¯å¤§ããªåˆ†æ•£ã‚’æŒã¤ $2^{n^2}$ å€‹ã®å¯èƒ½ãªçŠ¶æ…‹ãŒã‚ã‚‹.
Next, we propose an efficient estimator to solve this stochastic binary optimization problem.
æ¬¡ã«ã€ã“ã®**ç¢ºç‡çš„binaryæœ€é©åŒ–å•é¡Œã‚’è§£ããŸã‚ã®åŠ¹ç‡çš„ãªæ¨å®šå™¨**ã‚’ææ¡ˆã™ã‚‹.

### 4.1.2. Efficient Gradient Computation 4.1.2. åŠ¹ç‡çš„ãªå‹¾é…è¨ˆç®—

Since $Z^{(l)}$ is jointly optimized with the original Transformer-based models, we combine Eq.(4) and Eq.(6) into one unified objective:
$Z^{(l)}$ ã¯ã‚ªãƒªã‚¸ãƒŠãƒ«ã®Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã¨å…±åŒã§æœ€é©åŒ–ã•ã‚Œã‚‹ã®ã§ã€å¼(4)ã¨å¼(6)ã‚’1ã¤ã®çµ±ä¸€ã•ã‚ŒãŸç›®çš„é–¢æ•°ã«ã¾ã¨ã‚ã‚‹:

$$
L(Z, \Theta) = L_{BCE}({A^{(l)} \odot Z^{(l)}}, \Theta) + \beta \cdot \sum_{l=1}^{L} \sum_{u=1}^{n} \sum_{v=1}^{n} I[Z_{u,v}^{(l)} \neq 0]
\tag{7}
$$

where $\beta$ controls the sparsity of masks and we denote $Z$ as $Z := \{Z^{(1)}, \cdots, Z^{(L)}\}$.
ã“ã“ã§ $\beta$ ã¯maskã®sparseæ€§ã‚’åˆ¶å¾¡ã—ã€$Z$ ã‚’ $Z := \{Z^{(1)}, \cdots, Z^{(L)}\}$ ã¨ã™ã‚‹.
We further consider each $Z^{(l)}_{u,v}$ is drawn from a Bernoulli distribution parameterized by $\Pi^{(l)}_{u,v}$ such that $Z^{(l)}_{u,v} \sim Bern(\Pi^{(l)}_{u,v})$ [34].
ã•ã‚‰ã«ã€$Z^{(l)}_{u,v} \sim Bern(\Pi^{(l)}_{u,v})$ ã®ã‚ˆã†ãª $\Pi^{(l)}_{u,v}$ ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚ŒãŸBernoulliåˆ†å¸ƒã‹ã‚‰ ãã‚Œãã‚Œã® $Z^{(l)}_{u,v}$ ãŒç”Ÿæˆã•ã‚Œã‚‹(=ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã‚‹?)ã¨è€ƒãˆã‚‹[34].
As the parameter $\Pi^{(l)}_{u,v}$ is jointly trained with the downstream tasks, a small value of $\Pi^{(l)}_{u,v}$ suggests that the attention $A^{(l)}_{u,v}$ is more likely to be irrelevant, and could be removed without side effects.
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\Pi^{(l)}_{u,v}$ ã¯ä¸‹æµã‚¿ã‚¹ã‚¯ã¨å…±åŒã§å­¦ç¿’ã•ã‚Œã‚‹ãŸã‚ã€$\Pi^{(l)}_{u,v}$ ã®å€¤ãŒå°ã•ã„ã¨ã€attention $A^{(l)}_{u,v}$ ã¯ç„¡é–¢ä¿‚ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ãã€å‰¯ä½œç”¨ãªãå‰Šé™¤ã§ãã‚‹ã€‚
By doing this, Eq.(7) becomes:
ã“ã†ã™ã‚‹ã“ã¨ã§ã€å¼(7)ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚‹ï¼š

$$
L(Z, \Theta) =
E_{Z \in \Pi_{l=1}^{L} Bern(Z^{(l)}; \Pi^{(l)})}[L_{BCE}(Z, \Theta)]
+ \beta \cdot \sum_{l=1}^{L} \sum_{u=1}^{n} \sum_{v=1}^{n} Z_{u,v}^{(l)}
\tag{8}
$$

where $E(\cdot)$ is the expectation.
ã“ã“ã§ $E(\cdot)$ ã¯æœŸå¾…å€¤ã§ã‚ã‚‹ã€‚
The regularization term is now continuous, but the first term $L_{BCE}(Z, \Theta)$ still involves the discrete variables $Z^{(l)}$.
æ­£å‰‡åŒ–é …ã¯é€£ç¶šã«ãªã£ãŸãŒã€ç¬¬ä¸€é … $L_{BCE}(Z, \Theta)$ ã¯ã¾ã é›¢æ•£å¤‰æ•°(i.e. binary parameters) $Z^{(l)}$ ã‚’å«ã‚“ã§ã„ã‚‹.
One can address this issue by using existing gradient estimators, such as REINFORCE [48] and Straight Through Estimator [3], etc.
REINFORCE [48]ã‚„ Straight Through Estimator [3]ãªã©ã®æ—¢å­˜ã®å‹¾é…æ¨å®šå™¨ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ã“ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ã“ã¨ãŒã§ãã‚‹.
These approaches, however, suffer from either biased gradients or high variance.
ã—ã‹ã—ã€ã“ã‚Œã‚‰ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€åã£ãŸå‹¾é…ã‚„é«˜ã„åˆ†æ•£ã«æ‚©ã¾ã•ã‚Œã¦ã„ã‚‹.
Alternatively, we directly optimize discrete variables using the recently proposed augment-REINFORCEmerge (ARM) [15, 16, 56], which is unbiased and has low variance.
ã‚ã‚‹ã„ã¯ã€æœ€è¿‘ææ¡ˆã•ã‚ŒãŸaugment-REINFORCEmergeï¼ˆARMï¼‰[15, 16, 56]ã‚’ä½¿ã£ã¦ç›´æ¥é›¢æ•£å¤‰æ•°ã‚’æœ€é©åŒ–ã™ã‚‹.

In particular, we adopt the reparameterization trick [25], which reparameterizes $\Pi_{u,v}^{(l)} \in [0, 1]$ to a deterministic function $g(\cdot)$ with parameter $\Phi_{u,v}^{(l)}$, such that:
ç‰¹ã«ã€$\Pi_{u,v}^{(l)} \in [0, 1]$ ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\Phi_{u,v}^{(l)}$ ã‚’æŒã¤æ±ºå®šè«–çš„é–¢æ•° $g(\cdot)$ ã«å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã™ã‚‹ **reparameterization trick**[25]ã‚’æ¡ç”¨ã™ã‚‹:

$$
\Pi_{u,v}^{(l)} = g(\Phi_{u,v}^{(l)})
\tag{9}
$$

since the deterministic function $g(\cdot)$ should be bounded within [0, 1], we simply choose the standard sigmoid function as our deterministic function: $g(x) = \frac{1}{(1 + e^{-x})}$.
æ±ºå®šè«–çš„é–¢æ•°(=é–¾å€¤ä»¥ä¸Šã§ã‚ã‚Œã°é™½æ€§ã€ã¿ãŸã„ãª?) $g(\cdot)$ ã¯[0, 1]å†…ã§æœ‰ç•Œã§ã‚ã‚‹ã¹ããªã®ã§ã€æ±ºå®šè«–çš„é–¢æ•°ã¨ã—ã¦æ¨™æº–ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã‚’é¸ã¶ï¼š $g(x) = \frac{1}{(1 + e^{-x})}$.
As such, the second term in Eq.(8) becomes differentiable with the continuous function $g(\cdot)$.
ãã®ãŸã‚ã€å¼(8)ã®ç¬¬2é …ã¯é€£ç¶šé–¢æ•° $g(\cdot)$ ã§å¾®åˆ†å¯èƒ½ã«ãªã‚‹.
We next present the ARM estimator for computing the gradients of binary variables in the first term of Eq.(8) [15, 16, 56].
æ¬¡ã«ã€å¼(8)ã®ç¬¬1é …ã®binaryå¤‰æ•°(= Lå€‹ã®binary maskè¡Œåˆ— $\mathbf{Z}$ ã®äº‹!)ã®å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã®ARMæ¨å®šå™¨ã‚’ç¤ºã™[15, 16, 56].

According to Theorem 1 in ARM [56], we can compute the gradients for Eq.(8) as:
ARM [56]ã®å®šç†1ã«ã‚ˆã‚Œã°ã€å¼(8)ã®å‹¾é…ã¯æ¬¡ã®ã‚ˆã†ã«è¨ˆç®—ã§ãã‚‹(é›£ã—ã„...!!:thinking:):

$$
\Delta_{\Phi}^{ARM} L(\Phi, \Theta) =
\\
E_{\mathbf{U} \in \Pi_{l=1}^{L} Uni(\mathbf{U}^{(l)}; 0, 1)}
[L_{BCE}(I[\mathbf{U} > g(-\Phi)], \Theta) - L_{BCE}(I[\mathbf{U} < g(\Phi)], \Theta) \cdot (\mathbf{U} - \frac{1}{2})]
\\
+ \beta \Delta_{\Phi} g(\Phi)
\tag{10}
$$

where $Uni(0, 1)$ denotes the Uniform distribution within [0, 1], and $L_{BCE}(I[\mathbf{U} > g(-\Phi)], \Theta)$ is the cross-entropy loss obtained by setting the binary masks $\mathbf{Z}^{(l)}$ to 1 if $\mathbf{U}^{(l)} > g(- \Phi^{(l)})$ in the forward pass, and 0 otherwise.
ã“ã“ã§ $Uni(0, 1)$ ã¯[0, 1]å†…ã®ä¸€æ§˜åˆ†å¸ƒã‚’è¡¨ã—ã€$L_{BCE}(I[\mathbf{U} > g(-\Phi)], \Theta)$ ã¯ã€ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹(i.e. æ¨è«–æ™‚ã®é †ä¼æ¬ã®äº‹??)ã«ãŠã„ã¦ã€$\mathbf{U}^{(l)} > g(- \Phi^{(l)})$ ã®å ´åˆã«ãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯ $\mathbf{Z}^{(l)}$ ã‚’1ã«è¨­å®šã—ãã‚Œä»¥å¤–ã®å ´åˆã«0ã«è¨­å®šã™ã‚‹(=è¦ã¯indicator functionã‚’ä½¿ã£ã¦ã‚‹ã ã‘ã­!)ã“ã¨ã§å¾—ã‚‰ã‚Œã‚‹ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±. ($L_{BCE}[hoge, fuga]$ ãŒã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±é–¢æ•°ã‹...!:thinking:)
The same strategy is applied to Lğµğ¶ğ¸ (I[U < ğ‘”(Î¦)], Î˜).
åŒã˜æˆ¦ç•¥ã‚’ $L_{BCE}(I[\mathbf{U} < g(\Phi)], \Theta)$ ã«ã‚‚é©ç”¨ã—ã¦ã„ã‚‹.
Moreover, ARM is an unbiased estimator due to the linearity of expectations.
ã•ã‚‰ã«ã€ARM ã¯æœŸå¾…å€¤ã®ç·šå½¢æ€§ã«ã‚ˆã‚Šä¸åæ¨å®šé‡ã¨ãªã‚‹. (çœŸã®å°é–¢æ•°ã®?:thinking:)

Note that we need to evaluate Lğµğ¶ğ¸ $L_{BCE}(\cdot)$ twice to compute gradients in Eq.(10).
å¼(10)ã®å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã«ã€$L_{BCE}(\cdot)$ ã‚’2å›è©•ä¾¡ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã€‚
Given the fact that $u \sim Uni(0, 1)$ implies $(1 âˆ’ u) \sim Uni(0, 1)$, we can replace $\mathbf{U}$ with $(1 âˆ’ \mathbf{U})$ in the indicator function inside $L_{BCE}(I[\mathbf{U} > g(-\Phi)],  \Theta)$:
$u \sim Uni(0, 1)$ ãŒ $(1 âˆ’ u) \sim Uni(0, 1)$ ã‚’æ„å‘³ã™ã‚‹ã“ã¨ã‹ã‚‰ã€$L_{BCE}(I[\mathbf{U} > g(-\Phi)],  \Theta)$ å†…ã® indicator function ã«ãŠã„ã¦ã€$\mathbf{U}$ ã‚’ $(1 âˆ’ \mathbf{U})$ ã«ç½®ãæ›ãˆã‚‹ã“ã¨ãŒã§ãã‚‹:

$$
I[\mathbf{U} > g(-\Phi)]
\rightleftarrows I[1 - \mathbf{U} > g(-\Phi)]
\\
\rightleftarrows I[\mathbf{U} < 1 - g(-\Phi)]
\\
\rightleftarrows I[\mathbf{U} < g(\Phi)]
$$

To this end, we can further reduce the complexity by considering the variants of ARM â€“ Augment-Reinforce (AR) [56]:
ã“ã®ãŸã‚ã€ARMã®å¤‰å½¢ã§ã‚ã‚‹**Augment-Reinforce(AR)**[56]ã‚’è€ƒæ…®ã™ã‚‹ã“ã¨ã§ã€ã•ã‚‰ã«è¤‡é›‘ã•ã‚’è»½æ¸›ã™ã‚‹ã“ã¨ãŒã§ãã‚‹:

$$
\Delta_{\Theta}^{AR} L(\Phi, \Theta)
\\
= E_{\mathbf{U} \in \Pi_{l=1}^{L} Uni(\mathbf{U}^{(l)}; 0, 1)}
[L_{BCE}(I[\mathbf{U} < g(\Phi)], \Theta) \cdot (1 - 2 \mathbf{U})]
\\
+ \beta \Delta_{\Phi} g(\Phi)
\tag{11}
$$

where only requires one-forward pass.
ã“ã“ã§å¿…è¦ãªã®ã¯ **one-forward pass** ã ã‘ã .(ãªã«ãã‚Œ?:thinking:)
The gradient estimator $\Delta_{\Theta}^{AR} L(\Phi, \Theta)$ is still unbiased but may pose higher variance, comparing to $\Delta_{\Theta}^{ARM} L(\Phi, \Theta)$.
å‹¾é…æ¨å®šé‡ $\Delta_{\Theta}^{AR} L(\Phi, \Theta)$ ã¯ä¾ç„¶ã¨ã—ã¦ä¸åã§ã‚ã‚‹ãŒã€$\Delta_{\Theta}^{ARM} L(\Phi, \Theta)$ ã«æ¯”ã¹ã¦åˆ†æ•£ãŒå¤§ãããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
In the experiments, we can trade off the variance of the estimator with complexity.
å®Ÿé¨“ã§ã¯ã€æ¨å®šé‡ã®åˆ†æ•£ã¨è¤‡é›‘ã•ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã«ã™ã‚‹ã“ã¨ãŒã§ãã‚‹.(ARMæ¨å®šé‡ã®æ–¹ãŒåˆ†æ•£ãŒå°ã•ã„ã€ARæ¨å®šé‡ã®æ–¹ãŒè¤‡é›‘æ€§ãŒä½ã„.)

In the training stage, we update âˆ‡Î¦L (Î¦, Î˜) (either Eq.(10) or Eq.(11)) and âˆ‡Î˜L (Î¦, Î˜) 3 during the back propagation.
å­¦ç¿’æ®µéšã§ã¯ã€é€†ä¼æ’­ä¸­ã« $\Delta_{\Phi} L(\Phi, \Theta)$ (å¼ï¼ˆ10) ã¾ãŸã¯å¼ï¼ˆ11ï¼‰ï¼‰ã¨ $\Delta_{\Theta} L(\Phi, \Theta)$ ã‚’æ›´æ–°ã™ã‚‹.($\Phi$ ã¨ $\Theta$ ã‚’ãã‚Œãã‚Œæ›´æ–°ã™ã‚‹ç‚ºã‹...:thinking:)
In the inference stage, we can use the expectation of $Z_{u,v}^{(l)} \sim Bern(\Pi_{u,v}^{(l)})$ as the mask in Eq.(5), i.e., $E(Z_{u,v}^{(l)}) = \Pi_{u,v}^{(l)} = g(\Phi_{u,v}^{(l)})$.
æ¨è«–æ®µéšã§ã¯ã€$Z_{u,v}^{(l)} \sim Bern(\Pi_{u,v}^{(l)})$ ã®æœŸå¾…å€¤ã‚’å¼(5)ã®ãƒã‚¹ã‚¯ã¨ã—ã¦ä½¿ã†ã“ã¨ãŒã§ãã‚‹ã€ã™ãªã‚ã¡ã€$E(Z_{u,v}^{(l)}) = \Pi_{u,v}^{(l)} = g(\Phi_{u,v}^{(l)})$.
Nevertheless, this will not yield a sparse attention M(ğ‘™) since the sigmoid function is smooth unless the hard sigmoid function is used in Eq.(9).
ã¨ã¯ã„ãˆã€å¼(9)ã§hard sigmoidé–¢æ•°(hardã¨ã¯??:thinking:)ã‚’ä½¿ã‚ãªã„é™ã‚Šã€ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã¯å¹³æ»‘ãªã®ã§ã€ã“ã‚Œã§ã¯sparse attention $M(l)$ ã¯å¾—ã‚‰ã‚Œãªã„.(=ãªã‚ã‚‰ã‹ãªattention weight åˆ†å¸ƒã«ãªã£ã¦ã—ã¾ã†ã€ã£ã¦æ„å‘³...!)
Here we simply clip those values $g(\Phi_{u,v}^{(l)}) \leq 0.5$ to zeros such that a sparse attention matrix is guaranteed and the corresponding noisy attentions are eventually eliminated.
ã“ã“ã§ã¯ã€sparse attetnionè¡Œåˆ—ãŒä¿è¨¼ã•ã‚Œã€å¯¾å¿œã™ã‚‹ãƒã‚¤ã‚¸ãƒ¼ãª attentionãŒæœ€çµ‚çš„ã«æ’é™¤ã•ã‚Œã‚‹ã‚ˆã†ã«ã€å˜ã« $g(\Phi_{u,v}^{(l)}) \leq 0.5$ ã‚’ã‚¼ãƒ­ã«åˆ‡ã‚Šå–ã‚‹.(**é–¾å€¤ã§0 or 1ã‚’æ±ºã‚ã‚‹**:thinking:)

## 4.2. Jacobian Regularization

As recently proved by [28], the standard dot-product self-attention is not Lipschitz continuous and is vulnerable to the quality of input sequences.
æœ€è¿‘[28]ã«ã‚ˆã£ã¦è¨¼æ˜ã•ã‚ŒãŸã‚ˆã†ã«ã€**æ¨™æº–çš„ãªdot-product self-attentionã¯Lipschitz continuous(ãƒªãƒ—ã‚·ãƒƒãƒ„é€£ç¶š?)ã§ã¯ãªã**ã€å…¥åŠ›sequenceã®å“è³ªã«å¼±ã„.
Let $f^{(l)}$ be the $l$-th Transformer block (Sec 3.2.2) that contains both a self-attention layer and a point-wise feed-forward layer, and x be the input.
$f^{(l)}$ ã‚’ $l$ ç•ªç›®ã®Transformerãƒ–ãƒ­ãƒƒã‚¯(ç¬¬3.2.2ç¯€)ã¨ã—ã€self-attentionå±¤ã¨point-wise ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã‚’å«ã‚€ã¨ã™ã‚‹.
We can measure the robustness of the Transformer block using the residual error: $f^{(ğ‘™)}(x + \epsilon) âˆ’ f^{(ğ‘™)}(x)$, where $\epsilon$ is a small perturbation vector and the norm of $\epsilon$ is bounded by a small scalar $\delta$, i.e., $|\epsilon|_{2} \leq \delta$.
$f^{(l)}(x + \epsilon) âˆ’ f^{(l)}(x)$ (=é–¢æ•°ã®å‡ºåŠ›å€¤ã®å·®)ã‚’ä½¿ã£ã¦ã€Transformerãƒ–ãƒ­ãƒƒã‚¯ã®ãƒ­ãƒã‚¹ãƒˆæ€§ã‚’æ¸¬ã‚‹ã“ã¨ãŒã§ãã‚‹. ã“ã“ã§ã€$\epsilon$ ã¯å°ã•ãªæ‘‚å‹•ãƒ™ã‚¯ãƒˆãƒ«ã§ã‚ã‚Šã€ğã®ãƒãƒ«ãƒ (i.e. å¤§ãã•!)ã¯å°ã•ãªã‚¹ã‚«ãƒ©ãƒ¼ $\delta$ ã§å¢ƒç•Œä»˜ã‘ã‚‰ã‚Œã¦ã„ã‚‹. i.e. $|\epsilon|_{2} \leq \delta$.
Following the Taylor expansion, we have:
ãƒ†ã‚¤ãƒ©ãƒ¼å±•é–‹ã«å¾“ã†ã¨ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚‹:

$$
f^{(l)}_{i}(x + \epsilon) âˆ’ f^{(ğ‘™)}_{i}(x) \eqsim [\frac{\partial f^{(ğ‘™)}_{i}(x)}{\partial x}]^{T} \epsilon
$$

Let $J^{(ğ‘™)}(x)$ represent the Jacobian matrix at $x$ where $\frac{\partial f^{(ğ‘™)}_{i}(x)}{\partial x}$.
$J^{(l)}(x)$ ã¯ å…¥åŠ› $x$ ã«ãŠã‘ã‚‹ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—ã‚’è¡¨ã—ã€$\frac{\partial f^{(ğ‘™)}_{i}(x)}{\partial x}$ ã¨ã™ã‚‹.
Then, we set $J^{(l)}_{i}(x) = \frac{\partial f^{(ğ‘™)}_{i}(x)}{\partial x}$ to denote the $i$-th row of $J^{(ğ‘™)}(x)$.
ãã—ã¦ã€$J^{(ğ‘™)}(x)$ ã®$i$ç•ªç›®ã®è¡Œã‚’è¡¨ã™ãŸã‚ã«ã€$J^{(l)}_{i}(x) = \frac{\partial f^{(ğ‘™)}_{i}(x)}{\partial x}$ ã¨ã™ã‚‹.
According to HÃ¶lderâ€™s inequality4 , we have:
[HÃ¶lderã®ä¸ç­‰å¼](https://en.wikipedia.org/wiki/HÃ¶lderâ€™s_inequality)ã«ã‚ˆã‚Œã°ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚‹:

$$
||f^{(l)}_{i}(x + \epsilon) âˆ’ f^{(ğ‘™)}_{i}(x)||_{2}
\eqsim
||f^{(ğ‘™)}_{i}(x)^T \epsilon||_{2} \leq ||f^{(ğ‘™)}_{i}(x)^T \epsilon||_{2} \cdot ||\epsilon||_{2}
$$

Above inequality indicates that regularizing the L2 norm on the Jacobians enforces a Lipschitz constraint at least locally, and the residual error is strictly bounded.
ä¸Šè¨˜ã®ä¸ç­‰å¼ã¯ã€**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®L2ãƒãƒ«ãƒ ã‚’æ­£å‰‡åŒ–ã™ã‚‹ã“ã¨ã§ã€å°‘ãªãã¨ã‚‚å±€æ‰€çš„ã«ã¯ãƒªãƒ—ã‚·ãƒƒãƒ„åˆ¶ç´„ãŒå¼·åˆ¶ã•ã‚Œ**ã€æ®‹å·®(=é–¢æ•°ã®å‡ºåŠ›å€¤ã®å·®)ã¯å³å¯†ã«æœ‰ç•Œ(=å®šæ•° $L$ ä»¥ä¸‹!)ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹.(ã“ã®å ´åˆã®æ­£ã®å®šæ•°Lã£ã¦ $||f^{(ğ‘™)}_{i}(x)^T \epsilon||_{2} \cdot ||\epsilon||_{2}$ ã‹...!:thinking:)
Thus, we propose to regularize Jacobians with Frobenius norm for each Transformer block as:
ãã“ã§ã€å„Transformerãƒ–ãƒ­ãƒƒã‚¯ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã‚’ **Frobenius norm(?) ã§æ­£å‰‡åŒ–**ã™ã‚‹ã“ã¨ã‚’ææ¡ˆã™ã‚‹: 

$$
R_{J} = \sum_{l=1}^{L} ||J^{(l)}||^{2}_{F}
\tag{12}
$$

Importantly, $|J^{(l)}|^{2}_{F}$ can be approximated via various Monte-Carlo estimators [23, 37].
é‡è¦ãªã“ã¨ã¯ã€$|J^{(l)}|^{2}_{F}$ ã¯æ§˜ã€…ãªãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ¨å®šé‡[23, 37]ã«ã‚ˆã£ã¦è¿‘ä¼¼ã§ãã‚‹ã“ã¨ã§ã™ã€‚
In this work, we adopt the classical Hutchinson estimator [23].
æœ¬ç ”ç©¶ã§ã¯ã€å¤å…¸çš„ãªHutchinsonæ¨å®šé‡[23](??)ã‚’æ¡ç”¨ã™ã‚‹.
For each Jocobian matrix J (ğ‘™) âˆˆ R ğ‘›Ã—ğ‘› , we have:
å„ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ— $J^{(l)} \in \mathbb{R}^{n \times n}$ ã«å¯¾ã—ã¦ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚‹ï¼š

$$
||J^{(l)}||^{2}_{F} = Tr(J^{(l)} J^{(l)}^{T})
= E_{\nu \in N(0, I_{n})} [|| \nu^{T} J^{(l)}||^{2}_{F}]
\tag{}
$$

where $\nu \in N(0, I_{n})$ is the normal distribution vector.
ã“ã“ã§ã€$\nu \in N(0, I_{n})$ ã¯æ­£è¦åˆ†å¸ƒãƒ™ã‚¯ãƒˆãƒ«ã§ã‚ã‚‹.(å…±åˆ†æ•£è¡Œåˆ—ãŒå˜ä½è¡Œåˆ—ãªã®ã§ã€å„è¦ç´ ã¯ç‹¬ç«‹...!:thinking:)
We further make use of random projections to compute the norm of Jacobians $R_{j}$ and its gradient $\Delta_{\Theta} R_{j}(\Theta)$ [21], which significantly reduces the running time in practice.
ã•ã‚‰ã«ã€ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®ãƒãƒ«ãƒ  $R_{j}$ ã¨ãã®å‹¾é… $\Delta_{\Theta} R_{j}(\Theta)$ [21]ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã«random projections(ãƒ©ãƒ³ãƒ€ãƒ ãªé‡ã¿ã«ã‚ˆã‚‹linear projectionã®æ„å‘³??:thinking:)ã‚’åˆ©ç”¨ã™ã‚‹.

## 4.3. Optimization

### 4.3.1. Joint Training 4.3.1. åˆåŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

Putting together loss in Eq.(4), Eq.(6), and Eq.(12), the overall objective of Rec-Denoiser is:
å¼(4)ã€å¼(6)ã€å¼(12)ã®æå¤±ã‚’ã¾ã¨ã‚ã‚‹ã¨ã€Rec-Denoiserã®å…¨ä½“çš„ãªç›®çš„ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚‹ï¼š

$$
L_{Rec-Denoiser} = L_{BCE} + \beta \cdot R_{M} + \gamma \cdot R_{J}
\tag{13}
$$

where $\beta$ and $\gamma$ are regularizers to control the sparsity and robustness of self-attention networks, respectively.
ã“ã“ã§ $\beta$ ã¨ $\gamma$ ã¯ã€**ãã‚Œãã‚Œself-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã¨ãƒ­ãƒã‚¹ãƒˆæ€§ã‚’åˆ¶å¾¡ã™ã‚‹ regularizer(æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) ã§ã‚ã‚‹**.
Algorithm 1 summarizes the overall training of Rec-Denoiser with the AR estimator.
ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 1ã¯ã€ARæ¨å®šå™¨ã‚’ç”¨ã„ãŸRec-Denoiserã®å…¨ä½“çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã¾ã¨ã‚ãŸã‚‚ã®ã§ã‚ã‚‹ã€‚

![]()

Lastly, it is worth mentioning that our Rec-Denoiser is compatible to many Transformer-based sequential recommender models since our differentiable masks and gradient regularizations will not change their main architectures.
æœ€å¾Œã«ã€æˆ‘ã€…ã®Rec-Denoiserã¯ã€å¾®åˆ†å¯èƒ½ãªmask ã¨ å‹¾é…æ­£å‰‡åŒ–ã¯ã€ãã‚Œã‚‰ã®ä¸»è¦ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å¤‰æ›´ã—ãªã„ã®ã§ã€**å¤šãã®Transformerãƒ™ãƒ¼ã‚¹ã®é€æ¬¡æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã¨äº’æ›æ€§ãŒã‚ã‚‹**ã“ã¨ã‚’è¨€åŠã™ã‚‹ä¾¡å€¤ãŒã‚ã‚‹.
If we simply set all masks $Z^{(l)}$ to be all-ones matrix and $\beta = \gamma = 0$, our model boils down to their original designs.
å˜ç´”ã«ã™ã¹ã¦ã®ãƒã‚¹ã‚¯ $Z^{(l)}$ ($\forall l=1, \cdots, L$ :thinking:)ã‚’ã‚ªãƒ¼ãƒ«1ã®è¡Œåˆ—ã¨ã—ã€$\beta = \gamma = 0$ ã¨ã™ã‚‹ã¨ã€**ãƒ¢ãƒ‡ãƒ«ã¯å…ƒã®è¨­è¨ˆã«å¸°ç€ã™ã‚‹**.(åˆ†ã‹ã‚‹åˆ†ã‹ã‚‹...!)
If we randomly set subset of masks Z (ğ‘™) to be zeros, it is equivalent to structured Dropout like LayerDrop [17], DropHead [60].
ãƒã‚¹ã‚¯ã®ã‚µãƒ–ã‚»ãƒƒãƒˆ(i.e. Lå€‹ã®binary mask matrixã®ã†ã¡ã®ã„ãã¤ã‹) $Z^{(l)}$ ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚¼ãƒ­ã«è¨­å®šã™ã‚‹ã¨ã€LayerDrop [17]ã‚„DropHead [60]ã®ã‚ˆã†ãª **structured Dropout ã¨ç­‰ä¾¡**ã«ãªã‚‹.
In addition, our Rec-Denoiser can work together with linearized self-attention networks [27, 59] to further reduce the complexity of attentions.
ã•ã‚‰ã«ã€ç§ãŸã¡ã®Rec-Denoiserã¯ã€ç·šå½¢åŒ–ã•ã‚ŒãŸself-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯[27, 59]ã¨é€£æº(ã‚ˆãã‚¤ãƒ¡ãƒ¼ã‚¸ãŒæ¹§ã„ã¦ãªã„...?)ã™ã‚‹ã“ã¨ãŒã§ãã€attentionã®è¤‡é›‘ã•ã‚’ã•ã‚‰ã«è»½æ¸›ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
We leave this extension in the future.
ç§ãŸã¡ã¯ã“ã®å»¶é•·ã‚’å°†æ¥ã«æ®‹ã™.

### 4.3.2. Model Complexity 4.3.2. ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•

The complexity of Rec-Denoiser comes from three parts: a basic Transformer, differentiable masks, and Jacobian regularization.
Rec-Denoiserã®complexity(è¨ˆç®—é‡?)ã¯ã€åŸºæœ¬çš„ãªTransformerã€å¾®åˆ†å¯èƒ½ãªmaskã€Jacobianæ­£å‰‡åŒ–ã¨ã„ã†3ã¤ã®éƒ¨åˆ†ã‹ã‚‰æ¥ã¦ã„ã‚‹ã€‚
The complexity of basic Transformer keeps the same as SASRec [26] or BERT4Rec [41].
åŸºæœ¬çš„ãªTransformerã®complexity(è¨ˆç®—é‡?)ã¯ã€SASRec [26]ã‚„BERT4Rec [41]ã¨åŒã˜ã§ã‚ã‚‹ã€‚
The complexity of differentiable masks requires either one-forward pass (e.g., AR with high variance) or two-forward pass (e.g., ARM with low variance) of the model.
å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã®è¨ˆç®—é‡ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®one-forward pass(é«˜åˆ†æ•£ã®ARãªã©)ã¾ãŸã¯two-forward pass(ä½åˆ†æ•£ã®ARMãªã©)ã‚’å¿…è¦ã¨ã™ã‚‹.(n-forward passã®æ„å‘³ãŒã‚ã‹ã£ã¦ãªã„...:thinking:)
In sequential recommenders, the number of Transformer blocks is often very small (e.g., ğ¿ = 2 in SASRec [26] and BERT4Rec [41] ).
**é€æ¬¡ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã§ã¯ã€Transformerãƒ–ãƒ­ãƒƒã‚¯ã®æ•°ã¯éå¸¸ã«å°‘ãªã„ã“ã¨ãŒå¤šã„**ï¼ˆä¾‹ãˆã°ã€SASRec [26]ã¨BERT4Rec [41] ã§ã¯ L = 2 ï¼‰ã€‚
It is thus reasonable to use the ARM estimator without heavy computations.
å¾“ã£ã¦ã€é‡ã„è¨ˆç®—ã‚’ã›ãšã«ARMæ¨å®šé‡ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯åˆç†çš„ã§ã‚ã‚‹ã€‚
Besides, we compare the performance of AR and ARM estimators in Sec 5.3.
åŠ ãˆã¦ã€5.3ç« ã§ARæ¨å®šé‡ã¨ARMæ¨å®šé‡ã‚’ç”¨ã„ã‚‹å ´åˆã®performanceã‚’æ¯”è¼ƒã—ãŸ.

Moreover, the random project techniques are surprisingly efficient to compute the norms of Jacobians [21].
ã•ã‚‰ã«ã€random projectæŠ€æ³•ã¯ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®ãƒãƒ«ãƒ ã‚’è¨ˆç®—ã™ã‚‹ã®ã«é©šãã»ã©åŠ¹ç‡çš„ã§ã‚ã‚‹[21].
As a result, the overall computational complexity remains the same order as the original Transformers during the training.
ãã®çµæœã€**å…¨ä½“çš„ãªè¨ˆç®—é‡ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã®ã‚ªãƒªã‚¸ãƒŠãƒ«ã®Transformersã¨åŒã˜ã‚ªãƒ¼ãƒ€ãƒ¼ã®ã¾ã¾**ã§ã‚ã‚‹.
However, during the inference, our attention maps are very sparse, which enables much faster feed-forward computations.
ã—ã‹ã—ã€æ¨è«–ä¸­ã® attention map ã¯éå¸¸ã«ç–ãªãŸã‚ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰è¨ˆç®—ã‚’ã‚ˆã‚Šé«˜é€Ÿã«è¡Œã†ã“ã¨ãŒã§ãã‚‹. (æ¨è«–ã¯å…ƒã€…ã®Transformerã‚ˆã‚Šã‚‚é«˜é€Ÿã£ã¦ã“ã¨ã­...!)

# 5. Experiments 5. å®Ÿé¨“

Here we present our empirical results.
ã“ã“ã§ã¯å®Ÿè¨¼çš„ãªçµæœã‚’ç´¹ä»‹ã™ã‚‹
Our experiments are designed to answer the following research questions:
æˆ‘ã€…ã®å®Ÿé¨“ã¯ã€ä»¥ä¸‹ã®ç ”ç©¶èª²é¡Œã«ç­”ãˆã‚‹ãŸã‚ã«ãƒ‡ã‚¶ã‚¤ãƒ³ã•ã‚ŒãŸï¼š

- RQ1: How effective is the proposed Rec-Denoiser compared to the state-of-the-art sequential recommenders? RQ1: ææ¡ˆã™ã‚‹Rec-Denoiserã¯ã€**æœ€æ–°ã®é€æ¬¡æ¨è–¦å™¨ã¨æ¯”è¼ƒã—ã¦ã©ã®ç¨‹åº¦æœ‰åŠ¹**ã‹ï¼Ÿ
- RQ2: How can Rec-Denoiser reduce the negative impacts of noisy items in a sequence? RQ2ï¼šRec-Denoiserã¯ã€**sequenceå†…ã®ãƒã‚¤ã‚ºã®å¤šã„ã‚¢ã‚¤ãƒ†ãƒ ã®æ‚ªå½±éŸ¿ã‚’ã©ã®ã‚ˆã†ã«è»½æ¸›ã§ãã¾ã™ã‹**ï¼Ÿ

- RQ3: How do different components (e.g., differentiable masks and Jacobian regularization) affect the overall performance of Rec-Denoiser? RQ3ï¼š**ç•°ãªã‚‹æ§‹æˆè¦ç´ (å¾®åˆ†å¯èƒ½ mask ã‚„ Jacobian æ­£å‰‡åŒ–ãªã©ï¼‰ã¯ã€Rec-Denoiserã®å…¨ä½“çš„ãªæ€§èƒ½ã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹**ï¼Ÿ

## 5.1. Experimental Setting # 5.1.Experimental Setting

### 5.1.1. Dataset 5.1.1. ãƒ‡ãƒ¼ã‚¿é›†åˆ

We evaluate our models on five benchmark datasets: Movielens5 , Amazon6 (we choose the three commonly used categories: Beauty, Games, and Movies&TV), and Steam7 [30].
æˆ‘ã€…ã¯5ã¤ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ï¼š Movielens5ã€Amazon6ï¼ˆç¾å®¹ã€ã‚²ãƒ¼ãƒ ã€æ˜ ç”»ï¼†TVã®3ã¤ã®ã‚ˆãä½¿ã‚ã‚Œã‚‹ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’é¸æŠï¼‰ã€Steam7 [30]ã§ã‚ã‚‹ã€‚
Their statistics are shown in Table 1.
çµ±è¨ˆã¯è¡¨1ã®é€šã‚Šã§ã‚ã‚‹ã€‚
Among different datasets, MovieLens is the most dense one while Beauty has the fewest actions per user.
ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸­ã§ã€MovieLensã¯æœ€ã‚‚å¯†åº¦ãŒé«˜ãã€Beautyã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚ãŸã‚Šã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°ãŒæœ€ã‚‚å°‘ãªã„ã€‚
We use the same procedure as [26, 30, 39] to perform preprocessing and split data into train/valid/test sets, i.e., the last item of each userâ€™s sequence for testing, the second-to-last for validation, and the remaining items for training.
26,30,39]ã¨åŒã˜æ‰‹é †ã§å‰å‡¦ç†ã‚’è¡Œã„ã€ãƒ‡ãƒ¼ã‚¿ã‚’train/valid/testã‚»ãƒƒãƒˆã«åˆ†å‰²ã™ã‚‹ã€‚ã¤ã¾ã‚Šã€å„ãƒ¦ãƒ¼ã‚¶ã®sequenceã®æœ€å¾Œã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’testingã«ã€æœ€å¾Œã‹ã‚‰2ç•ªç›®ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’validationã«ã€æ®‹ã‚Šã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’trainingã«ä½¿ç”¨ã™ã‚‹ã€‚

### 5.1.2. Baselines 5.1.2. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

Here we include two groups of baselines.
ã“ã“ã§ã¯ã€2ã¤ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ»ã‚°ãƒ«ãƒ¼ãƒ—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã€‚
The first group includes general sequential methods (Sec 5.2): 1) FPMC [39]: a mixture of matrix factorization and first-order Markov chains model; 2) GRU4Rec [20]: a RNN-based method that models user action sequences; 3) Caser [42]: a CNN-based framework that captures high-order relationships via convolutional operations; 4) SASRec [26]: a Transformer-based method that uses left-to-right selfattention layers; 5) BERT4Rec [41]: an architecture that is similar to SASRec, but using bidirectional self-attention layers; 6) TiSASRec [30]: a time-aware self-attention model that further considers the relative time intervals between any two items; 7) SSE-PT [50]: a framework that introduces personalization into self-attention layers; 8) Rec-Denoiser: our proposed Rec-Denoiser that can choose any self-attentive models as its backbone.
æœ€åˆã®ã‚°ãƒ«ãƒ¼ãƒ—ã«ã¯ã€ä¸€èˆ¬çš„ãªé€æ¬¡çš„æ‰‹æ³•ãŒå«ã¾ã‚Œã‚‹ï¼ˆSec.5.2ï¼‰ï¼š

- 1. FPMC [39]ï¼šè¡Œåˆ—åˆ†è§£ã¨ä¸€æ¬¡ãƒãƒ«ã‚³ãƒ•é€£é–ãƒ¢ãƒ‡ãƒ«ã®æ··åˆ
- 2. GRU4Rec [20]ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹RNNãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•
- 3. Caser [42]ï¼šç•³ã¿è¾¼ã¿æ¼”ç®—ã«ã‚ˆã£ã¦é«˜æ¬¡ã®é–¢ä¿‚ã‚’æ‰ãˆã‚‹CNNãƒ™ãƒ¼ã‚¹ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€
- 4. SASRec [26]ï¼šå·¦ã‹ã‚‰å³ã¸ã®è‡ªå·±æ³¨æ„å±¤ã‚’ä½¿ç”¨ã™ã‚‹Transformerãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã€
- 5. BERT4Rec [41]ï¼š
- 6. TiSASRec [30]ï¼šä»»æ„ã®2ã¤ã®ã‚¢ã‚¤ãƒ†ãƒ ã®é–“ã®ç›¸å¯¾çš„ãªæ™‚é–“é–“éš”ã‚’ã•ã‚‰ã«è€ƒæ…®ã™ã‚‹ã€æ™‚é–“ã‚’è€ƒæ…®ã—ãŸè‡ªå·±æ³¨æ„ãƒ¢ãƒ‡ãƒ«ã€
- 7. SSE-PT [50]ï¼šself-attentionå±¤ã«ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã‚’å°å…¥ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€
- 8. Rec-Denoiserï¼šãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦ä»»æ„ã®self-attentionãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã§ãã‚‹ã€æˆ‘ã€…ã®ææ¡ˆã™ã‚‹Rec-Denoiserã€‚

The second group contains sparse Transformers (Sec 5.3):
2ç•ªç›®ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«ã¯ã‚¹ãƒ‘ãƒ¼ã‚¹å¤‰æ›å™¨ï¼ˆSec.5.3ï¼‰ãŒå«ã¾ã‚Œã‚‹ï¼š

- 1. Sparse Transformer [10]: it uses a fixed attention pattern, where only specific cells summarize previous locations in the attention layers; **å›ºå®šã•ã‚ŒãŸatteniton ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨**ã—ã€ç‰¹å®šã®ã‚»ãƒ«ã ã‘ãŒattetnionå±¤ã®å‰ã®ä½ç½®ã‚’è¦ç´„ã™ã‚‹ï¼›
- 2. ğ›¼-entmax sparse attention [12]: it simply replaces softmax with ğ›¼-entmax to achieve sparsity. ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã‚’é”æˆã™ã‚‹ãŸã‚ã«ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚’ ğ›¼-entmax ã«ç½®ãæ›ãˆãŸã‚‚ã®ã€‚

Note that we do not compare with other popular sparse Transformers like Star Transformer [18], Longformer [2], and BigBird [58].
ãªãŠã€Star Transformer [18]ã€Longformer [2]ã€BigBird [58]ã®ã‚ˆã†ãªä»–ã®æœ‰åãªã‚¹ãƒ‘ãƒ¼ã‚¹å¤‰æ›å™¨ã¨ã¯æ¯”è¼ƒã—ã¦ã„ãªã„ã€‚
These Transformers are specifically designed for thousands of tokens or longer in the language modeling tasks.
ã“ã‚Œã‚‰ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹æ•°åƒä»¥ä¸Šã®ãƒˆãƒ¼ã‚¯ãƒ³ç”¨ã«ç‰¹åˆ¥ã«è¨­è¨ˆã•ã‚Œã¦ã„ã‚‹ã€‚(ã ã‹ã‚‰ã­!)
We leave their explorations for recommendations in the future.
å½¼ã‚‰ã®æ¢æ±‚ã¯ä»Šå¾Œã®æè¨€ã«å§”ã­ãŸã„ã€‚
We also do not compare with LayerDrop [17] and DropHead [60] since the number of Transformer blocks and heads are often very small (e.g., ğ¿ = 2 in SARRec) in sequential recommendation.
ã¾ãŸã€LayerDrop[17]ã‚„DropHead[60]ã¨ã®æ¯”è¼ƒã¯è¡Œã‚ãªã„ã€‚ãªãœãªã‚‰ã€é€æ¬¡æ¨è–¦ã§ã¯ã€Transformerãƒ–ãƒ­ãƒƒã‚¯ã‚„ãƒ˜ãƒƒãƒ‰ã®æ•°ãŒéå¸¸ã«å°‘ãªã„(ä¾‹ãˆã°ã€SARRecã§ã¯ğ¿ = 2)ã“ã¨ãŒå¤šã„ã‹ã‚‰ã§ã‚ã‚‹ã€‚
Other sequential architectures like memory networks [9, 22] and graph neural networks [4, 51] have been outperformed by the above baselines, we simply omit these baselines and focus on Transformer-based models.
ãƒ¡ãƒ¢ãƒªãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯[9, 22]ã‚„ã‚°ãƒ©ãƒ•ãƒ»ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯[4, 51]ã®ã‚ˆã†ãªä»–ã®é€æ¬¡ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€ä¸Šè¨˜ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã‚‹.
The goal of experiments is to see whether the proposed differentiable mask techniques can reduce the negative impacts of noisy items in the self-attention layers.
**å®Ÿé¨“ã®ç›®çš„ã¯ã€ææ¡ˆã•ã‚ŒãŸå¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯æŠ€è¡“ãŒã€self-attentionå±¤ã«ãŠã‘ã‚‹ãƒã‚¤ã‚ºã‚¢ã‚¤ãƒ†ãƒ ã®æ‚ªå½±éŸ¿ã‚’è»½æ¸›ã§ãã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã™ã‚‹ã“ã¨**ã§ã‚ã‚‹ã€‚

### 5.1.3. Evaluation metrics 5.1.3. è©•ä¾¡æŒ‡æ¨™

For easy comparison, we adopt two common Top-N metrics, Hit@ğ‘ and NDCG@ğ‘ (with default value $N = 10$), to evaluate the performance of sequential models [26, 30, 41].
æ¯”è¼ƒã‚’å®¹æ˜“ã«ã™ã‚‹ãŸã‚ã«ã€é€æ¬¡ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€2ã¤ã®ä¸€èˆ¬çš„ãªTop-Nãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€**Hit@N**ã¨**NDCG@N**(ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤Ç” = 10)ã‚’æ¡ç”¨ã™ã‚‹[26, 30, 41].
Typically, Hit@ğ‘ counts the rates of the ground-truth items among top-ğ‘ items, while NDCG@ğ‘ considers the position and assigns higher weights to higher positions.
ä¸€èˆ¬çš„ã«ã€Hit@N ã¯ã€top-Nã‚¢ã‚¤ãƒ†ãƒ ã®ä¸­ã§ã‚°ãƒ©ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ã®å‰²åˆã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã€NDCG@N ã¯ä½ç½®(ãƒ©ãƒ³ã‚­ãƒ³ã‚°å†…)ã‚’è€ƒæ…®ã—ã€é«˜ã„ä½ç½®ã«é«˜ã„é‡ã¿ã‚’å‰²ã‚Šå½“ã¦ã‚‹.
Following the work [26, 30], for each user, we randomly sample 100 negative items, and rank these items with the ground-truth item.
[26,30]ã«å¾“ã„ã€å„ãƒ¦ãƒ¼ã‚¶ã«ã¤ã„ã¦100å€‹ã®ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚¢ã‚¤ãƒ†ãƒ (ex. ã¾ã è³¼å…¥ã—ã¦ãªã„ã‚¢ã‚¤ãƒ†ãƒ )ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ã“ã‚Œã‚‰ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ground-truthã‚¢ã‚¤ãƒ†ãƒ (=testãƒ‡ãƒ¼ã‚¿ã®1 item)ã¨ä¸€ç·’ã«é †ä½ä»˜ã‘ã™ã‚‹.
We calculate Hit@10 and NDCG@10 based on the rankings of these 101 items.
ã“ã®**101é …ç›®ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°**ã‚’ã‚‚ã¨ã«Hit@10ã¨NDCG@10ã‚’ç®—å‡ºã—ãŸ.

### 5.1.4. Parameter settings 5.1.4. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š(ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š)

For all baselines, we initialize the hyper-parameters as the ones suggested by their original work.
ã™ã¹ã¦ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«ã¤ã„ã¦ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å½¼ã‚‰ã®ã‚ªãƒªã‚¸ãƒŠãƒ«ç ”ç©¶ã§ææ¡ˆã•ã‚ŒãŸã‚‚ã®ã¨ã—ã¦åˆæœŸåŒ–ã—ãŸã€‚
They are then well tuned on the validation set to achieve optimal performance.
They are then well tuned on the validation set to achieve optimal performance.
ãã—ã¦ã€æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã™ã‚‹ãŸã‚ã«ã€ã‚»ãƒƒãƒˆä¸Šã§ååˆ†ã«èª¿æ•´ã•ã‚Œã‚‹ã€‚
The final results are conducted on the test set.
æœ€çµ‚çš„ãªçµæœã¯testã‚»ãƒƒãƒˆã§å®Ÿæ–½ã•ã‚Œã‚‹.
We search the dimension size of items within {10, 20, 30, 40, 50}.
itemã®(embeddingã®?)æ¬¡å…ƒã‚µã‚¤ã‚ºã‚’{10, 20, 30, 40, 50}ã®ç¯„å›²ã§æ¤œç´¢ã™ã‚‹ã€‚
As our Rec-Denoiser is a general plugin, we use the same hyper-parameters as the basic Transformers, e.g., number of Transformer blocks, batch size, learning rate in Adam optimizer, etc.
æˆ‘ã€…ã®Rec-Denoiserã¯ä¸€èˆ¬çš„ãªãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã§ã‚ã‚‹ãŸã‚ã€åŸºæœ¬çš„ãªTransformerã¨åŒã˜ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹ã€‚ä¾‹ãˆã°ã€Transformerãƒ–ãƒ­ãƒƒã‚¯ã®æ•°ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã€Adamã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®å­¦ç¿’ç‡ãªã©ã§ã‚ã‚‹ã€‚
According to Table 1, we set the maximum length of item sequence $n = 50$ for dense datasets MovieLens and Movies&TV, and $n = 25$ for sparse datasets Beauty, Games, and Steam.
è¡¨1ã«ã‚ˆã‚‹ã¨ã€ã‚¢ã‚¤ãƒ†ãƒ åˆ—ã®æœ€å¤§é•·ã‚’ã€å¯†ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹MovieLensã¨Movies&TVã«ã¯ $n = 50$ ã‚’ã€ç–ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹Beautyã€Gamesã€Steamã«ã¯ $n = 25$ ã‚’è¨­å®šã—ãŸ.
In addition, we set the number of Transformer blocks ğ¿ = 2, and the number of heads ğ» = 2 for self-attentive models.
ã•ã‚‰ã«ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ã®æ•° $L = 2$ã€self-attentionãƒ¢ãƒ‡ãƒ«ã®ãƒ˜ãƒƒãƒ‰æ•°$H = 2$ã¨ã—ãŸ. (Lã‚‚Hã‚‚next-token predictionã‚¿ã‚¹ã‚¯ã¨æ¯”è¼ƒã—ã¦å°‘ãªã‚ãªã‚“ã ãª...!:thinking:)
For Rec-Denoiser, two extra regularizers ğ›½ and ğ›¾ are both searched within {10âˆ’1 , 10âˆ’2 , . . . , 10âˆ’5 }
Rec-Denoiserã§ã¯ã€2ã¤ã®æ­£å‰‡åŒ–å­ $\beta$ ã¨ $\gamma$ ãŒ ${10-1 , 10-2 , ... , 10-5}$ å†…ã§æ¢ç´¢ã•ã‚Œã‚‹.
We choose ARM estimator due to the shallow structures of self-attentive recommenders.
æˆ‘ã€…ã¯ã€self-attentionå‹æ¨è–¦è€…ã®æ§‹é€ ãŒæµ…ã„ã“ã¨ã‹ã‚‰ã€ARMæ¨å®šå™¨ã‚’é¸æŠã—ãŸ.

## 5.2. Overall Performance(RQ1) 5.2.ç·åˆæˆç¸¾ï¼ˆRQ1ï¼‰

![table2]()

Table 2 presents the overall recommendation performance of all methods on the five datasets.
è¡¨2ã¯ã€5ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã‘ã‚‹ã™ã¹ã¦ã®æ‰‹æ³•ã®ç·åˆçš„ãªæ¨è–¦æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
Our proposed Recdenoisers consistently obtain the best performance for all datasets.
æˆ‘ã€…ã®ææ¡ˆã™ã‚‹**Recdenoisersã¯ã€ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ä¸€è²«ã—ã¦æœ€é«˜ã®æ€§èƒ½ã‚’å¾—ãŸ**ã€‚
Additionally, we have the following observations:
ã•ã‚‰ã«ã€æ¬¡ã®ã‚ˆã†ãªè¦‹è§£ã‚‚ã‚ã‚‹ï¼š

- The self-attentive sequential models (e.g., SASRec, BERT4Rec, TiSASRec, and SSE-PT) generally outperform FPMC, GRU4Rec, and Caser with a large margin, verifying that the self-attention networks have good ability of capture long-range item dependencies for the task of sequential recommendation. self-attentionå‹é€æ¬¡æ¨è–¦ãƒ¢ãƒ‡ãƒ«ï¼ˆSASRecã€BERT4Recã€TiSASRecã€SSE-PTãªã©ï¼‰ã¯ã€ä¸€èˆ¬ã«FPMCã€GRU4Recã€Caserã‚’å¤§ããªãƒãƒ¼ã‚¸ãƒ³ã‚’ã‚‚ã£ã¦ä¸Šå›ã‚‹çµæœã ã£ãŸ. **self-attentionå‹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒé€æ¬¡æ¨è–¦ã®ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦é•·è·é›¢é …ç›®ä¾å­˜æ€§ã‚’æ•æ‰ã™ã‚‹å„ªã‚ŒsãŸèƒ½åŠ›ã‚’æŒã¤ã“ã¨ãŒæ¤œè¨¼ã•ã‚ŒãŸ**.

- Comparing the original SASRec and its variants BERT4Rec, TiSASRec and SSE-PT, we find that the self-attentive models can gets benefit from incorporating additional information such as bi-directional attentions, time intervals, and user personalization. Such auxiliary information is important to interpret the dynamic behaviors of users. ã‚ªãƒªã‚¸ãƒŠãƒ«ã®SASRecã¨ãã®å¤‰ç¨®ã§ã‚ã‚‹BERT4Recã€TiSASRecã€SSE-PTã‚’æ¯”è¼ƒã™ã‚‹ã¨ã€**self-attentionsãƒ¢ãƒ‡ãƒ«ã¯ã€bi-directional attentionsã€time intervalsã€user personalizationãªã©ã®è¿½åŠ æƒ…å ±ã‚’å–ã‚Šå…¥ã‚Œã‚‹ã“ã¨ã§åˆ©ç›Šã‚’å¾—ã‚‹**ã“ã¨ãŒã§ãã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚ ã“ã®ã‚ˆã†ãªè£œåŠ©æƒ…å ±ã¯ã€ãƒ¦ãƒ¼ã‚¶ã®ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ãªè¡Œå‹•ã‚’è§£é‡ˆã™ã‚‹ãŸã‚ã«é‡è¦ã§ã‚ã‚‹.

- The relative improvements of Rec-denoisers over their backbones are significant for all cases. For example, SASRec+Denoiser has on average 8.04% improvement with respect to Hit@10 and over 12.42% improvements with respect to NDCG@10. Analogously, BERT4Rec+Denoiser outperforms the vanilla BERT4Rec by average 7.47% in Hit@10 and 11.64% in NDCG@10. We also conduct the significant test between Rec-denoisers and their backbones, where all ğ‘-values< 1ğ‘’ âˆ’6 , showing that the improvements of Rec-denoisers are statistically significant in all cases. **Rec-denoisersã®ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã«å¯¾ã™ã‚‹ç›¸å¯¾çš„ãªå‘ä¸Šã¯ã€ã™ã¹ã¦ã®ã‚±ãƒ¼ã‚¹ã§é¡•è‘—ã§ã‚ã‚‹**ã€‚ ä¾‹ãˆã°ã€SASRec+Denoiserã¯ã€Hit@10ã«å¯¾ã—ã¦å¹³å‡8.04%ã€NDCG@10ã«å¯¾ã—ã¦å¹³å‡12.42%ä»¥ä¸Šã®æ”¹å–„ãŒè¦‹ã‚‰ã‚Œã‚‹ã€‚ åŒæ§˜ã«ã€BERT4Rec+Denoiserã¯ã€Hit@10ã§ã¯å¹³å‡7.47%ã€NDCG@10ã§ã¯å¹³å‡11.64%ã§ã€ãƒãƒ‹ãƒ©BERT4Recã‚’ä¸Šå›ã£ã¦ã„ã‚‹ã€‚ ã¾ãŸã€Rec-denoiserã¨ãã®ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã®é–“ã®æœ‰æ„å·®æ¤œå®šã‚‚è¡Œã£ãŸã€‚ã“ã“ã§ã¯ã€ã™ã¹ã¦ã® p < 1ğ‘’ -6ã§ã‚ã‚Šã€Rec-denoiserã®æ”¹å–„ãŒã™ã¹ã¦ã®ã‚±ãƒ¼ã‚¹ã§çµ±è¨ˆçš„ã«æœ‰æ„ã§ã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸ.

These improvements of our proposed models are mainly attributed to the following reasons: 1) Rec-denoisers inherit full advantages of the self-attention networks as in SASRec, BERT4Rec, TiSASRec, and SSE-PT; 2) Through differentiable masks, irrelevant item-item dependencies are removed, which could largely reduce the negative impacts of noisy data; 3) Jacobian regularization enforces the smoothness of gradients, limiting quick changes of the output against input perturbations.
ææ¡ˆãƒ¢ãƒ‡ãƒ«ã®ã“ã‚Œã‚‰ã®æ”¹å–„ã¯ä¸»ã«ä»¥ä¸‹ã®ç†ç”±ã«ã‚ˆã‚‹ï¼š

- 1ï¼‰Rec-denoisersã¯ã€SASRecã€BERT4Recã€TiSASRecã€SSE-PTã®ã‚ˆã†ãª**self-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åˆ©ç‚¹ã‚’å®Œå…¨ã«å—ã‘ç¶™ã„ã§ã„ã‚‹** (ã ã‹ã‚‰æ€§èƒ½ãŒä¸‹ãŒã‚‹ã¯ãšãŒãªã„ã‹:thinking:)
- 2ï¼‰å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’é€šã—ã¦ã€ç„¡é–¢ä¿‚ãªitem-itemä¾å­˜æ€§ãŒé™¤å»ã•ã‚Œã€ãƒã‚¤ã‚ºã®å¤šã„ãƒ‡ãƒ¼ã‚¿ã®æ‚ªå½±éŸ¿ã‚’å¤§å¹…ã«è»½æ¸›ã§ãã‚‹
- 3ï¼‰Jacobianæ­£å‰‡åŒ–ã¯å‹¾é…ã®æ»‘ã‚‰ã‹ã•ã‚’å¼·åˆ¶ã—ã€å…¥åŠ›æ‘‚å‹•ã«å¯¾ã™ã‚‹å‡ºåŠ›ã®ç´ æ—©ã„å¤‰åŒ–ã‚’åˆ¶é™ã™ã‚‹ã€‚(Jacobianæ­£å‰‡åŒ–ã‚’å°å…¥ã—ãªã„ã¨ã€å‹¾é…ãŒæ€¥ã«ãªã‚‹ã£ã¦ã“ã¨ã ã‚ã†ã‹??)In general, smoothness improves the generalization of sequential recommendation.ä¸€èˆ¬çš„ã«ã€æ»‘ã‚‰ã‹ã•ã¯é€æ¬¡æ¨è–¦ã®ä¸€èˆ¬æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚

Overall, the experimental results demonstrate the superiority of our Rec-Denoisers.
å…¨ä½“ã¨ã—ã¦ã€å®Ÿé¨“çµæœã¯æˆ‘ã€…ã®Rec-Denoisersã®å„ªä½æ€§ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

## 5.3. Robustness to Noises(RQ2)

As discussed before, the observed item sequences often contain some noisy items that are uncorrelated to each other.
å‰è¿°ã—ãŸã‚ˆã†ã«ã€è¦³æ¸¬ã•ã‚ŒãŸitem sequence ã«ã¯ã€ã—ã°ã—ã°**äº’ã„ã«ç„¡ç›¸é–¢ãªãƒã‚¤ã‚ºitem**ãŒå«ã¾ã‚Œã‚‹.(=çµ¶å¯¾çš„ã«noisyãªitemã¨ã„ã†ã‚ˆã‚Šã¯ã€ç›¸å¯¾çš„ã«noisyãªitemã®æ„å‘³åˆã„:thiking:)
Generally, the performance of self-attention networks is sensitive to noisy input.
ä¸€èˆ¬ã«ã€self-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ€§èƒ½ã¯ãƒã‚¤ã‚ºã®å¤šã„å…¥åŠ›ã«æ•æ„Ÿã§ã‚ã‚‹ã€‚
Here we analyze how robust our training strategy is for noisy sequences.
ã“ã“ã§ã¯ã€**ãƒã‚¤ã‚ºã®å¤šã„sequenceã«å¯¾ã—ã¦ã€æˆ‘ã€…ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ãŒã©ã®ç¨‹åº¦ãƒ­ãƒã‚¹ãƒˆã§ã‚ã‚‹ã‹**ã‚’åˆ†æã™ã‚‹ã€‚
To achieve this, we follow the strategy [35] that corrupts the training data by randomly replacing a portion of the observed items in the training set with uniformly sampled items that are not in the validation or test set.
ã“ã‚Œã‚’é”æˆã™ã‚‹ãŸã‚ã«ã€æˆ‘ã€…ã¯ã€**ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã§è¦³æ¸¬ã•ã‚ŒãŸitemã®ä¸€éƒ¨ã‚’ã€æ¤œè¨¼ã‚»ãƒƒãƒˆã‚„ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«ã¯ãªã„ä¸€æ§˜ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸitemã§ãƒ©ãƒ³ãƒ€ãƒ ã«ç½®ãæ›ãˆã‚‹**ã“ã¨ã«ã‚ˆã£ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ç ´æã•ã›ã‚‹æˆ¦ç•¥[35]ã«å¾“ã†.
We range the ratio of the corrupted training data from 0% to 25%.
**ç ´æã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®æ¯”ç‡ã¯ã€0%ã‹ã‚‰25%ã®ç¯„å›²**ã§ã‚ã‚‹.
We only report the results of SASRec and SASRec-Denoiser in terms of Hit@10.
SASRecã¨SASRec-Denoiserã®çµæœã¯ã€Hit@10ã§ã®ã¿å ±å‘Šã™ã‚‹ã€‚
The performance of other self-attentive models is the same and omitted here due to page limitations.
ä»–ã®self-attentionãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚‚åŒæ§˜ã§ã€ã“ã“ã§ã¯ãƒšãƒ¼ã‚¸ã®éƒ½åˆä¸Šçœç•¥ã—ãŸã€‚
In addition, we compare with two recent sparse Transformers: Sparse Transformer [10] and ğ›¼-entmax sparse attention [12].
ã•ã‚‰ã«ã€æœ€è¿‘ã®2ã¤ã®ã‚¹ãƒ‘ãƒ¼ã‚¹å¤‰æ›å™¨ã¨æ¯”è¼ƒã™ã‚‹ï¼š Sparse Transformer [10]ã¨ â†ªL_1D6FCâ†©-entmax sparse attention [12]ã§ã‚ã‚‹.

![figure2]()

All the simulated experiments are repeated five times and the average results are shown in Figure 2.
ã™ã¹ã¦ã®æ¨¡æ“¬å®Ÿé¨“ã‚’5å›ç¹°ã‚Šè¿”ã—ã€ãã®å¹³å‡çµæœã‚’å›³2ã«ç¤ºã™ã€‚
Clearly, the performance of all models degrades with the increasing noise ratio.
æ˜ã‚‰ã‹ã«ã€**ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯ã€ãƒã‚¤ã‚ºæ¯”ç‡ã®å¢—åŠ ã¨ã¨ã‚‚ã«ä½ä¸‹ã™ã‚‹**ã€‚
We observe that our Rec-denoiser (use either ARM or AR estimators) consistently outperforms ğ›¼-entmax and Sparse Transformer under different ratios of noise on all datasets.
æˆ‘ã€…ã¯ã€æˆ‘ã€…ã®**Rec-denoiser(ARMã¾ãŸã¯ARæ¨å®šé‡ã‚’ä½¿ç”¨)ãŒã€å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã„ã¦ã€ç•°ãªã‚‹ãƒã‚¤ã‚ºæ¯”ç‡ã®ä¸‹ã§ä¸€è²«ã—ã¦ $\alpha$-entmaxã¨Sparse Transformerã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’è¦³å¯Ÿã—ãŸ**ã€‚
ğ›¼-entmax heavily relies on one trainable parameter ğ›¼ to filter out the noise, which may be over tuned during the training, while Sparse Transformer adopts a fixed attention pattern, which may lead to uncertain results, especially for short item sequences like Beauty and Games.
ğ›¼-entmax ã¯ã€ãƒã‚¤ã‚ºã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ãŸã‚ã®1ã¤ã®è¨“ç·´å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ğ›¼ã«å¤§ããä¾å­˜ã—ã¦ãŠã‚Šã€è¨“ç·´ä¸­ã«éå‰°ã«èª¿æ•´ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™. ä¸€æ–¹ã€Sparse Transformerã¯å›ºå®šã•ã‚ŒãŸattenitionãƒ‘ã‚¿ãƒ¼ãƒ³(maskãƒ‘ã‚¿ãƒ¼ãƒ³)ã‚’æ¡ç”¨ã—ã¦ãŠã‚Šã€ç‰¹ã«Beautyã‚„Gamesã®ã‚ˆã†ãªçŸ­ã„ã‚¢ã‚¤ãƒ†ãƒ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§ã¯ã€ä¸ç¢ºå®Ÿãªçµæœã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
In contrast, our differentaible masks have much more flexibility to adapt to noisy sequences.
å¯¾ç…§çš„ã«ã€æˆ‘ã€…ã®**å¾®åˆ†å¯èƒ½ãƒã‚¹ã‚¯ã¯ã€ãƒã‚¤ã‚ºã®å¤šã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«é©å¿œã™ã‚‹æŸ”è»Ÿæ€§ãŒã¯ã‚‹ã‹ã«é«˜ã„**ã€‚
The Jacobian regularization further encourages the smoothness of our gradients, leading to better generalization.
**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–ã¯ã€å‹¾é…ã®æ»‘ã‚‰ã‹ã•ã‚’ã•ã‚‰ã«ä¿ƒé€²ã—**ã€ã‚ˆã‚Šè‰¯ã„æ±åŒ–ã‚’ã‚‚ãŸã‚‰ã™ã€‚
From the results, the AR estimator performs better than ğ›¼-entmax but worse than ARM.
ãã®çµæœã€ARæ¨å®šå™¨ã¯ğ›¼-entmaxã‚ˆã‚Šã¯è‰¯ã„ãŒã€ARMã‚ˆã‚Šã¯æ‚ªã„ã€‚
This result is expected since ARM has much low variance.
**ARMã¯åˆ†æ•£ãŒå°‘ãªã„ã®ã§ã€ã“ã®çµæœã¯äºˆæƒ³é€šã‚Š**ã§ã‚ã‚‹ã€‚
In summary, both ARM and AR estimators are able to reduce the negative impacts of noisy sequences, which could improve the robustness of self-attentive models.
ã¾ã¨ã‚ã‚‹ã¨ã€ARMã¨ARã®ä¸¡æ¨å®šå™¨ã¯ã€ãƒã‚¤ã‚ºã®å¤šã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æ‚ªå½±éŸ¿ã‚’è»½æ¸›ã™ã‚‹ã“ã¨ãŒã§ãã€self-attentionãƒ¢ãƒ‡ãƒ«ã®é ‘å¥æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

## 5.4. Study of Rec-Denoiser(RQ3)

We further investigate the parameter sensitivity of Rec-Denoiser.
ã•ã‚‰ã«**Rec-Denoiserã®parameter sensitivity**ã‚’èª¿ã¹ãŸ.
For the number of blocks $L$ and the number of heads $H$, we find that self-attentive models typically benefit from small values (e.g., $H, L \leq 4$), which is similar to [31, 41].
**ãƒ–ãƒ­ãƒƒã‚¯æ•° $L$ ã¨ãƒ˜ãƒƒãƒ‰æ•° $H$ ã«ã¤ã„ã¦ã¯ã€self-attentionãƒ¢ãƒ‡ãƒ«ã¯ä¸€èˆ¬çš„ã«å°ã•ãªå€¤ï¼ˆä¾‹ãˆã°ã€$H, L \leq 4$ï¼‰ãŒæœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Š**ã€ã“ã‚Œã¯[31, 41]ã¨åŒæ§˜ã§ã‚ã‚‹ã€‚
In this section, we mainly study the following hyper-parameters: 1) the maximum length ğ‘›, 2) the regularizers ğ›½ and ğ›¾ to control the sparsity and smoothness.
æœ¬ç¯€ã§ã¯ã€ä¸»ã«ä»¥ä¸‹ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç ”ç©¶ã™ã‚‹ï¼š 1)sequenceæœ€å¤§é•·$n$ã€2)ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã¨å¹³æ»‘æ€§ã‚’åˆ¶å¾¡ã™ã‚‹æ­£å‰‡åŒ–å­$\beta$ã¨$\gamma$.
Here we only study the SASRec and SASRec-Denoiser due to page limitations.
ã“ã“ã§ã¯ã€ãƒšãƒ¼ã‚¸ã®éƒ½åˆä¸Šã€SASRecã¨SASRec-Denoiserã«ã¤ã„ã¦ã®ã¿è¿°ã¹ã‚‹ã€‚

![figure3]()

Fig.3.Effect of maximum length ğ‘› on ranking performance (Hit@10).
å›³3.æœ€å¤§é•·ğ‘›ãŒãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿ï¼ˆHit@10ï¼‰.

![figure4]()

Fig.4.Effect of regularizers ğ›½ and ğ›¾ on ranking performance (Hit@10).
å›³4.æ­£å‰‡åŒ–é‡Ç½ã¨Ç–ãŒãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿(Hit@10FE6)ã€‚

### 5.4.1. Maximum Length $n$ 5.4.1. sequence ã® æœ€å¤§é•· $n$

Figure 3 shows the Hit@10 for maximum length ğ‘› from 20 to 80 while keeping other optimal hyper-parameters unchanged.
å›³3ã¯ã€ä»–ã®æœ€é©ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰ãˆãšã«ã€æœ€å¤§é•·$n$ã‚’20ã‹ã‚‰80ã¾ã§å¤‰åŒ–ã•ã›ãŸå ´åˆã®Hit@10ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
We only test on the densest and sparsest datasets: MovieLeans and Beauty.
æœ€ã‚‚é«˜å¯†åº¦ã§ç–ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®ã¿ãƒ†ã‚¹ãƒˆã™ã‚‹ï¼š MovieLeansã¨Beautyã§ã‚ã‚‹ã€‚
Intuitively, the larger sequence we have, the larger probability that the sequence contains noisy items.
ç›´è¦³çš„ã«ã¯ã€**ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒå¤§ãã‘ã‚Œã°å¤§ãã„ã»ã©ã€ãã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ãƒã‚¤ã‚ºã®ã‚ã‚‹itemãŒå«ã¾ã‚Œã‚‹ç¢ºç‡ãŒé«˜ããªã‚‹**ã€‚
We observed that our SASRec-Denoiser improves the performance dramatically with longer sequences.
**æˆ‘ã€…ã®SASRec-Denoiserã¯ã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§åŠ‡çš„ã«æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹**ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚
This demonstrates that our design is more suitable for longer inputs, without worrying about the quality of sequences.
ã“ã‚Œã¯ã€æˆ‘ã€…ã®è¨­è¨ˆãŒã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®è³ªã‚’æ°—ã«ã™ã‚‹ã“ã¨ãªãã€ã‚ˆã‚Šé•·ã„å…¥åŠ›ã«é©ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

### 5.4.2. The regularizers $\beta$ and $\gamma$ 5.4.2. æ­£å‰‡åŒ–è¨˜å·$beta$ã¨$gamma$ã€‚

There are two major regularization parameters ğ›½ and ğ›¾ for sparsity and gradient smoothness, respectively.
æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Ç½ã¨ğ›¾ã¯ã€ãã‚Œãã‚Œã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã¨å‹¾é…å¹³æ»‘æ€§ã‚’è¡¨ã™ã€‚
Figure 4 shows the performance by changing one parameter while fixing the other as 0.01.
å›³4ã¯ã€ä¸€æ–¹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’0.01ã«å›ºå®šã—ã€ã‚‚ã†ä¸€æ–¹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’å¤‰æ›´ã—ãŸå ´åˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã¦ã„ã‚‹
As can be seen, our performance is relatively stable with respect to different settings.
è¦‹ã¦ã‚ã‹ã‚‹ã‚ˆã†ã«ã€**æˆ‘ã€…ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ç•°ãªã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã«å¯¾ã—ã¦æ¯”è¼ƒçš„å®‰å®šã—ã¦ã„ã‚‹**
In the experiments, the best performance can be achieved at ğ›½ = 0.01 and ğ›¾ = 0.001 for the MovieLens dataset.
å®Ÿé¨“ã§ã¯ã€MovieLensãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ã€â†ªLl_1D6FD = 0.01ã¨Ç– = 0.001ã§æœ€é«˜ã®æ€§èƒ½ã‚’é”æˆã™ã‚‹ã“ã¨ãŒã§ããŸã€‚

# 6. Conclusion and Fture Work 6. çµè«–ã¨ä»Šå¾Œã®èª²é¡Œ

In this work, we propose Rec-Denoiser to adaptively eliminate the negative impacts of the noisy items for self-attentive recommender systems.
æœ¬ç ”ç©¶ã§ã¯ã€Rec-Denoiserã‚’ææ¡ˆã—ã€self-attentionå‹ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹ãƒã‚¤ã‚ºã‚¢ã‚¤ãƒ†ãƒ ã®æ‚ªå½±éŸ¿ã‚’é©å¿œçš„ã«é™¤å»ã™ã‚‹ã€‚
The proposed Rec-Denoiser employs differentiable masks for the self-attention layers, which can dynamically prune irrelevant information.
ææ¡ˆã™ã‚‹Rec-Denoiserã¯ã€self-attentionå±¤ã«å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’æ¡ç”¨ã—ã€ç„¡é–¢ä¿‚ãªæƒ…å ±ã‚’å‹•çš„ã«é™¤å»ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
To further tackle the vulnerability of self-attention networks to small perturbations, Jacobian regularization is applied to the Transformer blocks to improve the robustness.
å°ã•ãªæ‘‚å‹•ã«å¯¾ã™ã‚‹self-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è„†å¼±æ€§ã«ã•ã‚‰ã«å–ã‚Šçµ„ã‚€ãŸã‚ã€ãƒ­ãƒã‚¹ãƒˆæ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–ãŒãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ»ãƒ–ãƒ­ãƒƒã‚¯ã«é©ç”¨ã•ã‚Œã‚‹.
Our experimental results on multiple real-world sequential recommendation tasks illustrate the effectiveness of our design.
å®Ÿä¸–ç•Œã®è¤‡æ•°ã®é€æ¬¡æ¨è–¦ã‚¿ã‚¹ã‚¯ã«é–¢ã™ã‚‹å®Ÿé¨“çµæœã¯ã€æˆ‘ã€…ã®è¨­è¨ˆã®æœ‰åŠ¹æ€§ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

Our proposed Rec-Denoiser framework (e.g., differentiable masks and Jacobian regularization) can be easily applied to any Transformer-based models in many tasks besides sequential recommendation.
**æˆ‘ã€…ã®ææ¡ˆã™ã‚‹Rec-Denoiserãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆå¾®åˆ†å¯èƒ½ãƒã‚¹ã‚¯ã‚„ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–ãªã©ï¼‰ã¯ã€é€æ¬¡æ¨è–¦ä»¥å¤–ã®å¤šãã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€ã‚ã‚‰ã‚†ã‚‹Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã«å®¹æ˜“ã«é©ç”¨**ã§ãã‚‹.(æ¨è–¦ä»¥å¤–ã«ã‚‚ã€Transformerã‚’ç”¨ã„ã‚‹æ±ã‚†ã‚‹ã‚¿ã‚¹ã‚¯?? next-token predictionã¨ã‹ã«ã‚‚??:thinking:)
In the future, we will continue to dmonstrate the contributions of our design in many real-world applications.
å°†æ¥çš„ã«ã¯ã€å¤šãã®å®Ÿä¸–ç•Œã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ãŠã„ã¦ã€æˆ‘ã€…ã®è¨­è¨ˆã®è²¢çŒ®ã‚’å®Ÿè¨¼ã—ã¦ã„ãã¤ã‚‚ã‚Šã§ã‚ã‚‹ã€‚
