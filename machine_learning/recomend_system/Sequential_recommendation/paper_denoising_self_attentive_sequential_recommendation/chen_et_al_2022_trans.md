## 0.1. link 0.1. ãƒªãƒ³ã‚¯

- https://dl.acm.org/doi/abs/10.1145/3523227.3546788 httpsã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

- https://arxiv.org/pdf/2212.04120.pdf httpsã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

## 0.2. title 0.2. ã‚¿ã‚¤ãƒˆãƒ«

Denoising Self-Attentive Sequential Recommendation
ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°è‡ªå·±èª¿æ•´å‹é€æ¬¡æ¨è–¦æ³•

## 0.3. abstract 0.3. æŠ½è±¡çš„

Transformer-based sequential recommenders are very powerful for capturing both short-term and long-term sequential item dependencies.
ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã«åŸºã¥ãsequentialæ¨è–¦å™¨ã¯ã€çŸ­æœŸçš„ãŠã‚ˆã³é•·æœŸçš„ãª**sequentialãªã‚¢ã‚¤ãƒ†ãƒ é–“ã®ä¾å­˜é–¢ä¿‚**ã‚’æ‰ãˆã‚‹ã®ã«éå¸¸ã«å¼·åŠ›ã§ã‚ã‚‹.
This is mainly attributed to their unique self-attention networks to exploit pairwise item-item interactions within the sequence.
ã“ã‚Œã¯ä¸»ã«ã€sequenceå†…ã®ãƒšã‚¢ãƒ¯ã‚¤ã‚ºã‚¢ã‚¤ãƒ†ãƒ -ã‚¢ã‚¤ãƒ†ãƒ ç›¸äº’ä½œç”¨ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã®ã€ç‹¬è‡ªã®self-attention ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«èµ·å› ã—ã¦ã„ã‚‹.
However, real-world item sequences are often noisy, which is particularly true for implicit feedback.
ã—ã‹ã—ã€**å®Ÿä¸–ç•Œã®item sequencesã¯ã—ã°ã—ã°ãƒã‚¤ã‚ºãŒå¤šãã€ç‰¹ã«æš—é»™çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«ã¯ãã‚ŒãŒå½“ã¦ã¯ã¾ã‚‹**ã€‚
For example, a large portion of clicks do not align well with user preferences, and many products end up with negative reviews or being returned.
ä¾‹ãˆã°ã€**ã‚¯ãƒªãƒƒã‚¯ã®å¤§éƒ¨åˆ†ã¯ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã«åˆã‚ãªã„ã—**ã€å¤šãã®è³¼å…¥ã—ãŸè£½å“ã¯å¦å®šçš„ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ãŸã‚Šã€è¿”å“ã•ã‚Œã‚‹ã“ã¨ã«ãªã‚‹.
As such, the current user action only depends on a subset of items, not on the entire sequences.
ã“ã®ã‚ˆã†ã«ã€ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã§ã¯ãªãã€ã‚¢ã‚¤ãƒ†ãƒ ã®subset(éƒ¨åˆ†é›†åˆ)ã«ã®ã¿ä¾å­˜ã™ã‚‹ã€‚
Many existing Transformer-based models use full attention distributions, which inevitably assign certain credits to irrelevant items.
æ—¢å­˜ã®Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã®å¤šãã¯ã€å®Œå…¨ãªattentionåˆ†å¸ƒã‚’ä½¿ç”¨ã—ã¦ãŠã‚Šã€å¿…ç„¶çš„ã«ç„¡é–¢ä¿‚ãªã‚¢ã‚¤ãƒ†ãƒ ã«ä¸€å®šã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã«ãªã‚‹ã€‚
This may lead to sub-optimal performance if Transformers are not regularized properly.
ã“ã‚Œã¯TransformerãŒé©åˆ‡ã«æ­£å‰‡åŒ–ã•ã‚Œãªã„å ´åˆã€æœ€é©ã§ãªã„æ€§èƒ½ã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹.

Here we propose the Rec-denoiser model for better training of self-attentive recommender systems.
æœ¬è«–æ–‡ã§ã¯ã€è‡ªå·±èª¿æ•´å‹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ã‚ˆã‚Šè‰¯ã„å­¦ç¿’ã®ãŸã‚ã«ã€**Rec-denoiserãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆ**ã™ã‚‹ã€‚
In Rec-denoiser, we aim to adaptively prune noisy items that are unrelated to the next item prediction.
Rec-denoiserã§ã¯ã€**next item prediction**ã«é–¢ä¿‚ã®ãªã„ãƒã‚¤ã‚ºã®å¤šã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’é©å¿œçš„ã«åˆˆã‚Šå–ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã™ã‚‹.(ãã£ã‹ã€NLPã®next-token predictionã¿ãŸã„ãªæ„Ÿã˜...!)
To achieve this, we simply attach each self-attention layer with a trainable binary mask to prune noisy attentions, resulting in sparse and clean attention distributions.
ãã®ãŸã‚ã€**å„self-attentionå±¤ã«å­¦ç¿’å¯èƒ½ãªbinary maskã‚’ä»˜åŠ **ã—ã€ãƒã‚¤ã‚ºã®å¤šã„attentionã‚’é™¤å»ã™ã‚‹ã“ã¨ã§ã€ã‚¹ãƒ‘ãƒ¼ã‚¹ã§ã‚¯ãƒªãƒ¼ãƒ³ãªattentionåˆ†å¸ƒãŒå¾—ã‚‰ã‚Œã‚‹ã€‚
This largely purifies item-item dependencies and provides better model interpretability.
ã“ã‚Œã«ã‚ˆã‚Šã€item-itemã®ä¾å­˜æ€§(ã®å¤šã??)ãŒã»ã¼é™¤å»ã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆå¯èƒ½æ€§ãŒå‘ä¸Šã™ã‚‹ã€‚
In addition, the self-attention network is typically not Lipschitz continuous and is vulnerable to small perturbations.
ã•ã‚‰ã«ã€self-attentionãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ä¸€èˆ¬çš„ã«ãƒªãƒ—ã‚·ãƒƒãƒ„é€£ç¶šã§ã¯ãªãã€å°ã•ãªæ‘‚å‹•ã«å¼±ã„.(ã“ã®ã¸ã‚“ã‚ˆãã‚ã‹ã‚‰ã‚“...!)
Jacobian regularization is further applied to the Transformer blocks to improve the robustness of Transformers for noisy sequences.
**ã•ã‚‰ã«ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–ã‚’Transformerãƒ–ãƒ­ãƒƒã‚¯ã«é©ç”¨ã—ã€ãƒã‚¤ã‚ºã®å¤šã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã™ã‚‹Transformerã®é ‘å¥æ€§ã‚’å‘ä¸Šã•ã›ã‚‹**ã€‚
Our Rec-denoiser is a general plugin that is compatible to many Transformers.
**æˆ‘ã€…ã®Rec-denoiserã¯å¤šãã®Transformerã«å¯¾å¿œã™ã‚‹æ±ç”¨ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã§ã‚ã‚‹**. (general pluginã€ç´ æ™´ã‚‰ã—ã„...!!)
Quantitative results on real-world datasets show that our Rec-denoiser outperforms the state-of-the-art baselines.
å®Ÿä¸–ç•Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã‘ã‚‹å®šé‡çš„ãªçµæœã¯ã€æˆ‘ã€…ã®Rec-denoiserãŒæœ€å…ˆç«¯ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’å‡Œé§•ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹.

# 1. introduction 1.ã¯ã˜ã‚ã«

Sequential recommendation aims to recommend the next item based on a userâ€™s historical actions [20, 35, 39, 44, 47], e.g., to recommend a bluetooth headphone after a user purchases a smart phone.
**Sequential recommendationã®ç›®çš„ã¯ï¼Œãƒ¦ãƒ¼ã‚¶ã®éå»ã®è¡Œå‹•ã«åŸºã¥ã„ã¦æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ¨è–¦ã™ã‚‹ã“ã¨(next-item-prediction)**ã§ã‚ã‚‹[20, 35, 39, 44, 47]ï¼ä¾‹ãˆã°ï¼Œ**ãƒ¦ãƒ¼ã‚¶ãŒã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã‚’è³¼å…¥ã—ãŸå¾Œã«Bluetoothãƒ˜ãƒƒãƒ‰ãƒ›ãƒ³ã‚’æ¨è–¦ã™ã‚‹ã‚ˆã†ãªå ´åˆ**ã§ã‚ã‚‹.
Learning sequential user behaviors is, however, challenging since a userâ€™s choices on items generally depend on both long-term and short-term preferences.
ã—ã‹ã—ï¼Œä¸€èˆ¬ã«ãƒ¦ãƒ¼ã‚¶ã®ã‚¢ã‚¤ãƒ†ãƒ é¸æŠã¯**é•·æœŸçš„å—œå¥½(long-term preference)**ã¨**çŸ­æœŸçš„å—œå¥½(short-term preference)**ã®ä¸¡æ–¹ã«ä¾å­˜ã™ã‚‹ãŸã‚ï¼Œé€æ¬¡çš„(sequential)ãªãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã®å­¦ç¿’ã¯å›°é›£ã§ã‚ã‚‹.
Early Markov Chain models [19, 39] have been proposed to capture short-term item transitions by assuming that a userâ€™s next decision is derived from a few preceding actions, while neglecting long-term preferences.
åˆæœŸã®ãƒãƒ«ã‚³ãƒ•é€£é–ãƒ¢ãƒ‡ãƒ«[19, 39]ã¯ã€ãƒ¦ãƒ¼ã‚¶ã®æ¬¡ã®æ±ºå®šãŒã„ãã¤ã‹ã®å…ˆè¡Œè¡Œå‹•ã‹ã‚‰å°ã‹ã‚Œã‚‹ã¨ä»®å®šã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€çŸ­æœŸçš„ãªã‚¢ã‚¤ãƒ†ãƒ ã®é·ç§»ã‚’æ‰ãˆã‚‹ãŸã‚ã«ææ¡ˆã•ã‚ŒãŸãŒã€é•·æœŸçš„ãªå—œå¥½ã¯ç„¡è¦–ã•ã‚ŒãŸã¾ã¾ã§ã‚ã£ãŸ.
To alleviate this limitation, many deep neural networks have been proposed to model the entire usersâ€™ sequences and achieve great success, including recurrent neural networks [20, 53] and convolutional neural networks [42, 54, 57].
ã“ã®é™ç•Œã‚’ç·©å’Œã™ã‚‹ãŸã‚ã«ã€ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯[20, 53]ã‚„ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯[42, 54, 57]ãªã©ã€**ãƒ¦ãƒ¼ã‚¶ã®sequenceå…¨ä½“ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹å¤šãã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒææ¡ˆã•ã‚Œã€å¤§ããªæˆåŠŸã‚’åã‚ã¦ã„ã‚‹**.

Recently, Transformers have shown promising results in various tasks, such as machine translation [43].
æœ€è¿‘ã€Transformersã¯æ©Ÿæ¢°ç¿»è¨³ã®ã‚ˆã†ãªæ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã§æœ‰æœ›ãªçµæœã‚’ç¤ºã—ã¦ã„ã‚‹[43].
One key component of Transformers is the self-attention network, which is capable of learning long-range dependencies by computing attention weights between each pair of objects in a sequence.
Transformersã®ä¸»è¦ãªæ§‹æˆè¦ç´ ã®1ã¤ã¯self-attention networkã§ã‚ã‚Šã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸­ã®å„å¯¾è±¡é–“ã®attention weightã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã§**é•·è·é›¢ä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã‚‹**.
Inspired by the success of Transformers, several self-attentive sequential recommenders have been proposed and achieve the state-of-the-art performance [26, 41, 49, 50].
Transformersã®æˆåŠŸã«è§¦ç™ºã•ã‚Œã€ã„ãã¤ã‹ã®self-attentive sequential recommendersãŒææ¡ˆã•ã‚Œã€æœ€æ–°ã®æ€§èƒ½ã‚’é”æˆã—ã¦ã„ã‚‹[26, 41, 49, 50].
For example, SASRec [26] is the pioneering framework to adopt self-attention network to learn the importance of items at different positions.
ä¾‹ãˆã°ã€SASRec [26]ã¯ã€ç•°ãªã‚‹ä½ç½®ã«ã‚ã‚‹itemã®é‡è¦åº¦ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã€self-attention networkã‚’æ¡ç”¨ã—ãŸå…ˆé§†çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚‹.
BERT4Rec [41] further models the correlations of items from both left-to-right and right-to-left directions.
BERT4Rec [41]ã¯ã€ã•ã‚‰ã«å·¦ã‹ã‚‰å³ã€å³ã‹ã‚‰å·¦ã®ä¸¡æ–¹å‘ã®itemã®ç›¸é–¢ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹.
SSE-PT [50] is a personalized Transformer model that provides better interpretability of engagement patterns by introducing user embeddings.
SSE-PT [50]ã¯ã€user embeddingsã‚’å°å…¥ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®è§£é‡ˆå¯èƒ½æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºãƒ‰ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹.
LSAN [31] adopts a novel twin-attention sequential framework, which can capture both long-term and short-term user preference signals.
LSAN [31]ã¯æ–°ã—ã„twin-attention sequential frameworkã‚’æ¡ç”¨ã—ã€é•·æœŸã¨çŸ­æœŸã®ä¸¡æ–¹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å—œå¥½ã‚·ã‚°ãƒŠãƒ«ã‚’æ‰ãˆã‚‹ã“ã¨ãŒã§ãã‚‹.
Recently, Transformers4Rec [14] performs an empirical analysis with broad experiments of various Transformer architectures for the task of sequential recommendation.
æœ€è¿‘ã€Transformers4Rec [14]ã¯ã€é€æ¬¡æ¨è–¦ã®ã‚¿ã‚¹ã‚¯ã®ãŸã‚ã«ã€æ§˜ã€…ãªTransformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å¹…åºƒã„å®Ÿé¨“ã«ã‚ˆã‚‹å®Ÿè¨¼åˆ†æã‚’è¡Œã£ã¦ã„ã‚‹.

Although encouraging performance has been achieved, the robustness of sequential recommenders is far less studied in the literature.
ã—ã‹ã—ã€**é€æ¬¡æ¨è–¦å™¨ã®robustnessã«ã¤ã„ã¦ã¯ã‚ã¾ã‚Šç ”ç©¶ã•ã‚Œã¦ã„ãªã„**.
Many real-world item sequences are naturally noisy, containing both true-positive and false-positive interactions [6, 45, 46].
å®Ÿä¸–ç•Œã®å¤šãã®ã‚¢ã‚¤ãƒ†ãƒ åˆ—ã¯è‡ªç„¶ã«ãƒã‚¤ã‚ºãŒå¤šãã€çœŸé™½æ€§(true-positive)ã¨å½é™½æ€§(false-positive. ex. **å¥½ãã˜ã‚ƒãªã„ã‘ã©ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã—ã¾ã£ãŸ. è³¼å…¥ã—ã¦ã¿ãŸãŒå«Œã„ã ã£ãŸ...??**)ã®ä¸¡æ–¹ã®ç›¸äº’ä½œç”¨ã‚’å«ã‚“ã§ã„ã‚‹ [6, 45, 46].
For example, a large portion of clicks do not align well with user preferences, and many products end up with negative reviews or being returned.
ä¾‹ãˆã°ã€ã‚¯ãƒªãƒƒã‚¯ã®å¤§éƒ¨åˆ†ã¯ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã«åˆã‚ãšã€å¤šãã®è£½å“ã¯å¦å®šçš„ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã§çµ‚ã‚ã£ãŸã‚Šã€è¿”å“ã•ã‚ŒãŸã‚Šã™ã‚‹.
In addition, there is no any prior knowledge about how a userâ€™s historical actions should be generated in online systems.
ã¾ãŸã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ã®éå»ã®è¡Œå‹•ã‚’ã©ã®ã‚ˆã†ã«ç”Ÿæˆã™ã¹ãã‹ã¨ã„ã†äº‹å‰çŸ¥è­˜ã¯å­˜åœ¨ã—ãªã„.
Therefore, developing robust algorithms to defend noise is of great significance for sequential recommendation.
ãã®ãŸã‚ã€**ãƒã‚¤ã‚ºã«å¼·ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é–‹ç™ºã™ã‚‹ã“ã¨ã¯ã€é€æ¬¡æ¨è–¦ã«ãŠã„ã¦å¤§ããªæ„ç¾©ãŒã‚ã‚‹**.

Clearly, not every item in a sequence is aligned well with user preferences, especially for implicit feedbacks (e.g., clicks, views, etc.) [8].
ç‰¹ã«ã€æš—é»™ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼ˆã‚¯ãƒªãƒƒã‚¯ã€ãƒ“ãƒ¥ãƒ¼ãªã©ï¼‰ã®å ´åˆã€**sequenceå†…ã®ã™ã¹ã¦ã®itemãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å—œå¥½ã¨ã†ã¾ãæ•´åˆã—ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªã„ã“ã¨ã¯æ˜ã‚‰ã‹**ã§ã‚ã‚‹ [8].
Unfortunately, the vanilla self-attention network is not Lipschitz continuous1 , and is vulnerable to the quality of input sequences [28].
æ®‹å¿µãªãŒã‚‰ã€vanilla self-attention networkã¯**Lipschitzé€£ç¶šã§ã¯ãªã(?)**ã€**å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®è³ªã«å¼±ã„ã¨ã„ã†å•é¡Œ**ãŒã‚ã‚‹[28]ã€‚
Recently, in the tasks of language modeling, people found that a large amount of BERTâ€™s attentions focus on less meaningful tokens, like "[SEP]" and ".
æœ€è¿‘ã€è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€BERT ã®æ³¨æ„ã®å¤šããŒã€"[SEP]" ã‚„ "." ã®ã‚ˆã†ãªã‚ã¾ã‚Šæ„å‘³ã®ãªã„ãƒˆãƒ¼ã‚¯ãƒ³ã«é›†ä¸­ã™ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ãŸ.
", which leads to a misleading explanation [11].
ã€Œã®ã‚ˆã†ãªã€ã‚ã¾ã‚Šæ„å‘³ã®ãªã„ãƒˆãƒ¼ã‚¯ãƒ³ã« BERT ã®æ³¨æ„ãŒé›†ä¸­ã—ã€èª¤è§£ã‚’æ‹›ãèª¬æ˜ã«ãªã£ã¦ã„ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ã¦ã„ã‚‹[11]ã€‚
It is thus likely to obtain sub-optimal performance if self-attention networks are not well regularized for noisy sequences.
ã“ã®ã‚ˆã†ã«ã€è‡ªå·±æ³¨æ„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒãƒã‚¤ã‚ºã®å¤šã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã—ã¦ã†ã¾ãæ­£å‰‡åŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã€æœ€é©ã¨ã¯è¨€ãˆãªã„æ€§èƒ½ãŒå¾—ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹.
We use the following example to further explain above concerns.
ä»¥ä¸‹ã®ä¾‹ã‚’ç”¨ã„ã¦ã€ä¸Šè¨˜ã®æ‡¸å¿µã«ã¤ã„ã¦ã•ã‚‰ã«èª¬æ˜ã™ã‚‹.

Figure 1 illustrates an example of left-to-right sequential recommendation where a userâ€™s sequence contains some noisy or irrelevant items.
å›³1ã¯ã€å·¦ã‹ã‚‰å³ã¸ã®sequential recommendationã®ä¸€ä¾‹ã§ã‚ã‚‹.
For example, a father may interchangeably purchase (phone, headphone, laptop) for his son, and (bag, pant) for his daughter, resulting in a sequence: (phone, bag, headphone, pant, laptop).
ä¾‹ãˆã°ã€ã‚ã‚‹çˆ¶è¦ªãŒæ¯å­ã«ï¼ˆæºå¸¯é›»è©±ã€ãƒ˜ãƒƒãƒ‰ãƒ•ã‚©ãƒ³ã€ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ï¼‰ã€å¨˜ã«ï¼ˆã‚«ãƒãƒ³ã€ã‚ºãƒœãƒ³ï¼‰ã‚’è³¼å…¥ã™ã‚‹å ´åˆã€ï¼ˆæºå¸¯é›»è©±ã€ã‚«ãƒãƒ³ã€ãƒ˜ãƒƒãƒ‰ãƒ•ã‚©ãƒ³ã€ã‚ºãƒœãƒ³ã€ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ï¼‰ã¨ã„ã†é †åºã«ãªã‚‹.
In the setting of sequential recommendation, we intend to infer the next item, e.g., laptop, based on the userâ€™s previous actions, e.g., (phone, bag, headphone, pant).
é€æ¬¡æ¨è–¦ã®è¨­å®šã§ã¯ã€**ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä»¥å‰ã®è¡Œå‹•ã€ä¾‹ãˆã°ï¼ˆé›»è©±ã€ã‚«ãƒãƒ³ã€ãƒ˜ãƒƒãƒ‰ãƒ›ãƒ³ã€ã‚ºãƒœãƒ³ï¼‰ã‹ã‚‰ã€æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã€ä¾‹ãˆã°ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ã‚’æ¨è«–ã™ã‚‹ã“ã¨**ã‚’æ„å›³ã—ã¦ã„ã‚‹.
However, the correlations among items are unclear, and intuitively pant and laptop are neither complementary nor compatible to each other, which makes the prediction untrustworthy.
ã—ã‹ã—ã€ã‚¢ã‚¤ãƒ†ãƒ é–“ã®ç›¸é–¢ãŒä¸æ˜ç¢ºã§ã‚ã‚Šã€**ç›´æ„Ÿçš„ã«pantã¨laptopã¯è£œå®Œé–¢ä¿‚ã«ã‚‚ç›¸å®¹ã‚Œãªã„**ãŸã‚ã€ã“ã®äºˆæ¸¬ã¯ä¿¡é ¼ã§ããªã„.
A trustworthy model should be able to only capture correlated items while ignoring these irrelevant items within sequences.
**ä¿¡é ¼ã§ãã‚‹ãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ã“ã‚Œã‚‰ã®ç„¡é–¢ä¿‚ãªã‚¢ã‚¤ãƒ†ãƒ ã‚’ç„¡è¦–ã—ã€ç›¸é–¢ã®ã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®ã¿ã‚’æ‰ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã¯ãš**ã§ã‚ã‚‹.
Existing self-attentive sequential models (e.g., SASRec [26] and BERT4Rec [41]) are insufficient to address noisy items within sequences.
æ—¢å­˜ã®self-attentive sequential modelï¼ˆä¾‹ãˆã°ã€SASRec [26]ã‚„BERT4Rec [41]ï¼‰ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ãƒã‚¤ã‚ºã®å¤šã„ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾å‡¦ã™ã‚‹ã«ã¯ä¸ååˆ†ã§ã‚ã‚‹.
The reason is that their full attention distributions are dense and would assign certain credits to all items, including irrelevant items.
ãã®ç†ç”±ã¯ã€ãã‚Œã‚‰ã®full attention distributionsãŒå¯†ã§ã‚ã‚Šã€ç„¡é–¢ä¿‚ãªitemã‚’å«ã‚€ã™ã¹ã¦ã®itemã«ä¸€å®šã®creditã‚’å‰²ã‚Šå½“ã¦ã¦ã—ã¾ã†ã‹ã‚‰ã§ã‚ã‚‹.
This causes a lack of focus and makes models less interpretable [10, 58].
ã“ã®ãŸã‚ã€æ³¨ç›®åº¦(focus)ãŒä¸è¶³ã—ã€ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆæ€§ãŒä½ä¸‹ã™ã‚‹[10, 58].

To address the above issues, one straightforward strategy is to design sparse Transformer architectures that sparsify the connections in the attention layers, which have been actively investigated in language modeling tasks [10, 58].
ä¸Šè¨˜ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€ä¸€ã¤ã®ç°¡å˜ãªæˆ¦ç•¥ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã§æ´»ç™ºã«ç ”ç©¶ã•ã‚Œã¦ã„ã‚‹**attention layersã®æ¥ç¶šã‚’sparseã«ã—ãŸ**Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ã§ã‚ã‚‹[10, 58].
Several representative models are Star Transformer [18], Sparse Transformer [10], Longformer [2], and BigBird [58].
ã„ãã¤ã‹ã®ä»£è¡¨çš„ãªãƒ¢ãƒ‡ãƒ«ã¯Star Transformer [18], Sparse Transformer [10], Longformer [2], ãã—ã¦BigBird [58]ã§ã‚ã‚‹.
These sparse attention patterns could mitigate noisy issues and avoid allocating credits to unrelated contents for the query of interest.
ã“ã‚Œã‚‰ã®sparse attentionãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã€ãƒã‚¤ã‚ºã®å•é¡Œã‚’è»½æ¸›ã—ã€é–¢å¿ƒã®ã‚ã‚‹ã‚¯ã‚¨ãƒªã«ç„¡é–¢ä¿‚ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã‚’å›é¿ã™ã‚‹ã“ã¨ãŒã§ãã‚‹.
However, these models largely rely on pre-defined attention schemas, which lacks flexibility and adaptability in practice.
ã—ã‹ã—ã€ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯äº‹å‰ã«å®šç¾©ã•ã‚ŒãŸattention schemas(??)ã«å¤§ããä¾å­˜ã—ã¦ãŠã‚Šã€å®Ÿéš›ã®ã¨ã“ã‚æŸ”è»Ÿæ€§ã‚„é©å¿œæ€§ã«æ¬ ã‘ã¦ã„ã‚‹.
Unlike end-to-end training approaches, whether these sparse patterns could generalize well to sequential recommendation remains unknown and is still an open research question.
ã¾ãŸã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®å­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã¯ç•°ãªã‚Šã€ã“ã‚Œã‚‰ã®ç–ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãŒé€æ¬¡æ¨è–¦ã«ã†ã¾ãä¸€èˆ¬åŒ–ã§ãã‚‹ã‹ã©ã†ã‹ã¯ä¸æ˜ã§ã‚ã‚Šã€ã¾ã æœªè§£æ±ºã®ç ”ç©¶èª²é¡Œã§ã‚ã‚‹.

## 1.1. Contributions.

è²¢çŒ®åº¦
In this work, we propose to design a denoising strategy, Rec-Denoiser, for better training of selfattentive sequential recommenders.
æœ¬ç ”ç©¶ã§ã¯ã€è‡ªå·±æ³¨æ„å‹é€æ¬¡æ¨è–¦å™¨ã‚’ã‚ˆã‚Šè‰¯ãå­¦ç¿’ã•ã›ã‚‹ãŸã‚ã®ãƒã‚¤ã‚ºé™¤å»æˆ¦ç•¥ã€Rec-Denoiserã‚’ææ¡ˆã™ã‚‹.
Our idea stems from the recent findings that not all attentions are necessary and simply pruning redundant attentions could further improve the performance [10, 12, 40, 55, 58].
æˆ‘ã€…ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã€å…¨ã¦ã®attentionsã¯å¿…è¦ã§ã¯ãªãã€å†—é•·ãªattentionsã‚’åˆˆã‚Šå–ã‚‹ã“ã¨ã§ã•ã‚‰ã«æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã¨ã„ã†æœ€è¿‘ã®çŸ¥è¦‹ã«ç”±æ¥ã™ã‚‹[10, 12, 40, 55, 58].
Rather than randomly dropping out attentions, we introduce differentiable masks to drop task-irrelevant attentions in the self-attention layers, which can yield exactly zero attention scores for noisy items.
æˆ‘ã€…ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ã«attentionsã‚’å‰Šé™¤ã™ã‚‹ã®ã§ã¯ãªãã€**å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’å°å…¥ã—ã€ã‚¿ã‚¹ã‚¯ã¨ç„¡é–¢ä¿‚ãªattentionsã‚’self-attention layersã§å‰Šé™¤ã™ã‚‹**ã“ã¨ã§ã€ãƒã‚¤ã‚ºã®å¤šã„itemã«å¯¾ã—ã¦attentionã‚¹ã‚³ã‚¢ã‚’æ­£ç¢ºã«ã‚¼ãƒ­ã«ã™ã‚‹ã“ã¨ãŒã§ãã‚‹.
The introduced sparsity in the self-attention layers has several benefits:
self-attention layersã«å°å…¥ã•ã‚ŒãŸã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã«ã¯ã€ã„ãã¤ã‹ã®åˆ©ç‚¹ãŒã‚ã‚‹.

1. Irrelevant attentions with parameterized masks can be learned to be dropped in a data-driven way.
2. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚ŒãŸãƒã‚¹ã‚¯ã‚’æŒã¤ç„¡é–¢ä¿‚ãªattentionã¯ã€ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ã®æ–¹æ³•ã§å‰Šé™¤ã•ã‚Œã‚‹ã‚ˆã†ã«å­¦ç¿’ã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹.
   - Taking Figure 1 as an example, our Rec-denoiser would prune the sequence (phone, bag, headphone) for pant, and (phone, bag, headphone, pant) for laptop in the attention maps.å›³1ã‚’ä¾‹ã«ã¨ã‚‹ã¨ã€Rec-denoiserã¯ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ—ã«ãŠã„ã¦ã€ã‚ºãƒœãƒ³ã«ã¯(phone, bag, headphone)ã€ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ã«ã¯(phone, bag, headphone, pant)ã¨ã„ã†é †åºã‚’åˆ‡ã‚Šæ¨ã¦ã‚‹ã“ã¨ã«ãªã‚‹.
   - Namely, we seek next item prediction explicitly based on a subset of more informative items. ã¤ã¾ã‚Šã€ã‚ˆã‚Šæƒ…å ±é‡ã®å¤šã„ã‚¢ã‚¤ãƒ†ãƒ ã®éƒ¨åˆ†é›†åˆ(subset)ã«åŸºã¥ãã€æ˜ç¤ºçš„ã«æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ äºˆæ¸¬ã‚’è¡Œã†ã®ã§ã™.
3. Our Rec-Denoiser still takes full advantage of Transformers as it does not change their architectures, but only the attention distributions.
4. æˆ‘ã€…ã®Rec-Denoiserã¯Transformerã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å¤‰æ›´ã›ãšã€**ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³åˆ†å¸ƒã®ã¿ã‚’å¤‰æ›´ã™ã‚‹**ãŸã‚ã€Transformerã‚’æœ€å¤§é™ã«æ´»ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã‚‹.
   - As such, Rec-Denoiser is easy to implement and is compatible to any Transformers, making them less complicated as well as improving their interpretability. ãã®ãŸã‚ã€Rec-Denoiserã¯å®Ÿè£…ãŒå®¹æ˜“ã§ã€ã‚ã‚‰ã‚†ã‚‹Transformerã¨äº’æ›æ€§ãŒã‚ã‚Šã€Transformerã®è¤‡é›‘ã•ã‚’è»½æ¸›ã—ã€ãã®è§£é‡ˆå¯èƒ½æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹.

In our proposed Rec-Denoiser, there are two major challenges.
æˆ‘ã€…ãŒææ¡ˆã™ã‚‹Rec-Denoiserã§ã¯ã€2ã¤ã®å¤§ããªèª²é¡ŒãŒã‚ã‚‹.
First, the discreteness of binary masks (i.e., 0 is dropped while 1 is kept) is, however, intractable in the back-propagation.
ã¾ãšã€2å€¤ãƒã‚¹ã‚¯ã®é›¢æ•£æ€§ï¼ˆã™ãªã‚ã¡ã€0ã¯å‰Šé™¤ã•ã‚Œã€1ã¯ä¿æŒã•ã‚Œã‚‹ï¼‰ã¯ã€ã—ã‹ã—ã€ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯å®Ÿè¡Œä¸å¯èƒ½ã§ã‚ã‚‹.
To remedy this issue, we relax the discrete variables with a continuous approximation through probabilistic reparameterization [25].
ã“ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€æˆ‘ã€…ã¯ç¢ºç‡çš„å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–[25]ã«ã‚ˆã‚Šã€é›¢æ•£å¤‰æ•°ã‚’é€£ç¶šçš„ãªè¿‘ä¼¼å€¤ã§ç·©å’Œã™ã‚‹.
As such, our differentiable masks can be trained jointly with original Transformers in an end-to-end fashion.
ã“ã®ã‚ˆã†ã«ã€æˆ‘ã€…ã®å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã¯ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã®Transformerã¨ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§å…±åŒã—ã¦å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã‚‹.
In addition, the scaled dot-product attention is not Lipschitz continuous and is thus vulnerable to input perturbations [28].
ã¾ãŸã€ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒƒãƒˆãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã®æ³¨ç›®ç‚¹ã¯Lipschitzé€£ç¶š(?)ã§ã¯ãªã„ãŸã‚ã€å…¥åŠ›æ‘‚å‹•ã«å¯¾ã—ã¦è„†å¼±ã§ã‚ã‚‹[28].
In this work, Jacobian regularization [21, 24] is further applied to the entire Transformer blocks, to improve the robustness of Transformers for noisy sequences.
ã“ã®ç ”ç©¶ã§ã¯ã€**ãƒã‚¤ã‚ºã®å¤šã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã™ã‚‹Transformerã®robustnessã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚**ã«ã€Transformerãƒ–ãƒ­ãƒƒã‚¯å…¨ä½“ã«ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–[21, 24]ã‚’ã•ã‚‰ã«é©ç”¨ã—ã¦ã„ã‚‹.
Experimental results on real-world benchmark datasets demonstrate the effectiveness and robustness of the proposed Rec-Denoiser.
å®Ÿä¸–ç•Œã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹å®Ÿé¨“çµæœã‹ã‚‰ã€ææ¡ˆã™ã‚‹Rec-Denoiserã®æœ‰åŠ¹æ€§ã¨é ‘å¥æ€§ã‚’å®Ÿè¨¼ã™ã‚‹.
In summary, our contributions are:
ã¾ã¨ã‚ã‚‹ã¨ã€æˆ‘ã€…ã®è²¢çŒ®ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã‚ã‚‹.

- We introduce the idea of denoising item sequences for better of training self-attentive sequential recommenders, which greatly reduces the negative impacts of noisy items. æœ¬è«–æ–‡ã§ã¯ã€è‡ªå·±èªè­˜å‹é€æ¬¡æ¨è–¦å™¨ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã€itemåˆ—ã®ãƒã‚¤ã‚ºé™¤å»ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ç´¹ä»‹ã—ã€ãƒã‚¤ã‚ºã®å¤šã„itemã«ã‚ˆã‚‹æ‚ªå½±éŸ¿ã‚’å¤§å¹…ã«è»½æ¸›ã™ã‚‹.

- We present a general Rec-Denoiser framework with differentiable masks that can achieve sparse attentions by dynamically pruning irrelevant information, leading to better model performance. æˆ‘ã€…ã¯ã€å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’æŒã¤ä¸€èˆ¬çš„ãªRec-Denoiserãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æç¤ºã—ã€ç„¡é–¢ä¿‚ãªæƒ…å ±ã‚’å‹•çš„ã«åˆˆã‚Šå–ã‚‹ã“ã¨ã§ç–ãªæ³¨æ„ã‚’é”æˆã—ã€ã‚ˆã‚Šè‰¯ã„ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’å°ãã“ã¨ãŒå¯èƒ½ã§ã‚ã‚‹.

- We propose an unbiased gradient estimator to optimize the binary masks, and apply Jacobian regularization on the gradients of Transformer blocks to further improve its robustness. ãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯ã®æœ€é©åŒ–ã®ãŸã‚ã«ä¸åå‹¾é…æ¨å®šå™¨ã‚’ææ¡ˆã—ã€Transformerãƒ–ãƒ­ãƒƒã‚¯ã®å‹¾é…ã«ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–ã‚’é©ç”¨ã—ã¦ã€ã•ã‚‰ã«é ‘å¥æ€§ã‚’å‘ä¸Šã•ã›ã‚‹.

- The experimental results demonstrate significant improvements that Rec-Denoiser brings to self-attentive recommenders (5.05% âˆ¼ 19.55% performance gains), as well as its robustness against input perturbations. å®Ÿé¨“çµæœã¯ã€Rec-DenoiserãŒè‡ªå·±æ³¨æ„å‹æ¨è–¦å™¨ã«ã‚‚ãŸã‚‰ã™å¤§ããªæ”¹å–„ï¼ˆ5.05% âˆ¼ 19.55%ã®æ€§èƒ½å‘ä¸Šï¼‰ã¨ã€å…¥åŠ›ã®æ‘‚å‹•ã«å¯¾ã™ã‚‹é ‘å¥æ€§ã‚’ç¤ºã—ã¦ã„ã‚‹.

# 2. Related Work 2. é–¢é€£ä½œå“

In this section, we briefly review the related work on sequential recommendation and sparse Transformers.
æœ¬ç¯€ã§ã¯ã€é€æ¬¡æ¨è–¦ã¨ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã«é–¢ã™ã‚‹é–¢é€£ç ”ç©¶ã‚’ç°¡å˜ã«ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹ã€‚
We also highlight the differences between the existing efforts and ours.
ã¾ãŸã€æ—¢å­˜ã®å–ã‚Šçµ„ã¿ã¨æˆ‘ã€…ã®å–ã‚Šçµ„ã¿ã¨ã®ç›¸é•ç‚¹ã‚’å¼·èª¿ã™ã‚‹ã€‚

## 2.1. Sequential Recommendation 2.1. ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³

Leveraging sequences of user-item interactions is crucial for sequential recommendation.
é€æ¬¡æ¨è–¦ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®ç›¸äº’ä½œç”¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã‚ã‚‹ã€‚
User dynamics can be caught by Markov Chains for inferring the conditional probability of an item based on the previous items [19, 39].
ãƒ¦ãƒ¼ã‚¶ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã¯ãƒãƒ«ã‚³ãƒ•é€£é–ã«ã‚ˆã£ã¦æ•æ‰ã•ã‚Œã€å‰ã®ã‚¢ã‚¤ãƒ†ãƒ ã«åŸºã¥ãã‚¢ã‚¤ãƒ†ãƒ ã®æ¡ä»¶ä»˜ãç¢ºç‡ã‚’æ¨è«–ã™ã‚‹ã“ã¨ãŒã§ãã‚‹[19, 39]ã€‚
More recently, growing efforts have been dedicated to deploying deep neural networks for sequential recommendation such as recurrent neural networks [20, 53], convolutional neural networks [42, 54, 57], memory networks [9, 22], and graph neural networks [4, 7, 51].
æœ€è¿‘ã§ã¯ã€ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ [20, 53]ã€ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ [42, 54, 57]ã€ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ [9, 22]ã€ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ [4, 7, 51] ãªã©ã®æ·±ã„ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’é€æ¬¡æ¨è–¦ã«åˆ©ç”¨ã™ã‚‹å–ã‚Šçµ„ã¿ãŒç››ã‚“ã«ãªã£ã¦ã„ã‚‹ã€‚
For example, GRU4Rec [20] employs a gated recurrent unit to study temporal behaviors of users.
ä¾‹ãˆã°ã€GRU4Rec[20]ã¯ãƒ¦ãƒ¼ã‚¶ã®æ™‚é–“çš„è¡Œå‹•ã‚’ç ”ç©¶ã™ã‚‹ãŸã‚ã«ã‚²ãƒ¼ãƒ†ãƒƒãƒ‰ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ¦ãƒ‹ãƒƒãƒˆã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã€‚
Caser [42] learns sequential patterns by using convolutional filters on local sequences.
Caser [42]ã¯å±€æ‰€çš„ãªé…åˆ—ã«å¯¾ã—ã¦ç•³ã¿è¾¼ã¿ãƒ•ã‚£ãƒ«ã‚¿ã‚’ç”¨ã„ã¦é€£ç¶šçš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹ã€‚
MANN [9] adopts memory-augmented neural networks to model user historical records.
MANN [9]ã¯ãƒ¦ãƒ¼ã‚¶ã®å±¥æ­´è¨˜éŒ²ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ãŸã‚ã«ãƒ¡ãƒ¢ãƒªè£œå¼·å‹ãƒ‹ãƒ¥ãƒ¼ ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ¡ç”¨ã™ã‚‹ã€‚
SR-GNN [51] converts session sequences into graphs and uses graph neural networks to capture complex item-item transitions.
SR-GNN [51]ã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ã‚°ãƒ©ãƒ•ã«å¤‰æ›ã—ã€ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦è¤‡é›‘ãªã‚¢ã‚¤ãƒ†ãƒ -ã‚¢ã‚¤ãƒ†ãƒ é·ç§»ã‚’æ‰ãˆã‚‹ã€‚

Transformer-based models have shown promising potential in sequential recommendation [5, 26, 30, 32, 33, 41, 49, 50], due to their ability of modeling arbitrary dependencies in a sequence.
ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã«åŸºã¥ããƒ¢ãƒ‡ãƒ«ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸­ã®ä»»æ„ã®ä¾å­˜é–¢ä¿‚ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã§ãã‚‹ãŸã‚ã€é€æ¬¡æ¨è–¦ã«ãŠã„ã¦æœ‰æœ›ãªå¯èƒ½æ€§ã‚’ç¤ºã—ã¦ã„ã‚‹[5, 26, 30, 32, 33, 41, 49, 50]ï¼
For example, SASRec [26] first adopts self-attention network to learn the importance of items at different positions.
ä¾‹ãˆã°ã€SASRec [26]ã§ã¯ã€ã¾ãšã€ç•°ãªã‚‹ä½ç½®ã«ã‚ã‚‹é …ç›®ã®é‡è¦åº¦ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã€è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã€‚
In the follow-up studies, several Transformer variants have been designed for different scenarios by adding bidirectional attentions [41], time intervals [30], personalization [50], importance sampling [32], and sequence augmentation [33].
ãã®å¾Œã€åŒæ–¹å‘ã®æ³¨æ„[41]ã€æ™‚é–“é–“éš”[30]ã€å€‹äººåŒ–[50]ã€é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°[32]ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ‹¡å¼µ[33]ã‚’è¿½åŠ ã—ã€ç•°ãªã‚‹ã‚·ãƒŠãƒªã‚ªã®ãŸã‚ã«ã„ãã¤ã‹ã®å¤‰ç¨®ãŒè¨­è¨ˆã•ã‚Œã¦ããŸã€‚
However, very few studies pay attention to the robustness of self-attentive recommender models.
ã—ã‹ã—ã€è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®é ‘å¥æ€§ã«æ³¨ç›®ã—ãŸç ”ç©¶ã¯éå¸¸ã«å°‘ãªã„ã€‚
Typically, usersâ€™ sequences contain lots of irrelevant items since they may subsequently purchase a series of products with different purposes [45].
ä¸€èˆ¬ã«ã€ãƒ¦ãƒ¼ã‚¶ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ã¯ç„¡é–¢ä¿‚ãªé …ç›®ãŒå¤šãå«ã¾ã‚Œã‚‹ã€‚
As such, the current user action only depends on a subset of items, not on the entire sequences.
ã“ã®ã‚ˆã†ãªå ´åˆã€ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ã®è¡Œå‹•ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã§ã¯ãªãã€ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã«ã®ã¿ä¾å­˜ã™ã‚‹ã€‚
However, the self-attention module is known to be sensitive to noisy sequences [28], which may lead to sub-optimal generalization performance.
ã—ã‹ã—ã€è‡ªå·±æ³¨æ„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ãƒã‚¤ã‚ºã®å¤šã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«æ•æ„Ÿã§ã‚ã‚‹ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ãŠã‚Š[28]ã€ã“ã‚Œã¯æœ€é©ã§ãªã„æ±åŒ–æ€§èƒ½ã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
In this paper, we aim to reap the benefits of Transformers while denoising the noisy item sequences by using learnable masks in an end-to-end fashion.
æœ¬è«–æ–‡ã§ã¯ã€å­¦ç¿’å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§ç”¨ã„ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€ãƒã‚¤ã‚ºã®å¤šã„ã‚¢ã‚¤ãƒ†ãƒ åˆ—ã‚’ãƒã‚¤ã‚ºåŒ–ã—ã¤ã¤ã€Transformersã®åˆ©ç‚¹ã‚’äº«å—ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚

## 2.2. Sparce Transformer 2.2. ã‚¹ãƒ‘ãƒ¼ã‚¹å¤‰åœ§å™¨

Recently, many lightweight Transformers seek to achieve sparse attention maps since not all attentions carry important information in the self-attention layers [2, 10, 18, 29, 58].
æœ€è¿‘ã€å¤šãã®è»½é‡ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãŒã€å…¨ã¦ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãŒè‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã®é‡è¦ãªæƒ…å ±ã‚’æŒã£ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªã„ã®ã§ã€ç–ãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ—ã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã‚’è¿½æ±‚ã—ã¦ã„ã‚‹[2, 10, 18, 29, 58]ã€‚
For instance, Reformer [29] computes attentions based on locality-sensitive hashing, leading to lower memory consumption.
ä¾‹ãˆã°ã€Reformer [29]ã¯å±€æ‰€æ€§ã‚’è€ƒæ…®ã—ãŸãƒãƒƒã‚·ãƒ¥ã«åŸºã¥ã„ã¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—ã—ã€ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã®ä½æ¸›ã«ã¤ãªãŒã‚‹ã€‚
Star Transformer [18] replaces the fully-connected structure of self-attention with a star-shape topology.
Star Transformer [18]ã¯ã€è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Œå…¨é€£çµæ§‹é€ ã‚’æ˜Ÿå½¢ã®ãƒˆãƒãƒ­ã‚¸ãƒ¼ã«ç½®ãæ›ãˆãŸã‚‚ã®ã§ã‚ã‚‹ã€‚
Sparse Transformer [10] and Longformer [2] achieve sparsity by using various sparse patterns, such as diagonal sliding windows, dilated sliding windows, local and global sliding windows.
Sparse Transformer [10] ã¨ Longformer [2] ã¯ã€æ–œã‚ã‚¹ãƒ©ã‚¤ãƒ‰çª“ã€æ‹¡å¼µã‚¹ãƒ©ã‚¤ãƒ‰çª“ã€ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒ©ã‚¤ãƒ‰çª“ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ãƒ©ã‚¤ãƒ‰çª“ãªã©ã€æ§˜ã€…ãªã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”¨ã„ã¦ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã‚’å®Ÿç¾ã™ã‚‹ã€‚
BigBird [58] uses random and several fixed patterns to build sparse blocks.
BigBird [58]ã§ã¯ï¼Œãƒ©ãƒ³ãƒ€ãƒ ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã„ãã¤ã‹ã®å›ºå®šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”¨ã„ã¦ï¼Œç–ãªãƒ–ãƒ­ãƒƒã‚¯ã‚’æ§‹ç¯‰ã—ã¦ã„ã‚‹ï¼
It has been shown that these sparse attentions can obtain the state-of-the-art performance and greatly reduce computational complexity.
ã“ã‚Œã‚‰ã®ç–ãªæ³¨æ„ã¯ã€æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã€è¨ˆç®—é‡ã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚
However, many of them rely on fixed attention schemas that lack flexibility and require tremendous engineering efforts.
ã—ã‹ã—ã€ã“ã‚Œã‚‰ã®å¤šãã¯ã€æŸ”è»Ÿæ€§ã«æ¬ ã‘ã€è†¨å¤§ãªå·¥å­¦çš„åŠªåŠ›ã‚’å¿…è¦ã¨ã™ã‚‹å›ºå®šçš„ãªæ³¨æ„ã‚¹ã‚­ãƒ¼ãƒã«ä¾å­˜ã—ã¦ã„ã‚‹ã€‚

Another line of work is to use learnable attention distributions [12, 36, 38, 40].
ã¾ãŸã€å­¦ç¿’å¯èƒ½ãªæ³¨æ„åˆ†å¸ƒ[12, 36, 38, 40]ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€‚
Mostly, they calculate attention weights with variants of sparsemax that replaces the softmax normalization in the self-attention networks.
ã»ã¨ã‚“ã©ã®å ´åˆã€è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã‘ã‚‹ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹æ­£è¦åŒ–ã‚’ç½®ãæ›ãˆã‚‹sparsemaxã®å¤‰ç¨®ã‚’ç”¨ã„ã¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã‚’è¨ˆç®—ã™ã‚‹ã€‚
This allows to produce both sparse and bounded attentions, yielding a compact and interpretable set of alignments.
ã“ã‚Œã«ã‚ˆã‚Šã€ç–ã§å¢ƒç•Œã®ã‚ã‚‹æ³¨æ„ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã€ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã§è§£é‡ˆå¯èƒ½ãªã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆã®é›†åˆã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
Our Rec-denoiser is related to this line of work.
æˆ‘ã€…ã®Rec-denoiserã¯ã€ã“ã®ç ”ç©¶ã«é–¢é€£ã—ã¦ã„ã‚‹ã€‚
Instead of using sparsemax, we design a trainable binary mask for the self-attention network.
ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¹ã‚’ç”¨ã„ã‚‹ä»£ã‚ã‚Šã«ã€æˆ‘ã€…ã¯è‡ªå·±æ³¨æ„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«å¯¾ã—ã¦å­¦ç¿’å¯èƒ½ãªãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯ã‚’è¨­è¨ˆã™ã‚‹ã€‚
As a result, our proposed Rec-denoiser can automatically determine which self-attention connections should be deleted or kept in a data-driven way.
ãã®çµæœã€æˆ‘ã€…ã®ææ¡ˆã™ã‚‹Rec-denoiserã¯ã€ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ã®æ–¹æ³•ã§ã€ã©ã®è‡ªå·±æ³¨æ„ã®æ¥ç¶šã‚’å‰Šé™¤ã™ã¹ãã‹ã€ã‚ã‚‹ã„ã¯ä¿æŒã™ã¹ãã‹ã‚’è‡ªå‹•çš„ã«æ±ºå®šã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

# 3. Problem and Background 3. å•é¡Œç‚¹ã¨èƒŒæ™¯

In this section, we first formulate the problem of sequential recommendation, and then revisit several self-attentive models.
æœ¬ç¯€ã§ã¯ã€ã¾ãšé€æ¬¡æ¨è–¦ã®å•é¡Œã‚’å®šå¼åŒ–ã—ã€æ¬¡ã«ã„ãã¤ã‹ã®è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’å†æ¤œè¨ã™ã‚‹ã€‚
We further discuss the limitations of the existing work.
ã•ã‚‰ã«ã€æ—¢å­˜ã®ç ”ç©¶ã®é™ç•Œã«ã¤ã„ã¦è­°è«–ã™ã‚‹ã€‚

## 3.1. Problem Setup 3.1. å•é¡Œã®è¨­å®š

In sequential recommendation, let $U$ be a set of users, $I$ a set of items, and $S = {S^1, S^2,\cdots, S^{
U

We user $S^u = (S^u*1, S^u_2, \cdots, S^u*{
S^u

Given the interaction history $S^u$, sequential recommendation seeks to predict the next item $S^u_{S^u+1}$ at time step $
S^u+1

During the training process [26, 41], it will be convenient to regard the modelâ€™s input as $(S^u*1, S^u_2, \cdots, S^u*{
S^u - 1

## 3.2. Self-attenvive Recommenders 3.2. è‡ªå·±ã‚¢ãƒ†ãƒ³ãƒã‚¤ãƒ–å‹ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼

Owing to the ability of learning long sequences, Transformer architectures [43] have been widely used in sequential recommendation, like **SASRec** [26], BERT4Rec [41], and TiSASRec [30].
Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£[43]ã¯é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ãŸã‚ã€é€æ¬¡æ¨è–¦ã«ãŠã„ã¦**SASRec** [26], BERT4Rec [41], TiSASRec [30] ãªã©åºƒãåˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚
Here we briefly introduce the design of SASRec and discuss its limitations.
ã“ã“ã§ã¯ã€SASRecã®è¨­è¨ˆã‚’ç°¡å˜ã«ç´¹ä»‹ã—ã€ãã®é™ç•Œã«ã¤ã„ã¦è€ƒå¯Ÿã™ã‚‹ã€‚

### 3.2.1. Embedding Layer 3.2.1. ã‚¨ãƒ³ãƒ™ãƒ‡ãƒƒãƒ‰ãƒ¬ã‚¤ãƒ¤ãƒ¼

Transformer-based recommenders maintain an item embedding table $T \in R^{
I

$$
\hat{E} = E + P \tag{1}
$$

where $\hat{E}
ã“ã“ã§ã€$hat{E}
\in R^{n\times d}$ is an order-aware embedding, which can be directly fed to any Transformer-based models.
\in R^{n}times d}$ ã¯é †åºã‚’è€ƒæ…®ã—ãŸåŸ‹ã‚è¾¼ã¿ã§ã€Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã«ç›´æ¥ä¸ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

### 3.2.2. Transformer Block 3.2.2. å¤‰åœ§å™¨ãƒ–ãƒ­ãƒƒã‚¯

A Transformer block consists of a self-attention layer and a point-wise feed-forward layer.
Transformerãƒ–ãƒ­ãƒƒã‚¯ã¯ã€è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã¨ãƒã‚¤ãƒ³ãƒˆãƒ¯ã‚¤ã‚ºãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚

**Self-attention Layer**:
**è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤**ã€‚
The key component of Transformer block is the self-attention layer that is highly efficient to uncover sequential dependencies in a sequence [43].
Transformerãƒ–ãƒ­ãƒƒã‚¯ã®ã‚­ãƒ¼ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸­ã®é †åºä¾å­˜æ€§ã‚’ç™ºè¦‹ã™ã‚‹ãŸã‚ã«éå¸¸ã«åŠ¹ç‡çš„ã§ã‚ã‚‹è‡ªå·±æ³¨æ„å±¤ã§ã‚ã‚‹[43]ã€‚
The scaled dot-product attention is a popular attention kernel:
ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ‰ãƒƒãƒˆãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¯ã€ä¸€èˆ¬çš„ãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚«ãƒ¼ãƒãƒ«ã§ã‚ã‚‹ã€‚

$$
\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d}}) V
\tag{2}
$$

where $\text{Attention}(Q, K, V) \in R^{n \times d}$ is the output item representations; $Q = \hat{E}W^Q$, $K =\hat{E}W^K$, and $V = \hat{E}W^V$ are the queries, keys and values, respectively; ${W^Q, W^K, W^V}
ã“ã“ã§ã€$text{Attention}(Q, K, V) \in R^{n \times d}$ã¯å‡ºåŠ›é …ç›®è¡¨ç¾ã€$Q = \hat{E}W^Q$, $K = \hat{E}W^K$, $V = \hat{E}W^V$ ã¯ãã‚Œãã‚Œã‚¯ã‚¨ãƒªãƒ¼ã€ã‚­ãƒ¼ã€å€¤ã€ ${W^Q, W^K, W^V} , ã¯3ã¤ã®å°„å½±å¤‰æ›è¡Œåˆ—ã§ã‚ã‚Šã€$Î¸sqrt{time, tpm, tpm}$ã¯æ³¨æ„ã®åˆ†å¸ƒã‚’æŸ”ã‚‰ã‹ãã™ã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ã§ã‚ã‚‹ã€‚
\in R^{d \times d}$ are three projection matrices; and $\sqrt{d}$ is the scale factor to produce a softer attention distribution.
\W^Q, W^K, W^V} ã¯3ã¤ã®å°„å½±è¡Œåˆ—ã€$sqrt{d}$ ã¯ã‚ˆã‚ŠæŸ”ã‚‰ã‹ã„æ³¨ç›®åˆ†å¸ƒã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ã‚¹ã‚±ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ã§ã‚ã‚‹ã€‚
In sequential recommendation, one can utilize either left-to-right unidirectional attentions (e.g., SASRec [26] and TiSASRec [30]) or bidirectional attentions (e.g., BERT4Rec [41]) to predict the next item.
é€æ¬¡æ¨è–¦ã§ã¯ã€å·¦ã‹ã‚‰å³ã¸ã®ä¸€æ–¹å‘ã®æ³¨ç›®ï¼ˆä¾‹ï¼šSASRec [26], TiSASRec [30]ï¼‰ã‚„åŒæ–¹å‘ã®æ³¨ç›®ï¼ˆä¾‹ï¼šBERT4Rec [41] ï¼‰ã‚’ç”¨ã„ã¦æ¬¡ã®é …ç›®ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
Moreover, one can apply H attention functions in parallel to enhance expressiveness: $H <- \text{MultiHead}(\hat{E})$ [43].
ã•ã‚‰ã«ã€Hå€‹ã®æ³¨æ„é–¢æ•°ã‚’ä¸¦åˆ—ã«é©ç”¨ã—ã¦è¡¨ç¾åŠ›ã‚’é«˜ã‚ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ï¼š$H <- \{MultiHead}(\hat{E})$ [43]ã€‚

**Point-wise Feed-forward Layer**:
**ãƒã‚¤ãƒ³ãƒˆãƒ»ãƒ¯ã‚¤ã‚ºãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤**ã€‚
As the self attention layer is built on linear projections, we can endow the nonlinearity by introducing a point-wise feed-forward layers:
è‡ªå·±æ³¨æ„å±¤ã¯ç·šå½¢æŠ•å°„ã§æ§‹ç¯‰ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ãƒã‚¤ãƒ³ãƒˆãƒ¯ã‚¤ã‚ºãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã‚’å°å…¥ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€éç·šå½¢æ€§ã‚’ä»˜ä¸ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã‚ã‚‹ã€‚

$$
F_i = FFN(H_i) = \text{ReLU}(H_i W^{(1)} + b^{(1)})W^{(2)} + b^{(2)}
\tag{3}
$$

where $W^{(*)} \in R^{d \times d}$, $b^{(*)} \in R^d$ are the learnable weights and bias.
ã“ã“ã§ã€$W^{(*)} \in R^{d \times d}$, $b^{(*)} \in R^d$ã¯å­¦ç¿’å¯èƒ½ãªé‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã§ã‚ã‚‹ã€‚

In practice, it is usually beneficial to learn hierarchical item dependencies by stacking more Transformer blocks.
å®Ÿéš›ã«ã¯ã€ã‚ˆã‚Šå¤šãã®Transformerãƒ–ãƒ­ãƒƒã‚¯ã‚’ç©ã¿é‡ã­ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€éšå±¤çš„ãªé …ç›®ã®ä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒé€šå¸¸æœ‰ç›Šã§ã‚ã‚‹ã€‚
Also, one can adopt the tricks of residual connection, dropout, and layer normalization for stabilizing and accelerating the training.
ã¾ãŸã€å­¦ç¿’ã®å®‰å®šåŒ–ã¨é«˜é€ŸåŒ–ã®ãŸã‚ã«ã€æ®‹å·®æ¥ç¶šã€ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã€å±¤æ­£è¦åŒ–ãªã©ã®ãƒˆãƒªãƒƒã‚¯ã‚’æ¡ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
Here we simply summarize the output of $L$ Transformer blocks as:
ã“ã“ã§ã¯ã€$L$å€‹ã®Transformerãƒ–ãƒ­ãƒƒã‚¯ã®å‡ºåŠ›ã‚’å˜ç´”ã«ä»¥ä¸‹ã®ã‚ˆã†ã«ã¾ã¨ã‚ã‚‹ã€‚
$F^{(L)} <- \text{Transformer}(\hat{E})$.
F^{(L)} <- \text{Transformer}(\hat{E})$.

### 3.2.3. Learning Objective 3.2.3. å­¦ç¿’ç›®æ¨™

After stacked $L$ Transformer blocks, one can predict the next item (given the first $t$ items) based on $F_t^{(L)}$.
L$å€‹ã®Transformerãƒ–ãƒ­ãƒƒã‚¯ã‚’ç©ã¿é‡ã­ãŸå¾Œã€$F_t^{(L)}$ã«åŸºã¥ã„ã¦ã€ï¼ˆæœ€åˆã®$t$å€‹ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒã‚ã‚Œã°ï¼‰æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
In this work, we use inner product to predict the relevance of item $i$ as:
æœ¬ç ”ç©¶ã§ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ $i$ã®é–¢é€£æ€§ã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã«ã€å†…ç©ã‚’æ¬¡ã®ã‚ˆã†ã«ä½¿ç”¨ã™ã‚‹ã€‚

$$
r_{i, t} = <F_{t}^{(L)}, T_i>,
$$

where $T_i \in R^d$ is the embedding of item $i$.
ã“ã“ã§ã€$T_i \in R^d$ ã¯é …ç›®$i$ã®åŸ‹ã‚è¾¼ã¿ã§ã‚ã‚‹ã€‚
Recall that the model inputs a sequence $s = (s_1, s_2, \cdots, s_n)$ and its desired output is a shifted version of the same sequence $o = (o_1, o_2, \cdots, o_n)$, we can adopt the binary cross-entropy loss as:
ãƒ¢ãƒ‡ãƒ«ã¯é…åˆ—$s = (s_1, s_2, \cdots, s_n)$ã‚’å…¥åŠ›ã—ã€ãã®æœ›ã¾ã—ã„å‡ºåŠ›ã¯åŒã˜é…åˆ—$o = (o_1, o_2, \cdots, o_n)$ã®ã‚·ãƒ•ãƒˆç‰ˆã§ã‚ã‚‹ã¨ã™ã‚‹ã¨ã€2å€¤ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã¨ã—ã¦æ¡ç”¨ã§ãã‚‹ã“ã¨ã‚’æƒ³èµ·ã—ã¦ãã ã•ã„ã€‚

$$
L_{BCE} = - \sum_{S^u \in S} \sum_{t=1}^{n}{[\log{\sigma(r_{o_t, t})} + \log{1 - \sigma (r_{o_t', t})}]} + a \cdot ||\theta||^2_F
\tag{4}
$$

where $\theta$ is the mode parameters, $a$ is the reqularizer to prevent over-fitting, $o'_t \notin S^u$ is a negative sample corresponding to $o_t$, and $\sigma(\cdot)$ is the sigmoid function.
ã“ã“ã§ã€$theta$ã¯ãƒ¢ãƒ¼ãƒ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€$a$ã¯ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’é˜²ããŸã‚ã®reqularizerã€$o'_t \notin S^u$ã¯$o_t$ã«å¯¾å¿œã™ã‚‹è² ã®ã‚µãƒ³ãƒ—ãƒ«ã€$sigma( \cdot)$ ã¯ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã§ã‚ã‚‹ã€‚

More details can be found in SASRec [26] and BERT4Rec [41].
è©³ç´°ã¯ã€SASRec [26]ã¨BERT4Rec [41]ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚

## 3.3. The Noisy Attentions Problem 3.3. ãƒã‚¤ã‚¸ãƒ¼ãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å•é¡Œ

Despite the success of SASRec and its variants, we argue that they are insufficient to address the noisy items in sequences.
SASRecã¨ãã®å¤‰ç¨®ã®æˆåŠŸã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€æˆ‘ã€…ã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸­ã®ãƒã‚¤ã‚ºã®å¤šã„ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾å‡¦ã™ã‚‹ã«ã¯ä¸ååˆ†ã§ã‚ã‚‹ã¨ä¸»å¼µã™ã‚‹ã€‚
The reason is that the full attention distributions (e.g., Eq. (2)) are dense and would assign certain credits to irrelevant items.
ãã®ç†ç”±ã¯ã€å®Œå…¨ãªæ³¨ç›®åº¦åˆ†å¸ƒï¼ˆä¾‹ãˆã°ã€å¼ï¼ˆ2ï¼‰ï¼‰ã¯å¯†ã§ã‚ã‚Šã€ç„¡é–¢ä¿‚ãªã‚¢ã‚¤ãƒ†ãƒ ã«ä¸€å®šã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å‰²ã‚Šå½“ã¦ã¦ã—ã¾ã†ã‹ã‚‰ã§ã‚ã‚‹ã€‚
This complicates the item-item dependencies, increases the training difficulty, and even degrades the model performance.
ã“ã‚Œã¯ã€é …ç›®-é …ç›®ä¾å­˜é–¢ä¿‚ã‚’è¤‡é›‘ã«ã—ã€å­¦ç¿’ã®é›£æ˜“åº¦ã‚’ä¸Šã’ã€ã•ã‚‰ã«ã¯ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ä½ä¸‹ã•ã›ã‚‹ã€‚
To address this issue, several attempts have been proposed to manually define sparse attention schemas in language modeling tasks [2, 10, 18, 58].
ã“ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ç–ãªæ³¨æ„ã‚¹ã‚­ãƒ¼ãƒã‚’æ‰‹å‹•ã§å®šç¾©ã™ã‚‹è©¦ã¿ãŒã„ãã¤ã‹ææ¡ˆã•ã‚Œã¦ã„ã‚‹[2, 10, 18, 58]ã€‚
However, these fixed sparse attentions cannot adapt to the input data [12], leading to sub-optimal performance.
ã—ã‹ã—ã€ã“ã‚Œã‚‰ã®å›ºå®šçš„ãªç–ãªæ³¨æ„ã¯å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«é©å¿œã§ããš[12]ã€æœ€é©ã¨ã¯è¨€ãˆãªã„æ€§èƒ½ã«ãªã‚‹ã€‚

On the other hand, several dropout techniques are specifically designed for Transformers to keep only a small portion of attentions, including LayerDrop [17], DropHead [60], and UniDrop [52].
ä¸€æ–¹ã€LayerDrop [17], DropHead [60], UniDrop [52]ãªã©ã€ã„ãã¤ã‹ã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆæŠ€è¡“ã¯ã€TransformersãŒæ³¨ç›®ã®ã”ãä¸€éƒ¨ã ã‘ã‚’ä¿æŒã™ã‚‹ã‚ˆã†ã«ç‰¹åˆ¥ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚
Nevertheless, these randomly dropout approaches are susceptible to bias: the fact that attentions can be dropped randomly does not mean that the model allows them to be dropped, which may lead to over-aggressive pruning issues.
ã—ã‹ã—ãªãŒã‚‰ã€ã“ã‚Œã‚‰ã®ãƒ©ãƒ³ãƒ€ãƒ ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ãƒã‚¤ã‚¢ã‚¹ã®å½±éŸ¿ã‚’å—ã‘ã‚„ã™ã„ï¼šæ³¨æ„ãŒãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ‰ãƒ­ãƒƒãƒ—ã§ãã‚‹ã¨ã„ã†äº‹å®Ÿã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒãã‚Œã‚‰ã‚’ãƒ‰ãƒ­ãƒƒãƒ—ã™ã‚‹ã“ã¨ã‚’è¨±å¯ã—ã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ãªã„ã®ã§ã€éåº¦ã«æ”»æ’ƒçš„ãªåˆˆã‚Šè¾¼ã¿ã®å•é¡Œã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
In contrast, we propose a simple yet effective data-driven method to mask out irrelevant attentions by using differentiable masks.
ã“ã‚Œã«å¯¾ã—ã€æˆ‘ã€…ã¯å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’ç”¨ã„ã¦ç„¡é–¢ä¿‚ãªæ³¨æ„ã‚’ãƒã‚¹ã‚¯ã™ã‚‹ã€ã‚·ãƒ³ãƒ—ãƒ«ã‹ã¤åŠ¹æœçš„ãªãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ã®æ–¹æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚

# 4. Rec-Denoiser 4. ãƒ¬ãƒƒã‚¯ãƒ‡ãƒã‚¤ã‚¶ãƒ¼

In this section, we present our Rec-Denoiser that consists of two parts: differentiable masks for self-attention layers and Jacobian regularization for Transformer blocks.
æœ¬ç¯€ã§ã¯ã€Rec-Denoiserã‚’ç´¹ä»‹ã™ã‚‹ã€‚Rec-Denoiserã¯ã€è‡ªå·±èªè­˜å±¤ã«å¯¾ã™ã‚‹å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã¨Transformerãƒ–ãƒ­ãƒƒã‚¯ã«å¯¾ã™ã‚‹ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–ã®2ã¤ã®éƒ¨åˆ†ã‹ã‚‰æ§‹æˆã•ã‚Œã¦ã„ã‚‹ã€‚

## 4.1. Differentiable Masks 4.1. å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯

The self-attention layer is the cornerstone of Transformers to capture long-range dependencies.
è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã¯é•·è·é›¢ä¾å­˜é–¢ä¿‚ã‚’æ‰ãˆã‚‹ãŸã‚ã®Transformersã®è¦ã§ã‚ã‚‹ã€‚
As shown in Eq. (2), the softmax operator assigns a non-zero weight to every item.
å¼(2)ã«ç¤ºã™ã‚ˆã†ã«ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹æ¼”ç®—å­ã¯å…¨ã¦ã®é …ç›®ã«0ã§ãªã„é‡ã¿ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã€‚
However, full attention distributions may not always be advantageous since they may cause irrelevant dependencies, unnecessary computation, and unexpected explanation.
ã—ã‹ã—ã€å®Œå…¨ãªæ³¨æ„åˆ†å¸ƒã¯ã€ç„¡é–¢ä¿‚ãªä¾å­˜é–¢ä¿‚ã€ä¸å¿…è¦ãªè¨ˆç®—ã€äºˆæœŸã›ã¬èª¬æ˜ã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€å¿…ãšã—ã‚‚æœ‰åˆ©ã¨ã¯è¨€ãˆãªã„ã€‚
We next put forward differentiable masks to address this concern.
æ¬¡ã«ã€ã“ã®æ‡¸å¿µã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’æå”±ã™ã‚‹ã€‚

### 4.1.1. Learnable Sparse Attentions 4.1.1. å­¦ç¿’å¯èƒ½ãªã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³

Not every item in a sequence is aligned well with user preferences in the same sense that not all attentions are strictly needed in self-attention layers.
ã“ã‚Œã¯ã€è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã§å…¨ã¦ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãŒå³å¯†ã«å¿…è¦ã¨ã•ã‚Œã‚‹ã‚ã‘ã§ã¯ãªã„ã®ã¨åŒã˜æ„å‘³ã§ã‚ã‚‹ã€‚
Therefore, we attach each self-attention layer with a trainable binary mask to prune noisy or task-irrelevant attentions.
ãã“ã§ã€å„è‡ªå·±èªè­˜å±¤ã«å­¦ç¿’å¯èƒ½ãªãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯ã‚’ä»˜åŠ ã—ã€ãƒã‚¤ã‚ºã‚„ã‚¿ã‚¹ã‚¯ã¨ç„¡é–¢ä¿‚ãªèªè­˜ã‚’æ’é™¤ã™ã‚‹ã€‚
Formally, for the ğ‘™-th self-attention layer in Eq. (2), we introduce a binary matrix $Z^{(l)} \in {0, 1}^{n\times n}$, where $Z^{(l)}_{u,v}$ denotes whether the connection between query $u$ and key $v$ is present.
ã“ã“ã§ã€$Z^{(l)}_{u,v}$ã¯ã‚¯ã‚¨ãƒª$u$ã¨ã‚­ãƒ¼$v$ã®é–“ã®æ¥ç¶šãŒå­˜åœ¨ã™ã‚‹ã‹ã©ã†ã‹ã‚’è¡¨ã™ã€‚
As such, the $l$-th self-attention layer becomes:
ã“ã®ã‚ˆã†ã«ã€$l$ç•ªç›®ã®è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚‹ã€‚

$$
A^{(l)} = \text{softmax}(\frac{Q^{(l)} K^{(l)T}}{\sqrt{d}}),
\\
M^{(l)} = A^{(l)} \odot Z^{(l)},
\\
\text{Attention}(Q^{(l)}, K^{(l)}, V^{(l)}) = M^{(l)} V^{(l)},
\tag{5}
$$

where $A^{(l)}$ is the original full attentions, $M^{(l)}$ denotes the sparse attentions, and $\odot$ is the element-wise product.
ã“ã“ã§ã€$A^{(l)}$ã¯å…ƒã®å®Œå…¨æ³¨æ„ã€$M^{(l)}$ã¯ç–æ³¨æ„ã€$Î˜odot$ã¯è¦ç´ ãƒ¯ã‚¤ã‚ºç©ã§ã‚ã‚‹ã“ã¨ã‚’è¡¨ã—ã¦ã„ã‚‹ã€‚
Intuitively, the mask $Z^{(l)}$ (e.g., 1 is kept and 0 is dropped) requires minimal changes to the original self-attention layer.
ç›´æ„Ÿçš„ã«ã¯ã€ãƒã‚¹ã‚¯$Z^{(l)}$ï¼ˆä¾‹ãˆã°ã€1ã‚’æ®‹ã—0ã‚’è½ã¨ã™ï¼‰ã¯ã€å…ƒã®è‡ªå·±æ³¨æ„å±¤ã«æœ€å°é™ã®å¤‰æ›´ã‚’åŠ ãˆã‚‹ã ã‘ã§æ¸ˆã‚€ã€‚
More importantly, they are capable of yielding exactly zero attention scores for irrelevant dependencies, resulting in better interpretability.
ã•ã‚‰ã«é‡è¦ãªã“ã¨ã¯ã€ç„¡é–¢ä¿‚ãªä¾å­˜é–¢ä¿‚ã«å¯¾ã—ã¦æ­£ç¢ºã«ã‚¼ãƒ­ã®æ³¨æ„ã‚¹ã‚³ã‚¢ã‚’ã‚‚ãŸã‚‰ã™ã“ã¨ãŒã§ãã€çµæœã¨ã—ã¦è§£é‡ˆå¯èƒ½æ€§ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
The idea of differentiable masks is not new.
å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã®è€ƒãˆæ–¹ã¯æ–°ã—ã„ã‚‚ã®ã§ã¯ãªã„ã€‚
In the language modeling, differentiable masks have been shown to be very powerful to extract short yet sufficient sentences, which achieves better performance [1, 13].
è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ãŠã„ã¦ã€å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã¯ã€çŸ­ã„ãŒååˆ†ãªæ–‡ç« ã‚’æŠ½å‡ºã™ã‚‹ã®ã«éå¸¸ã«å¼·åŠ›ã§ã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ãŠã‚Šã€ã‚ˆã‚Šè‰¯ã„æ€§èƒ½ã‚’é”æˆã—ã¦ã„ã‚‹[1, 13]ã€‚

One way to encourage sparsity of $M^{(l)}$ is to explicitly penalize the number of non-zero entries of $Z^{(l)}$, for $1 \leq l \leq L$, by minimizing:
M^{(l)}$ ã®ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã‚’ä¿ƒã™æ–¹æ³•ã®ä¸€ã¤ã¨ã—ã¦ã€$Z^{(l)}$ ã®0ã§ãªã„ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã®æ•°ã‚’ã€1 \l L$ ã«å¯¾ã—ã¦ã€æ˜ç¤ºçš„ã«ãƒšãƒŠãƒ«ãƒ†ã‚£ã¨ã—ã¦ã€æœ€å°åŒ–ã™ã‚‹æ–¹æ³•ãŒã‚ã‚Šã¾ã™ã€‚

$$
R_M = \sum_{l=1}^{L}||Z^{l}||_{0}
= \sum_{l=1}^{L} \sum_{u=1}^{n} \sum_{v=1}^{n} I[Z_{u,v}^{(l)} \neq 0],
\tag{6}
$$

where $I[c]$ is an indicator that is equal to 1 if the condition $c$ holds and 0 otherwise; and $
|\cdot

However, there are two challenges for optimizing $Z^{(l)}$: non-differentiability and large variance.
ã—ã‹ã—ã€$Z^{(l)}$ã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã«ã¯ã€éå¾®åˆ†æ€§ã¨å¤§ããªåˆ†æ•£ã®2ã¤ã®èª²é¡ŒãŒã‚ã‚‹ã€‚
$L_0$ is discontinuous and has zero derivatives almost everywhere.
L_0$ã¯ä¸é€£ç¶šã§ã‚ã‚Šã€ã»ã¼å…¨åŸŸã§ã‚¼ãƒ­å¾®åˆ†ã¨ãªã‚‹ã€‚
Additionally, there are $2^{n^2}$ possible states for the binary mask $Z^{(l)}$ with large variance.
ã•ã‚‰ã«ã€ãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯$Z^{(l)}$ã®å¯èƒ½ãªçŠ¶æ…‹ã¯$2^{n^2}$ã§ã‚ã‚Šã€åˆ†æ•£ãŒå¤§ãã„ã€‚
Next, we propose an efficient estimator to solve this stochastic binary optimization problem.
æ¬¡ã«ã€ã“ã®ç¢ºç‡çš„äºŒå€¤æœ€é©åŒ–å•é¡Œã‚’è§£ããŸã‚ã®åŠ¹ç‡çš„ãªæ¨å®šå™¨ã‚’ææ¡ˆã™ã‚‹ã€‚

### 4.1.2. Efficient Gradient Computation 4.1.2. åŠ¹ç‡çš„ãªå‹¾é…è¨ˆç®—

$$
L(Z, \Theta) = L_{BCE}({A^{(l)} \odot Z^{(l)}}, \Theta) + \beta \cdot \sum_{l=1}^{L} \sum_{u=1}^{n} \sum_{v=1}^{n} I[Z_{u,v}^{(l)} \neq 0]
\tag{7}
$$

## 4.2. Jacobian Regularization 4.2. ãƒ¤ã‚³ãƒ“ã‚¢ã®æ­£å‰‡åŒ–

## 4.3. Optimization 4.3. æœ€é©åŒ–

### 4.3.1. Joint Training 4.3.1. åˆåŒè¨“ç·´

### 4.3.2. Model Complexity 4.3.2. ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•

# 5. Experiments 5. å®Ÿé¨“

## 5.1. Experimental Setting 5.1. å®Ÿé¨“è¨­å®š

### 5.1.1. Dataset 5.1.1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

### 5.1.2. Baselines 5.1.2. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

### 5.1.3. Evaluation metrics 5.1.3. è©•ä¾¡æŒ‡æ¨™

### 5.1.4. Parameter settings 5.1.4. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š

## 5.2. Overall Performance(RQ1) 5.2. ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹(RQ1)

## 5.3. Robustness to Noises(RQ2) 5.3. ãƒã‚¤ã‚ºã«å¯¾ã™ã‚‹ãƒ­ãƒã‚¹ãƒˆæ€§(RQ2)

## 5.4. Study of Rec-Denoiser(RQ3) 5.4. Rec-Denoiserã®ç ”ç©¶(RQ3)

We further investigate the parameter sensitivity of Rec-Denoiser.
ã•ã‚‰ã«ã€Rec-Denoiserã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ„Ÿåº¦ã‚’èª¿æŸ»ã—ãŸã€‚
For the number of blocks ğ¿ and the number of heads ğ», we find that self-attentive models typically benefit from small values (e.g., ğ», ğ¿ â‰¤ 4), which is similar to [31, 41].
ãƒ–ãƒ­ãƒƒã‚¯æ•°áµƒã¨ãƒ˜ãƒƒãƒ‰æ•°áµƒã«ã¤ã„ã¦ã€è‡ªå·±æ³¨æ„å‹ãƒ¢ãƒ‡ãƒ«ã¯å…¸å‹çš„ã«å°ã•ãªå€¤ï¼ˆä¾‹ãˆã°ã€áµƒâ‰¤ 4ï¼‰ã‹ã‚‰æ©æµã‚’å—ã‘ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã€ã“ã‚Œã¯[31, 41]ã¨åŒæ§˜ã§ã‚ã‚‹ã€‚
In this section, we mainly study the following hyper-parameters:
æœ¬ç¯€ã§ã¯ã€ä¸»ã«ä»¥ä¸‹ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦æ¤œè¨ã™ã‚‹ã€‚

1. the maximum length ğ‘›, 2) the regularizers ğ›½ and ğ›¾ to control the sparsity and smoothness.
1. æœ€å¤§é•· ğ‘›, 2) æ­£å‰‡åŒ– Ç– ã¨ ğ›¾ ã§ç–å¯†ã¨å¹³æ»‘æ€§ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚
   Here we only study the SASRec and SASRec-Denoiser due to page limitations.
   ã“ã“ã§ã¯ã€ãƒšãƒ¼ã‚¸ã®åˆ¶é™ã‹ã‚‰SASRecã¨SASRec-Denoiserã«ã¤ã„ã¦ã®ã¿æ¤œè¨ã™ã‚‹ã€‚

Fig. 3.
å›³3.
Effect of maximum length ğ‘› on ranking performance (Hit@10).
æœ€å¤§é•·ğ‘›ãŒãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿ï¼ˆHit@10ï¼‰ã€‚

Fig. 4.
å›³ 4.
Effect of regularizers ğ›½ and ğ›¾ on ranking performance (Hit@10).
æ­£å‰‡åŒ– Ç– ã¨ Ç– ãŒãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿(Hit@10)ã€‚

### 5.4.1. Maximum Length $n$ 5.4.1. æœ€å¤§é•· $n$

Figure 3 shows the Hit@10 for maximum length ğ‘› from 20 to 80 while keeping other optimal hyper-parameters unchanged.
å›³3ã¯ã€ä»–ã®æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰æ›´ã›ãšã€æœ€å¤§é•·ğ‘›ã‚’20ã‹ã‚‰80ã«ã—ãŸå ´åˆã®Hit@10ã‚’ç¤ºã—ãŸã‚‚ã®ã§ã™ã€‚
We only test on the densest and sparsest datasets:
æœ€ã‚‚é«˜å¯†åº¦ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨æœ€ã‚‚ç–ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®ã¿ãƒ†ã‚¹ãƒˆã—ã¦ã„ã¾ã™ã€‚
MovieLeans and Beauty.
MovieLeansã¨Beautyã®2ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ†ã‚¹ãƒˆã—ã¦ã„ã¾ã™ã€‚
Intuitively, the larger sequence we have, the larger probability that the sequence contains noisy items.
ç›´æ„Ÿçš„ã«ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒå¤§ãã‘ã‚Œã°å¤§ãã„ã»ã©ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ãƒã‚¤ã‚ºã®å¤šã„é …ç›®ãŒå«ã¾ã‚Œã‚‹ç¢ºç‡ãŒé«˜ããªã‚‹ã€‚
FWe observed that our SASRec-Denoiser improves the performance dramatically with longer sequences.
ãã®çµæœã€æˆ‘ã€…ã®SASRec-Denoiserã¯é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§åŠ‡çš„ã«æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚
This demonstrates that our design is more suitable for longer inputs, without worrying about the quality of sequences.
ã“ã‚Œã¯ã€æˆ‘ã€…ã®è¨­è¨ˆãŒã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å“è³ªã‚’æ°—ã«ã™ã‚‹ã“ã¨ãªãã€ã‚ˆã‚Šé•·ã„å…¥åŠ›ã«é©ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

### 5.4.2. The regularizers $\beta$ and $\gamma$ 5.4.2. æ­£å‰‡åŒ–å¤‰æ•°$Î²$ã¨$Î˜gamma$ã€‚

There are two major regularization parameters ğ›½ and ğ›¾ for sparsity and gradient smoothness, respectively.
æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¯ã€ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã¨å‹¾é…å¹³æ»‘æ€§ã®ãŸã‚ã«ã€ãã‚Œãã‚ŒÇ–ã¨áµ¯ã®2ã¤ãŒã‚ã‚‹ã€‚
Figure 4 shows the performance by changing one parameter while fixing the other as 0.01.
å›³4ã¯ã€ä¸€æ–¹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’0.01ã«å›ºå®šã—ã€ä»–æ–¹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰åŒ–ã•ã›ãŸå ´åˆã®æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
As can be seen, our performance is relatively stable with respect to different settings.
è¦‹ã¦ã‚ã‹ã‚‹ã‚ˆã†ã«ã€æˆ‘ã€…ã®æ€§èƒ½ã¯ç•°ãªã‚‹è¨­å®šã«å¯¾ã—ã¦æ¯”è¼ƒçš„å®‰å®šã—ã¦ã„ã‚‹ã€‚
In the experiments, the best performance can be achieved at ğ›½ = 0.01 and ğ›¾ = 0.001 for the MovieLens dataset.
å®Ÿé¨“ã§ã¯ï¼ŒMovieLensãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã„ã¦ï¼ŒÇ– = 0.01 ã¨Ç– = 0.001 ã§æœ€é«˜ã®æ€§èƒ½ã‚’é”æˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼

# 6. Conclusion and Fture Work 6. çµè«–ã¨ãƒ•ãƒ¥ãƒ¼ãƒãƒ£ãƒ¼ãƒ¯ãƒ¼ã‚¯

In this work, we propose Rec-Denoiser to adaptively eliminate the negative impacts of the noisy items for self-attentive recommender systems.
æœ¬ç ”ç©¶ã§ã¯ã€è‡ªå·±æ³¨è¦–å‹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã„ã¦ã€ãƒã‚¤ã‚ºã®å¤šã„ã‚¢ã‚¤ãƒ†ãƒ ã®æ‚ªå½±éŸ¿ã‚’é©å¿œçš„ã«é™¤å»ã™ã‚‹Rec-Denoiserã‚’ææ¡ˆã™ã‚‹ã€‚
The proposed Rec-Denoiser employs differentiable masks for the self-attention layers, which can dynamically prune irrelevant information.
ææ¡ˆã™ã‚‹Rec-Denoiserã¯ã€è‡ªå·±æ³¨è¦–å±¤ã«å¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ç„¡é–¢ä¿‚ãªæƒ…å ±ã‚’å‹•çš„ã«åˆˆã‚Šå–ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
To further tackle the vulnerability of self-attention networks to small perturbations, Jacobian regularization is applied to the Transformer blocks to improve the robustness.
ã•ã‚‰ã«ã€å°ã•ãªæ‘‚å‹•ã«å¯¾ã™ã‚‹è‡ªå·±æ³¨è¦–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è„†å¼±æ€§ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€Transformerãƒ–ãƒ­ãƒƒã‚¯ã«ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–ã‚’é©ç”¨ã—ã€é ‘å¥æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚
Our experimental results on multiple real-world sequential recommendation tasks illustrate the effectiveness of our design.
å®Ÿä¸–ç•Œã®è¤‡æ•°ã®é€æ¬¡æ¨è–¦ã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹å®Ÿé¨“çµæœã‹ã‚‰ã€æˆ‘ã€…ã®è¨­è¨ˆã®æœ‰åŠ¹æ€§ãŒç¤ºã•ã‚Œã‚‹ã€‚

Our proposed Rec-Denoiser framework (e.g., differentiable masks and Jacobian regularization) can be easily applied to any Transformer-based models in many tasks besides sequential recommendation.
æˆ‘ã€…ã®ææ¡ˆã™ã‚‹Rec-Denoiserãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆå¾®åˆ†å¯èƒ½ãªãƒã‚¹ã‚¯ã‚„ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ­£å‰‡åŒ–ãªã©ï¼‰ã¯ã€é€æ¬¡æ¨è–¦ä»¥å¤–ã®å¤šãã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã«å®¹æ˜“ã«é©ç”¨ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã‚ã‚‹ã€‚
In the future, we will continue to demonstrate the contributions of our design in many real-world applications.
ä»Šå¾Œã€å¤šãã®å®Ÿä¸–ç•Œã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ãŠã„ã¦ã€æˆ‘ã€…ã®è¨­è¨ˆã®è²¢çŒ®åº¦ã‚’å®Ÿè¨¼ã—ã¦ã„ãäºˆå®šã§ã‚ã‚‹ã€‚
