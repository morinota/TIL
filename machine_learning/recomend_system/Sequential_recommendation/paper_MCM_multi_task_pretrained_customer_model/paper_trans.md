## 0.1. Link ãƒªãƒ³ã‚¯

- https://dl.acm.org/doi/10.1145/3604915.3608868 https://dl.acm.org/doi/10.1145/3604915.3608868

## 0.2. title ã‚¿ã‚¤ãƒˆãƒ«

MCM: A Multi-task Pre-trained Customer Model for Personalization
MCMï¼š
ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯äº‹å‰å­¦ç¿’æ¸ˆã¿é¡§å®¢ãƒ¢ãƒ‡ãƒ«

## 0.3. abstract æŠ„éŒ²

Personalization plays a critical role in helping customers discover the products and contents they prefer for e-commerce stores.Personalized recommendations differ in contents, target customers, and UI.
ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé¡§å®¢ã€UIãŒç•°ãªã‚Šã¾ã™ã€‚
However, they require a common core capability - the ability to deeply understand customersâ€™ preferences and shopping intents.
ã—ã‹ã—ã€ã“ã‚Œã‚‰ã«ã¯**å…±é€šã®ã‚³ã‚¢èƒ½åŠ›**ãŒå¿…è¦: ã¤ã¾ã‚Šé¡§å®¢ã®å—œå¥½ã‚„è³¼è²·æ„å›³ã‚’æ·±ãç†è§£ã™ã‚‹èƒ½åŠ›ã€‚(ä¸‹æµã‚¿ã‚¹ã‚¯æ¯ã«è‰²ã€…æ¡ä»¶ã¯ç•°ãªã‚‹ã‘ã©ã€ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã‚„è¡Œå‹•æ„å›³ã‚’ç†è§£ã™ã‚‹ã¨ã„ã†ç‚¹ã¯å…±é€šã—ã¦ã‚‹ã‚ˆã­...!ã£ã¦è©±:thinking:)
In this paper, we introduce the MCM (Multi-task pre-trained Customer Model), a large pre-trained BERT-based multi-task customer model with 10 million trainable parameters for e-commerce stores.
æœ¬è«–æ–‡ã§ã¯ã€1,000ä¸‡å€‹ã®å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ã€**BERTãƒ™ãƒ¼ã‚¹ã®å¤§è¦æ¨¡ãªäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯é¡§å®¢ãƒ¢ãƒ‡ãƒ«**ã§ã‚ã‚‹**MCMï¼ˆMulti-task pre-trained Customer Modelï¼‰**ã‚’ç´¹ä»‹ã™ã‚‹ã€‚
This model aims to empower all personalization projects by providing commonly used preference scores for recommendations, customer embeddings for transfer learning, and a pre-trained model for fine-tuning.
ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆãä½¿ã‚ã‚Œã‚‹ãƒ—ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ãƒ»ã‚¹ã‚³ã‚¢ã€è»¢ç§»å­¦ç¿’ã®ãŸã‚ã®é¡§å®¢embeddingã€fine-tuningã®ãŸã‚ã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã€**ã‚ã‚‰ã‚†ã‚‹ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«åŠ›ã‚’ä¸ãˆã‚‹ã“ã¨**ã‚’ç›®çš„ã¨ã—ã¦ã„ã‚‹ã€‚
In this work, we improve the SOTA BERT4Rec framework to handle heterogeneous customer signals and multi-task training as well as innovate new data augmentation method that is suitable for recommendation task.
æœ¬ç ”ç©¶ã§ã¯ã€SOTA BERT4Recãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æ”¹è‰¯ã—ã€**heterogeneous customer signals(ç•°ç¨®ãƒ¦ãƒ¼ã‚¶ä¿¡å·?)**ã‚„**multi-task training**ã‚’æ‰±ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹ã¨ã¨ã‚‚ã«ã€æ¨è–¦ã‚¿ã‚¹ã‚¯ã«é©ã—ãŸæ–°ãŸãªdata augmentation method(ç”»åƒãƒ‡ãƒ¼ã‚¿ã¿ãŸã„ãªãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ??:thinking:)ã‚’è€ƒæ¡ˆã—ãŸã€‚
Experimental results show that MCM outperforms the original BERT4Rec by 17% on on NDCG@10 of next action prediction tasks.
å®Ÿé¨“ã®çµæœã€æ¬¡ã®è¡Œå‹•äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã®NDCG@10ã«ãŠã„ã¦ã€MCMã¯ã‚ªãƒªã‚¸ãƒŠãƒ«ã®BERT4Recã‚’17%ä¸Šå›ã£ãŸã€‚
Additionally, we demonstrate that the model can be easily fine-tuned to assist a specific recommendation task.
ã•ã‚‰ã«ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ç‰¹å®šã®æ¨è–¦ã‚¿ã‚¹ã‚¯ã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã«å®¹æ˜“ã«fine-tuningã§ãã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã™ã‚‹ã€‚(=ç›®çš„é–¢æ•°ã‚„å‡ºåŠ›å±¤å‘¨ã‚Šã‚’å¤‰ãˆã¦ã€å­¦ç¿’ã—ç›´ã™ã€ã£ã¦ã‚¤ãƒ¡ãƒ¼ã‚¸:thinking:)
For instance, after fine-tuning MCM for an incentive based recommendation project, performance improves by 60% on the conversion prediction task and 25% on the click-through prediction task compared to a baseline tree-based GBDT model.
ä¾‹ãˆã°ã€ã‚¤ãƒ³ã‚»ãƒ³ãƒ†ã‚£ãƒ–ãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç”¨ã«MCMã‚’å¾®èª¿æ•´ã—ãŸçµæœã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ãƒ„ãƒªãƒ¼ãƒ™ãƒ¼ã‚¹ã®GBDTãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦ã€ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã§60%ã€ã‚¯ãƒªãƒƒã‚¯ã‚¹ãƒ«ãƒ¼äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã§25%ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Šã—ãŸã€‚

# 1. Introduction ã¯ã˜ã‚ã«

In a personalized recommendation system, it is critical to understand each customerâ€™s preference and shopping intent holistically based on customersâ€™ profile as well as comprehensive historical behaviors like browsing, searching and purchasing signals.
ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºãƒ‰ãƒ»ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€é¡§å®¢ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã ã‘ã§ãªãã€ãƒ–ãƒ©ã‚¦ã‚¸ãƒ³ã‚°ã€æ¤œç´¢ã€è³¼è²·ã‚·ã‚°ãƒŠãƒ«ãªã©ã®åŒ…æ‹¬çš„ãªéå»ã®è¡Œå‹•ã«åŸºã¥ã„ã¦ã€å„é¡§å®¢ã®å—œå¥½ã¨ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã®æ„å›³ã‚’ç·åˆçš„ã«ç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã‚ã‚‹ã€‚
Ecommerce stores usually have tens of billions of customer historical behavior data.
Eã‚³ãƒãƒ¼ã‚¹ã‚¹ãƒˆã‚¢ã¯é€šå¸¸ã€**ä½•ç™¾å„„ã¨ã„ã†é¡§å®¢ã®éå»ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿**ã‚’æŒã£ã¦ã„ã‚‹ã€‚
These signals, if learned with a large capacity model, can help to provide the grounding of customers understanding and be utilized by thousands of downstream use cases.
**ã“ã‚Œã‚‰ã®ä¿¡å·ã¯ã€å¤§å®¹é‡ãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’ã™ã‚Œã°ã€é¡§å®¢ã®ç†è§£ã®åŸºç¤ã¨ãªã‚Šã€ä½•åƒã‚‚ã®ä¸‹æµã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã§æ´»ç”¨ã•ã‚Œã‚‹**ã€‚
Bert-based pretrained model like GPT has been successful in NLP and CV [4, 5, 8], Researchers in recommendation field explore Bert-based model on specific task like session-based recommendation [9], under the setting of sequential recommendation [2, 3, 6, 7].
GPTã®ã‚ˆã†ãªBertãƒ™ãƒ¼ã‚¹ã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯ã€NLPã‚„CV[4, 5, 8]ã§æˆåŠŸã—ã¦ãŠã‚Šã€æ¨è–¦åˆ†é‡ã®ç ”ç©¶è€…ã¯ã€Sequentialæ¨è–¦[2, 3, 6, 7]ã®è¨­å®šä¸‹ã§ã€**ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ã®æ¨è–¦[9]ã®ã‚ˆã†ãªç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã§**Bertãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ç ”ç©¶ã—ã¦ã„ã‚‹ã€‚(ã‚ãã¾ã§ã€session-basedãªæ¨è–¦ã®ã¿ã«ãŠã„ã¦ä½¿ã‚ã‚Œã¦ã‚‹...!!:thinking:)
However, large-scale pre-training with Bert is still largely under-explored in the field of recommender systems.
ã—ã‹ã—ã€Bertã‚’ä½¿ã£ãŸå¤§è¦æ¨¡ãªäº‹å‰å­¦ç¿’ã¯ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®åˆ†é‡ã§ã¯ã¾ã ã»ã¨ã‚“ã©ç ”ç©¶ã•ã‚Œã¦ã„ãªã„ã€‚

In this paper, we propose Multi-task pre-trained Customer Model (MCM) model: a large-capacity Bert-based multi-task model with 10 million trainable parameters, pre-trained on a vast amount of behavior data from a large e-commence service.
æœ¬ç¨¿ã§ã¯ã€ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯äº‹å‰å­¦ç¿’æ¸ˆã¿é¡§å®¢ãƒ¢ãƒ‡ãƒ«(MCM)ã‚’ææ¡ˆã™ã‚‹ï¼š
ã“ã‚Œã¯ã€å¤§è¦æ¨¡ãªeã‚³ãƒãƒ¼ã‚¹ã‚µãƒ¼ãƒ“ã‚¹ã®è†¨å¤§ãªè¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦äº‹å‰å­¦ç¿’ã•ã‚ŒãŸã€1,000ä¸‡å€‹ã®å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤å¤§å®¹é‡ã®Bertãƒ™ãƒ¼ã‚¹ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã€‚
We make architectural improvements for better pre-training, which will be explained in more detail in the model section.
ã‚ˆã‚Šè‰¯ã„äº‹å‰å­¦ç¿’ã®ãŸã‚ã«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ”¹è‰¯ã—ã¦ã„ã‚‹ãŒã€ã“ã‚Œã«ã¤ã„ã¦ã¯ãƒ¢ãƒ‡ãƒ«ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§è©³ã—ãèª¬æ˜ã™ã‚‹ã€‚
Offline results show that MCM outperform the original Bert4rec on multi-tasks by average 17% on NDCG@10 of next action prediction.
ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã®çµæœã€MCMã¯ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€æ¬¡ã®è¡Œå‹•äºˆæ¸¬ã®NDCG@10ã«ãŠã„ã¦ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã®Bert4recã‚’å¹³å‡17%ä¸Šå›ã£ãŸã€‚
In order to evaluate the ability to support new personalization tasks, we fine-tune the model for an incentive offer recommendation task, the performance improves by 60% on conversion rate and 25% on click-through rate, compared to the baseline GBDT model.
æ–°ã—ã„ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€incentive offerã®æ¨è–¦ã‚¿ã‚¹ã‚¯ç”¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸã¨ã“ã‚ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®GBDTãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦ã€ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡ã§60%ã€ã‚¯ãƒªãƒƒã‚¯ã‚¹ãƒ«ãƒ¼ç‡ã§25%ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Šã—ã¾ã—ãŸã€‚
(incentive offer = ãƒ¦ãƒ¼ã‚¶ã«å¯¾ã—ã¦å•†å“ã®è³¼å…¥ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã®åˆ©ç”¨ã‚’ã™ã‚‹éš›ã®å‹•æ©Ÿã¥ã‘ã¨ã—ã¦ã€å‰²å¼•ã€ç‰¹å…¸ã€ãƒã‚¤ãƒ³ãƒˆç­‰ã®incentiveã‚’æä¾›ã™ã‚‹ã“ã¨)
(session-basedãªæ¨è–¦ã‚¿ã‚¹ã‚¯ã§äº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ã€incentive offerç”¨ã«fine-tuningã™ã‚‹äº‹ã§ã€end-to-endã§å­¦ç¿’ã•ã›ãŸGBDTã‚ˆã‚Šã‚‚æ€§èƒ½ãŒå‘ä¸Šã—ãŸã‚ˆã€ã¨ã„ã†è©±ã‹ãª:thinking:)

# 2. Methodology æ–¹æ³•è«–

## 2.1. Model Framework ãƒ¢ãƒ‡ãƒ«ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

MCM consists of three modules: embedding module, sequential encoding module and readout module.
MCMã¯3ã¤ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹: embedding moduleã€sequential encoding moduleã€readout module1ã€‚
We inherited the sequential encoding module from Bert4rec, while make algorithm improvements on the other two modules, which we will introduce in more detail in the following sessions.
Bert4recã®sequential encoding moduleã‚’ç¶™æ‰¿ã—ã¤ã¤ã€ä»–ã®2ã¤ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ”¹è‰¯ã‚’åŠ ãˆãŸã€‚

![figure1]()

### 2.1.1. Heterogeneous(ç•°ç¨®ã®) Embedding Module.

In this module, we convert the raw inputs into distributed representations through embedding lookup, as is typically done in bert-based models.
ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã¯ã€bert-basedã®ãƒ¢ãƒ‡ãƒ«ã§ä¸€èˆ¬çš„ã«è¡Œã‚ã‚Œã‚‹ã‚ˆã†ã«ã€embedding lookupã‚’é€šã—ã¦ç”Ÿã®å…¥åŠ›ã‚’åˆ†æ•£è¡¨ç¾(=distributed representations=embedding?)ã«å¤‰æ›ã™ã‚‹ã€‚
(embedding lookup=sparseãƒ™ã‚¯ãƒˆãƒ«ã‚’denceãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã€‚"lookup" = ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚„ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ç‰¹å®šã®æƒ…å ±ã‚’æ¢ã—å‡ºã™æ“ä½œã€‚ã˜ã‚ƒã‚ã€embedding lookupã¯ã€embedding tableçš„ãªã‚‚ã®ã‹ã‚‰å¯¾è±¡entityã®embeddingã‚’æ¢ã—å‡ºã™æ“ä½œã€ã¿ãŸã„ãªã‚¤ãƒ¡ãƒ¼ã‚¸??:thinking:)

The original input is a heterogeneous interaction sequence including purchase actions and non-purchase actions.
å…ƒã®å…¥åŠ›ã¯ã€**è³¼å…¥è¡Œå‹•ã¨éè³¼å…¥è¡Œå‹•ã‚’å«ã‚€heterogeneousãª(ç•°ç¨®ãª)interaction sequence**ã§ã‚ã‚‹ã€‚(=ã„ã‚ã‚“ãªç¨®é¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ­ã‚°)
Non-purchase actions are actions that are High Value Actions (HVAs) customers have with the e-commerce stores, e.g.member sign-up, mobile camera search, click records of products etc.
**éè³¼è²·ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**ã¨ã¯ã€ä¼šå“¡ç™»éŒ²ã€æºå¸¯ã‚«ãƒ¡ãƒ©æ¤œç´¢ã€å•†å“ã®ã‚¯ãƒªãƒƒã‚¯è¨˜éŒ²ãªã©ã€é¡§å®¢ãŒECåº—èˆ—ã§è¡Œã†**HVA(High Value Action)**(??)ã¨ãªã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã“ã¨ã€‚
Instead of feeding the raw inputs into the embedding module, we choose to represent each interaction $i$ with a set of features $f_{i}^{j}$, $j \in 1, 2, \cdots, |J|$, where $|J|$ denotes the total number of features for each interaction.
åŸ‹ã‚è¾¼ã¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ç”Ÿã®å…¥åŠ›ã‚’å…¥åŠ›ã™ã‚‹ä»£ã‚ã‚Šã«ã€**å„ç›¸äº’ä½œç”¨ $i$ ã‚’ç‰¹å¾´é‡ $f_{i}^{j}$ ã®é›†åˆã§è¡¨ç¾ã™ã‚‹**ã“ã¨ã«ã™ã‚‹($j \in 1, 2, \cdots, |J|$)ã€ã“ã“ã§ã€$|J|$ã¯å„ç›¸äº’ä½œç”¨ã®ç‰¹å¾´é‡ã®ç·æ•°ã‚’è¡¨ã™ã€‚
Now the inputs of each customer $c$ are $|J|$ sequences, with the $j$-th sequence in the form of $F^{j} = [f_{1}^{j}, f_{2}^{j}, \cdots, f_{n_i}^{j}]$.
ã“ã“ã§ã€å„é¡§å®¢ $c$ ã®å…¥åŠ›ã¯ $|J|$ å€‹ã®ç³»åˆ—ã§ã‚ã‚Šã€$j$ ç•ªç›®ã®ç³»åˆ—ã¯ $F^{j} = [f_{1}^{j}, f_{2}^{j}, \cdots, f_{n_i}^{j}]$ ã®å½¢ã§ã‚ã‚‹ã€‚
Currently, the features include the hierarchical structures of the product: product line, category, subcategory as well as brand.
ç¾åœ¨ã€ã“ã®ç‰¹å¾´é‡ã«ã¯å•†å“ã®éšå±¤æ§‹é€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹ï¼š
è£½å“ãƒ©ã‚¤ãƒ³ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼ã€ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼ã€ãƒ–ãƒ©ãƒ³ãƒ‰ã€‚
Additionally, we design a feature called token type to handle heterogeneous input, making it easier for the model to differentiate different types of interactions.
ã•ã‚‰ã«ã€"token type"ã¨å‘¼ã°ã‚Œã‚‹ç‰¹å¾´é‡ã‚’è¨­è¨ˆã—ã€ç•°ç¨®å…¥åŠ›ã‚’æ‰±ã†ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®ç›¸äº’ä½œç”¨ã‚’åŒºåˆ¥ã—ã‚„ã™ãã—ã¦ã„ã‚‹ã€‚

Each distinct feature value is assigned a unique embedding vector.
å„ç‰¹å¾´é‡($f_{1}^{j}$ ã®äº‹?)ã«ã¯å›ºæœ‰ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã‚‹ã€‚
After the embedding look-up, the inputs are converted to $|J|$ sequences of embeddings, we perform average pooling to these sequences, producing a single embedding sequence $\mathbf{E} = [\mathbf{e}_1, \mathbf{e}_2, \cdots, \mathbf{e}_{n_i}]$.
åŸ‹ã‚è¾¼ã¿ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å¾Œã€å…¥åŠ›ã‚’ $|J|$ å€‹ã®åŸ‹ã‚è¾¼ã¿åˆ—ã«å¤‰æ›ã—ã€ã“ã‚Œã‚‰ã®åˆ—ã«average poolingã‚’è¡Œã„ã€1ã¤ã®åŸ‹ã‚è¾¼ã¿åˆ— $\mathbf{E} = [\mathbf{e}_1, \mathbf{e}_2, \cdots, \mathbf{e}_{n_i}]$ ã‚’ç”Ÿæˆã™ã‚‹ã€‚
The bert model requires position embedding to capture the order of sequences, we adopt learnable position embeddings $P$ for better performance, which is randomly initialized learnable parameters.
bertãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é †åºã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹ãŸã‚ã«ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’å¿…è¦ã¨ã™ã‚‹ãŸã‚ã€ã‚ˆã‚Šè‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¾—ã‚‹ãŸã‚ã«å­¦ç¿’å¯èƒ½ãªä½ç½®åŸ‹ã‚è¾¼ã¿ $P$ ã‚’æ¡ç”¨ã—ã€ã“ã‚Œã¯ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã•ã‚ŒãŸå­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚‹ã€‚
The output of the embedding module is then
åŸ‹ã‚è¾¼ã¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å‡ºåŠ›ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚‹ã€‚

$$
\mathbf{H} = \mathbf{E} + \mathbf{P}
\tag{1}
$$

### 2.1.2. Task-Aware Attentional Readout Module. ã‚¿ã‚¹ã‚¯ã‚’æ„è­˜ã—ãŸæ³¨æ„èª­ã¿å‡ºã—ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚

The output of the sequential encoding module is a sequence of hidden vectors, with the same length as the input sequence.
sequential encoding moduleã®å‡ºåŠ›ã¯ã€å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹(=$\mathbf{H}$)ã¨åŒã˜é•·ã•ã®éš ã‚Œãƒ™ã‚¯ãƒˆãƒ«ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§ã‚ã‚‹ã€‚(sequential encoding moduleã¯ã€BERT4Recã¨åŒã˜ã€‚)
Previous work[9] computes the inner product between the last hidden vector and the item embedding to produce the score for the corresponding item.
å…ˆè¡Œç ”ç©¶[9]ã¯ã€æœ€å¾Œ(=æ¨è«–æ™‚ã«sequenceã®æœ€å¾Œã«è¿½åŠ ã™ã‚‹ç‰¹æ®Štoken "mask":thinking:)ã®éš ã‚Œãƒ™ã‚¯ãƒˆãƒ«ã¨itemåŸ‹ã‚è¾¼ã¿ã¨ã®å†…ç©ã‚’è¨ˆç®—ã—ã€å¯¾å¿œã™ã‚‹itemã®ã‚¹ã‚³ã‚¢(next-itemç¢ºç‡!)ã‚’ç”Ÿæˆã™ã‚‹ã€‚(=ã‚·ãƒ³ãƒ—ãƒ«ã«dot-productã«ã‚ˆã‚‹score prediction module:thinking:)
This can be sub-optimal since the last hidden vector is a fixed representation of the whole behavior sequence, which is not aware of the specific item or task to predict.
æœ€å¾Œ(=sequenceã®æœ€å¾Œã®token?:thinking:)ã®éš ã‚Œãƒ™ã‚¯ãƒˆãƒ«ã¯è¡Œå‹•sequenceå…¨ä½“ã®å›ºå®šè¡¨ç¾ã§ã‚ã‚Šã€**äºˆæ¸¬ã™ã‚‹ç‰¹å®šã®ã‚¢ã‚¤ãƒ†ãƒ ã‚„ã‚¿ã‚¹ã‚¯ã‚’æ„è­˜ã—ã¦ã„ãªã„ãŸã‚ã€ã“ã‚Œã¯æœ€é©ã¨ã¯è¨€ãˆãªã„å¯èƒ½æ€§ãŒã‚ã‚‹**ã€‚(=multi-taskãªãƒ¢ãƒ‡ãƒ«ã‚’ç›®æŒ‡ã—ã¦ã‚‹ã‹ã‚‰??:thinking:)
Different tasks may be related to different behaviors within the whole behavior sequence.
ç•°ãªã‚‹ã‚¿ã‚¹ã‚¯ã¯ã€è¡Œå‹•ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã®ä¸­ã§ç•°ãªã‚‹è¡Œå‹•ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚

We propose a novel task-aware attentional readout module, which allows different items (labels in each task) and different tasks to attend to different subsequences of the hidden sequence with attention mechanism , in order to produce a task-specific representation.
æˆ‘ã€…ã¯ã€task-awareãªæ–°ã—ã„attentional readout modulã‚’ææ¡ˆã™ã‚‹ã€‚ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸè¡¨ç¾ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ã€ç•°ãªã‚‹ã‚¢ã‚¤ãƒ†ãƒ (=å„ã‚¿ã‚¹ã‚¯ã®å›ç­”)ã¨ç•°ãªã‚‹ã‚¿ã‚¹ã‚¯ãŒã€attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«ã‚ˆã£ã¦ã€éš ã‚Œsequenceã®ç•°ãªã‚‹éƒ¨åˆ†sequenceã«æ³¨ç›®ã™ã‚‹ã“ã¨ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚
Specifically, let $\mathbf{h}_{i}$ denote the $i$-th embedding of the output of the sequential encoding module, and let $\mathbf{e}_{w}$ denote the embedding for a particular item $w$ in a certain task, the attentional readout operation can be described as:
å…·ä½“çš„ã«ã¯ã€secuential encoding moduleã®å‡ºåŠ›ã® $i$ ç•ªç›®ã®åŸ‹ã‚è¾¼ã¿ã‚’$\mathbf{h}_{i}$ ã¨ã—ã€ã‚ã‚‹ã‚¿ã‚¹ã‚¯ã®ç‰¹å®šã®item (=æ¨è–¦å€™è£œã‚¢ã‚¤ãƒ†ãƒ ?) $w$ ã«å¯¾ã™ã‚‹åŸ‹ã‚è¾¼ã¿ã‚’ $\mathbf{e}_{w}$ ã¨ã™ã‚‹ã¨ã€attentional readoutæ“ä½œã¯æ¬¡ã®ã‚ˆã†ã«è¨˜è¿°ã§ãã‚‹:

$$
a_{w, i} = \frac{\exp((\mathbf{e}_{w})^T \mathbf{h}_{i})}{\sum_{i} \exp((\mathbf{e}_{w})^T \mathbf{h}_{i})}
\\
\mathbf{r}_{w} = \sum_{i} a_{w, i} \mathbf{h}_{i}
\tag{2}
$$

where $\mathbf{r}_w$ is the representation of the input sequence, specific to item $w$.
ã“ã“ã§ã€$\mathbf{r}_w$ ã¯ã‚¢ã‚¤ãƒ†ãƒ  $w$ ã«å›ºæœ‰ã®å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®è¡¨ç¾ã§ã‚ã‚‹ã€‚
The predicted score for item ğ‘¤ is:
item $w$ ã®äºˆæ¸¬ã‚¹ã‚³ã‚¢ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚‹:

$$
s(w) = \mathbf{r}_{w}^T \mathbf{e}_{w}
\tag{3}
$$

Softmax operation is performed on the scores to produce the final distribution.
æœ€çµ‚çš„ãªåˆ†å¸ƒã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ã€ã‚¹ã‚³ã‚¢ã«å¯¾ã—ã¦ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹æ¼”ç®—ãŒå®Ÿè¡Œã•ã‚Œã‚‹ã€‚

## 2.2. Model Learning with prefix-augmentation ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹æ‹¡å¼µã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«å­¦ç¿’

Previous work[9] on sequential recommendation adopt a popular augmentation method in NLP called masked language model, which randomly masks out some tokens in the input sequence and asks the model to predict them based on all other tokens.
é€æ¬¡æ¨è–¦ã«é–¢ã™ã‚‹å…ˆè¡Œç ”ç©¶[9]ã§ã¯ã€**maskedè¨€èªãƒ¢ãƒ‡ãƒ«**ã¨å‘¼ã°ã‚Œã‚‹ã€è‡ªç„¶è¨€èªå‡¦ç†ã§ã‚ˆãä½¿ã‚ã‚Œã‚‹augumentationæ‰‹æ³•ã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã€‚ã“ã‚Œã¯ã€å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ä¸­ã®ã„ãã¤ã‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒã‚¹ã‚¯ã—ã€ä»–ã®ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åŸºã«äºˆæ¸¬ã™ã‚‹ã‚ˆã†ãƒ¢ãƒ‡ãƒ«ã«æ±‚ã‚ã‚‹ã‚‚ã®ã§ã‚ã‚‹ã€‚(masked-item-predictionã‚’äº‹å‰å­¦ç¿’ã‚¿ã‚¹ã‚¯ã§ä½¿ã£ã¦ã‚‹ã£ã¦è©±ã‹ãª?:thinking:)
Such augmentation is suitable for language modeling, but we believe it can be problematic for recommendation tasks since it leaks future information.
ã“ã®ã‚ˆã†ãªè£œå¼·ã¯è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ã¯é©ã—ã¦ã„ã‚‹ãŒã€**æ¨è–¦ã‚¿ã‚¹ã‚¯ã§ã¯å°†æ¥ã®æƒ…å ±ãŒæ¼ã‚Œã¦ã—ã¾ã†ç‚º(ã†ã‚“ã†ã‚“ç¢ºã‹ã«:thinking:)ã€å•é¡ŒãŒã‚ã‚‹**ã¨æˆ‘ã€…ã¯è€ƒãˆã¦ã„ã‚‹ã€‚
We propose a new augmentation method called random prefix augmentation, which randomly samples a prefix from the whole input sequence, and ask the model to predict the last item.
æˆ‘ã€…ã¯ã€**random prefix augmentation**ã¨å‘¼ã°ã‚Œã‚‹æ–°ã—ã„augmentationæ–¹æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚ã“ã‚Œã¯ã€å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«prefixã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ãƒ¢ãƒ‡ãƒ«ã«æœ€å¾Œã®itemã‚’äºˆæ¸¬ã•ã›ã‚‹ã‚‚ã®ã§ã‚ã‚‹ã€‚
In this case, the input will only include the items before the last item, so that our augmentation avoids leaking future information.
ã“ã®å ´åˆã€å…¥åŠ›ã«ã¯æœ€å¾Œã®itemã‚ˆã‚Šå‰ã®itemsã—ã‹å«ã¾ã‚Œãªã„ã®ã§ã€æˆ‘ã€…ã®augumentationæ‰‹æ³•ã¯**å°†æ¥ã®æƒ…å ±ãŒæ¼ã‚Œã‚‹ã®ã‚’é¿ã‘ã‚‹**ã“ã¨ãŒã§ãã‚‹ã€‚
For example, suppose the original input sequence is [ğ‘–1,ğ‘–2,ğ‘–3], valid prefixes include [ğ‘–1] and [ğ‘–1,ğ‘–2].
ä¾‹ãˆã°ã€å…ƒã®å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒ[ğ‘–1,ğ‘–2,ğ‘–3]ã ã¨ã™ã‚‹ã¨ã€æœ‰åŠ¹ãªæ¥é ­è¾ã¯[ğ‘–1]ã¨[ğ‘–1,ğ‘–2]ã§ã‚ã‚‹ã€‚
The augmentation is performed at batch time rather than during data pre-processing, in order to save memory.
ãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„ã™ã‚‹ãŸã‚ã€augumentationã¯ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ä¸­ã§ã¯ãªãã€batchæ™‚ã«å®Ÿè¡Œã•ã‚Œã‚‹ã€‚

The loss function for each prefix is defined as the negative loglikelihood of the label item (the last item):
å„prefixã®æå¤±é–¢æ•°ã¯ã€label item(=æœ€å¾Œã®item)ã®è² ã®å¯¾æ•°å°¤åº¦ã¨ã—ã¦å®šç¾©ã•ã‚Œã‚‹:

$$
\mathcal{L} = - \log P(i = i_{gt}|S)
\tag{4}
$$

where $i_{gt}$ denotes the ground truth item, S denotes the input sequence, which contains all items but the last one.
ã“ã“ã§ã€$i_{gt}$ ã¯ground truth itemã‚’è¡¨ã—ã€$S$ ã¯æœ€å¾Œã®itemä»¥å¤–ã®ã™ã¹ã¦ã®itemã‚’å«ã‚€å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’è¡¨ã™ã€‚(leave-one-outçš„ãªã‚„ã¤ã !:thinking:)
For multi-task training, the loss of all tasks are summed together.
ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ»ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯ã€ã™ã¹ã¦ã®ã‚¿ã‚¹ã‚¯ã®æå¤±ãŒåˆè¨ˆã•ã‚Œã‚‹ã€‚

# 3. Experiment å®Ÿé¨“

## 3.1. Experiment Setup å®Ÿé¨“ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 3.1.1. Datasets and Evaluation Details. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨è©•ä¾¡ã®è©³ç´°

To train our models, we use data from a large e-commerce service.
ç§ãŸã¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹ãŸã‚ã«ã€å¤§è¦æ¨¡ãªé›»å­å•†å–å¼•ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
We use customer behaviour data sampled from 6 years customer history.
6å¹´é–“ã®é¡§å®¢å±¥æ­´ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãŸé¡§å®¢è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
The dataset consists of 40M customers and 10B interactions.
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯40Mã®é¡§å®¢ã¨10Bã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ã€‚
The behavior sequence includes three types of interactions: item purchases, item clicks and customer valuable actions.
**è¡Œå‹•ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ã¯3ç¨®é¡ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãŒå«ã¾ã‚Œã‚‹**:
ã‚¢ã‚¤ãƒ†ãƒ ã®è³¼å…¥ã€ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚¯ãƒªãƒƒã‚¯ã€é¡§å®¢ã®ä¾¡å€¤ã‚ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã§ã™ã€‚
For each purchase and click interaction, we use productsâ€™ hierarchical features including product line (PL), category, subcategory as well as brands.
å„è³¼è²·ã¨ã‚¯ãƒªãƒƒã‚¯ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã«ã¯ã€ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒ©ã‚¤ãƒ³ï¼ˆPLï¼‰ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼ã€ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼ã€ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’å«ã‚€**å•†å“ã®éšå±¤çš„ç‰¹å¾´é‡**ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
To note, these features are also the tasks we train the model to predict on customerâ€™s next preferences.
ã“ã‚Œã‚‰ã®ç‰¹å¾´é‡ã¯ã€é¡§å®¢ã®æ¬¡ã®å—œå¥½ã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã‚‚ã‚ã‚‹ã€‚(ã‚ã€next-item-predictionãªã‚‰ã¬ã€next-category-predictionã¨è¨€ã†æ„Ÿã˜ã§è©•ä¾¡ã™ã‚‹ã£ã¦äº‹ã­!:thinking:)
As suggested in [1], we split the dataset into training, validation and testing dataset by time to avoid leaking future information.
[1]ã§ææ¡ˆã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«ã€**å°†æ¥ã®æƒ…å ±æ¼ãˆã„ã‚’é¿ã‘ã‚‹ãŸã‚ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ™‚é–“ã”ã¨ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€æ¤œè¨¼ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åˆ†å‰²ã™ã‚‹**ã€‚
We adopt ranking metrics for evaluation, the primary metric is NDCG (Normalized Discounted Cumulative Gain), as well as recall and precision.
**è©•ä¾¡ã«ã¯ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ¡ç”¨**ã—ã€ä¸»ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯NDCGï¼ˆæ­£è¦åŒ–å‰²å¼•ç´¯ç©åˆ©å¾—ï¼‰ã€ãŠã‚ˆã³ãƒªã‚³ãƒ¼ãƒ«ã¨ç²¾åº¦ã§ã‚ã‚‹ã€‚
The Bert encoder consists of three transfomer layers, the head number is four.
ãƒãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¯3ã¤ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼å±¤ã§æ§‹æˆã•ã‚Œã€ãƒ˜ãƒƒãƒ‰æ•°ã¯4ã§ã‚ã‚‹ã€‚(ã†ã‚“ã†ã‚“ã€æ¨è–¦ç”¨ã®architectureã¯å°ã•ãã¦è‰¯ã„ã‚“ã ã‚ˆã­!:thinking:)(L=3, h=4)
The maximum sequence length is truncated to 300.
æœ€å¤§é…åˆ—é•·ã¯300ã«åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã‚‹ã€‚(N=300)

## 3.2. Quality of Preference Scores

![fig1]()

We first compare the performance between our model and a SOTA sequential recommendation model Bert4rec.
ã¾ãšã€æˆ‘ã€…ã®ãƒ¢ãƒ‡ãƒ«ã¨SOTAã®sequentialæ¨è–¦ãƒ¢ãƒ‡ãƒ« Bert4rec ã®æ€§èƒ½ã‚’æ¯”è¼ƒã™ã‚‹ã€‚
As illustrated in Table.1, MCM_final significantly outperforms bert4rec by about 11%.
è¡¨.1ã«ç¤ºã™ã‚ˆã†ã«ã€MCM_finalã¯bert4recã‚’ç´„11%å¤§å¹…ã«ä¸Šå›ã‚‹ã€‚
We also conduct ablation experiments with variants of MCM model MCM_Single and MCM_MTL.
ã¾ãŸã€MCMãƒ¢ãƒ‡ãƒ«MCM_Singleã¨MCM_MTLã®å¤‰ç•°ä½“ã‚’ç”¨ã„ãŸã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“ã‚‚è¡Œã£ãŸã€‚
Compared to Bert4rec, MCM_Single utilizs heterogeneous interaction sequence and contextual features to train each single task, and MCM_MTL utilizs attentional readout and MTL.
Bert4recã¨æ¯”è¼ƒã—ã¦ã€MCM_Singleã¯å„å˜ä¸€ã‚¿ã‚¹ã‚¯ã®å­¦ç¿’ã«ç•°ç¨®interactionã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨contexutualç‰¹å¾´é‡ã‚’åˆ©ç”¨ã—ã€MCM_MTLã¯attenional readoutã¨MTLã‚’åˆ©ç”¨ã™ã‚‹ã€‚(ã¨ã„ã†ã‹ã€Bert4Recã¯id-Onlyãªæ‰‹æ³•ã ã£ã‘??:thinking:)
MCM-Final utilizes heterogeneous data, attentional readout and pre-fix data augmentation.
MCM-Finalã¯ã€ç•°ç¨®ãƒ‡ãƒ¼ã‚¿ã€attenitional readoutã€prefix data augumentationã‚’åˆ©ç”¨ã™ã‚‹ã€‚
The results show that richer input signals contribute to the incrementality the most, while MTL and prefix augmentation are also helpful.
ãã®çµæœã€è±Šå¯Œãªå…¥åŠ›ä¿¡å·ãŒã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒªãƒ†ã‚£ã«æœ€ã‚‚å¯„ä¸ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã€MTLã¨æ¥é ­è¾è£œå¼·ã‚‚æœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸã€‚

## 3.3. Extensibility of MCM MCMã®æ‹¡å¼µæ€§

To demonstrate the flexibility and extensibility of MCM, we give a detailed example showing how to modify and fine-tune the model on a next action recommendation use case.
MCMã®æŸ”è»Ÿæ€§ã¨æ‹¡å¼µæ€§ã‚’å®Ÿè¨¼ã™ã‚‹ãŸã‚ã«ã€æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¨è–¦ã™ã‚‹usecaseã«ã¤ã„ã¦ã€ã©ã®ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã‚’ä¿®æ­£ã—ã€fine-tuningã™ã‚‹ã‹ã‚’ç¤ºã™è©³ç´°ãªä¾‹ã‚’ç¤ºã™ã€‚
This task aims to encourage customers to complete one action task by providing incentives (e.g.cash backs).
ã“ã®ã‚¿ã‚¹ã‚¯ã¯ã€ã‚¤ãƒ³ã‚»ãƒ³ãƒ†ã‚£ãƒ–ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒƒã‚¯ãªã©ï¼‰ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã€é¡§å®¢ã«1ã¤ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã®å®Œäº†ã‚’ä¿ƒã™ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã‚‹ã€‚
There are in total 28 tasks, including purchasing from a new product line (i.e., the product line that the customer never bought before) and trying new services like camera search or prime video streaming service.
ã‚¿ã‚¹ã‚¯ã¯å…¨éƒ¨ã§28ã‚ã‚Š(=ãŸã¶ã‚“next-action recommendationã‚¿ã‚¹ã‚¯ã®è¡Œå‹•ç©ºé–“ã®å¤§ãã•?)ã€æ–°ã—ã„å•†å“ãƒ©ã‚¤ãƒ³(ã¤ã¾ã‚Šã€é¡§å®¢ãŒã“ã‚Œã¾ã§è²·ã£ãŸã“ã¨ã®ãªã„å•†å“ãƒ©ã‚¤ãƒ³)ã‹ã‚‰ã®è³¼å…¥ã‚„ã€ã‚«ãƒ¡ãƒ©æ¤œç´¢ã‚„ãƒ—ãƒ©ã‚¤ãƒ ãƒ»ãƒ“ãƒ‡ã‚ªãƒ»ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚ˆã†ãªæ–°ã—ã„ã‚µãƒ¼ãƒ“ã‚¹ã‚’è©¦ã™ã“ã¨ã‚‚å«ã¾ã‚Œã‚‹ã€‚

The multi-task prediction scores from MCM model have covered the tasks and can be directly utilized, however it may not reflect customersâ€™ behaviors with incentives.
MCMãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯äºˆæ¸¬ã‚¹ã‚³ã‚¢ã¯ã€ã‚¿ã‚¹ã‚¯ã‚’ã‚«ãƒãƒ¼ã—ã¦ãŠã‚Šã€ç›´æ¥æ´»ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ãŒã€ã‚¤ãƒ³ã‚»ãƒ³ãƒ†ã‚£ãƒ–ã«ã‚ˆã‚‹é¡§å®¢ã®è¡Œå‹•ã‚’åæ˜ ã—ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
So we add a new head on top of the sequential encoder to predict the incentive effect and fine-tune the model with customer behavior data under incentives: 30K action clicks and completion(conversion) records.
ãã“ã§ã€sequential encoderã®ä¸Šã«æ–°ã—ã„ãƒ˜ãƒƒãƒ‰ã‚’è¿½åŠ ã—ã€ã‚¤ãƒ³ã‚»ãƒ³ãƒ†ã‚£ãƒ–åŠ¹æœã‚’äºˆæ¸¬ã—ã€ã‚¤ãƒ³ã‚»ãƒ³ãƒ†ã‚£ãƒ–ä¸‹ã®é¡§å®¢è¡Œå‹•ãƒ‡ãƒ¼ã‚¿(30Kã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¯ãƒªãƒƒã‚¯ã¨å®Œäº†ï¼ˆã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰ã®è¨˜éŒ²)ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’fine-tuningã™ã‚‹ã€‚
From the results, MCM significantly outperforms a tree-based GBDT task prediction model by 25% on conversion NDCG, and fine-tuned model (MCM_finetuned) outperforms the MCM model without fine-tuning by 35% on conversion NDCG, which in total drives over 60% improvement on conversion rate as compared to tree-based GBDT model.
ãã®çµæœã€MCMã¯æœ¨ãƒ™ãƒ¼ã‚¹ã®GBDTã‚¿ã‚¹ã‚¯äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›NDCGã§25%å¤§ããä¸Šå›ã‚Šã€å¾®èª¿æ•´ãƒ¢ãƒ‡ãƒ«ï¼ˆMCM_finetunedï¼‰ã¯å¾®èª¿æ•´ãªã—ã®MCMãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›NDCGã§35%ä¸Šå›ã‚Šã€åˆè¨ˆã§æœ¨ãƒ™ãƒ¼ã‚¹ã®GBDTãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦å¤‰æ›ç‡ã‚’60%ä»¥ä¸Šæ”¹å–„ã—ãŸã€‚

# 4. Conclusion çµè«–

In this paper, we introduce MCM, a large pre-trained customer model that serves as a sequential multi-task recommendation model to support diverse personalization projects.
æœ¬ç¨¿ã§ã¯ã€å¤šæ§˜ãªãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ”¯æ´ã™ã‚‹é€æ¬¡ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ã€**äº‹å‰å­¦ç¿’ã•ã‚ŒãŸå¤§è¦æ¨¡ãªé¡§å®¢ãƒ¢ãƒ‡ãƒ«MCM**ã‚’ç´¹ä»‹ã™ã‚‹ã€‚
Through experiments, we demonstrate the modelâ€™s ability to provide highly accurate preference predictions, which surpass the performance of other baseline models.
å®Ÿé¨“ã‚’é€šã˜ã¦ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ãŒä»–ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å‡Œé§•ã™ã‚‹é«˜ç²¾åº¦ã®å—œå¥½äºˆæ¸¬ã‚’æä¾›ã§ãã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã™ã‚‹ã€‚
We also showcase a detailed use case on a recommendation project, demonstrating how MCM can be extended to new tasks and deliver significant performance improvements.
ã¾ãŸã€ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«é–¢ã™ã‚‹è©³ç´°ãªãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã‚’ç´¹ä»‹ã—ã€MCMãŒã„ã‹ã«æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã«æ‹¡å¼µã•ã‚Œã€å¤§å¹…ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã‚’å®Ÿç¾ã§ãã‚‹ã‹ã‚’å®Ÿè¨¼ã™ã‚‹ã€‚

# 5. Speaker bio ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ã®çµŒæ­´

Rui Luo and Tianxin Wang are applied scientists in Amazon, they work on recommender systems to improve customer shopping experience.
Rui Luoã¨Tianxin Wangã¯ã‚¢ãƒã‚¾ãƒ³ã®å¿œç”¨ç§‘å­¦è€…ã§ã€é¡§å®¢ã®è²·ã„ç‰©ä½“é¨“ã‚’å‘ä¸Šã•ã›ã‚‹ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ãƒ»ã‚·ã‚¹ãƒ†ãƒ ã‚’ç ”ç©¶ã—ã¦ã„ã‚‹ã€‚
