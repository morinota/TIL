## link

- https://arxiv.org/pdf/2104.07413.pdf

## title

Empowering News Recommendation with Pre-trained Language Models

## abstract

Personalized news recommendation is an essential technique for online news services. News articles usually contain rich textual content, and accurate news modeling is important for personalized news recommendation. Existing news recommendation methods mainly model news texts based on traditional text modeling methods, which is not optimal for mining the deep semantic information in news texts. Pre-trained language models (PLMs) are powerful for natural language understanding, which has the potential for better news modeling. However, there is no public report that show PLMs have been applied to news recommendation. In this paper, we report our work on exploiting pre-trained language models to empower news recommendation. Offline experimental results on both monolingual and multilingual news recommendation datasets show that leveraging PLMs for news modeling can effectively improve the performance of news recommendation. Our PLM-empowered news recommendation models have been deployed to the Microsoft News platform, and achieved significant gains in terms of both click and pageview in both English-speaking and global markets.

# Introduction

News recommendation techniques have played critical roles in many online news platforms to alleviate the information overload of users [15]. News modeling is an important step in news recommendation, because it is a core technique to understand the content of candidate news and a prerequisite for inferring user interests from clicked news. Since news articles usually have rich textual information, news texts modeling is the key for understanding news content for news recommendation. Existing news recommendation methods usually model news texts based on traditional NLP models [15, 19, 20, 22, 23, 25, 26]. For example, Wang et al. [20] proposed to use a knowledge-aware CNN network to learn news representations from embeddings of words and entities in news title. Wu et al. [23] proposed to use multi-head self-attention network to learn news representations from news titles. However, it is difficult for these shallow models to understand the deep semantic information in news texts [18]. In addition, their models are only learned from the supervisions in the news recommendation task, which may not be optimal for capturing the semantic information. Pre-trained language models (PLMs) have achieved great success in NLP due to their strong ability in text modeling [2, 5, 6, 11, 12, 14, 28]. Different from traditional models that are usually directly trained with labeled data in specific tasks, PLMs are usually first pre-trained on a large unlabeled corpus via self-supervision to encode universal text information [5]. Thus, PLMs can usually provide better initial point for finetuning in downstream tasks [16]. In addition, different from many traditional NLP methods with shallow models [9, 10, 24, 29], PLMs are usually much deeper with a huge number of parameters. For example, the BERT-Base model contains 12 Transformer layers and up to 109M parameters [5]. Thus, PLMs may have greater ability in modeling the complicated contextual information in news text, which have the potentials to improve news text modeling for news recommendation. In this paper, we present our work on empowering large-scale news recommendation with pre-trained language models.1 Different from existing news recommendation methods that use shallow NLP models for news modeling, we explore to model news with pre-trained language models and finetune them with the news recommendation task. Offline experiments on real-world English and multilingual news recommendation datasets validate that incorporating PLMs into news modeling can consistently improve the news recommendation performance. In addition, our PLM-empowered news recommendation models have been deployed to the Microsoft News platform.2 To our best knowledge, this is the first reported effort to empower large-scale news recommender systems with PLMs. The online flight experiments show that our PLM-empowered news recommendation models achieved 8.53% click and 2.63% pageview gains in English-speaking markets, and 10.68% click and 6.04% pageview gains in other 43 global markets.

# Methodology

In this section, we introduce the details of PLM-empowered news recommendation. We first introduce the general news recommendation model framework, and then introduce how to incorporate PLMs into this framework to empower news modeling.

## General News Recommendation Framework

The general framework of news recommendation used in many existing methods [1, 15, 21, 23] is shown in Fig. 1. The core components in this framework include a news encoder that aims to learn the embeddings of news from their texts, a user encoder that learns user embedding from the embeddings of clicked news, and a click prediction module that computes personalized click score for news ranking based on the relevance between user embedding and candidate news embedding. We assume a user has ùëá historical clicked news, which are denoted as [ùê∑1, ùê∑2, ..., ùê∑ùëá ]. The news encoder processes these clicked news of a user and each candidate news ùê∑ùëê to obtain their embeddings, which are denoted as [h1, h2, ..., hùëá ] and hùëê , respectively. It can be implemented by various NLP models, such as CNN [10] and self-attention [18]. The user encoder receives the sequence of clicked news embeddings as input, and outputs a user embedding u that summarizes user interest information. It can also be implemented by various models, such as the GRU network used in [15], the attention network used in [21] and the combination of multi-head self-attention and additive attention networks used in [23]. The click prediction module takes the user embedding u and hùëê as inputs, and compute the click score ùë¶ÀÜ by evaluating their relevance. It can also be implemented by various methods such as inner product [15], neural network [20] and factorization machine [7].

## PLM Empowered News Recommendation

Next, we introduce the framework of PLM empowered news recommendation, as shown in Fig. 2. We instantiate the news encoder with a pre-trained language model to capture the deep contexts in news texts and an attention network to pool the output of PLM. We denote an input news text with ùëÄ tokens as [ùë§1,ùë§2, ...,ùë§ùëÄ ]. The PLM converts each token into its embedding, and then learns the hidden representations of words through several Transformer [18] layers. We denote the hidden token representation sequence as [r1, r2, ..., rùëÄ ]. We use an attention [29] network to summarize the hidden token representations into a unified news embedding. The news embeddings learned by the PLM and attention network are further used for user modeling and candidate matching.

## Model Training

Following [22, 23], we also use negative sampling techniques to build labeled samples from raw news impression logs, and we use the cross-entropy loss function for model training by classifying which candidate news is clicked. By optimizing the loss function via backward-propagation, the parameters in the recommendation model and PLMs can be tuned for the news recommendation task.

# Experiments

## Datasets and Experimental Settings

Our offline experiments are conducted on two real-world datasets. The first one is MIND [27], which is an English dataset for monolingual news recommendation. It contains the news click logs of 1 million users on Microsoft News in six weeks.3 The second one is a multilingual news recommendation dataset (denoted as Multilingual) collected by ourselves on MSN News platform from Dec. 1, 2020 to Jan. 14, 2021. It contains users from 7 countries with different language usage, and their market language codes are EN-US, DE-DE, FR-FR, IT-IT, JA-JP, ES-ES and KO-KR, respectively. We randomly sample 200,000 impression logs in each market. The logs in the last week are used for test and the rest are used for training and validation (9:1 split). The detailed statistics of the two datasets are shown in Table 1. In our experiments, we used the ‚ÄúBase‚Äù version of different pretrained language models if not specially mentioned. We finetuned the last two Transformer layers because we find there is only a very small performance difference between finetuning all layers and the last two layers. Following [27], we used the titles of news for news modeling. We used Adam [3] as the optimization algorithm and the learning rate was 1e-5. The batch size was 128.4 These hyperparameters are developed on the validation sets. We used average AUC, MRR, nDCG@5 and nDCG@10 over all impressions as the performance metrics. We repeated each experiment 5 times independently and reported the average performance.

## Offline Performance Evaluation

We first compare the performance of several methods on the MIND dataset to validate the effectiveness of PLM-based models in monolingual news recommendation. We compared several recent news recommendation methods including EBNR [15], NAML [21], NPA [22], LSTUR [1], NRMS [23] and their variants empowered by different pre-trained language models, including BERT [5], RoBERTa [14] and UniLM [2]. The results are shown in Table 2. Referring to this table, we find that incorporating pre-trained language models can consistently improve the performance of basic models.5 This is because pre-trained language models have stronger text modeling ability than the shallow models learned from scratch in the news recommendation. In addition, we find that the models based on RoBERTa are better than those based on BERT. This may be because RoBERTa has better hyperparameter settings than BERT and is pre-trained on larger corpus for a longer time. Besides, the models based on UniLM achieve the best performance. This may be due to UniLM can exploit the self-supervision information in both text understanding and generation tasks, which can help learn a higher-quality PLM. In addition, we conduct experiments on the Multilingual dataset to validate the effectiveness of PLMs in multilingual news recommendation. We compare the performance of EBNR, NAML, NPA, LSTUR and NRMS with different multilingual text modeling methods, including: (1) MUSE [13], using modularizing unsupervised sense embeddings; (2) Unicoder [8], a universal language encoder pre-trained by cross-lingual self-supervision tasks; and (3) InfoXLM [4], a contrastively pre-trained cross-lingual language model based on information-theoretic framework. In these methods, following [8] we mix up the training data in different languages. In addition, we also compare the performance of independently learned monolingual models based on MUSE for each market (denoted as Single). The results of different methods in terms of AUC are shown in Table 3. We find that multilingual models usually outperform the independently learned monolingual models. This may be because different languages usually have some inherent relatedness and users in different countries may also have some similar interests. Thus, jointly training models with multilingual data can help learn a more accurate recommendation model. It also provides the potential to use a unified recommendation model to serve users in different countries with diverse language usage (e.g., IndoEuropean and Altaic), which can greatly reduce the computation and memory cost of online serving. In addition, the performance methods based on multilingual PLMs are better than those based on MUSE embeddings. This may be because PLMs are also stronger than word embeddings in capturing the complicated multilingual semantic information. In addition, InfoXLM can better empower multilingual news recommendation than Unicoder. This may be because InfoXLM uses better contrastive pre-training strategies than Unicoder to help learn more accurate models.

## Influence of Model Size

Next, we explore the influence of PLM size on the recommendation performance. We compare the performance of two representative methods (i.e., NAML and NRMS) with different versions of BERT, including BERT-Base (12 layers), BERT-Medium (8 layers), BERTSmall (4 layers) and BERT-Tiny (2 layers). The results on MIND are shown in Fig. 3. We find that using larger PLMs with more parameters usually yields better recommendation performance. This may be because larger PLMs usually have stronger abilities in capturing the deep semantic information of news, and the performance may be further improved if more giant PLMs (e.g., BERT-Large) are incorporated. However, since huge PLMs are too cumbersome for online applications, we prefer the base version of PLMs.

## Influence of Different Pooling Methods

We also explore using different pooling methods for learning news embeddings from the hidden states of PLMs. We compare three methods, including: (1) CLS, using the representation of the ‚Äú[CLS]‚Äù token as news embedding, which is a widely used method for obtaining sentence embedding; (2) Average, using the average of hidden states of PLM; (3) Attention, using an attention network to learn news embeddings from hidden states. The results of NAML-BERT and NRMS-BERT on MIND are shown in Fig. 4.6 We find it is very interesting that the CLS method yields the worst performance. This may be because it cannot exploit all output hidden states of the PLM. In addition, Attention outperforms Average. This may be because attention networks can distinguish the informativeness of hidden states, which can help learn more accurate news representations. Thus, we choose attention mechanism as the pooling method.

## Visualization of News Embedding

We also study the differences between the news embeddings learned by shallow models and PLM-empowered models. We use t-SNE [17] to visualize the news embeddings learned by NRMS and NRMSUniLM, and the results are shown in Fig. 5. We find an interesting phenomenon that the news embeddings learned by NRMS-UniLM are much more discriminative than NRMS. This may be because the shallow self-attention network in NRMS cannot effectively model the semantic information in news texts. Since user interests are also inferred from embeddings of clicked news, it is difficult for NRMS to accurately model user interests from non-discriminative news representations. In addition, we observe that the news embeddings learned by NRMS-UniLM form several clear clusters. This may be because the PLM-empowered model can disentangle different kinds of news for better user interest modeling and news matching. These results demonstrate that deep PLMs have greater ability than shallow NLP models in learning discriminative text representations, which is usually beneficial for accurate news recommendation.

## Online Flight Experiments

We have deployed our PLM-empowered news recommendation models into the Microsoft News platform. Our NAML-UniLM model was used to serve users in English-speaking markets, including EN-US, EN-GB, EN-AU, EN-CA and EN-IN. The online flight experimental results have shown a gain of 8.53% in click and 2.63% in pageview against the previous news recommendation model without pre-trained language model. In addition, our NAML-InfoXLM model was used to serve users in other 43 markets with different languages. The online flight results show an improvement of 10.68% in click and 6.04% in pageview. These results validate that incorporating pre-trained language models into news recommendation can effectively improve the recommendation performance and user experience of online news services.

# Conclusion

In this paper, we present our work on empowering personalized news recommendation with pre-trained language models. We conduct extensive offline experiments on both English and multilingual news recommendation datasets, and the results show incorporating pre-trained language models can effectively improve news modeling for news recommendation. In addition, our PLM-empowered news recommendation models have been deployed to a commercial news platform, which is the first public reported effort to empower real-world large-scale news recommender systems with PLMs. The online flight results show significant improvement in both click and pageview in a large number of markets with different languages.
