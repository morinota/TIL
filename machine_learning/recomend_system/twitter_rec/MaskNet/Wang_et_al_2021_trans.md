## link ãƒªãƒ³ã‚¯

- https://arxiv.org/abs/2102.07619 https://arxiv.org/abs/2102.07619

## title ã‚¿ã‚¤ãƒˆãƒ«

MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models by Instance-Guided Mask
MaskNetï¼š ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã«ã‚ˆã‚‹CTRãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¸ã®ç‰¹å¾´çš„ãªä¹—ç®—ã®å°å…¥

## abstract ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆ

Click-Through Rate(CTR) estimation has become one of the most fundamental tasks in many real-world applications and it's important for ranking models to effectively capture complex high-order features.
ã‚¯ãƒªãƒƒã‚¯ã‚¹ãƒ«ãƒ¼ç‡(CTR)ã®æ¨å®šã¯ã€å¤šãã®å®Ÿä¸–ç•Œã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ãŠã„ã¦æœ€ã‚‚åŸºæœ¬çš„ãªã‚¿ã‚¹ã‚¯ã®1ã¤ã¨ãªã£ã¦ãŠã‚Šã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã«ã¨ã£ã¦è¤‡é›‘ãªé«˜æ¬¡ã®ç‰¹å¾´ã‚’åŠ¹æœçš„ã«æ‰ãˆã‚‹ã“ã¨ãŒé‡è¦ã§ã‚ã‚‹.
Shallow feed-forward network is widely used in many state-of-the-art DNN models such as FNN, DeepFM and xDeepFM to implicitly capture high-order feature interactions.
æµ…ã„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€FNNã€DeepFMã€xDeepFMãªã©ã®å¤šãã®æœ€å…ˆç«¯ã®DNNãƒ¢ãƒ‡ãƒ«ã§ã€é«˜æ¬¡ã®ç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’æš—é»™ã®ã†ã¡ã«æ‰ãˆã‚‹ãŸã‚ã«åºƒãä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚
However, some research has proved that addictive feature interaction, particular feed-forward neural networks, is inefficient in capturing common feature interaction.
ã—ã‹ã—ã€ä¸­æ¯’æ€§ã®ã‚ã‚‹ç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã€ç‰¹ã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€å…±é€šã®ç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’æ‰ãˆã‚‹ã«ã¯éåŠ¹ç‡ã§ã‚ã‚‹ã“ã¨ãŒè¨¼æ˜ã•ã‚ŒãŸç ”ç©¶ã‚‚ã‚ã‚Šã¾ã™ã€‚
To resolve this problem, we introduce specific multiplicative operation into DNN ranking system by proposing instance-guided mask which performs element-wise product both on the feature embedding and feed-forward layers guided by input instance.
ã“ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€æˆ‘ã€…ã¯ã€å…¥åŠ›ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«å°ã‹ã‚ŒãŸç‰¹å¾´åŸ‹ã‚è¾¼ã¿å±¤ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®ä¸¡æ–¹ã§è¦ç´ ã”ã¨ã®ç©ã‚’è¡Œã†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰ãƒã‚¹ã‚¯ã‚’ææ¡ˆã—ã€DNNãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã«ç‰¹å®šã®ä¹—ç®—æ¼”ç®—ã‚’å°å…¥ã™ã‚‹ã€‚
We also turn the feed-forward layer in DNN model into a mixture of addictive and multiplicative feature interactions by proposing MaskBlock in this paper.
ã¾ãŸã€æœ¬è«–æ–‡ã§MaskBlockã‚’ææ¡ˆã™ã‚‹ã“ã¨ã§ã€DNNãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã‚’åŠ ç®—å‹ã¨ä¹—ç®—å‹ã®ç‰¹å¾´é‡ç›¸äº’ä½œç”¨ã®æ··åˆå‹ã«å¤‰ãˆã¦ã„ã¾ã™ã€‚
MaskBlock combines the layer normalization, instance-guided mask, and feed-forward layer and it is a basic building block to be used to design new ranking model under various configurations.
MaskBlockã¯ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ­£è¦åŒ–ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰ä»˜ããƒã‚¹ã‚¯ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã‚’çµ„ã¿åˆã‚ã›ãŸã‚‚ã®ã§ã€æ§˜ã€…ãªæ§‹æˆã§æ–°ã—ã„ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’è¨­è¨ˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹åŸºæœ¬çš„ãªãƒ“ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ãƒ–ãƒ­ãƒƒã‚¯ã§ã‚ã‚‹ã€‚
The model consisting of MaskBlock is called MaskNet in this paper and two new MaskNet models are proposed to show the effectiveness of MaskBlock as basic building block for composing high performance ranking systems.
æœ¬è«–æ–‡ã§ã¯ã€MaskBlockã‹ã‚‰ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’MaskNetã¨å‘¼ã³ã€MaskBlockãŒé«˜æ€§èƒ½ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹æˆã™ã‚‹ãŸã‚ã®åŸºæœ¬æ§‹æˆè¦ç´ ã¨ã—ã¦ã®æœ‰åŠ¹æ€§ã‚’ç¤ºã™ãŸã‚ã«ã€2ã¤ã®æ–°ã—ã„MaskNetãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã™ã‚‹ã€‚
The experiment results on three real-world datasets demonstrate that our proposed MaskNet models outperform state-of-the-art models such as DeepFM and xDeepFM significantly, which implies MaskBlock is an effective basic building unit for composing new high performance ranking systems.
3ã¤ã®å®Ÿä¸–ç•Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãŸå®Ÿé¨“ã®çµæœã€ææ¡ˆã—ãŸMaskNetãƒ¢ãƒ‡ãƒ«ã¯DeepFMã‚„xDeepFMãªã©ã®æœ€å…ˆç«¯ãƒ¢ãƒ‡ãƒ«ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹ã“ã¨ãŒå®Ÿè¨¼ã•ã‚Œã€MaskBlockãŒæ–°ã—ã„é«˜æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹æˆã™ã‚‹ãŸã‚ã®æœ‰åŠ¹ãªåŸºæœ¬æ§‹æˆå˜ä½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚

# Introduction åºç« 

**Click-through rate (CTR) prediction** is to predict the probability of a user clicking on the recommended items.
ã‚¯ãƒªãƒƒã‚¯ã‚¹ãƒ«ãƒ¼ç‡ï¼ˆCTRï¼‰äºˆæ¸¬ã¯ã€ãƒ¦ãƒ¼ã‚¶ãŒæ¨è–¦ã‚¢ã‚¤ãƒ†ãƒ ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ç¢ºç‡ã‚’äºˆæ¸¬ã™ã‚‹ã‚‚ã®.
It plays important role in personalized advertising and recommender systems.
ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºãƒ‰åºƒå‘Šã‚„ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã„ã¦é‡è¦ãªå½¹å‰²ã‚’æ‹…ã£ã¦ã„ã‚‹.
Many models have been proposed to resolve this problem such as Logistic Regression (LR) [16], Polynomial-2 (Poly2) [17], tree-based models [7], tensor-based models [12], Bayesian models [5], and Field-aware Factorization Machines (FFMs) [11].
ã“ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆLRï¼‰[16]ã€å¤šé …å¼2ï¼ˆPoly2ï¼‰[17]ã€æœ¨ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«[7]ã€ãƒ†ãƒ³ã‚½ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«[12]ã€ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«[5]ã€ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚¢ã‚¦ã‚§ã‚¢å› å­åˆ†è§£ãƒã‚·ãƒ³ï¼ˆFFMs[11] ãªã©å¤šãã®ãƒ¢ãƒ‡ãƒ«ãŒæç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚ï¼½
In recent years, employing DNNs for CTR estimation has also been a research trend in this field and some deep learning based models have been introduced such as Factorization-Machine Supported Neural Networks(FNN)[24], Attentional Factorization Machine (AFM)[3], wide & deep(W&D)[22], DeepFM[6], xDeepFM[13] etc.
è¿‘å¹´ã€**CTRæ¨å®šã«DNNã‚’æ¡ç”¨ã™ã‚‹ã“ã¨ã‚‚ã“ã®åˆ†é‡ã®ç ”ç©¶å‹•å‘**ã§ã‚ã‚Šã€Factorization-Machine Supported Neural Networksï¼ˆFNNï¼‰[24]ã€Attentional Factorization Machineï¼ˆAFMï¼‰[3]ã€wide & deepï¼ˆW&Dï¼‰[22]ã€DeepFMï¼ˆ6ï¼‰ã€xDeepFMï¼ˆ13ï¼‰ãªã©ã®æ·±å±¤å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ãŒç´¹ä»‹ã•ã‚Œã¦ã„ã‚‹.

Feature interaction is critical for CTR tasks and itâ€™s important for ranking model to effectively capture these complex features.
**CTRã‚¿ã‚¹ã‚¯ã§ã¯ç‰¹å¾´é‡ã®ç›¸äº’ä½œç”¨ãŒé‡è¦**ã§ã‚ã‚Šã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¯ã“ã‚Œã‚‰ã®è¤‡é›‘ãªç‰¹å¾´ã‚’åŠ¹æœçš„ã«æ‰ãˆã‚‹ã“ã¨ãŒé‡è¦ã§ã‚ã‚‹.
Most DNN ranking models such as FNN , W&D, DeepFM and xDeepFM use the shallow MLP layers to model high-order interactions in an implicit way and itâ€™s an important component in current state-ofthe-art ranking systems.
FNN , W&D, DeepFM, xDeepFMãªã©ã®ã»ã¨ã‚“ã©ã®DNNãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¯ã€**æµ…ã„MLPå±¤**(=Multi Layer Perceptronå±¤=å¤šå±¤ã®å…¨çµåˆå±¤)ã‚’ä½¿ã£ã¦é«˜æ¬¡ã®ç›¸äº’ä½œç”¨ã‚’æš—é»™ã®ã†ã¡ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¦ãŠã‚Šã€ç¾åœ¨ã®æœ€æ–°é‹­ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã„ã¦é‡è¦ãªè¦ç´ ã¨ãªã£ã¦ã„ã‚‹.

However, Alex Beutel et.al [2] have proved that addictive feature interaction, particular feed-forward neural networks, is inefficient in capturing common feature crosses.
ã—ã‹ã—ã€Alex Beutelã‚‰[2]ã¯ã€ä¸­æ¯’æ€§ã®ã‚ã‚‹ç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã€ç‰¹ã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€å…±é€šã®ç‰¹å¾´ã®äº¤å·®ã‚’æ‰ãˆã‚‹ã®ã«åŠ¹ç‡ãŒæ‚ªã„ã“ã¨ã‚’è¨¼æ˜ã—ã¾ã—ãŸã€‚
They proposed a simple but effective approach named "latent cross" which is a kind of multiplicative interactions between the context embedding and the neural network hidden states in RNN model.
å½¼ã‚‰ã¯ã€RNNãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã¨ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®éš ã‚ŒçŠ¶æ…‹ã®é–“ã®ä¸€ç¨®ã®ä¹—æ³•çš„ç›¸äº’ä½œç”¨ã§ã‚ã‚‹"æ½œåœ¨ã‚¯ãƒ­ã‚¹"ã¨ã„ã†ã‚·ãƒ³ãƒ—ãƒ«ã§åŠ¹æœçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¾ã—ãŸã€‚
Recently, Rendle et.alâ€™s work [18] also shows that a carefully configured dot product baseline largely outperforms the MLP layer in collaborative filtering.
æœ€è¿‘ã€Rendleã‚‰ã®ç ”ç©¶[18]ã§ã‚‚ã€**å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ãŠã„ã¦ã€æ³¨æ„æ·±ãè¨­å®šã•ã‚ŒãŸãƒ‰ãƒƒãƒˆç©ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³(MFã®ã‚„ã¤ã‹ãª?)ãŒMLPå±¤ã‚’å¤§ããä¸Šå›ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹**.
While a MLP can in theory approximate any function, they show that it is non-trivial to learn a dot product with an MLP and learning a dot product with high accuracy for a decently large embedding dimension requires a large model capacity as well as many training data.
MLPã¯ç†è«–çš„ã«ã¯ã‚ã‚‰ã‚†ã‚‹é–¢æ•°ã‚’è¿‘ä¼¼ã§ãã‚‹ãŒã€MLPã§ãƒ‰ãƒƒãƒˆãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã¯éè‡ªæ˜ã§ã‚ã‚Šã€ãã“ãã“å¤§ããªåŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã«å¯¾ã—ã¦é«˜ã„ç²¾åº¦ã§ãƒ‰ãƒƒãƒˆãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚’å­¦ç¿’ã™ã‚‹ã«ã¯ã€å¤šãã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨åŒæ§˜ã«å¤§ããªãƒ¢ãƒ‡ãƒ«å®¹é‡ãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹.
Their work also proves the inefficiency of MLP layerâ€™s ability to model complex feature interactions.
ã¾ãŸã€å½¼ã‚‰ã®ç ”ç©¶ã¯ã€è¤‡é›‘ãªç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹MLPå±¤ã®èƒ½åŠ›ãŒéåŠ¹ç‡ã§ã‚ã‚‹ã“ã¨ã‚’è¨¼æ˜ã—ã¦ã„ã‚‹.

Inspired by "latent cross"[2] and Rendleâ€™s work [18], we care about the following question: Can we improve the DNN ranking systems by introducing specific multiplicative operation into it to make it efficiently capture complex feature interactions?
"latent cross"[2]ã¨Rendleã®ç ”ç©¶[18]ã«è§¦ç™ºã•ã‚Œã€æˆ‘ã€…ã¯ä»¥ä¸‹ã®å•ã„ã‚’æ°—ã«ã—ã¦ã„ã‚‹ï¼š **DNNãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã«ç‰¹å®šã®ä¹—ç®—æ¼”ç®—ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€è¤‡é›‘ãªç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’åŠ¹ç‡çš„ã«æ‰ãˆã‚‰ã‚Œã‚‹ã‚ˆã†ã«æ”¹å–„ã§ããªã„ã‹**ï¼Ÿ

In order to overcome the problem of inefficiency of feed-forward layer to capture complex feature cross, we introduce a special kind of multiplicative operation into DNN ranking system in this paper.
**feed-forwardå±¤**(=ãƒ‡ãƒ¼ã‚¿ã®æµã‚ŒãŒä¸€æ–¹å‘ã§ã‚ã‚‹æ§‹é€ ã€‚ãƒ‡ãƒ¼ã‚¿ãŒè¡Œã£ãŸã‚Šæ¥ãŸã‚Šã€ã‚ã‚‹ã„ã¯ãƒ«ãƒ¼ãƒ—ç­‰ã‚’ã—ãªã„ã€‚)ãŒè¤‡é›‘ãªç‰¹å¾´é‡ã®äº¤å·®ã‚’æ‰ãˆã‚‹ã®ã«åŠ¹ç‡ãŒæ‚ªã„ã¨ã„ã†å•é¡Œã‚’å…‹æœã™ã‚‹ãŸã‚ã«ã€æœ¬è«–æ–‡ã§ã¯DNNãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã«ç‰¹æ®Šãªä¹—ç®—æ¼”ç®—ã‚’å°å…¥ã—ãŸ.
First, we propose an instance-guided mask performing elementwise production on the feature embedding and feed-forward layer.
ã¾ãšã€ç‰¹å¾´åŸ‹ã‚è¾¼ã¿å±¤ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã«å¯¾ã—ã¦ã€**è¦ç´ åˆ†è§£ã‚’è¡Œã†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯**ã‚’ææ¡ˆã™ã‚‹.(??)
The instance-guided mask utilizes the global information collected from input instance to dynamically highlight the informative elements in feature embedding and hidden layer in a unified manner.
ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰å‹ãƒã‚¹ã‚¯ã¯ã€å…¥åŠ›ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ã‚‰åé›†ã—ãŸã‚°ãƒ­ãƒ¼ãƒãƒ«ãªæƒ…å ±ã‚’åˆ©ç”¨ã—ã€ç‰¹å¾´é‡åŸ‹è¾¼å±¤ã¨éš ã‚Œå±¤ã®**æƒ…å ±é‡ã®å¤šã„è¦ç´ ã‚’çµ±ä¸€çš„ã«å‹•çš„ã«ãƒã‚¤ãƒ©ã‚¤ãƒˆ**ã™ã‚‹.(æƒ…å ±é‡ã‚’é›†ç´„ã—ã¦ã„ã‚‹ã€å¿…è¦ãªæƒ…å ±ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦ã‚‹ã‚¤ãƒ¡ãƒ¼ã‚¸??)
There are two main advantages for adopting the instance-guided mask: 
instance-guided maskã‚’é©ç”¨ã™ã‚‹äº‹ã®åˆ©ç‚¹ã¯ä¸»ã«ï¼’ã¤.
firstly, the element-wise product between the mask and hidden layer or feature embedding layer brings multiplicative operation into DNN ranking system in unified way to more efficiently capture complex feature interaction.
ç¬¬ä¸€ã«ã€ãƒã‚¹ã‚¯ã¨éš ã‚Œå±¤ã¾ãŸã¯ç‰¹å¾´åŸ‹ã‚è¾¼ã¿å±¤ã®é–“ã®è¦ç´ ã”ã¨ã®ç©ãŒã€DNNãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã«çµ±ä¸€çš„ãªæ–¹æ³•ã§ä¹—ç®—æ¼”ç®—ã‚’ã‚‚ãŸã‚‰ã—ã€**è¤‡é›‘ãªç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’ã‚ˆã‚ŠåŠ¹ç‡çš„ã«æ•ã‚‰ãˆã‚‹ã“ã¨ãŒã§ãã‚‹**ã“ã¨.
Secondly, itâ€™s a kind of finegained bit-wise attention guided by input instance which can both weaken the influence of noise in feature embedding and MLP layers while highlight the informative signals in DNN ranking systems.
ç¬¬äºŒã«ã€DNNãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã„ã¦**æƒ…å ±é‡ã®å¤šã„ä¿¡å·ã‚’å¼·èª¿**ã—ãªãŒã‚‰ã€ç‰¹å¾´åŸ‹ã‚è¾¼ã¿ã‚„MLPå±¤ã«ãŠã‘ã‚‹**ãƒã‚¤ã‚ºã®å½±éŸ¿ã‚’å¼±ã‚ã‚‹ã“ã¨ãŒã§ãã‚‹**ã€å…¥åŠ›ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«ã‚ˆã£ã¦å°ã‹ã‚Œã‚‹ä¸€ç¨®ã®ç´°ã‹ã„bit-wise attention(??Attentionå±¤??)ã§ã‚ã‚‹ã“ã¨ã§ã‚ã‚‹.

By combining instance-guided mask, a following feed-forward layer and layer normalization, MaskBlock is proposed by us to turn the commonly used feed-forward layer into a mixture of addictive and multiplicative feature interactions.
ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã€å¾Œç¶šã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã€å±¤ã®æ­£è¦åŒ–ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€**ä¸€èˆ¬çš„ã«ç”¨ã„ã‚‰ã‚Œã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã‚’åŠ æ³•çš„ãƒ»ä¹—æ³•çš„ãªç‰¹å¾´é‡ç›¸äº’ä½œç”¨ã®æ··åˆã«å¤‰ãˆã‚‹MaskBlock**ãŒç§ãŸã¡ã«ã‚ˆã£ã¦ææ¡ˆã•ã‚Œã¦ã„ã‚‹.
The instance-guided mask introduces multiplicative interactions and the following feedforward hidden layer aggregates the masked information in order to better capture the important feature interactions.
**ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã¯ä¹—ç®—çš„ãªç›¸äº’ä½œç”¨ã‚’å°å…¥**ã—ã€æ¬¡ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰éš ã‚Œå±¤ã¯ã€é‡è¦ãªç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’ã‚ˆã‚Šã‚ˆãæ•ã‚‰ãˆã‚‹ãŸã‚ã«ã€ãƒã‚¹ã‚¯ã•ã‚ŒãŸæƒ…å ±ã‚’é›†ç´„ã™ã‚‹.
While the layer normalization can ease optimization of the network.
ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ­£è¦åŒ–ã¯ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€é©åŒ–ã‚’å®¹æ˜“ã«ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ãŒã€‚

MaskBlock can be regarded as a basic building block to design new ranking models under some kinds of configuration.
MaskBlockã¯ã€ã‚ã‚‹ç¨®ã®æ§‹æˆã®ã‚‚ã¨ã§æ–°ã—ã„ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’è¨­è¨ˆã™ã‚‹ãŸã‚ã®åŸºæœ¬çš„ãªãƒ“ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ãƒ–ãƒ­ãƒƒã‚¯ã¨ã¿ãªã™ã“ã¨ãŒã§ãã‚‹.
The model consisting of MaskBlock is called MaskNet in this paper and two new MaskNet models are proposed to show the effectiveness of MaskBlock as basic building block for composing high performance ranking systems.
æœ¬è«–æ–‡ã§ã¯ã€**MaskBlockã‹ã‚‰ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’MaskNetã¨å‘¼ã³**ã€MaskBlockãŒé«˜æ€§èƒ½ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹æˆã™ã‚‹ãŸã‚ã®åŸºæœ¬æ§‹æˆè¦ç´ ã¨ã—ã¦ã®æœ‰åŠ¹æ€§ã‚’ç¤ºã™ãŸã‚ã«ã€2ã¤ã®æ–°ã—ã„MaskNetãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã™ã‚‹:

The contributions of our work are summarized as follows:
æœ¬ç ”ç©¶ã®è²¢çŒ®ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¦ç´„ã•ã‚Œã‚‹ï¼š

- (1) In this work, we propose an instance-guided mask performing element-wise product both on the feature embedding and feed-forward layers in DNN models. The global context information contained in the instance-guided mask is dynamically incorporated into the feature embedding and feed-forward layer to highlight the important elements. (1) æœ¬è«–æ–‡ã§ã¯ã€DNNãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´åŸ‹ã‚è¾¼ã¿å±¤(=å…¥åŠ›å±¤ã¨ã¯é•ã†?è‡ªç„¶è¨€èªãƒ†ã‚­ã‚¹ãƒˆã‚’embeddingã«å¤‰æ›ã™ã‚‹å±¤ã®ã‚ˆã†ãªã‚¤ãƒ¡ãƒ¼ã‚¸?)ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®ä¸¡æ–¹ã«ãŠã„ã¦ã€è¦ç´ ã”ã¨ã®ç©ã‚’è¡Œã†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã‚’ææ¡ˆã™ã‚‹. ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã«å«ã¾ã‚Œã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’ã€ç‰¹å¾´åŸ‹ã‚è¾¼ã¿å±¤ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã«å‹•çš„ã«å–ã‚Šè¾¼ã¿ã€é‡è¦ãªè¦ç´ ã‚’å¼·èª¿ã™ã‚‹ã“ã¨ãŒã§ãã‚‹.

- (2) We propose a basic building block named MaskBlock which consists of three key components: instance-guided mask, a following feed-forward hidden layer and layer normalization module. In this way, we turn the widely used feed-forward layer of a standard DNN model into a mixture of addictive and multiplicative feature interactions. (2) ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰ä»˜ããƒã‚¹ã‚¯ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰éš ã‚Œå±¤ã€å±¤æ­£è¦åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®3ã¤ã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‹ã‚‰ãªã‚‹MaskBlockã¨ã„ã†åŸºæœ¬æ§‹æˆãƒ–ãƒ­ãƒƒã‚¯ã‚’ææ¡ˆã™ã‚‹. ã“ã®ã‚ˆã†ã«ã—ã¦ã€æ¨™æº–çš„ãªDNNãƒ¢ãƒ‡ãƒ«ã®åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã‚’ã€**åŠ æ³•çš„ãƒ»ä¹—æ³•çš„ãªç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã®æ··åˆã«å¤‰ãˆã‚‹**.

- (3) We also propose a new ranking framework named MaskNet to compose new ranking system by utilizing MaskBlock as basic building unit. To be more specific, the serial MaskNet model and parallel MaskNet model are designed based on the MaskBlock in this paper. The serial rank model stacks MaskBlock block by block while the parallel rank model puts many MaskBlocks in parallel on a sharing feature embedding layer. (3) ã¾ãŸã€MaskBlockã‚’åŸºæœ¬æ§‹æˆå˜ä½ã¨ã—ã¦ã€æ–°ã—ã„ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹æˆã™ã‚‹MaskNetã¨ã„ã†æ–°ã—ã„ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã¦ã„ã‚‹. å…·ä½“çš„ã«ã¯ã€æœ¬è«–æ–‡ã§ã¯**MaskBlockã‚’ãƒ™ãƒ¼ã‚¹ã«ã‚·ãƒªã‚¢ãƒ«MaskNetãƒ¢ãƒ‡ãƒ«ã€ãƒ‘ãƒ©ãƒ¬ãƒ«MaskNetãƒ¢ãƒ‡ãƒ«ã‚’è¨­è¨ˆã—ã¦ã„ã‚‹**. ã‚·ãƒªã‚¢ãƒ«ãƒ©ãƒ³ã‚¯ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€MaskBlockã‚’ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã«ç©ã¿é‡ã­ã€ãƒ‘ãƒ©ãƒ¬ãƒ«ãƒ©ãƒ³ã‚¯ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€å¤šæ•°ã®MaskBlockã‚’å…±æœ‰æ©Ÿèƒ½åŸ‹ã‚è¾¼ã¿å±¤ä¸Šã«ä¸¦åˆ—ã«é…ç½®ã™ã‚‹.

- (4) Extensive experiments are conduct on three real-world datasets and the experiment results demonstrate that our proposed two MaskNet models outperform state-of-the-art models significantly. The results imply MaskBlock indeed enhance DNN modelâ€™s ability of capturing complex feature interactions through introducing multiplicative operation into DNN models by instance-guided mask. (4) 3ã¤ã®å®Ÿä¸–ç•Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å¤§è¦æ¨¡ãªå®Ÿé¨“ã‚’è¡Œã„ã€ææ¡ˆã™ã‚‹2ã¤ã®MaskNetãƒ¢ãƒ‡ãƒ«ãŒã€æœ€å…ˆç«¯ãƒ¢ãƒ‡ãƒ«ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã—ãŸ. ãã®çµæœã€MaskBlockã¯ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã«ã‚ˆã£ã¦DNNãƒ¢ãƒ‡ãƒ«ã«ä¹—ç®—æ¼”ç®—ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€è¤‡é›‘ãªç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’æ‰ãˆã‚‹DNNãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’ç¢ºã‹ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚ŒãŸ.

The rest of this paper is organized as follows.
æœ¬ç¨¿ã®æ®‹ã‚Šã®éƒ¨åˆ†ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«æ§‹æˆã•ã‚Œã¦ã„ã‚‹.
Section 2 introduces some related works which are relevant with our proposed model.
ç¬¬2ç¯€ã§ã¯ã€æœ¬ææ¡ˆãƒ¢ãƒ‡ãƒ«ã¨é–¢é€£æ€§ã®ã‚ã‚‹ã„ãã¤ã‹ã®é–¢é€£ä½œå“ã‚’ç´¹ä»‹ã™ã‚‹.
We introduce our proposed models in detail in Section 3.
ã‚»ã‚¯ã‚·ãƒ§ãƒ³3ã§ã¯ã€ææ¡ˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’è©³ã—ãç´¹ä»‹ã™ã‚‹.
The experimental results on three real world datasets are presented and discussed in Section 4.
3ã¤ã®å®Ÿä¸–ç•Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å®Ÿé¨“çµæœã‚’ç¤ºã—ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³4ã§è­°è«–ã™ã‚‹.
Section 5 concludes our work in this paper.
ç¬¬5ç¯€ã§ã¯ã€æœ¬è«–æ–‡ã«ãŠã‘ã‚‹æˆ‘ã€…ã®ç ”ç©¶ã‚’ç· ã‚ããã‚‹.

# Related Work é–¢é€£ä½œå“

## State-Of-The-Art CTR Models æœ€å…ˆç«¯ã®CTRãƒ¢ãƒ‡ãƒ«

Many deep learning based CTR models have been proposed in recent years and it is the key factor for most of these neural network based models to effectively model the feature interactions.
è¿‘å¹´ã€å¤šãã®æ·±å±¤å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®CTRãƒ¢ãƒ‡ãƒ«ãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ãŒã€ã“ã‚Œã‚‰ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã®å¤šãã«ã¨ã£ã¦ã€**ç‰¹å¾´é‡ã®ç›¸äº’ä½œç”¨ã‚’åŠ¹æœçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ã“ã¨ã¯é‡è¦ãªè¦ç´ ã¨ãªã£ã¦ã„ã‚‹**.

Factorization-Machine Supported Neural Networks (FNN)[24] is a feed-forward neural network using FM to pre-train the embedding layer.
FNNï¼ˆFactorization-Machine Supported Neural Networksï¼‰[24]ã¯ã€FMã‚’ç”¨ã„ã¦åŸ‹ã‚è¾¼ã¿å±¤ã®äº‹å‰å­¦ç¿’ã‚’è¡Œã†ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚
Wide & Deep Learning[22] jointly trains wide linear models and deep neural networks to combine the benefits of memorization and generalization for recommender systems.
Wide & Deep Learning[22]ã¯ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ãŸã‚ã®è¨˜æ†¶ã¨æ±åŒ–ã®åˆ©ç‚¹ã‚’çµ„ã¿åˆã‚ã›ã‚‹ãŸã‚ã«ã€åºƒã„ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã¨æ·±ã„ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å…±åŒã§å­¦ç¿’ã—ã¾ã™ã€‚
However, expertise feature engineering is still needed on the input to the wide part of Wide & Deep model.
ã—ã‹ã—ã€Wide & Deepãƒ¢ãƒ‡ãƒ«ã®Wideéƒ¨åˆ†ã¸ã®å…¥åŠ›ã«ã¯ã€ã¾ã å°‚é–€çš„ãªãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãŒå¿…è¦ã§ã‚ã‚‹ã€‚
To alleviate manual efforts in feature engineering, DeepFM[6] replaces the wide part of Wide & Deep model with FM and shares the feature embedding between the FM and deep component.
DeepFM[6]ã¯ã€feature engineeringã®æ‰‹ä½œæ¥­ã‚’è»½æ¸›ã™ã‚‹ãŸã‚ã«ã€Wide & Deepãƒ¢ãƒ‡ãƒ«ã®Wideéƒ¨åˆ†ã‚’FMã«ç½®ãæ›ãˆã€FMã¨Deepã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé–“ã§ç‰¹å¾´åŸ‹è¾¼ã¿ã‚’å…±æœ‰ã™ã‚‹ã€‚

While most DNN ranking models process high-order feature interactions by MLP layers in implicit way, some works explicitly introduce high-order feature interactions by sub-network.
å¤šãã®DNNãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¯MLPå±¤ã«ã‚ˆã‚‹é«˜æ¬¡ç‰¹å¾´é‡ç›¸äº’ä½œç”¨ã‚’æš—é»™ã®ã†ã¡ã«å‡¦ç†ã—ã¦ã„ã‚‹ãŒã€ã„ãã¤ã‹ã®ä½œå“ã¯ã‚µãƒ–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹é«˜æ¬¡ç‰¹å¾´é‡ç›¸äº’ä½œç”¨ã‚’æ˜ç¤ºçš„ã«å°å…¥ã—ã¦ã„ã‚‹ã€‚
Deep & Cross Network (DCN)[21] efficiently captures feature interactions of bounded degrees in an explicit fashion.
Deep & Cross Network (DCN)[21]ã¯ã€æœ‰ç•Œåº¦ã®ç‰¹å¾´çš„ãªç›¸äº’ä½œç”¨ã‚’æ˜ç¤ºçš„ã«åŠ¹ç‡ã‚ˆãæ‰ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
Similarly, eXtreme Deep Factorization Machine (xDeepFM) [13] also models the loworder and high-order feature interactions in an explicit way by proposing a novel Compressed Interaction Network (CIN) part.
åŒæ§˜ã«ã€eXtreme Deep Factorization Machine (xDeepFM) [13]ã‚‚ã€æ–°ã—ã„Compressed Interaction Network (CIN) éƒ¨åˆ†ã‚’ææ¡ˆã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€ä½æ¬¡ã¨é«˜æ¬¡ã®ç‰¹å¾´ç›¸äº’ä½œç”¨ã‚’æ˜ç¤ºçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¦ã„ã¾ã™ã€‚
AutoInt[19] uses a multi-head self-attentive neural network to explicitly model the feature interactions in the low-dimensional space.
AutoInt[19]ã¯ã€ä½æ¬¡å…ƒç©ºé–“ã«ãŠã‘ã‚‹ç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’æ˜ç¤ºçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ãŸã‚ã«ã€ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰è‡ªå·±èª¿æ•´å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
FiBiNET[9] can dynamically learn feature importance via the Squeeze-Excitation network (SENET) mechanism and feature interactions via bilinear function.
FiBiNET[9]ã¯ã€Squeeze-Excitation networkï¼ˆSENETï¼‰ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«ã‚ˆã‚‹ç‰¹å¾´ã®é‡è¦åº¦ã€ãƒã‚¤ãƒªãƒ‹ã‚¢é–¢æ•°ã«ã‚ˆã‚‹ç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’å‹•çš„ã«å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

## Feature-Wise Mask Or Gating Feature-Wise Mask Or Gatingï¼ˆãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ¯ã‚¤ã‚ºãƒ»ãƒã‚¹ã‚¯ãƒ»ã‚ªã‚¢ãƒ»ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

Feature-wise mask or gating has been explored widely in vision [8, 20], natural language processing [4] and recommendation system[14, 15].
ç‰¹å¾´é‡ã«å¿œã˜ãŸãƒã‚¹ã‚¯ã‚„ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¯ã€è¦–è¦š[8, 20]ã€è‡ªç„¶è¨€èªå‡¦ç†[4]ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ [14, 15]ãªã©ã§åºƒãç ”ç©¶ã•ã‚Œã¦ã„ã‚‹ã€‚
For examples, Highway Networks [20] utilize feature gating to ease gradient-based training of very deep networks.
ä¾‹ãˆã°ã€Highway Networks [20]ã¯ã€éå¸¸ã«æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‹¾é…ãƒ™ãƒ¼ã‚¹ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®¹æ˜“ã«ã™ã‚‹ãŸã‚ã«ç‰¹å¾´ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
Squeezeand-Excitation Networks[8] recalibrate feature responses by explicitly multiplying each channel with learned sigmoidal mask values.
Squeezeand-Excitation Networks[8]ã¯ã€å„ãƒãƒ£ãƒ³ãƒãƒ«ã«å­¦ç¿’ã—ãŸã‚·ã‚°ãƒ¢ã‚¤ãƒ‰ãƒã‚¹ã‚¯å€¤ã‚’æ˜ç¤ºçš„ã«ä¹—ç®—ã™ã‚‹ã“ã¨ã§ç‰¹å¾´å¿œç­”ã‚’å†è¼ƒæ­£ã™ã‚‹ã€‚
Dauphin et al.[4] proposed gated linear unit (GLU) to utilize it to control what information should be propagated for predicting the next word in the language modeling task.
Dauphinã‚‰[4]ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€æ¬¡ã®å˜èªã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã«ä¼æ’­ã•ã›ã‚‹æƒ…å ±ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã«ã€ã‚²ãƒ¼ãƒˆç·šå½¢ãƒ¦ãƒ‹ãƒƒãƒˆï¼ˆGLUï¼‰ã‚’ææ¡ˆã—ã€ã“ã‚Œã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹ã€‚
Gating or mask mechanisms are also adopted in recommendation systems.
ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚„ãƒã‚¹ã‚¯ã®ä»•çµ„ã¿ã‚‚æ¡ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
Ma et al.[15] propose a novel multi-task learning approach, Multi-gate Mixture-of-Experts (MMoE), which explicitly learns to model task relationships from data.
Maã‚‰[15]ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¿ã‚¹ã‚¯ã®é–¢ä¿‚ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ã“ã¨ã‚’æ˜ç¤ºçš„ã«å­¦ç¿’ã™ã‚‹ã€æ–°ã—ã„ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€Multi-gate Mixture-of-Experts (MMoE) ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚
Ma et al.[14] propose a hierarchical gating network (HGN) to capture both the long-term and short-term user interests.
Maã‚‰[14]ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é•·æœŸçš„ãªèˆˆå‘³ã¨çŸ­æœŸçš„ãªèˆˆå‘³ã®ä¸¡æ–¹ã‚’æ‰ãˆã‚‹ãŸã‚ã«ã€éšå±¤çš„ãªã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆHGNï¼‰ã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚
The feature gating and instance gating modules in HGN select what item features can be passed to the downstream layers from the feature and instance levels, respectively.
HGNã®feature gatingã¨instance gatingãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ãã‚Œãã‚Œfeatureãƒ¬ãƒ™ãƒ«ã¨instanceãƒ¬ãƒ™ãƒ«ã‹ã‚‰ä¸‹æµå±¤ã«æ¸¡ã™ã“ã¨ãŒã§ãã‚‹é …ç›®ã®ç‰¹å¾´ã‚’é¸æŠã—ã¾ã™ã€‚

## Normalization æ­£è¦åŒ–

Normalization techniques have been recognized as very effective components in deep learning.
æ­£è¦åŒ–æŠ€è¡“ã¯ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦éå¸¸ã«æœ‰åŠ¹ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¨ã—ã¦èªè­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚
Many normalization approaches have been proposed with the two most popular ones being BatchNorm [10] and LayerNorm [1] .
å¤šãã®æ­£è¦åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒææ¡ˆã•ã‚Œã¦ãŠã‚Šã€æœ€ã‚‚ä¸€èˆ¬çš„ãªã‚‚ã®ã¯ BatchNorm [10] ã¨ LayerNorm [1] ã®2ã¤ã§ã‚ã‚‹ã€‚
Batch Normalization (Batch Norm or BN)[10] normalizes the features by the mean and variance computed within a mini-batch.
ãƒãƒƒãƒæ­£è¦åŒ–ï¼ˆBatch Normã¾ãŸã¯BNï¼‰[10]ã¯ã€ãƒŸãƒ‹ãƒãƒƒãƒå†…ã§è¨ˆç®—ã•ã‚ŒãŸå¹³å‡ã¨åˆ†æ•£ã«ã‚ˆã£ã¦ç‰¹å¾´ã‚’æ­£è¦åŒ–ã—ã¾ã™ã€‚
Another example is layer normalization (Layer Norm or LN)[1] which was proposed to ease optimization of recurrent neural networks.
ã¾ãŸã€ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€é©åŒ–ã‚’å®¹æ˜“ã«ã™ã‚‹ãŸã‚ã«ææ¡ˆã•ã‚ŒãŸãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ï¼ˆLayer Normã¾ãŸã¯LNï¼‰[1]ã‚‚ãã®ä¸€ä¾‹ã§ã‚ã‚‹ã€‚
Statistics of layer normalization are not computed across the ğ‘ samples in a mini-batch but are estimated in a layer-wise manner for each sample independently.
å±¤æ­£è¦åŒ–ã®çµ±è¨ˆã¯ã€ãƒŸãƒ‹ãƒãƒƒãƒã®Ç”ã‚µãƒ³ãƒ—ãƒ«å…¨ä½“ã§è¨ˆç®—ã•ã‚Œã‚‹ã®ã§ã¯ãªãã€å„ã‚µãƒ³ãƒ—ãƒ«ã«ã¤ã„ã¦ç‹¬ç«‹ã—ã¦å±¤çŠ¶ã«æ¨å®šã•ã‚Œã¾ã™ã€‚
Normalization methods have shown success in accelerating the training of deep networks.
æ­£è¦åŒ–æ‰‹æ³•ã¯ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å­¦ç¿’ã‚’åŠ é€Ÿã•ã›ã‚‹ã“ã¨ã«æˆåŠŸã—ã¾ã—ãŸã€‚

# Our Proposed Model ç§ãŸã¡ã®ææ¡ˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«

In this section, we first describe the feature embedding layer.
æœ¬ç¯€ã§ã¯ã€ã¾ãšã€ç‰¹å¾´é‡åŸ‹ã‚è¾¼ã¿å±¤ã«ã¤ã„ã¦èª¬æ˜ã™ã‚‹ã€‚
Then the details of the instance-guided mask, MaskBlock and MaskNet structure we proposed will be introduced.
ç¶šã„ã¦ã€æˆ‘ã€…ãŒææ¡ˆã—ãŸã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã€MaskBlockã€MaskNetã®æ§‹é€ ã®è©³ç´°ã«ã¤ã„ã¦ç´¹ä»‹ã™ã‚‹ã€‚
Finally the log loss as a loss function is introduced.
æœ€å¾Œã«ã€æå¤±é–¢æ•°ã¨ã—ã¦ã®log lossã‚’ç´¹ä»‹ã™ã‚‹ã€‚

## Embedding Layer ã‚¨ãƒ³ãƒ™ãƒ‡ãƒƒãƒ‰ãƒ¬ã‚¤ãƒ¤ãƒ¼

The input data of CTR tasks usually consists of sparse and dense features and the sparse features are mostly categorical type.
CTRã‚¿ã‚¹ã‚¯ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã¯é€šå¸¸ã€ç–ã¨å¯†ã®ç‰¹å¾´ã§æ§‹æˆã•ã‚Œã€ç–ã®ç‰¹å¾´ã¯ä¸»ã«ã‚«ãƒ†ã‚´ãƒªãƒ¼å‹ã§ã‚ã‚‹ã€‚
Such features are encoded as one-hot vectors which often lead to excessively high-dimensional feature spaces for large vocabularies.
ã“ã®ã‚ˆã†ãªç‰¹å¾´ã¯ã€ä¸€ç™ºãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦ç¬¦å·åŒ–ã•ã‚Œã‚‹ãŸã‚ã€å¤§è¦æ¨¡ãªèªå½™ã®å ´åˆã€éå‰°ã«é«˜æ¬¡å…ƒã®ç‰¹å¾´ç©ºé–“ã¨ãªã‚‹ã“ã¨ãŒå¤šã„ã€‚
The common solution to this problem is to introduce the embedding layer.
ã“ã®å•é¡Œã«å¯¾ã™ã‚‹ä¸€èˆ¬çš„ãªè§£æ±ºç­–ã¯ã€ã‚¨ãƒ³ãƒ™ãƒ‡ãƒƒãƒ‰ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã™ã€‚
Generally, the sparse input can be formulated as:
ä¸€èˆ¬ã«ã€ã‚¹ãƒ‘ãƒ¼ã‚¹å…¥åŠ›ã¯æ¬¡ã®ã‚ˆã†ã«å®šå¼åŒ–ã§ãã‚‹ï¼š

$$
\tag{1}
$$

where ğ‘“ denotes the number of fields, and ğ‘¥ğ‘– âˆˆ R ğ‘› denotes a onehot vector for a categorical field with ğ‘› features and ğ‘¥ğ‘– âˆˆ R ğ‘› is vector with only one value for a numerical field.
ã“ã“ã§ã€á‘“ã¯å ´ã®æ•°ã‚’è¡¨ã—ã€Å¥ğ‘–âˆˆR ğ‘›ã¯ğ‘›å€‹ã®ç‰¹å¾´ã‚’æŒã¤ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å ´ã«å¯¾ã™ã‚‹onehotãƒ™ã‚¯ãƒˆãƒ«ã€Å¥ğ‘–âˆˆR ğ‘›ã¯æ•°å€¤å ´ã«å¯¾ã™ã‚‹å”¯ä¸€ã®å€¤ã‚’æŒã¤ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¡¨ã™ã€‚
We can obtain feature embedding ğ‘’ğ‘– for one-hot vector ğ‘¥ğ‘– via:
ã‚’ä»‹ã—ã¦ã€ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆãƒ™ã‚¯ãƒˆãƒ«Å¥ğ‘–ã®ç‰¹å¾´åŸ‹ã‚è¾¼ã¿á‘’ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼š

$$
\tag{2}
$$

where ğ‘Šğ‘’ âˆˆ R ğ‘˜Ã—ğ‘› is the embedding matrix of ğ‘› features and ğ‘˜ is the dimension of field embedding.
ã“ã“ã§ã€ğ‘Šğ‘’âˆˆRÃ—ğ‘›ã¯ğ‘›ç‰¹å¾´ã®åŸ‹ã‚è¾¼ã¿è¡Œåˆ—ã€ğ‘˜ã¯ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åŸ‹ã‚è¾¼ã¿ã®æ¬¡å…ƒã§ã‚ã‚‹ã€‚
The numerical feature ğ‘¥ğ‘— can also be converted into the same low-dimensional space by:
æ•°å€¤ç‰¹å¾´é‡Å¥á‘—ã‚‚ã€åŒã˜ã‚ˆã†ã«ä½æ¬¡å…ƒç©ºé–“ã«å¤‰æ›ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼š

$$
\tag{3}
$$

where ğ‘‰ğ‘— âˆˆ R ğ‘˜ is the corresponding field embedding with size ğ‘˜.
ã“ã“ã§ã€Ç”âˆˆRâ†ªLl458â†©ã¯ã‚µã‚¤ã‚ºğ‘˜ã®å¯¾å¿œã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åŸ‹ã‚è¾¼ã¿ã§ã‚ã‚‹ã€‚

Through the aforementioned method, an embedding layer is applied upon the raw feature input to compress it to a low dimensional, dense real-value vector.
å‰è¿°ã®æ–¹æ³•ã«ã‚ˆã‚Šã€å…¥åŠ›ã•ã‚ŒãŸç”Ÿã®ç‰¹å¾´é‡ã«å¯¾ã—ã¦åŸ‹ã‚è¾¼ã¿å±¤ã‚’é©ç”¨ã—ã€ä½æ¬¡å…ƒã§å¯†ãªå®Ÿæ•°å€¤ã®ãƒ™ã‚¯ãƒˆãƒ«ã«åœ§ç¸®ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
The result of embedding layer is a wide concatenated vector:
åŸ‹ã‚è¾¼ã¿å±¤ã®çµæœã¯ã€å¹…ã®åºƒã„é€£çµã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã§ã™ï¼š

$$
\tag{}
$$

where ğ‘“ denotes the number of fields, and eğ‘– âˆˆ R ğ‘˜ denotes the embedding of one field.
ã“ã“ã§ã€ğ‘“ã¯å ´ã®æ•°ã‚’è¡¨ã—ã€eğ‘–âˆˆR ğ‘˜ã¯1ã¤ã®å ´ã®åŸ‹ã‚è¾¼ã¿ã‚’è¡¨ã—ã¾ã™ã€‚
Although the feature lengths of input instances can be various, their embedding are of the same length ğ‘“ Ã— ğ‘˜, where ğ‘˜ is the dimension of field embedding.
å…¥åŠ›ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ç‰¹å¾´é‡ã¯æ§˜ã€…ã§ã‚ã‚‹ãŒã€ãã®åŸ‹ã‚è¾¼ã¿ã¯åŒã˜é•·ã•áµ…Ã—458 (ğ‘˜ã¯ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åŸ‹ã‚è¾¼ã¿ã®æ¬¡å…ƒ) ã§ã‚ã‚‹ã€‚

We use instance-guided mask to introduce the multiplicative operation into DNN ranking system and here the so-called "instance" means the feature embedding layer of current input instance in the following part of this paper.
æœ¬ç¨¿ã§ã¯ã€DNNã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã«ä¹—ç®—æ¼”ç®—ã‚’å°å…¥ã™ã‚‹ãŸã‚ã«ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã‚’ç”¨ã„ã‚‹ã€‚ã“ã“ã§ã„ã†ã€Œã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€ã¨ã¯ã€æœ¬ç¨¿ã®ä»¥ä¸‹ã®éƒ¨åˆ†ã«ãŠã„ã¦ã€ç¾åœ¨ã®å…¥åŠ›ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ç‰¹å¾´åŸ‹ã‚è¾¼ã¿å±¤ã‚’æŒ‡ã™ã€‚

## Instance-Guided Mask ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ»ã‚¬ã‚¤ãƒ‰ãƒ»ãƒã‚¹ã‚¯

We utilize the global information collected from input instance by instance-guided mask to dynamically highlight the informative elements in feature embedding and feed-forward layer.
ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã«ã‚ˆã£ã¦å…¥åŠ›ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ã‚‰åé›†ã•ã‚ŒãŸã‚°ãƒ­ãƒ¼ãƒãƒ«ãªæƒ…å ±ã‚’æ´»ç”¨ã—ã€ç‰¹å¾´åŸ‹ã‚è¾¼ã¿ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã§æƒ…å ±é‡ã®å¤šã„è¦ç´ ã‚’å‹•çš„ã«å¼·èª¿ã—ã¾ã™ã€‚
For feature embedding, the mask lays stress on the key elements with more information to effectively represent this feature.
ç‰¹å¾´åŸ‹ã‚è¾¼ã¿ã§ã¯ã€ã“ã®ç‰¹å¾´ã‚’åŠ¹æœçš„ã«è¡¨ç¾ã™ã‚‹ãŸã‚ã«ã€ã‚ˆã‚Šå¤šãã®æƒ…å ±ã‚’æŒã¤é‡è¦ãªè¦ç´ ã«é‡ç‚¹ã‚’ç½®ã„ã¦ã„ã‚‹ã®ãŒç‰¹å¾´ã§ã™ã€‚
For the neurons in hidden layer, the mask helps those important feature interactions to stand out by considering the contextual information in the input instance.
éš ã‚Œå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã«ã¨ã£ã¦ã€ãƒã‚¹ã‚¯ã¯ã€å…¥åŠ›ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®æ–‡è„ˆæƒ…å ±ã‚’è€ƒæ…®ã™ã‚‹ã“ã¨ã§ã€é‡è¦ãªç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’éš›ç«‹ãŸã›ã‚‹ã®ã«å½¹ç«‹ã¤ã€‚
In addition to this advantage, the instance-guided mask also introduces the multiplicative operation into DNN ranking system to capture complex feature cross more efficiently.
ã“ã®åˆ©ç‚¹ã«åŠ ãˆã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã¯ã€DNNãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã«ä¹—ç®—æ¼”ç®—ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€è¤‡é›‘ãªç‰¹å¾´ã®äº¤å·®ã‚’ã‚ˆã‚ŠåŠ¹ç‡çš„ã«æ‰ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

As depicted in Figure 1, two fully connected (FC) layers with identity function are used in instance-guided mask.
å›³1ã«ç¤ºã™ã‚ˆã†ã«ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰å‹ãƒã‚¹ã‚¯ã§ã¯ã€åŒä¸€æ€§æ©Ÿèƒ½ã‚’æŒã¤2ã¤ã®å®Œå…¨æ¥ç¶šï¼ˆFCï¼‰å±¤ãŒä½¿ç”¨ã•ã‚Œã‚‹ã€‚
Notice that the input of instance-guided mask is always from the input instance, that is to say, the feature embedding layer.
ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã®å…¥åŠ›ã¯å¸¸ã«å…¥åŠ›ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ã‚‰ã€ã¤ã¾ã‚Šç‰¹å¾´åŸ‹ã‚è¾¼ã¿å±¤ã‹ã‚‰ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã™ã‚‹ã€‚

The first FC layer is called "aggregation layer" and it is a relatively wider layer compared with the second FC layer in order to better collect the global contextual information in input instance.
ç¬¬1FCå±¤ã¯ã€Œã‚¢ã‚°ãƒªã‚²ãƒ¼ã‚·ãƒ§ãƒ³å±¤ã€ã¨å‘¼ã°ã‚Œã€å…¥åŠ›ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’ã‚ˆã‚Šã‚ˆãåé›†ã™ã‚‹ãŸã‚ã«ã€ç¬¬2FCå±¤ã«æ¯”ã¹ã¦æ¯”è¼ƒçš„åºƒã„å±¤ã¨ãªã£ã¦ã„ã¾ã™ã€‚
The aggregation layer has parameters ğ‘Šğ‘‘1 and here ğ‘‘ denotes the ğ‘‘-th mask.
é›†è¨ˆå±¤ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿áµ„1ã‚’æŒã¡ã€ã“ã“ã§ğ‘‘ã¯áµ…ç•ªç›®ã®ãƒã‚¹ã‚¯ã‚’è¡¨ã™ã€‚
For feature embedding and different MLP layers, we adopt different instance-guided mask owning its parameters to learn to capture various information for each layer from input instance.
ç‰¹å¾´åŸ‹ã‚è¾¼ã¿ã¨ç•°ãªã‚‹MLPå±¤ã«ã¯ã€å…¥åŠ›ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ã‚‰å„å±¤ã«æ§˜ã€…ãªæƒ…å ±ã‚’å–ã‚Šè¾¼ã‚€ãŸã‚ã®å­¦ç¿’ã‚’è¡Œã†ãŸã‚ã€ãã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ç•°ãªã‚‹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰ä»˜ããƒã‚¹ã‚¯ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚

The second FC layer named "projection layer" reduces dimensionality to the same size as feature embedding layerğ‘‰ğ‘’ğ‘šğ‘ or hidden layer ğ‘‰â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘› with parameters ğ‘Šğ‘‘2, Formally,
æŠ•å½±å±¤ã€ã¨åä»˜ã‘ã‚‰ã‚ŒãŸ2ç•ªç›®ã®FCå±¤ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿â„Šâ„Šâ„Šâ„Šâ„Šã€å½¢å¼çš„ã«ã¯áµ„áµã¾ãŸã¯éš ã‚Œå±¤ğ‘…ã¨åŒã˜å¤§ãã•ã«æ¬¡å…ƒã‚’ç¸®å°ã—ã¾ã™ã€

$$
\tag{5}
$$

where ğ‘‰ğ‘’ğ‘šğ‘ âˆˆ R ğ‘š=ğ‘“ Ã—ğ‘˜ refers to the embedding layer of input instance,ğ‘Šğ‘‘1 âˆˆ R ğ‘¡Ã—ğ‘š andğ‘Šğ‘‘2 âˆˆ R ğ‘§Ã—ğ‘¡ are parameters for instanceguided mask, ğ‘¡ and ğ‘§ respectively denotes the neural number of aggregation layer and projection layer, ğ‘“ denotes the number of fields and ğ‘˜ is the dimension of field embedding.
ã“ã“ã§ã€ğ‘‰ğ‘’áµâˆˆR ğ‘š=ğ‘“ Ã—ğ‘˜ã¯å…¥åŠ›ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®åŸ‹ã‚è¾¼ã¿å±¤ã€â†ªL_1D461â†©âˆˆRğ‘šã€áµ„âˆˆRğ‘‘ğ‘§ã¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°ãƒã‚¹ã‚¯ç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒ‡ã™ã€ ğ‘¡ã¯é›†ç´„å±¤ã€ğ‘§ã¯æŠ•å½±å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ•°ã‚’ã€ğ‘“ã¯ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°ã€ğ‘˜ã¯ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åŸ‹è¾¼ã®æ¬¡å…ƒã‚’ãã‚Œãã‚Œè¡¨ã™ã€‚
ğ›½ğ‘‘1 âˆˆ R ğ‘¡Ã—ğ‘š and ğ›½ğ‘‘2 âˆˆ R ğ‘§Ã—ğ‘¡ are learned bias of the two FC layers.
ğ›½áµ…1âˆˆRğ‘¡Ã—áµ†ã€ğ›½áµ…2âˆˆRğ‘¡ã¯2ã¤ã®FCå±¤ã®å­¦ç¿’ãƒã‚¤ã‚¢ã‚¹ã€‚
Notice here that the aggregation layer is usually wider than the projection layer because the size of the projection layer is required to be equal to the size of feature embedding layer or MLP layer.
ã“ã“ã§æ³¨ç›®ã—ãŸã„ã®ã¯ã€æŠ•å½±å±¤ã®ã‚µã‚¤ã‚ºã¯ç‰¹å¾´åŸ‹ã‚è¾¼ã¿å±¤ã‚„MLPå±¤ã®ã‚µã‚¤ã‚ºã¨ç­‰ã—ã„ã“ã¨ãŒè¦æ±‚ã•ã‚Œã‚‹ãŸã‚ã€é›†è¨ˆå±¤ã¯é€šå¸¸ã€æŠ•å½±å±¤ã‚ˆã‚Šã‚‚åºƒã„ã“ã¨ã§ã™ã€‚
So we define the size ğ‘Ÿ = ğ‘¡/ğ‘§ as reduction ratio which is a hyper-parameter to control the ratio of neuron numbers of two layers.
ãã“ã§ã€2ã¤ã®å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ã®æ¯”ç‡ã‚’åˆ¶å¾¡ã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã§ã‚ã‚‹ç¸®å°ç‡ã¨ã—ã¦ã€ã‚µã‚¤ã‚ºáµ… =ğ‘¡/ğ‘§ã‚’å®šç¾©ã™ã‚‹ã“ã¨ã«ã—ã¾ã™ã€‚

Element-wise product is used in this work to incorporate the global contextual information aggregated by instance-guided mask into feature embedding or hidden layer as following:
æœ¬ä½œå“ã§ã¯ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã«ã‚ˆã£ã¦é›†ç´„ã•ã‚ŒãŸã‚°ãƒ­ãƒ¼ãƒãƒ«ãªæ–‡è„ˆæƒ…å ±ã‚’ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ç‰¹å¾´åŸ‹ã‚è¾¼ã¿ã‚„éš ã‚Œå±¤ã«å–ã‚Šè¾¼ã‚€ãŸã‚ã«ã€è¦ç´ åˆ¥ç©ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ï¼š

$$
\tag{6}
$$

where Vğ‘’ğ‘šğ‘ denotes embedding layer and Vâ„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘› means the feedforward layer in DNN model, âŠ™ means the element-wise production between two vectors as follows:
ã“ã“ã§ã€Vá‘’ã¯åŸ‹ã‚è¾¼ã¿å±¤ã€Vá‘šã¯DNNãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã‚’è¡¨ã—ã€âŠ™ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«2ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«é–“ã®è¦ç´ ã”ã¨ã®ç”Ÿæˆã‚’æ„å‘³ã—ã¾ã™ï¼š

$$
\tag{7}
$$

here ğ‘¢ is the size of vector ğ‘‰ğ‘– and ğ‘‰ğ‘—
ã“ã“ã§ã€ğ‘¢ã¯ãƒ™ã‚¯ãƒˆãƒ«ğ‘‰ğ‘–ã®å¤§ãã•ã€ğ‘‰ğ‘—ã¯

The instance-guided mask can be regarded as a special kind of bitwise attention or gating mechanism which uses the global context information contained in input instance to guide the parameter optimization during training.
ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰ãƒã‚¹ã‚¯ã¯ã€å…¥åŠ›ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«å«ã¾ã‚Œã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’ç”¨ã„ã¦ã€å­¦ç¿’ä¸­ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’ã‚¬ã‚¤ãƒ‰ã™ã‚‹ç‰¹æ®Šãªç¨®é¡ã®ãƒ“ãƒƒãƒˆå˜ä½ã®æ³¨æ„ã¾ãŸã¯ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹ã¨ã¿ãªã™ã“ã¨ãŒã§ãã¾ã™ã€‚
The bigger value inğ‘‰ğ‘šğ‘ğ‘ ğ‘˜ implies that the model dynamically identifies an important element in feature embedding or hidden layer.
ğ‘‰ã®å€¤ãŒå¤§ãã„ã»ã©ã€ãƒ¢ãƒ‡ãƒ«ãŒç‰¹å¾´åŸ‹è¾¼ã‚„éš ã‚Œå±¤ã®é‡è¦ãªè¦ç´ ã‚’å‹•çš„ã«è­˜åˆ¥ã—ã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚
It is used to boost the element in vector ğ‘‰ğ‘’ğ‘šğ‘ orğ‘‰â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›.
ãƒ™ã‚¯ãƒˆãƒ«ğ‘‰ğ‘’ğ‘‰ã¾ãŸã¯ğ‘…ğ‘ã®è¦ç´ ã‚’é«˜ã‚ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ã‚‚ã®ã§ã‚ã‚‹ã€‚
On the contrary, small value inğ‘‰ğ‘šğ‘ğ‘ ğ‘˜ will suppress the uninformative elements or even noise by decreasing the values in the corresponding vector ğ‘‰ğ‘’ğ‘šğ‘ or ğ‘‰â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›.
é€†ã«ğ‘‰ã®å€¤ãŒå°ã•ã„ã¨ã€å¯¾å¿œã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«ğ‘ ğ‘˜ã®å€¤ãŒå°ã•ããªã‚Šã€æƒ…å ±é‡ã®å°‘ãªã„è¦ç´ ã‚„ãƒã‚¤ã‚ºã¾ã§æŠ‘åˆ¶ã•ã‚Œã¾ã™ã€‚

The two main advantages in adopting the instance-guided mask are: firstly, the element-wise product between the mask and hidden layer or feature embedding layer brings multiplicative operation into DNN ranking system in unified way to more efficiently capture complex feature interaction.
ç¬¬ä¸€ã«ã€ãƒã‚¹ã‚¯ã¨éš ã‚Œå±¤ã¾ãŸã¯ç‰¹å¾´åŸ‹ã‚è¾¼ã¿å±¤ã®é–“ã®è¦ç´ ã”ã¨ã®ç©ãŒã€DNNãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã«çµ±ä¸€çš„ãªæ–¹æ³•ã§ä¹—ç®—æ¼”ç®—ã‚’ã‚‚ãŸã‚‰ã—ã€è¤‡é›‘ãªç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’ã‚ˆã‚ŠåŠ¹ç‡çš„ã«æ•ã‚‰ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã“ã¨ã§ã™ã€‚
Secondly, this kind of fine-gained bit-wise attention guided by input instance can both weaken the influence of noise in feature embedding and MLP layers while highlight the informative signals in DNN ranking systems.
ç¬¬äºŒã«ã€å…¥åŠ›ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«ã‚ˆã£ã¦å°ã‹ã‚Œã‚‹ã“ã®ã‚ˆã†ãªç´°ã‹ã„ãƒ“ãƒƒãƒˆå˜ä½ã®æ³¨æ„ã¯ã€ç‰¹å¾´åŸ‹ã‚è¾¼ã¿ã¨MLPå±¤ã«ãŠã‘ã‚‹ãƒã‚¤ã‚ºã®å½±éŸ¿ã‚’å¼±ã‚ã‚‹ä¸€æ–¹ã§ã€DNNãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹æœ‰ç›Šãªä¿¡å·ã‚’å¼·èª¿ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

## MaskBlock MaskBlock

To overcome the problem of the inefficiency of feed-forward layer to capture complex feature interaction in DNN models, we propose a basic building block named MaskBlock for DNN ranking systems in this work, as shown in Figure 2 and Figure 3.
DNNãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹è¤‡é›‘ãªç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’æ‰ãˆã‚‹ã«ã¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ãŒéåŠ¹ç‡ã§ã‚ã‚‹ã¨ã„ã†å•é¡Œã‚’å…‹æœã™ã‚‹ãŸã‚ã«ã€æœ¬ç ”ç©¶ã§ã¯å›³2ãŠã‚ˆã³å›³3ã«ç¤ºã™ã‚ˆã†ãªDNNãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®ãŸã‚ã®MaskBlockã¨ã„ã†åŸºæœ¬æ§‹æˆãƒ–ãƒ­ãƒƒã‚¯ã‚’ææ¡ˆã—ã¾ã™ã€‚
The proposed MaskBlock consists of three key components: layer normalization module ,instance-guided mask, and a feed-forward hidden layer.
ææ¡ˆã™ã‚‹MaskBlockã¯ã€å±¤æ­£è¦åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰éš ã‚Œå±¤ã®3ã¤ã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§æ§‹æˆã•ã‚Œã‚‹ã€‚
The layer normalization can ease optimization of the network.
ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ­£è¦åŒ–ã«ã‚ˆã‚Šã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€é©åŒ–ã‚’å®¹æ˜“ã«ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
The instance-guided mask introduces multiplicative interactions for feed-forward layer of a standard DNN model and feed-forward hidden layer aggregate the masked information in order to better capture the important feature interactions.
ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã¯ã€æ¨™æº–çš„ãªDNNãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã«ä¹—ç®—çš„ãªç›¸äº’ä½œç”¨ã‚’å°å…¥ã—ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰éš ã‚Œå±¤ã¯ã€é‡è¦ãªç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’ã‚ˆã‚Šã‚ˆãæ‰ãˆã‚‹ãŸã‚ã«ã€ãƒã‚¹ã‚¯ã•ã‚ŒãŸæƒ…å ±ã‚’é›†ç´„ã™ã‚‹ã‚‚ã®ã§ã‚ã‚‹ã€‚
In this way, we turn the widely used feed-forward layer of a standard DNN model into a mixture of addictive and multiplicative feature interactions.
ã“ã®ã‚ˆã†ã«ã—ã¦ã€æ¨™æº–çš„ãªDNNãƒ¢ãƒ‡ãƒ«ã®åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã‚’ã€åŠ æ³•çš„ãƒ»ä¹—æ³•çš„ãªç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã®æ··åˆã«å¤‰ãˆã‚‹ã®ã§ã™ã€‚

First, we briefly review the formulation of LayerNorm.
ã¾ãšã€LayerNormã®å®šå¼åŒ–ã‚’ç°¡å˜ã«èª¬æ˜ã™ã‚‹ã€‚

### Layer Normalization: ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ­£è¦åŒ–ï¼š

In general, normalization aims to ensure that signals have zero mean and unit variance as they propagate through a network to reduce "covariate shift" [10].
ä¸€èˆ¬ã«ã€æ­£è¦åŒ–ã¨ã¯ã€ä¿¡å·ãŒãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä¼æ¬ã™ã‚‹éš›ã«ã€å¹³å‡å€¤ãŒã‚¼ãƒ­ã§åˆ†æ•£ãŒå˜ä½ã¨ãªã‚‹ã‚ˆã†ã«ã—ã€ã€Œå…±å¤‰é‡ã‚·ãƒ•ãƒˆã€ã‚’æ¸›ã‚‰ã™ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™[10]ã€‚
As an example, layer normalization (Layer Norm or LN)[1] was proposed to ease optimization of recurrent neural networks.
ä¾‹ãˆã°ã€ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€é©åŒ–ã‚’å®¹æ˜“ã«ã™ã‚‹ãŸã‚ã«ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒãƒ¼ãƒãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆLayer Normã€LNï¼‰[1]ãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ã€‚
Specifically, let ğ‘¥ = (ğ‘¥1, ğ‘¥2, ..., ğ‘¥ğ» ) denotes the vector representation of an input of size ğ» to normalization layers.
å…·ä½“çš„ã«ã¯ã€ğ‘¥ = (ğ‘¥2, ..., â†ªLl_1D43B) ã¯ã€æ­£è¦åŒ–å±¤ã¸ã®ã‚µã‚¤ã‚ºáµƒã®å…¥åŠ›ã®ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ã‚’ç¤ºã™ã¨ã™ã‚‹ã€‚
LayerNorm re-centers and re-scales input x as
LayerNormã¯ã€å…¥åŠ›ã•ã‚ŒãŸxã‚’å†ä¸­å¿ƒåŒ–ã—ã€å†ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã€‚

$$
\tag{8}
$$

where â„ is the output of a LayerNorm layer.
ã“ã“ã§ã€â„ã¯LayerNormå±¤ã®å‡ºåŠ›ã§ã‚ã‚‹ã€‚
âŠ™ is an element-wise production operation.
âŠ™ã¯è¦ç´ ã”ã¨ã®ç”Ÿç”£æ“ä½œã§ã™ã€‚
ğœ‡ and ğ›¿ are the mean and standard deviation of input.
ğœ‡ ã¨ğ›¿ ã¯å…¥åŠ›ã®å¹³å‡ã¨æ¨™æº–åå·®ã§ã‚ã‚‹ã€‚
Bias b and gain g are parameters with the same dimension ğ».
ãƒã‚¤ã‚¢ã‚¹bã¨ã‚²ã‚¤ãƒ³gã¯åŒã˜æ¬¡å…ƒáµƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚‹ã€‚

As one of the key component in MaskBlock, layer normalization can be used on both feature embedding and feed- forward layer.
MaskBlockã®ä¸»è¦ãªæ§‹æˆè¦ç´ ã®1ã¤ã§ã‚ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã¯ã€ç‰¹å¾´åŸ‹ã‚è¾¼ã¿å±¤ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®ä¸¡æ–¹ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
For the feature embedding layer, we regard each featureâ€™s embedding as a layer to compute the mean, standard deviation, bias and gain of LN as follows:
ç‰¹å¾´åŸ‹ã‚è¾¼ã¿å±¤ã«ã¤ã„ã¦ã¯ã€å„ç‰¹å¾´ã®åŸ‹ã‚è¾¼ã¿ã‚’1ã¤ã®å±¤ã¨ã¿ãªã—ã¦ã€ä»¥ä¸‹ã®ã‚ˆã†ã«LNã®å¹³å‡ã€æ¨™æº–åå·®ã€ãƒã‚¤ã‚¢ã‚¹ã€ã‚²ã‚¤ãƒ³ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã«ã—ã¦ã„ã¾ã™ï¼š

$$
\tag{9}
$$

As for the feed-forward layer in DNN model, the statistics of ğ¿ğ‘ are estimated among neurons contained in the corresponding hidden layer as follows:
DNNãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã«ã¤ã„ã¦ã¯ã€å¯¾å¿œã™ã‚‹éš ã‚Œå±¤ã«å«ã¾ã‚Œã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é–“ã§ã€ä»¥ä¸‹ã®ã‚ˆã†ã«â†ªLu43Fâ†©ã®çµ±è¨ˆé‡ã‚’æ¨å®šã™ã‚‹ï¼š

$$
\tag{10}
$$

where X âˆˆ R ğ‘¡ refers to the input of feed-forward layer, Wğ‘– âˆˆ R ğ‘šÃ—ğ‘¡ are parameters for the layer, ğ‘¡ and ğ‘š respectively denotes the size of input layer and neural number of feed-forward layer.
ã“ã“ã§ã€XâˆˆR ğ‘¡ã¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®å…¥åŠ›ã€Wğ‘–âˆˆR ğ‘šÃ—ğ‘¡ã¯å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ğ‘¡ã¯å…¥åŠ›å±¤ã®ã‚µã‚¤ã‚ºã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ•°ã‚’ãã‚Œãã‚Œç¤ºã™ã€‚
Notice that we have two places to put normalization operation on the MLP: one place is before non-linear operation and another place is after non-linear operation.
MLPã«æ­£è¦åŒ–æ“ä½œã‚’å…¥ã‚Œã‚‹å ´æ‰€ãŒ2ã¤ã‚ã‚‹ã“ã¨ã«æ³¨ç›®ã—ã¦ãã ã•ã„ï¼š1ã¤ã¯éç·šå½¢æ“ä½œã®å‰ã€ã‚‚ã†1ã¤ã¯éç·šå½¢æ“ä½œã®å¾Œã§ã™ã€‚
We find the performance of the normalization before non-linear consistently outperforms that of the normalization after non-linear operation.
éç·šå½¢å‰ã®æ­£è¦åŒ–ã®æ€§èƒ½ãŒã€éç·šå½¢æ¼”ç®—å¾Œã®æ­£è¦åŒ–ã®æ€§èƒ½ã‚’ä¸€è²«ã—ã¦ä¸Šå›ã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚
So all the normalization used in MLP part is put before non-linear operation in our paper as formula (4) shows.
ãã®ãŸã‚ã€MLPéƒ¨åˆ†ã§ä½¿ç”¨ã•ã‚Œã‚‹æ­£è¦åŒ–ã¯ã€å¼ï¼ˆ4ï¼‰ãŒç¤ºã™ã‚ˆã†ã«ã€æœ¬è«–æ–‡ã§ã¯ã™ã¹ã¦éç·šå½¢æ¼”ç®—ã®å‰ã«ç½®ã‹ã‚Œã¾ã™ã€‚

### MaskBlock on Feature Embedding: MaskBlock on Feature Embeddingï¼š

We propose MaskBlock by combining the three key elements: layer normalization, instance-guided mask and a following feed-forward layer.
æˆ‘ã€…ã¯ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ­£è¦åŒ–ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰ä»˜ããƒã‚¹ã‚¯ã€å¾Œç¶šã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã¨ã„ã†3ã¤ã®é‡è¦ãªè¦ç´ ã‚’çµ„ã¿åˆã‚ã›ã¦MaskBlockã‚’ææ¡ˆã™ã‚‹ã€‚
MaskBlock can be stacked to form deeper network.
MaskBlockã¯ç©ã¿é‡ã­ã¦ã‚ˆã‚Šæ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å½¢æˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
According to the different input of each MaskBlock, we have two kinds of MaskBlocks: MaskBlock on feature embedding and MaskBlock on Maskblock.
å„MaskBlockã®å…¥åŠ›ã®é•ã„ã«å¿œã˜ã¦ã€2ç¨®é¡ã®MaskBlockã‚’ç”¨æ„ã—ã¾ã—ãŸï¼š MaskBlock on feature embeddingã¨MaskBlock on Maskblockã§ã‚ã‚‹ã€‚
We will firstly introduce the MaskBlock on feature embedding as depicted in Figure 2 in this subsection.
æœ¬ç¯€ã§ã¯ã€ã¾ãšå›³2ã«æã‹ã‚ŒãŸç‰¹å¾´åŸ‹ã‚è¾¼ã¿ã«é–¢ã™ã‚‹MaskBlockã‚’ç´¹ä»‹ã™ã‚‹ã€‚

The feature embedding Vğ‘’ğ‘šğ‘ is the only input for MaskBlock on feature embedding.
ç‰¹å¾´åŸ‹ã‚è¾¼ã¿Vá‘’ã¯ã€ç‰¹å¾´åŸ‹ã‚è¾¼ã¿ã®MaskBlockã®å”¯ä¸€ã®å…¥åŠ›ã§ã™ã€‚
After the layer normalization operation on embedding Vğ‘’ğ‘šğ‘.
åŸ‹ã‚è¾¼ã¿Vğ‘’ã®å±¤æ­£è¦åŒ–æ¼”ç®—ã‚’è¡Œã£ãŸå¾Œã€‚
MaskBlock utilizes instance-guided mask to highlight the informative elements in Vğ‘’ğ‘šğ‘ by element-wise product, Formally,
MaskBlockã¯ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã‚’åˆ©ç”¨ã—ã¦ã€Vá‘’á‘šã®æƒ…å ±é‡ã®å¤šã„è¦ç´ ã‚’è¦ç´ åˆ¥ç©ã§ãƒã‚¤ãƒ©ã‚¤ãƒˆã™ã‚‹ã€Formallyã€

$$
\tag{11}
$$

where âŠ™ means an element-wise production between the instanceguided mask and the normalized vector ğ¿ğ‘ğ¸ğ‘€ğµ(Vğ‘’ğ‘šğ‘), Vğ‘šğ‘ğ‘ ğ‘˜ğ‘’ğ‘‘ğ¸ğ‘€ğµ denote the masked feature embedding.
ã“ã“ã§ã€âŠ™ã¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰ã•ã‚ŒãŸãƒã‚¹ã‚¯ã¨æ­£è¦åŒ–ãƒ™ã‚¯ãƒˆãƒ«áµƒ(Vğ‘)ã¨ã®é–“ã®è¦ç´ ã”ã¨ã®ç”Ÿç”£ã€Vğ‘šáµ„ğµã¯ãƒã‚¹ã‚¯ã•ã‚ŒãŸç‰¹å¾´åŸ‹è¾¼ã‚’è¡¨ã™ã€‚
Notice that the input of instance-guided mask Vğ‘šğ‘ğ‘ ğ‘˜ is also the feature embedding ğ‘‰ğ‘’ğ‘šğ‘.
ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯Vá‘šá‘˜ã®å…¥åŠ›ã¯ã€ç‰¹å¾´åŸ‹ã‚è¾¼ã¿á‘”á‘’á‘”ã§ã‚‚ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

We introduce a feed-forward hidden layer and a following layer normalization operation in MaskBlock to better aggregate the masked information by a normalized non-linear transformation.
MaskBlockã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰éš ã‚Œå±¤ã¨ãã‚Œã«ç¶šãå±¤ã®æ­£è¦åŒ–æ“ä½œã‚’å°å…¥ã—ã€æ­£è¦åŒ–ã•ã‚ŒãŸéç·šå½¢å¤‰æ›ã«ã‚ˆã£ã¦ãƒã‚¹ã‚¯ã•ã‚ŒãŸæƒ…å ±ã‚’ã‚ˆã‚Šã‚ˆãé›†ç´„ã•ã›ã‚‹ã€‚
The output of MaskBlock can be calculated as follows:
MaskBlockã®å‡ºåŠ›ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«è¨ˆç®—ã§ãã¾ã™ï¼š

$$
\tag{12}
$$

where Wğ‘– âˆˆ R ğ‘Ã—ğ‘› are parameters of the feed-forward layer in the ğ‘–-th MaskBlock, ğ‘› denotes the size of Vğ‘šğ‘ğ‘ ğ‘˜ğ‘’ğ‘‘ğ¸ğ‘€ğµ and ğ‘ means the size of neural number of the feed-forward layer.
ã“ã“ã§ã€Wğ‘–âˆˆR ğ‘Ã—ğ‘›ã¯ğ‘–ç•ªç›®ã®MaskBlockã«ãŠã‘ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ğ‘›ã¯Vğ‘šğ‘ğ‘’ã®ã‚µã‚¤ã‚ºã€ğ‘ã¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ç•ªå·ã®å¤§ãã•ã‚’è¡¨ã™ã€‚

The instance-guided mask introduces the element-wise product into feature embedding as a fine-grained attention while normalization both on feature embedding and hidden layer eases the network optimization.
ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰å‹ãƒã‚¹ã‚¯ã¯ã€ç‰¹å¾´åŸ‹ã‚è¾¼ã¿ã«è¦ç´ ã”ã¨ã®ç©ã‚’å°å…¥ã—ã€ç‰¹å¾´åŸ‹ã‚è¾¼ã¿ã¨éš ã‚Œå±¤ã®ä¸¡æ–¹ã§æ­£è¦åŒ–ã™ã‚‹ã“ã¨ã§ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€é©åŒ–ã‚’å®¹æ˜“ã«ã—ã¾ã™ã€‚
These key components in MaskBlock help the feedforward layer capture complex feature cross more efficiently.
MaskBlockã®ã“ã‚Œã‚‰ã®ã‚­ãƒ¼ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ãŒè¤‡é›‘ãªç‰¹å¾´çš„ãªã‚¯ãƒ­ã‚¹ã‚’ã‚ˆã‚ŠåŠ¹ç‡çš„ã«æ‰ãˆã‚‹ã“ã¨ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

### MaskBlock on MaskBlock: MaskBlock on MaskBlockï¼š

In this subsection, we will introduce MaskBlock on MaskBlock as depicted in Figure 3.
ã“ã®å°ç¯€ã§ã¯ã€å›³3ã«æã‹ã‚ŒãŸMaskBlock on MaskBlockã‚’ç´¹ä»‹ã™ã‚‹ã€‚
There are two different inputs for this MaskBlock: feature embedding ğ‘‰ğ‘’ğ‘šğ‘ and the output ğ‘‰ ğ‘ ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ of the previous MaskBlock.
ã“ã®MaskBlockã®å…¥åŠ›ã¯ã€ç‰¹å¾´åŸ‹ã‚è¾¼ã¿Ç”â†ªLl452â†©ã¨ã€å‰ã®MaskBlockã®å‡ºåŠ›ğ‘‰â†ªLl452â†©â†ªLl452â†©ã®2ç¨®é¡ã§ã™ã€‚
The input of instance-guided mask for this kind of MaskBlock is always the feature embedding ğ‘‰ğ‘’ğ‘šğ‘.
ã“ã®ç¨®ã®MaskBlockã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã®å…¥åŠ›ã¯å¸¸ã«ç‰¹å¾´åŸ‹ã‚è¾¼ã¿Ç”á‘’á‘šã¨ãªã‚‹ã€‚
MaskBlock utilizes instance-guided mask to highlight the important feature interactions in previous MaskBlockâ€™s output ğ‘‰ ğ‘ ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ by element-wise product, Formally,
MaskBlockã¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã‚’åˆ©ç”¨ã—ã¦ã€ä»¥å‰ã®MaskBlockã®å‡ºåŠ›ğ‘‰ğ‘¡ğ‘¡ã‚’è¦ç´ åˆ¥ç©ã§å¼·èª¿ã—ã€Formallyã€

$$
\tag{13}
$$

where âŠ™ means an element-wise production between the instanceguided mask ğ‘‰ğ‘šğ‘ğ‘ ğ‘˜ and the previous MaskBlockâ€™s output ğ‘‰ ğ‘ ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡, ğ‘‰ğ‘šğ‘ğ‘ ğ‘˜ğ‘’ğ‘‘ğ» ğ¼ğ· denote the masked hidden layer.
where âŠ™ means an element-wise production between the instanceguided mask ğ‘‰ğ‘šğ‘ğ‘ ğ‘˜ and the previous MaskBlockâ€™s output ğ‘‰ ğ‘ ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡, ğ‘‰ğ‘šğ‘ğ‘ ğ‘˜ğ‘’ğ‘‘ğ» ğ¼ğ· denote the masked hidden layer.

In order to better capture the important feature interactions, another feed-forward hidden layer and a following layer normalization are introduced in MaskBlock .
é‡è¦ãªç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’ã‚ˆã‚Šã‚ˆãæ‰ãˆã‚‹ãŸã‚ã«ã€MaskBlock ã§ã¯ã€ã‚‚ã†ä¸€ã¤ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰éš ã‚Œå±¤ã¨ãã‚Œã«ç¶šãå±¤ã®æ­£è¦åŒ–ã‚’å°å…¥ã—ã¦ã„ã¾ã™ã€‚
In this way, we turn the widely used feed-forward layer of a standard DNN model into a mixture of addictive and multiplicative feature interactions to avoid the ineffectiveness of those addictive feature cross models.
ã“ã®ã‚ˆã†ã«ã—ã¦ã€æ¨™æº–çš„ãªDNNãƒ¢ãƒ‡ãƒ«ã®åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã‚’ã€åŠ æ³•çš„ç‰¹å¾´ç›¸äº’ä½œç”¨ã¨ä¹—æ³•çš„ç‰¹å¾´ç›¸äº’ä½œç”¨ã®æ··åˆã«å¤‰ãˆã€ãã‚Œã‚‰ã®åŠ æ³•çš„ç‰¹å¾´ç›¸äº’ä½œç”¨ãƒ¢ãƒ‡ãƒ«ã®éåŠ¹æœã‚’å›é¿ã—ã¦ã„ã‚‹ã®ã§ã™ã€‚
The output of MaskBlock can be calculated as follows:
MaskBlockã®å‡ºåŠ›ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«è¨ˆç®—ã§ãã¾ã™ï¼š

$$
\tag{14}
$$

where ğ‘Šğ‘– âˆˆ R ğ‘Ã—ğ‘› are parameters of the feed-forward layer in the ğ‘–-th MaskBlock, ğ‘› denotes the size of Vğ‘šğ‘ğ‘ ğ‘˜ğ‘’ğ‘‘ğ» ğ¼ğ· and ğ‘ means the size of neural number of the feed-forward layer.
ã“ã“ã§ã€ğ‘Šğ‘–âˆˆRâ†ªL_1D45Eâ†©Ã—ğ‘›ã¯ğ‘–ç•ªç›®ã®MaskBlockã«ãŠã‘ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ğ‘›ã¯Vğ‘šğ‘ˆğ‘’áµƒğ·ã€ğ‘ã¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ç•ªå·ã®å¤§ãã•ã‚’è¡¨ã™ã€‚

## MaskNet ãƒã‚¹ã‚¯ãƒãƒƒãƒˆ

Based on the MaskBlock, various new ranking models can be designed according to different configurations.
MaskBlockã‚’ãƒ™ãƒ¼ã‚¹ã«ã€æ§˜ã€…ãªæ§‹æˆã§æ–°ã—ã„ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
The rank model consisting of MaskBlock is called MaskNet in this work.
MaskBlockã§æ§‹æˆã•ã‚Œã‚‹ãƒ©ãƒ³ã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ã€æœ¬ä½œå“ã§ã¯MaskNetã¨å‘¼ã¶ã€‚
We also propose two MaskNet models by utilizing the MaskBlock as the basic building block.
ã¾ãŸã€MaskBlockã‚’åŸºæœ¬æ§‹æˆè¦ç´ ã¨ã—ã¦ã€2ã¤ã®MaskNetãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚

### Serial MaskNet: Serial MaskNetï¼š

We can stack one MaskBlock after another to build the ranking system , as shown by the left model in Figure 4.
å›³4ã®å·¦ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚ˆã†ã«ã€MaskBlockã‚’æ¬¡ã€…ã«ç©ã¿é‡ã­ã¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
The first block is a MaskBlock on feature embedding and all other blocks are MaskBlock on Maskblock to form a deeper network.
æœ€åˆã®ãƒ–ãƒ­ãƒƒã‚¯ã¯ç‰¹å¾´åŸ‹ã‚è¾¼ã¿ã«MaskBlockã€ãã‚Œä»¥å¤–ã®ãƒ–ãƒ­ãƒƒã‚¯ã¯Maskblockã«MaskBlockã—ã¦ã‚ˆã‚Šæ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å½¢æˆã—ã¾ã™ã€‚
The prediction layer is put on the final MaskBlockâ€™s output vector.
äºˆæ¸¬å±¤ã¯ã€æœ€çµ‚çš„ãªMaskBlockã®å‡ºåŠ›ãƒ™ã‚¯ãƒˆãƒ«ã«ã‹ã‘ã‚‹ã€‚
We call MaskNet under this serial configuration as SerMaskNet in our paper.
æœ¬ç¨¿ã§ã¯ã€ã“ã®ã‚ˆã†ãªã‚·ãƒªã‚¢ãƒ«æ§‹æˆã®MaskNetã‚’SerMaskNetã¨å‘¼ã¶ã“ã¨ã«ã™ã‚‹ã€‚
All inputs of instance-guided mask in every MaskBlock come from the feature embedding layer Vğ‘’ğ‘šğ‘ and this makes the serial MaskNet model look like a RNN model with sharing input at each time step.
å…¨ã¦ã®MaskBlockã«ãŠã‘ã‚‹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã®å…¥åŠ›ã¯å…¨ã¦ç‰¹å¾´åŸ‹ã‚è¾¼ã¿å±¤Vá‘’á‘šã‹ã‚‰æ¥ã‚‹ãŸã‚ã€ã‚·ãƒªã‚¢ãƒ«MaskNetãƒ¢ãƒ‡ãƒ«ã¯å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§å…¥åŠ›ã‚’å…±æœ‰ã™ã‚‹RNNãƒ¢ãƒ‡ãƒ«ã®ã‚ˆã†ã«è¦‹ãˆã‚‹ã€‚

### Parallel MaskNet: ãƒ‘ãƒ©ãƒ¬ãƒ«ãƒã‚¹ã‚¯ãƒãƒƒãƒˆ

We propose another MaskNet by placing several MaskBlocks on feature embedding in parallel on a sharing feature embedding layer, as depicted by the right model in Figure 4.
å›³4ã®å³ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚ˆã†ã«ã€ç‰¹å¾´åŸ‹ã‚è¾¼ã¿ã«é–¢ã™ã‚‹MaskBlockã‚’å…±æœ‰ã™ã‚‹ç‰¹å¾´åŸ‹ã‚è¾¼ã¿å±¤ä¸Šã«è¤‡æ•°ä¸¦åˆ—ã«é…ç½®ã™ã‚‹ã“ã¨ã§ã€åˆ¥ã®MaskNetã‚’ææ¡ˆã—ã¾ã™ã€‚
The input of each block is only the shared feature embedding Vğ‘’ğ‘šğ‘ under this configuration.
å„ãƒ–ãƒ­ãƒƒã‚¯ã®å…¥åŠ›ã¯ã€ã“ã®æ§‹æˆã§ã¯å…±æœ‰ç‰¹å¾´åŸ‹ã‚è¾¼ã¿Vá‘’á‘šã®ã¿ã§ã‚ã‚‹ã€‚
We can regard this ranking model as a mixture of multiple experts just as MMoE[15] does.
ã“ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¯ã€MMoE[15]ã¨åŒã˜ã‚ˆã†ã«ã€è¤‡æ•°ã®å°‚é–€å®¶ã®æ··åˆç‰©ã¨ã¿ãªã™ã“ã¨ãŒã§ãã‚‹ã€‚
Each MaskBlock pays attention to specific kind of important features or feature interactions.
å„MaskBlockã¯ã€ç‰¹å®šã®ç¨®é¡ã®é‡è¦ãªæ©Ÿèƒ½ã‚„æ©Ÿèƒ½ã®ç›¸äº’ä½œç”¨ã«æ³¨ç›®ã—ã¦ã„ã¾ã™ã€‚
We collect the information of each expert by concatenating the output of each MaskBlock as follows:
å„MaskBlockã®å‡ºåŠ›ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«é€£çµã™ã‚‹ã“ã¨ã§ã€å„ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®æƒ…å ±ã‚’åé›†ã™ã‚‹ï¼š

$$
\tag{15}
$$

where Vğ‘– ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ âˆˆ R ğ‘ is the output of the ğ‘–-th MaskBlock and ğ‘ means size of neural number of feed-forward layer in MaskBlock, ğ‘¢ is the MaskBlock number.
ã“ã“ã§ã€Vğ‘– ğ‘œğ‘¢ âˆˆ R ğ‘ ã¯ğ‘–ç•ªç›®ã®MaskBlockã®å‡ºåŠ›ã€ğ‘® ã¯MaskBlockã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ•°ã®ã‚µã‚¤ã‚ºã€áµ† ã¯MaskBlockç•ªå·ã§ã‚ã‚‹ã€‚

To further merge the feature interactions captured by each expert, multiple feed-forward layers are stacked on the concatenation information Vğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ .
å„ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãŒæ‰ãˆãŸç‰¹å¾´çš„ãªç›¸äº’ä½œç”¨ã‚’ã•ã‚‰ã«çµ±åˆã™ã‚‹ãŸã‚ã«ã€é€£çµæƒ…å ±Vğ‘šğ‘’ã«è¤‡æ•°ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã‚’ç©ã¿é‡ã­ã‚‹ã€‚
Let H0 = Vğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ denotes the output of the concatenation layer, then H0 is fed into the deep neural network and the feed forward process is:
H0 = Vğ‘šğ‘’ãŒé€£çµå±¤ã®å‡ºåŠ›ã‚’è¡¨ã™ã¨ã™ã‚‹ã¨ã€H0ã¯ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«æŠ•å…¥ã•ã‚Œã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å‡¦ç†ã¯

$$
\tag{16}
$$

where ğ‘™ is the depth and ReLU is the activation function.
ã“ã“ã§ã€â†ªL_1D459â†©ã¯æ·±åº¦ã€ReLUã¯æ´»æ€§åŒ–é–¢æ•°ã§ã‚ã‚‹ã€‚
Wğ‘¡ , ğ›½ğ‘¡ , Hğ‘™ are the model weight, bias and output of the ğ‘™-th layer.
Wğ‘¡ ,ğ‘¡ , Hâ†ªL_1D459â†© ã¯ã€ğ‘™ç¬¬1å±¤ã®ãƒ¢ãƒ‡ãƒ«é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹ã€å‡ºåŠ›ã§ã‚ã‚‹ã€‚
The prediction layer is put on the last layer of multiple feed-forward networks.
äºˆæ¸¬å±¤ã¯ã€è¤‡æ•°ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€çµ‚å±¤ã«ç½®ã‹ã‚Œã¾ã™ã€‚
We call this version MaskNet as "ParaMaskNet" in the following part of this paper.
æœ¬ç¨¿ã§ã¯ã€ã“ã®MaskNetã‚’ "ParaMaskNet "ã¨å‘¼ã¶ã“ã¨ã«ã™ã‚‹ã€‚

## Prediction Layer ãƒ—ãƒ¬ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒ³ãƒ¬ã‚¤ãƒ¤ãƒ¼

To summarize, we give the overall formulation of our proposed modelâ€™ s output as:
ã¾ã¨ã‚ã‚‹ã¨ã€ææ¡ˆãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã®å…¨ä½“çš„ãªå®šå¼åŒ–ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚‹ï¼š

$$
\tag{17}
$$

where ğ‘¦^ âˆˆ (0, 1) is the predicted value of CTR, ğ›¿ is the sigmoid function, ğ‘› is the size of the last MaskBlockâ€™s output(SerMaskNet) or feed-forward layer(ParaMaskNet), ğ‘¥ğ‘– is the bit value of feedforward layer and ğ‘¤ğ‘– is the learned weight for each bit value.
ã“ã“ã§ã€ğ‘¦âˆˆ (0, 1)ã¯CTRã®äºˆæ¸¬å€¤ã€ğ›¿ã¯ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã€ğ‘›ã¯æœ€å¾Œã®MaskBlockã®å‡ºåŠ›ï¼ˆSerMaskNetï¼‰ã¾ãŸã¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ï¼ˆParaMaskNetï¼‰ã®ã‚µã‚¤ã‚ºã€ğ‘¥ğ‘–ã¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®ãƒ“ãƒƒãƒˆå€¤ã€â†ªL_1D464â†©ğ‘–ã¯å„ãƒ“ãƒƒãƒˆå€¤ã®å­¦ç¿’é‡ã¿ã§ã™ã€‚

For binary classifications, the loss function is the log loss:
äºŒå€¤åˆ†é¡ã®å ´åˆã€æå¤±é–¢æ•°ã¯å¯¾æ•°æå¤±ã¨ãªã‚‹ï¼š

$$
\tag{18}
$$

where ğ‘ is the total number of training instances, ğ‘¦ğ‘– is the ground truth of ğ‘–-th instance and ğ‘¦^ğ‘– is the predicted CTR.
ã“ã“ã§ã€ğ‘ã¯è¨“ç·´ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ç·æ•°ã€áµ†ğ‘–ã¯ğ‘–ç•ªç›®ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ground truthã€ğ‘¦^ğ‘–ã¯äºˆæ¸¬CTRã§ã‚ã‚‹ã€‚
The optimization process is to minimize the following objective function:
æœ€é©åŒ–å‡¦ç†ã¯ã€ä»¥ä¸‹ã®ç›®çš„é–¢æ•°ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ï¼š

$$
\tag{19}
$$

where ğœ† denotes the regularization term and Î˜ denotes the set of parameters, including those in feature embedding matrix, instanceguided mask matrix, feed-forward layer in MaskBlock, and prediction part.
ã“ã“ã§ã€ğœ†ã¯æ­£å‰‡åŒ–é …ã€Î˜ã¯ç‰¹å¾´åŸ‹ã‚è¾¼ã¿è¡Œåˆ—ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰ä»˜ããƒã‚¹ã‚¯è¡Œåˆ—ã€MaskBlockã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã€äºˆæ¸¬éƒ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å«ã‚€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é›†åˆã‚’ç¤ºã™ã€‚

# Experimental Results å®Ÿé¨“çµæœ

In this section, we evaluate the proposed approaches on three realworld datasets and conduct detailed ablation studies to answer the following research questions:
æœ¬ç¯€ã§ã¯ã€ä»¥ä¸‹ã®ç ”ç©¶èª²é¡Œã«ç­”ãˆã‚‹ãŸã‚ã€3ã¤ã®å®Ÿä¸–ç•Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ææ¡ˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è©•ä¾¡ã—ã€è©³ç´°ãªã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¿ãƒ‡ã‚£ã‚’å®Ÿæ–½ã™ã‚‹ï¼š

- RQ1 Does the proposed MaskNet model based on the MaskBlock perform better than existing state-of-the-art deep learning based CTR models? RQ1 MaskBlockã«åŸºã¥ãMaskNetã®ææ¡ˆãƒ¢ãƒ‡ãƒ«ã¯ã€æ—¢å­˜ã®æœ€å…ˆç«¯ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã«åŸºã¥ãCTRãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šæ€§èƒ½ãŒé«˜ã„ã‹ï¼Ÿ

- RQ2 What are the influences of various components in the MaskBlock architecture? Is each component necessary to build an effective ranking system? RQ2 MaskBlockã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ãŠã‘ã‚‹æ§˜ã€…ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å½±éŸ¿åŠ›ã¯ã©ã®ã‚ˆã†ãªã‚‚ã®ã‹ï¼ŸåŠ¹æœçš„ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã«ã€å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯å¿…è¦ãªã®ã‹ï¼Ÿ

- RQ3 How does the hyper-parameter of networks influence the performance of our proposed two MaskNet models? RQ3 ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€æˆ‘ã€…ãŒææ¡ˆã™ã‚‹2ã¤ã®MaskNetãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã«ã©ã®ã‚ˆã†ã«å½±éŸ¿ã™ã‚‹ã‹ï¼Ÿ

- RQ4 Does instance-guided mask highlight the important elements in feature embedding and feed-forward layers according to the input instance? RQ4 ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰å‹ãƒã‚¹ã‚¯ã¯ã€å…¥åŠ›ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«å¿œã˜ã¦ã€ç‰¹å¾´åŸ‹ã‚è¾¼ã¿å±¤ã‚„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®é‡è¦ãªè¦ç´ ã‚’å¼·èª¿ã™ã‚‹ã®ã‹ï¼Ÿ

In the following, we will first describe the experimental settings, followed by answering the above research questions.
ä»¥ä¸‹ã§ã¯ã€ã¾ãšå®Ÿé¨“è¨­å®šã«ã¤ã„ã¦èª¬æ˜ã—ã€ãã®å¾Œã€ä¸Šè¨˜ã®ãƒªã‚µãƒ¼ãƒã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³ã«å›ç­”ã™ã‚‹ã€‚

## Experiment Setup å®Ÿé¨“ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### Datasets. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

The following three data sets are used in our experiments:
å®Ÿé¨“ã§ã¯ã€ä»¥ä¸‹ã®3ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ãŸï¼š

- (1) Criteo1 Dataset: As a very famous public real world display ad dataset with each ad display information and corresponding user click feedback, Criteo data set is widely used in many CTR model evaluation. There are 26 anonymous categorical fields and 13 continuous feature fields in Criteo data set. (1) Criteo1ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼š Criteoãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å„åºƒå‘Šè¡¨ç¤ºæƒ…å ±ã¨ãã‚Œã«å¯¾å¿œã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ãƒªãƒƒã‚¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æŒã¤éå¸¸ã«æœ‰åãªå…¬é–‹å®Ÿä¸–ç•Œãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤åºƒå‘Šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ã€å¤šãã®CTRãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã§åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚ Criteoã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ã€26ã®åŒ¿åã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨13ã®é€£ç¶šç‰¹å¾´ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚Šã¾ã™ã€‚

- (2) Malware2 Dataset: Malware is a dataset from Kaggle competitions published in the Microsoft Malware prediction. The goal of this competition is to predict a Windows machineâ€™s probability of getting infected. The malware prediction task can be formulated as a binary classification problem like a typical CTR estimation task does. (2) Malware2 Datasetï¼š Malwareã¯ã€Microsoftã®Malwareäºˆæ¸¬ã§å…¬é–‹ã•ã‚ŒãŸKaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚ ã“ã®ç«¶æŠ€ã®ç›®çš„ã¯ã€Windowsãƒã‚·ãƒ³ãŒæ„ŸæŸ“ã™ã‚‹ç¢ºç‡ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã§ã™ã€‚ ãƒãƒ«ã‚¦ã‚§ã‚¢äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã¯ã€å…¸å‹çš„ãªCTRæ¨å®šã‚¿ã‚¹ã‚¯ãŒãã†ã§ã‚ã‚‹ã‚ˆã†ã«ã€äºŒå€¤åˆ†é¡å•é¡Œã¨ã—ã¦å®šå¼åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

- (3) Avazu3 Dataset: The Avazu dataset consists of several days of ad click- through data which is ordered chronologically. For each click data, there are 23 fields which indicate elements of a single ad impression. (3) Avazu3ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼š Avazuã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ•°æ—¥åˆ†ã®åºƒå‘Šã‚¯ãƒªãƒƒã‚¯ã‚¹ãƒ«ãƒ¼ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚ç³»åˆ—ã«ä¸¦ã¹ãŸã‚‚ã®ã§ã‚ã‚‹ã€‚ å„ã‚¯ãƒªãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã«ã¯ã€1ã¤ã®åºƒå‘Šã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®è¦ç´ ã‚’ç¤ºã™23ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå­˜åœ¨ã—ã¾ã™ã€‚

We randomly split instances by 8 : 1 : 1 for training , validation and test while Table 1 lists the statistics of the evaluation datasets
è¡¨1ã«è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆå€¤ã‚’ç¤ºã™ã€‚

### Evaluation Metrics. è©•ä¾¡æŒ‡æ¨™

AUC (Area Under ROC) is used in our experiments as the evaluation metric.
å®Ÿé¨“ã§ã¯ã€è©•ä¾¡æŒ‡æ¨™ã¨ã—ã¦AUCï¼ˆArea Under ROCï¼‰ã‚’ç”¨ã„ã¦ã„ã¾ã™ã€‚
AUCâ€™s upper bound is 1 and larger value indicates a better performance.
AUCã®ä¸Šé™ã¯1ã§ã‚ã‚Šã€å€¤ãŒå¤§ãã„ã»ã©æ€§èƒ½ãŒå„ªã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚

RelaImp is also as work [23] does to measure the relative AUC improvements over the corresponding baseline model as another evaluation metric.
RelaImpã¯ã¾ãŸã€åˆ¥ã®è©•ä¾¡æŒ‡æ¨™ã¨ã—ã¦ã€å¯¾å¿œã™ã‚‹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹ç›¸å¯¾çš„ãªAUCã®æ”¹å–„ã‚’æ¸¬å®šã™ã‚‹ä½œæ¥­[23]ã¨åŒæ§˜ã§ã™ã€‚
Since AUC is 0.5 from a random strategy, we can remove the constant part of the AUC score and formalize the RelaImp as:
ãƒ©ãƒ³ãƒ€ãƒ æˆ¦ç•¥ã‹ã‚‰AUCã¯0.5ãªã®ã§ã€AUCã‚¹ã‚³ã‚¢ã®å®šæ•°éƒ¨åˆ†ã‚’å‰Šé™¤ã—ã¦RelaImpã‚’æ¬¡ã®ã‚ˆã†ã«å®šå¼åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼š

$$
\tag{20}
$$

### Models for Comparisons. æ¯”è¼ƒã®ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã€‚

We compare the performance of the following CTR estimation models with our proposed approaches: FM, DNN, DeepFM, Deep&Cross Network(DCN), xDeepFM and AutoInt Model, all of which are discussed in Section 2.
ä»¥ä¸‹ã®CTRæ¨å®šãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ã€æˆ‘ã€…ã®ææ¡ˆã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨æ¯”è¼ƒã™ã‚‹ï¼š FM, DNN, DeepFM, Deep&Cross Network(DCN), xDeepFM, AutoInt Modelã§ã‚ã‚Šã€ã“ã‚Œã‚‰ã¯ã™ã¹ã¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³2ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ã€‚
FM is considered as the base model in evaluation.
FMã¯è©•ä¾¡ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä½ç½®ã¥ã‘ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

### Implementation Details. å®Ÿæ–½å†…å®¹

We implement all the models with Tensorflow in our experiments.
å®Ÿé¨“ã§ã¯ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã‚’Tensorflowã§å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚
For optimization method, we use the Adam with a mini-batch size of 1024 and a learning rate is set to 0.0001.
æœ€é©åŒ–æ‰‹æ³•ã¨ã—ã¦ã¯ã€ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’1024ã€å­¦ç¿’ç‡ã‚’0.0001ã«è¨­å®šã—ãŸAdamã‚’ä½¿ç”¨ã™ã‚‹ã€‚
Focusing on neural networks structures in our paper, we make the dimension of field embedding for all models to be a fixed value of 10.
æœ¬ç¨¿ã§ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹é€ ã«ç€ç›®ã—ã€ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã®æ¬¡å…ƒã‚’10ã¨ã„ã†å›ºå®šå€¤ã«ã—ã¦ã„ã‚‹ã€‚
For models with DNN part, the depth of hidden layers is set to 3, the number of neurons per layer is 400, all activation function is ReLU.
DNNéƒ¨åˆ†ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ã¯ã€éš ã‚Œå±¤ã®æ·±ã•ã‚’3ã€1å±¤ã‚ãŸã‚Šã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ã‚’400ã€ã™ã¹ã¦ã®æ´»æ€§åŒ–é–¢æ•°ã‚’ReLUã¨ã—ãŸã€‚
For default settings in MaskBlock, the reduction ratio of instance-guided mask is set to 2.
MaskBlockã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§ã¯ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰ãƒã‚¹ã‚¯ã®ç¸®å°ç‡ã¯2ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚
We conduct our experiments with 2 Tesla ğ¾40 GPUs.
2å°ã®Tesla áµƒ40 GPUã§å®Ÿé¨“ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚

## Performance Comparison (RQ1) ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æ¯”è¼ƒï¼ˆRQ1ï¼‰

The overall performances of different models on three evaluation datasets are show in the Table 2.
3ã¤ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã‘ã‚‹ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã®ç·åˆçš„ãªæ€§èƒ½ã‚’è¡¨2ã«ç¤ºã—ã¾ã™ã€‚
From the experimental results, we can see that:
å®Ÿé¨“çµæœã‹ã‚‰ã€æ¬¡ã®ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ï¼š

- (1) Both the serial model and parallel model achieve better performance on all three datasets and obtains significant improvements over the state-of-the-art methods. It can boost the accuracy over the baseline FM by 3.12% to 11.40%, baseline DeepFM by 1.55% to 5.23%, as well as xDeepFM baseline by 1.27% to 4.46%. We also conduct a significance test to verify that our proposed models outperforms baselines with the significance level ğ›¼ = 0.01. Though maskNet model lacks similar module such as CIN in xDeepFM to explicitly capture high-order feature interaction, it still achieves better performance because of the existence of MaskBlock. The experiment results imply that MaskBlock indeed enhance DNN Modelâ€™s ability of capturing complex feature interactions through introducing multiplicative operation into DNN models by instance-guided mask on the normalized feature embedding and feed-forward layer. (1)ã‚·ãƒªã‚¢ãƒ«ãƒ¢ãƒ‡ãƒ«ã€ãƒ‘ãƒ©ãƒ¬ãƒ«ãƒ¢ãƒ‡ãƒ«ã¨ã‚‚ã«ã€3ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ˆã‚Šè‰¯ã„æ€§èƒ½ã‚’é”æˆã—ã€æœ€å…ˆç«¯ã®æ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦å¤§ããªæ”¹å–„ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³FMã‚’3.12%ã‹ã‚‰11.40%ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³DeepFMã‚’1.55%ã‹ã‚‰5.23%ã€xDeepFMãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’1.27%ã‹ã‚‰4.46%ã®ç²¾åº¦ã§å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ ã¾ãŸã€æœ‰æ„æ°´æº–ğ›¼= 0.01ã§ã€ææ¡ˆãƒ¢ãƒ‡ãƒ«ãŒãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã«æœ‰æ„æ€§æ¤œå®šã‚’å®Ÿæ–½ã—ãŸã€‚ maskNetãƒ¢ãƒ‡ãƒ«ã«ã¯ã€xDeepFMã®CINã®ã‚ˆã†ãªé«˜æ¬¡ç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’æ˜ç¤ºçš„ã«æ‰ãˆã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ãŒã€ãã‚Œã§ã‚‚MaskBlockã®å­˜åœ¨ã«ã‚ˆã‚Šã€ã‚ˆã‚Šé«˜ã„æ€§èƒ½ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚ å®Ÿé¨“çµæœã¯ã€MaskBlockãŒã€æ­£è¦åŒ–ã•ã‚ŒãŸç‰¹å¾´åŸ‹ã‚è¾¼ã¿å±¤ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰ä»˜ããƒã‚¹ã‚¯ã‚’é©ç”¨ã—ã€DNNãƒ¢ãƒ‡ãƒ«ã«ä¹—ç®—æ¼”ç®—ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€è¤‡é›‘ãªç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’æ‰ãˆã‚‹èƒ½åŠ›ã‚’ç¢ºã‹ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚

- (2) As for the comparison of the serial model and parallel model, the experimental results show comparable performance on three evaluation datasets. It explicitly proves that MaskBlock is an effective basic building unit for composing various high performance ranking systems. (2) ã‚·ãƒªã‚¢ãƒ«ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‘ãƒ©ãƒ¬ãƒ«ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒã«ã¤ã„ã¦ã¯ã€3ã¤ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã„ã¦ã€å®Ÿé¨“çµæœã¯åŒç­‰ã®æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ MaskBlockãŒæ§˜ã€…ãªé«˜æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹æˆã™ã‚‹ãŸã‚ã®æœ‰åŠ¹ãªåŸºæœ¬æ§‹æˆå˜ä½ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ç¤ºçš„ã«è¨¼æ˜ã—ã¦ã„ã¾ã™ã€‚

## Ablation Study of MaskBlock (RQ2) MaskBlockã®ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ (RQ2)

In order to better understand the impact of each component in MaskBlock, we perform ablation experiments over key components of MaskBlock by only removing one of them to observe the performance change, including mask module, layer normalization(LN) and feed-forward network(FFN).
MaskBlockã®å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å½±éŸ¿ã‚’ã‚ˆã‚Šç†è§£ã™ã‚‹ãŸã‚ã«ã€MaskBlockã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã‚ã‚‹ãƒã‚¹ã‚¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ï¼ˆLNï¼‰ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆFFNï¼‰ã®ã†ã¡1ã¤ã ã‘ã‚’å‰Šé™¤ã™ã‚‹ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“ã‚’è¡Œã„ã€æ€§èƒ½å¤‰åŒ–ã‚’è¦³å¯Ÿã—ã¾ã—ãŸã€‚
Table 3 shows the results of our two full version MaskNet models and its variants removing only one component.
è¡¨3ã¯ã€MaskNetã®2ã¤ã®ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã¨ã€1ã¤ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã ã‘ã‚’å–ã‚Šé™¤ã„ãŸãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã®çµæœã§ã™ã€‚

From the results in Table 3, we can see that removing either instance-guided mask or layer normalization will decrease modelâ€™s performance and this implies that both the instance-guided mask and layer normalization are necessary components in MaskBlock for its effectiveness.
è¡¨3ã®çµæœã‹ã‚‰ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰ä»˜ããƒã‚¹ã‚¯ã¨ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã®ã©ã¡ã‚‰ã‹ã‚’å‰Šé™¤ã™ã‚‹ã¨ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰ä»˜ããƒã‚¹ã‚¯ã¨ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã®ä¸¡æ–¹ãŒMaskBlockã®æœ‰åŠ¹æ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã«å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã‚ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã¾ã—ãŸã€‚
As for the feed-forward layer in MaskBlock, its effect on serial model or parallel model shows difference.
MaskBlockã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã«ã¤ã„ã¦ã¯ã€ã‚·ãƒªã‚¢ãƒ«ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‘ãƒ©ãƒ¬ãƒ«ãƒ¢ãƒ‡ãƒ«ã§åŠ¹æœãŒç•°ãªã‚‹ã€‚
The Serial modelâ€™s performance dramatically degrades while it seems do no harm to parallel model if we remove the feed-forward layer in MaskBlock.
MaskBlockã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã‚’å‰Šé™¤ã—ã¦ã‚‚ãƒ‘ãƒ©ãƒ¬ãƒ«ãƒ¢ãƒ‡ãƒ«ã«ã¯å®³ãŒãªã„ã‚ˆã†ã«è¦‹ãˆã‚‹ãŒã€Serialãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯åŠ‡çš„ã«ä½ä¸‹ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
We deem this implies that the feed-forward layer in MaskBlock is important for merging the feature interaction information after instance-guided mask.
ã“ã®ã“ã¨ã¯ã€MaskBlockã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ãŒã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã®å¾Œã«ç‰¹å¾´çš„ãªç›¸äº’ä½œç”¨æƒ…å ±ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹ãŸã‚ã«é‡è¦ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€‚
For parallel model, the multiple feed-forward layers above parallel MaskBlocks have similar function as feed-forward layer in MaskBlock does and this may produce performance difference between two models when we remove this component.
ä¸¦åˆ—ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€ä¸¦åˆ—MaskBlockã®ä¸Šã«ã‚ã‚‹è¤‡æ•°ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã¯ã€MaskBlockã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã¨åŒæ§˜ã®æ©Ÿèƒ½ã‚’æŒã¤ãŸã‚ã€ã“ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å‰Šé™¤ã™ã‚‹ã¨ã€2ã¤ã®ãƒ¢ãƒ‡ãƒ«é–“ã§æ€§èƒ½å·®ãŒç”Ÿã˜ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

## Hyper-Parameter Study(RQ3) ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ç ”ç©¶(RQ3)

In the following part of the paper, we study the impacts of hyperparameters on two MaskNet models, including 1) the number of feature embedding size; 2) the number of MaskBlock; and 3) the reduction ratio in instance-guided mask module.
æœ¬è«–æ–‡ã®ä»¥ä¸‹ã®éƒ¨åˆ†ã§ã¯ã€2ã¤ã®MaskNetãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ã«ã¤ã„ã¦ã€1ï¼‰ç‰¹å¾´åŸ‹ã‚è¾¼ã¿ã‚µã‚¤ã‚ºã®æ•°ã€2ï¼‰MaskBlockã®æ•°ã€3ï¼‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å‰Šæ¸›ç‡ã«ã¤ã„ã¦ç ”ç©¶ã—ã¦ã„ã‚‹ã€‚
The experiments are conducted on Criteo dataset via changing one hyper-parameter while holding the other settings.
Criteoãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã„ã¦ã€1ã¤ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰æ›´ã—ã€ä»–ã®è¨­å®šã‚’ç¶­æŒã—ãŸã¾ã¾å®Ÿé¨“ã‚’è¡Œã£ãŸã€‚
The hyper-parameter experiments show similar trend in other two datasets.
ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®Ÿé¨“ã§ã¯ã€ä»–ã®2ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚‚åŒæ§˜ã®å‚¾å‘ãŒè¦‹ã‚‰ã‚ŒãŸã€‚

### Number of Feature Embedding Size. ç‰¹å¾´é‡åŸ‹ã‚è¾¼ã¿ã‚µã‚¤ã‚ºã®æ•°ã€‚

The results in Table 4 show the impact of the number of feature embedding size on model performance.
è¡¨4ã®çµæœã¯ã€ç‰¹å¾´é‡åŸ‹ã‚è¾¼ã¿ã‚µã‚¤ã‚ºã®æ•°ãŒãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
It can be observed that the performances of both models increase when embedding size increases at the beginning.
ä¸¡ãƒ¢ãƒ‡ãƒ«ã¨ã‚‚ã€åŸ‹ã‚è¾¼ã¿ã‚µã‚¤ã‚ºãŒåˆæœŸã«å¤§ãããªã‚‹ã¨ã€æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã“ã¨ãŒç¢ºèªã§ãã¾ã™ã€‚
However, model performance degrades when the embedding size is set greater than 50 for SerMaskNet model and 30 for ParaMaskNet model.
ã—ã‹ã—ã€åŸ‹ã‚è¾¼ã¿ã‚µã‚¤ã‚ºã‚’SerMaskNetãƒ¢ãƒ‡ãƒ«ã§50ä»¥ä¸Šã€ParaMaskNetãƒ¢ãƒ‡ãƒ«ã§30ä»¥ä¸Šã«è¨­å®šã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹ã€‚
The experimental results tell us the models benefit from larger feature embedding size.
å®Ÿé¨“çµæœã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒã‚ˆã‚Šå¤§ããªç‰¹å¾´é‡åŸ‹ã‚è¾¼ã¿ã‚µã‚¤ã‚ºã®æ©æµã‚’å—ã‘ã‚‹ã“ã¨ã‚’ç‰©èªã£ã¦ã„ã¾ã™ã€‚

### Number of MaskBlock. MaskBlockã®æ•°

For understanding the influence of the number of MaskBlock on modelâ€™s performance, we conduct experiments to stack MaskBlock from 1 to 9 blocks for both MaskNet models.
MaskBlockã®æ•°ãŒãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’ç†è§£ã™ã‚‹ãŸã‚ã€MaskNetã®ä¸¡ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ã€MaskBlockã‚’1ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰9ãƒ–ãƒ­ãƒƒã‚¯ã¾ã§ç©ã¿ä¸Šã’ã‚‹å®Ÿé¨“ã‚’è¡Œã„ã¾ã—ãŸã€‚
The experimental results are listed in the Table 5.
å®Ÿé¨“çµæœã‚’è¡¨5ã«ç¤ºã—ã¾ã™ã€‚
For SerMaskNet model, the performance increases with more blocks at the beginning until the number is set greater than 5.
SerMaskNetãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€åˆæœŸã«ãƒ–ãƒ­ãƒƒã‚¯æ•°ã‚’å¢—ã‚„ã™ã¨ã€5å€‹ä»¥ä¸Šã«ãªã‚‹ã¾ã§æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚
While the performance slowly increases when we continually add more MaskBlock into ParaMaskNet model.
ä¸€æ–¹ã€ParaMaskNetã®ãƒ¢ãƒ‡ãƒ«ã«MaskBlockã‚’è¿½åŠ ã—ç¶šã‘ã‚‹ã¨ã€å¾ã€…ã«æ€§èƒ½ãŒå‘ä¸Šã—ã¾ã™ã€‚
This may indicates that more experts boost the ParaMaskNet modelâ€™s performance though itâ€™s more time consuming.
ã“ã‚Œã¯ã€ã‚ˆã‚Šå¤šãã®å°‚é–€å®¶ãŒParaMaskNetãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŒã€ã‚ˆã‚Šå¤šãã®æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€‚

### Reduction Ratio in Instance-Guided Mask. ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°ãƒã‚¹ã‚¯ã«ãŠã‘ã‚‹å‰Šæ¸›ç‡ã€‚

In order to explore the influence of the reduction ratio in instance-guided mask, We conduct some experiments to adjust the reduction ratio from 1 to 5 by changing the size of aggregation layer.
ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã«ãŠã‘ã‚‹å‰Šæ¸›ç‡ã®å½±éŸ¿ã‚’æ¢ã‚‹ãŸã‚ã€é›†ç´„å±¤ã®å¤§ãã•ã‚’å¤‰ãˆã¦å‰Šæ¸›ç‡ã‚’1ï½5ã¾ã§èª¿æ•´ã™ã‚‹å®Ÿé¨“ã‚’è¡Œã£ãŸã€‚
Experimental results are shown in Table 6 and we can observe that various reduction ratio has little influence on modelâ€™s performance.
å®Ÿé¨“çµæœã‚’è¡¨6ã«ç¤ºã™ãŒã€æ§˜ã€…ãªç¸®å°ç‡ãŒãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã«ã»ã¨ã‚“ã©å½±éŸ¿ã‚’ä¸ãˆãªã„ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
This indicates that we can adopt small reduction ratio in aggregation layer in real life applications for saving the computation resources.
ã“ã®ã“ã¨ã¯ã€å®Ÿéš›ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ãŠã„ã¦ã€è¨ˆç®—è³‡æºã‚’ç¯€ç´„ã™ã‚‹ãŸã‚ã«ã€é›†ç´„å±¤ã®ç¸®å°ç‡ã‚’å°ã•ãã™ã‚‹ã“ã¨ã‚’æ¡ç”¨ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

## Instance-Guided Mask Study(RQ4) ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ç ”ç©¶(RQ4)

As discussed in Section in 3.2, instance-guided mask can be regarded as a special kind of bit-wise attention mechanism to highlight important information based on the current input instance.
3.2ç¯€ã§è¿°ã¹ãŸã‚ˆã†ã«ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã¯ã€ç¾åœ¨ã®å…¥åŠ›ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«åŸºã¥ã„ã¦é‡è¦ãªæƒ…å ±ã‚’å¼·èª¿ã™ã‚‹ç‰¹æ®Šãªç¨®é¡ã®ãƒ“ãƒƒãƒˆå˜ä½ã®æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¨ã—ã¦ã¿ãªã™ã“ã¨ãŒã§ãã‚‹ã€‚
We can utilize instance-guided mask to boost the informative elements and suppress the uninformative elements or even noise in feature embedding and feed-forward layer.
ç‰¹å¾´åŸ‹ã‚è¾¼ã¿ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã§ã€æƒ…å ±é‡ã®å¤šã„è¦ç´ ã‚’é«˜ã‚ã€æƒ…å ±é‡ã®å°‘ãªã„è¦ç´ ã‚„ãƒã‚¤ã‚ºã‚’æŠ‘åˆ¶ã™ã‚‹ãŸã‚ã«ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰ä»˜ããƒã‚¹ã‚¯ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

To verify this, we design the following experiment: After training the SerMaskNet with 3 blocks, we input different instances into the model and observe the outputs of corresponding instance-guided masks.
ã“ã‚Œã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã«ã€æ¬¡ã®ã‚ˆã†ãªå®Ÿé¨“ã‚’è¨ˆç”»ã—ãŸï¼š SerMaskNetã‚’3ãƒ–ãƒ­ãƒƒã‚¯å­¦ç¿’ã•ã›ãŸå¾Œã€ç•°ãªã‚‹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã—ã€å¯¾å¿œã™ã‚‹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã®å‡ºåŠ›ã‚’è¦³å¯Ÿã™ã‚‹ã€‚

Firstly, we randomly sample 100000 different instances from Criteo dataset and observe the distributions of the produced values by instance-guided mask from different blocks.
ã¾ãšã€Criteoãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰100000å€‹ã®ç•°ãªã‚‹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ç•°ãªã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã«ã‚ˆã‚‹ç”Ÿæˆå€¤ã®åˆ†å¸ƒã‚’è¦³å¯Ÿã—ã¾ã™ã€‚
Figure 5 shows the result.
å›³5ã«ãã®çµæœã‚’ç¤ºã—ã¾ã™ã€‚
We can see that the distribution of mask values follow normal distribution.
ãƒã‚¹ã‚¯å€¤ã®åˆ†å¸ƒã¯æ­£è¦åˆ†å¸ƒã«å¾“ã†ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
Over 50% of the mask values are small number near zero and only little fraction of the mask value is a relatively larger number.
ãƒã‚¹ã‚¯å€¤ã®50ï¼…ä»¥ä¸Šã¯ã‚¼ãƒ­ã«è¿‘ã„å°ã•ãªæ•°ã§ã€ç›¸å¯¾çš„ã«å¤§ããªæ•°ã®ãƒã‚¹ã‚¯å€¤ã¯ã”ãã‚ãšã‹ã§ã™ã€‚
This implies that large fraction of signals in feature embedding and feed-forward layer is uninformative or even noise which is suppressed by the small mask values.
ã“ã®ã“ã¨ã¯ã€ç‰¹å¾´åŸ‹ã‚è¾¼ã¿å±¤ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®ä¿¡å·ã®å¤§éƒ¨åˆ†ã¯ã€å°ã•ãªãƒã‚¹ã‚¯å€¤ã«ã‚ˆã£ã¦æŠ‘åˆ¶ã•ã‚ŒãŸæƒ…å ±é‡ã®å°‘ãªã„ã€ã‚ã‚‹ã„ã¯ãƒã‚¤ã‚ºã§ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚
However, there is some informative information boosted by larger mask values through instance-guided mask.
ã—ã‹ã—ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰å‹ãƒã‚¹ã‚¯ã«ã‚ˆã‚Šã€ãƒã‚¹ã‚¯ã®å€¤ã‚’å¤§ããã™ã‚‹ã“ã¨ã§ã€æƒ…å ±é‡ãŒå¢—ãˆã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€‚

Secondly, we randomly sample two instances and compare the difference of the produced values by instance-guided mask.
æ¬¡ã«ã€2ã¤ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èª˜å°å‹ãƒã‚¹ã‚¯ã«ã‚ˆã‚‹ç”Ÿæˆå€¤ã®å·®ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚
The results are shown in Figure 6.
ãã®çµæœã‚’å›³6ã«ç¤ºã—ã¾ã™ã€‚
We can see that: As for the mask values for feature embedding, different input instances lead the mask to pay attention to various areas.
ã¨ã„ã†ã“ã¨ãŒã‚ã‹ã‚‹ï¼š ç‰¹å¾´åŸ‹è¾¼ã®ãŸã‚ã®ãƒã‚¹ã‚¯å€¤ã«ã¤ã„ã¦ã¯ã€å…¥åŠ›ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®é•ã„ã«ã‚ˆã‚Šã€ãƒã‚¹ã‚¯ãŒæ§˜ã€…ãªé ˜åŸŸã«æ³¨ç›®ã™ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
The mask outputs of instance A pay more attention to the first few features and the mask values of instance B focus on some bits of other features.
ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹Aã®ãƒã‚¹ã‚¯å‡ºåŠ›ã¯æœ€åˆã®æ•°å€‹ã®ç‰¹å¾´é‡ã«æ³¨ç›®ã—ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹Bã®ãƒã‚¹ã‚¯å€¤ã¯ä»–ã®ç‰¹å¾´é‡ã®ä¸€éƒ¨ã®ãƒ“ãƒƒãƒˆã«æ³¨ç›®ã™ã‚‹ã€‚
We can observe the similar trend in the mask values in feed-forward layer.
ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã§ã®ãƒã‚¹ã‚¯å€¤ã‚‚åŒæ§˜ã®å‚¾å‘ãŒè¦‹ã‚‰ã‚Œã‚‹ã€‚
This indicates the input instance indeed guide the mask to pay attention to the different part of the feature embedding and feed-forward layer.
ã“ã‚Œã¯ã€å…¥åŠ›ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒã€ç‰¹å¾´åŸ‹ã‚è¾¼ã¿å±¤ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®ç•°ãªã‚‹éƒ¨åˆ†ã«æ³¨æ„ã‚’æ‰•ã†ã‚ˆã†ã€ãƒã‚¹ã‚¯ã‚’ç¢ºã‹ã«èª˜å°ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

# Conclusion çµè«–

In this paper, we introduce multiplicative operation into DNN ranking system by proposing instance-guided mask which performs element-wise product both on the feature embedding and feedforward layers.
æœ¬è«–æ–‡ã§ã¯ã€ç‰¹å¾´åŸ‹ã‚è¾¼ã¿å±¤ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®ä¸¡æ–¹ã§è¦ç´ ã”ã¨ã®ç©ã‚’è¡Œã†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰ãƒã‚¹ã‚¯ã‚’ææ¡ˆã—ã€DNNãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã«ä¹—ç®—æ¼”ç®—ã‚’å°å…¥ã™ã‚‹ã€‚
We also turn the feed-forward layer in DNN model into a mixture of addictive and multiplicative feature interactions by proposing MaskBlock by bombing the layer normalization, instanceguided mask, and feed-forward layer.
ã¾ãŸã€å±¤æ­£è¦åŒ–ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰ä»˜ããƒã‚¹ã‚¯ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã‚’çˆ†æ’ƒã—ã¦MaskBlockã‚’ææ¡ˆã™ã‚‹ã“ã¨ã§ã€DNNãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã‚’åŠ æ³•çš„ãƒ»ä¹—æ³•çš„ãªç‰¹å¾´ç›¸äº’ä½œç”¨ã®æ··åˆã«å¤‰ãˆã¦ã„ã¾ã™ã€‚
MaskBlock is a basic building block to be used to design new ranking model.
MaskBlockã¯ã€æ–°ã—ã„ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’è¨­è¨ˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹åŸºæœ¬çš„ãªæ§‹æˆè¦ç´ ã§ã™ã€‚
We also propose two specific MaskNet models based on the MaskBlock.
ã¾ãŸã€MaskBlockã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸ2ã¤ã®å…·ä½“çš„ãªMaskNetãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚
The experiment results on three real-world datasets demonstrate that our proposed models outperform state-of-the-art models such as DeepFM and xDeepFM significantly.
3ã¤ã®å®Ÿä¸–ç•Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãŸå®Ÿé¨“çµæœã‹ã‚‰ã€ææ¡ˆãƒ¢ãƒ‡ãƒ«ãŒDeepFMã‚„xDeepFMãªã©ã®æœ€å…ˆç«¯ãƒ¢ãƒ‡ãƒ«ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹ã“ã¨ãŒå®Ÿè¨¼ã•ã‚ŒãŸã€‚
