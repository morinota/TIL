## link

- https://dl.acm.org/doi/10.1145/3394486.3403370

## title

SimClusters: Community-Based Representations for Heterogeneous Recommendations at Twitter

## abstract

Personalized recommendation products at Twitter target a multitude of heterogeneous items: Tweets, Events, Topics, Hashtags, and users. Each of these targets varies in their cardinality (which affects the scale of the problem) and their "shelf life'' (which constrains the latency of generating the recommendations). Although Twitter has built a variety of recommendation systems before dating back a decade, solutions to the broader problem were mostly tackled piecemeal. In this paper, we present SimClusters, a general-purpose representation layer based on overlapping communities into which users as well as heterogeneous content can be captured as sparse, interpretable vectors to support a multitude of recommendation tasks. We propose a novel algorithm for community discovery based on Metropolis-Hastings sampling, which is both more accurate and significantly faster than off-the-shelf alternatives. SimClusters scales to networks with billions of users and has been effective across a variety of deployed applications at Twitter.

# Introduction

Personalized recommendations lie at the heart of many different technology-enabled products, and Twitter is no exception. Our highlevel goal is to make content discovery effortless and to free the user from the need for manual curation. On the Twitter platform, a wide variety of content types are displayed in a multitude of contexts, requiring a variety of personalization approaches. For example, recommendations of interesting Tweets are an essential component of not only the Home tab, but also for dissemination via email or push notifications. The â€œWho To Followâ€ module with user follow recommendations is crucial, especially for new users, and was one of the first recommendations products to be launched on Twitter [11]. Trends and Events (previously called Moments) are essential for informing the user about currently ongoing news stories and topics of conversation on the platform, and the Explore tab shows the user a personalized list of these. The recently launched Topics feature1 lets users follow Topics (such as â€œMachine Learningâ€ or â€œK-Popâ€) and see the algorithmically curated best tweets about those Topics in their Home feed.

A summary of the diversity of personalized recommendation problems at Twitter is presented in Table 1. In all cases, we are making recommendations to users, but what weâ€™re recommending can be heterogeneous. For example, we could be suggesting users, Tweets, Events, Topics, or hashtags. There are two main dimensions of interest across the different recommendations problems: the cardinality of the items being recommended and the shelf life of the computed recommendations. The shelf life of our computed recommendations is closely related to the churn we observe in the recommended content. For example, since the follower graph changes relatively slowly, user follow recommendations can remain relevant for weeks (and thus can be computed in batch). At the other end of the spectrum, Tweet recommendations become stale much quicker and must be generated in an online system in close to real time; for this case, batch computations would not yield results fast enough to meet the product requirements. Naturally, in the case of recommendation problems where the item cardinality is large, being able to handle the scale of the problem is important.

Previously, Twitter built systems to tackle each of these different recommendation problems individually with little re-use or commonality. The original example here is the â€œWho To Followâ€ system that launched a decade ago [11] for user recommendations. Subsequently, Gupta et al. [12] described a specialized system to generate Tweet recommendations in real time, insights from which were later deployed in GraphJet [31]. GraphJet ingested the realtime stream of user-Tweet engagements to maintain a user-Tweet bipartite graph from which to generate recommendations, but the system was expensive to extend to new use cases. These aforementioned infrastructures were built mainly to generate candidates which got blended and scored subsequently. Twitter also built custom infrastructure for feature retrieval and scoring of arbitrarily generated candidates - examples include RealGraph [14] and RecService [9]. All of these systems were built with the aim of solving specific sub-problems in the recommendations landscape at Twitter and require separate development and maintenance. The central motivating question of this paper is: can we build a general system that helps us advance the accuracy of all or most of the Twitter products which require personalization and recommendations?

The solution proposed in this paper is built on the insight that we can construct, from the userâ€“user graph, a general-purpose representation based on community structure, where each community is characterized by a set of influencers that many people in that community follow. Each of the different kinds of content (i.e., the targets in Table 1) is represented as a vector in the space of these communities, with the entry corresponding to the ğ‘–-th community for item ğ‘— indicating how interested the ğ‘–-th community is in item ğ‘—. The end result is that we can represent heterogeneous recommendation targets as sparse, interpretable vectors in the same space, which enables solutions for a wide variety of recommendation and personalization tasks (see details in Section 6). There are two notable aspects of our design:

- 1. We avoid conventional matrix factorization methods that typically require solving massive numerical optimization problems, and instead rely on a combination of similarity search and community discovery, both of which are easier to scale. A key algorithmic innovation of our work is a new approach to community discovery â€” called Neighborhood-aware MH â€” which is 10Ã—-100Ã— faster, 3Ã—-4Ã— more accurate than off-theshelf baselines, and scales easily to graphs with âˆ¼109 nodes and âˆ¼1011 edges. It helps us discover âˆ¼105 communities on Twitter that are either organized around a common topic (e.g., â€œK-Popâ€ or â€œMachine Learningâ€) or based on social relationships (e.g., those who work together or went to high-school together). We have open-sourced the implementation of the new algorithm in https://github.com/twitter/sbf.

- 2.  Our overall architecture has a modular and extensible design to enable the use of whichever computing paradigm is most suited to a specific component â€“ batch-distributed, batch-multicore, or streaming-distributed. In particular, the ability to dynamically update representations using streaming-distributed components has proved crucial for accurately modeling Tweets which are Twitterâ€™s most important type of content.

We refer to our overall system as SimClusters (Similarity-based Clusters) and have deployed it in production for more than a year.
SimClusters also has the following features, which correspond to our design requirements:

- 1. Universal representations: SimClusters provides representations for both users and a variety of content in the same space. This removes the need to invest in expensive custom infrastructure for each type of content.
- 2. Computational scale: We are able to apply SimClusters at Twitter scale, with âˆ¼109 users, âˆ¼1011 edges between them, and 108 new Tweets every day with âˆ¼109 user engagements per day.
- 3. Accuracy beyond the head: SimClusters representations are accurate beyond just the most popular content (â€œheadâ€), primarily due to the ability to scale SimClusters to a very large representational space with âˆ¼105 dimensions.
- 4. Item and graph churn: The modular design of SimClusters makes it easy to extend to dynamic items which rapidly rise and diminish in popularity. Many of our important recommendations and engagement prediction problem involve items that churn rapidly â€“ most Tweets, Events, and Trends stay relevant for no more than a day or two, meaning that it is crucial to be able to efficiently learn representations of new items before they lose their relevance.
- 5. Interpretability: SimClusters representations are sparse and each dimension corresponds to a specific community, making them interpretable to a degree that is hard to obtain with alternatives such as matrix factorization or graph embeddings.
- 6. Efficient nearest neighbor search: Identifying nearest neighbors is core to many downstream tasks such as generating recommendations, similar item retrieval, and user targeting. The sparsity of SimClusters representations makes it easy to setup and maintain inverted indices for retrieving nearest neighbors, even for rapidly churning domains (see details in Section 4).

SimClusters has been applied to many recommendations and personalization problems at Twitter â€” even for mature products such as out-of-network Tweet recommendations and Personalized Trends, SimClusters has enabled double digit improvements in the engagement rates of recommendations. It has also accelerated the building of entirely new products, such as Similar Tweets and Topic Tweet recommendations. SimClusters continues to be actively developed internally and applied to new use cases.

# Overview of SimClusters

The SimClusters system (see Figure 1) consists of two stages:

- 1. In the first stage (detailed in Section 3), we discover bipartite communities from our user-user graph at scale, resulting in learning sparse, non-negative representations for our users. At the end of this stage, each user is associated with a list of communities they participate in, along with the scores quantifying the strength of their affiliation to each of those communities. We refer to this output as â€œUser Interest Representationsâ€ and it is made available in both offline data warehouses as well as low-latency online stores, indexed by the user id. This first stage is run in a batch-distributed setting, typically as a series of MapReduce jobs running on Hadoop.
- 2. The second stage (detailed in Section 4) consists of several jobs running in parallel, each of which calculates the representations for a specific recommendation target, using a userâ€“target bipartite graph formed from interaction logs on the platform. Each job in the second stage operates in either a batch-distributed setting or a streaming-distributed setting, depending on the shelf-life of the recommendation target and the churn in the corresponding userâ€“target bipartite graph.

The most important detail about our design is that itâ€™s based on discovering communities from the userâ€“user graph. While the other userâ€“target graphs on Twitter evolve rapidly, the userâ€“user graph is relatively long-term and stable, and the specific communities discovered from the graph often outlive specific edges or nodes in the graph. In addition, the user-user graph usually also has more coverage, in the sense that there are a lot more users who have a minimum number of edges in this graph compared to the other user-item graphs.

The other important aspect of our design is its modularity. The different parts of the pipeline depend on each other only via offline data sets or online key-value stores, meaning that they are robust to delays in the preceding steps. It is also easy to swap out existing implementations with new variants, or run multiple implementations in parallel, as long as the output is in the format expected by the downstream jobs. The system degrades gracefully in the presence of bugs and errors - if any one of the item representation jobs has an issue, the other item representation jobs can still service their applications. Our design also allows for gradually adding more modules to the system without needing to build it all in at the start â€” in fact, the first version of the system only output communities without any item representations, but this by itself had many useful applications.

# Stage 1: Community Discovery

This stage is about discovering communities from the Twitter userâ€“ user graph i.e. the directed graph of Follow relationships between users. Following seminal work in the analysis of directed graphs such as HITS [17] and SALSA [20], we find it convenient to reformulate the directed graph as a bipartite graph. We now frame our task as one of identifying bipartite communities i.e. communities consisting of members from left as well as right partitions, and where the edge density between the left and right member sets is high. The bipartite reformulation lets us more flexibly assign users to communities â€” similar to HITS, we decouple the communities a user is influential in from the communities in which a user is interested.

Problem Definition 1. Given a bipartite userâ€“user graph with left-partition ğ¿ and right-partition ğ‘…, find ğ‘˜ (possibly overlapping) bipartite communities from the graph, and assign each left-node and right-node to the communities with weights to indicate the strength of their memberships.

The other advantage of reformulating the directed graph as a bipartite graph is that we can choose to make ğ‘…, the right set of nodes, different from ğ¿, the left set of nodes â€“ in particular, since the majority of edges in a typical social network is directed towards a minority of users, it makes sense to pick a smaller ğ‘… than ğ¿. In Twitterâ€™s case, we find that weâ€™re able to cover the majority of edges (numbering âˆ¼1011) in the full graph by including the top âˆ¼107 most followed users in ğ‘…, while ğ¿ continues to include all users, which is âˆ¼109 . Our problem definition also asks to assign non-negative scores to both the left and the right members indicating the strength of association to a community. Therefore, we represent the left and right memberships as sparse, non-negative matrices U|ğ¿|Ã—ğ‘˜ and V|ğ‘…|Ã—ğ‘˜ , where ğ‘˜ is the number of communities. Hence, the problem of bipartite community discovery bears close similarities to the problem of sparse non-negative matrix factorization (NMF). The biggest challenge with adapting existing approaches such as NMF and their variants [1, 3, 35] is the inability to scale them to graphs with âˆ¼109 nodes and âˆ¼1011 edges â€“ all of our attempts internally to adapt these approaches (see e.g. [30]) have only worked at smaller scales and have been very difficult to debug and maintain.

With SimClusters, we instead adopt the following 3-step approach, illustrated with a toy example in Figure 2.

- 1. Similarity Graph of Right Nodes: We calculate the â€œright projectionâ€ of the bipartite graph, i.e., we calculate the similarity between the nodes in the right partition of the bipartite graph based on their incoming edges, and we form a weighted, undirected graph consisting only of the nodes in the right partition. More details in Section 3.1.
- 2. Communities of Right Nodes: We discover communities from this similarity graph, using a novel neighborhood-based sampling algorithm that is inspired by the work of [33] but is much more accurate, faster, and scales to graphs with billions of edges. More details in Section 3.2.
- 3. Communities of Left Nodes: We now assign nodes from the left partition to the communities discovered in step 2, and this is described in Section 3.3.

We note that the broad outlines of this approach have been independently discovered and suggested in the past literature (see e.g. [23, 28]), but it has been largely neglected as an option among similar deployments in industry. The primary reason we think this works is that while the original bipartite graph is massive and noisy, the similarity graph of right nodes is much smaller and has clearer community structure. From a scalability point of view, this approach shifts most of the computational burden to the first step of identifying pairs of similar users based on their followers, which is a problem that is largely solved at Twitter (see Section 3.1). From a matrix-factorization point of view, this 3-step approach closely mirrors one way of performing SVD of a matrix A, via the eigen-decomposition of A ğ‘‡ A.

Our 3-step approach also isolates the hard-to-parallelize step of community discovery into step 2, where it operates on a smaller graph that fits into the memory of a single machine, while the other two steps operate on much bigger inputs and out of necessity run in batch-distributed settings such as Hadoop MapReduce. This approach is also modular and allows for swapping out implementations of each of the above steps independent of the others. A possible concern with our 3-step approach is that it may lead to reduced accuracy compared to directly learning the communities on the input bipartite network â€“ we empirically test this in Supplemental Section A.2 and find this not to be an issue.

## Step 1: Similarity Graph of Right Nodes

The goal of this step is to construct a much smaller unipartite, undirected graph ğº over the nodes of the right partition. We define the weight between two users (ğ‘¢, ğ‘£) based on the cosine similarity of their followers on the left side of the bipartite graph. To elaborate, if ğ‘¥Â®ğ‘¢ and ğ‘¥Â®ğ‘£ represent the binary incidence vectors of ğ‘¢â€™s and ğ‘£â€™s followers, their cosine similarity is defined as ğ‘¥Â®ğ‘¢ Â· Â®ğ‘¥ğ‘£/ p âˆ¥ Â®ğ‘¥ğ‘¢ âˆ¥ âˆ¥ Â®ğ‘¥ğ‘£ âˆ¥. With this definition, two users would have non-zero similarity, or an edge in ğº simply by sharing one common neighbor in the bipartite graph. In order to avoid generating an extremely dense similarity graph, we discard the edges with similarity score lower than a certain threshold and additionally keep at most a certain number of neighbors with the largest similarity scores for each user.

The difficulty is that solving the similar users problem is very challenging at Twitter scale. But because this is a problem with important applications â€“ e.g. it is the foundation of applying itembased collaborative filtering for the â€œWho To Followâ€ module [11] â€“ we have invested significant resources to develop a robust solution. Our solution, called WHIMP, uses a combination of wedge sampling and Locality Sensitive Hashing (LSH) to scale to the Twitter graph and lends itself to implementation on Hadoop MapReduce [32]. WHIMP is able to identify similar users for users with either large or small followings, and has been vetted in a variety of ways internally.

Ultimately, this similarity graph step takes as input a directed/bipartite graph with âˆ¼109 nodes and âˆ¼1011 edges and outputs an undirected graph with âˆ¼107 nodes and âˆ¼109 edges. In other words, we go from shared-nothing cluster-computing scale to shared-memory multi-core scale. The transformation wrought by this step is also reminiscent of prior research which suggested that keeping only the most important edges in a graph can benefit community discovery methods [29].

## Step 2: Communities of Right Nodes

In this step, we wish to discover communities of densely connected nodes from the undirected, possibly-weighted similarity graph from the previous step. In order to accurately preserve the structure of the input similarity graph, we have observed that it is important for the communities to have hundreds of nodes, rather than thousands or tens or thousands. This means that we need algorithms that can process input graphs with âˆ¼107 nodes and âˆ¼109 edges to find âˆ¼105 communities. Despite the long history of community discovery algorithms, we were unable to find any existing solution that can satisfy these scale requirements. We next describe the algorithm we developed, called Neighborhood-aware Metropolis Hastings (henceforth Neighborhood-aware MH), to meet our requirements.

Our algorithm extends a Metropolis-Hastings sampling approach presented in [33] for discovering overlapping communities, which we first describe as background. Let Z|ğ‘…|Ã—ğ‘˜ be a sparse binary community assignments matrix and Z(ğ‘¢) denote the set of communities to which the vertex ğ‘¢ has been assigned (in other words, Z(ğ‘¢) gives the non-zero column indices from the ğ‘¢-th row in Z). Equation 1 specifies an objective function over Z.

$$
\tag{1}
$$

1 is the indicator function. F (Z) is the sum of two terms â€“ the first counts how many neighboring pairs of nodes in the graph share at least one community, while the second counts how many nonneighbor pairs of nodes in the graph do not share a community.2 Since most real, large-scale networks are very sparse, it is useful to upweight the contribution of the first term using the parameter ğ›¼ â€“ increasing values of ğ›¼ means that the objective function is better optimized by Z with more non-zeros. Note also that the objective function above is decomposable, in the sense that the overall objective function F (Z) can be expressed as a sum of a function ğ‘“ (ğ‘¢, Z) over individual vertices (below, N (ğ‘¢) denotes the set of neighbors of vertex ğ‘¢).

$$
\tag{2}
$$

Using the above background, we first describe the approach for discovering overlapping communities in a general way in Algorithm 1. After initializing Z, we run at most ğ‘‡ epochs of optimization, where in each epoch we iterate over all the vertices in the graph in a shuffled order. For each vertex ğ‘¢ we sample a new set of community assignments Z â€² (ğ‘¢) using the proposal function, and calculate the difference in objective function between the newly proposed Z â€² (ğ‘¢) and the current set of community assignments Z(ğ‘¢). If Z â€² (ğ‘¢) is better, then it is accepted; if not, it may still be accepted with a certain probability, indicated in line 6 of Algorithm 1. As noted in [33], one reason for preferring a randomized optimization procedure as opposed a deterministic optimization procedure is to avoid getting stuck in local minima.

The specific choices for the â€˜Initializeâ€™ and â€˜Proposalâ€™ functions made in [33] are described in Algorithm 2. Because these functions are implemented using purely random sampling, we refer to this approach as â€˜Random MHâ€™. The main practical drawback of Random MH is that it is extremely slow to obtain a satisfactorily accurate solution for even moderate values of ğ‘˜. This is not surprising considering that in each step, the proposal function generates a completely random community assignments vector and evaluates Algorithm 3: Initialize and Proposal functions for Neighborhood-aware MH 1: Function: Initialize(ğº, ğ‘˜) 2: for ğ‘– â† 1..ğ‘˜ do 3: Set ğ‘– ğ‘¡â„ column of Z as neighbors of a randomly picked node 4: end for 5: return Z 6: 7: Function: Proposal(ğ‘¢,ğº, Z, ğ‘˜,ğ‘™) // ğ‘™ << ğ‘˜ 8: ğ‘† â† columns of Z with â‰¥ 1 non-zero in rows of ğ‘ (ğ‘¢) // enumerateSubsets(ğ‘†,ğ‘™) returns all subsets of ğ‘† of size â‰¤ ğ‘™ 9: for ğ‘  â† enumerateSubsets(ğ‘†,ğ‘™) do 10: fMap(ğ‘ ) â† ğ‘“ (ğ‘¢, ğ‘ ) // Per Eqn 2 11: end for 12: return Sample ğ‘  from ğ‘† according to softmax(fMap) it w.r.t. the current vector; as ğ‘˜ increases, the space of community assignments increases exponentially which makes it very unlikely that the proposal will be able to generate an acceptable transition.

Instead, we propose Neighborhood-aware MH, specified in Algorithm 3. The proposal function in Neighborhood-aware MH is based on two insights or assumptions â€“ the first is that it is extremely unlikely that a node should belong to a community that none of its neighbors currently belongs to; the second is that for most practical applications, it is unnecessary to assign a node to more than a small number of communities. We design a two-step proposal function that works as follows. In the first step, for a given node ğ‘¢ we iterate over all the neighbors of ğ‘¢, look up their community assignments in Z, and identify the set of communities which are represented at least once, call it ğ‘†. In the second step, we iterate over all subsets of size â‰¤ ğ‘™ of ğ‘† from the first step, where ğ‘™ is a user-provided upper bound on how many communities a node can be assigned to. For each subset ğ‘ , we calculate the function ğ‘“ (ğ‘¢, ğ‘ ) from Eqn 2, and finally sample the subset ğ‘  with probability proportional to ğ‘’ ğ‘“ (ğ‘¢,ğ‘ ) i.e. we apply the softmax. The result of the sampling is then either accepted or rejected, as specified in lines 6 and 7 of Algorithm 1. As for initializing Z, we seed each community with the neighborhood for a randomly selected node in the graph.

We discuss a few important implementation details.

- Most of the complexity comes from evaluating the function ğ‘“ (ğ‘¢, ğ‘ ), which requires calculating the intersection between a nodeâ€™s neighbors and the union of the communities in ğ‘ . For many members of ğ‘† (the set computed in line 8 Algorithm 3), we can incrementally compute the summary statistics required for ğ‘“ (ğ‘¢, ğ‘ ) as we go through a nodeâ€™s neighborhood when executing line 8 of Algorithm 3, so that the subsequent inner loop in line 10 can execute much faster. Similarly, the acceptance probability for line 6 of Algorithm 1 can also reuse the ğ‘“ (ğ‘¢, Z) computed during the proposal process.
- Sampling from a softmax distribution can be accomplished efficiently in a single pass using the Gumbel-Max trick.
- In the important special case where we assign each node to at most one community only, each epoch of Neighborhood-aware MH can execute in ğ‘‚(|ğ¸|) time, using both of the above mentioned tricks.
- The algorithm lends itself well to parallelization. Specifically the for loop in line 4 of Algorithm 1 can be distributed among several threads which share access to Z, the rows of which can optionally be synchronized using read-write locks. In practice, we have found that removing synchronization has no effect on the accuracy and gives a slight boost in speed (similar to [24]).

## Step 3: Communities of Left Nodes

The output of the previous step is the matrix V|ğ‘…|Ã—ğ‘˜ in which the ğ‘–- th row specifies the communities to which the right-node ğ‘– has been assigned. The remaining problem that needs to be solved is coming up with the matrix U|ğ¿|Ã—ğ‘˜ such that the ğ‘–-th row specifies the communities to which the left-node ğ‘– has been assigned. A simple way to do this assignment is to assign a left-node to communities by looking at the communities that its neighbors (which will all be right-nodes, and hence already have assignments) have been assigned to. More formally, if A|ğ¿|Ã— |ğ‘…| is the adjacency matrix of the input bipartite graph, then we set U = ğ‘¡ğ‘Ÿğ‘¢ğ‘›ğ‘ğ‘ğ‘¡ğ‘’ (A Â· V), where the ğ‘¡ğ‘Ÿğ‘¢ğ‘›ğ‘ğ‘ğ‘¡ğ‘’ function keeps only up to a certain number of nonzeros per row to save on storage. This equation for calculating U is motivated by the fact that in the special case when V is an orthonormal matrix, i.e. V ğ‘‡ V = ğ¼, then U = A Â· V is the solution to A = U Â· V ğ‘‡ . We have experimented with situations both where V is orthonormal (this can be achieved by assigning each right-node to at most one community) as well as situations where V is not, and have found that in each case the resulting U provides accurate representations for the left-nodes. We refer to U as User Interest Representations, and it forms the main input for subsequent steps. The computation in this step can be scaled to our requirements easily by implementing in a batch-distributed computing paradigm such as Hadoop MapReduce.

# Stage 2: Item Representations

In this section, we describe how to compute representations for different items, such as Tweets, Hashtags, or users - which can be the targets for different recommendation problems. Along with the user interest representations U from Stage 1, this stage also relies on a userâ€“item bipartite graph that is formed from historical or on-going user engagements with those items on the platform.

Our general framework is to compute an itemâ€™s representation by aggregating the representations of all the users who engaged with it, i.e., the representation for item ğ‘— is

$$
\tag{3}
$$

where N (ğ‘—) denotes all the users who engaged with item ğ‘— in the corresponding userâ€“item bipartite graph, and W(ğ‘—) and U(ğ‘¢) are both vectors. The ğ‘ğ‘”ğ‘”ğ‘Ÿğ‘’ğ‘”ğ‘ğ‘¡ğ‘’ function can be chosen based on different applications, and can also be learned from a specific supervised task [13]. In our case, we opt for a relatively simple, interpretable ğ‘ğ‘”ğ‘”ğ‘Ÿğ‘’ğ‘”ğ‘ğ‘¡ğ‘’ function with the goal that W(ğ‘—, ğ‘) can be interpreted as the level of interest an average user of the community ğ‘ currently has in this item ğ‘—. We choose to use â€œexponentially time-decayed averageâ€ as our ğ‘ğ‘”ğ‘”ğ‘Ÿğ‘’ğ‘”ğ‘ğ‘¡ğ‘’ function, which exponentially decays the contribution of a user who interacted with the item based on how long ago that user engaged with the item. The half-life used for the exponential decay is item-dependent â€“ where the shelf-life of those items is longer (such as Topics), we set longer half-lives, while for shorter shelf life items such as Tweets, we set shorter half-lives.

The resulting matrix W is much denser than U and it is not useful to save all its non-zero values at scale. Instead, we maintain two additional views or indexes of W, each of which keeps a top-k view. The first view is R and R (ğ‘—) tracks the top communities for the item ğ‘—. The second view is C and C (ğ‘) tracks the top items for the community ğ‘. In the case of items with a long shelf life, the calculation of W, R, and C is straightforwardly done in a batch setting using e.g. Hadoop MapReduce.

However, handling items with short shelf life is more interesting. In this case, we realize a major advantage of an exponentially time-decayed average (as opposed to e.g. time-windowed average), which is that it lends itself to easy incremental updates for W. Specifically, we just need to keep two summary statistics for each cell in W - the current average itself and the last timestamp when it was updated. As detailed in Algorithm 4 lines 4â€“7, when a new userâ€“item engagement arrives, we are able to update W for the item by calculating a decay factor based on the time elapsed since the last update. In order to exactly track the row-wise and columnwise top-k views on W, it is necessary that we track the entirety of W - if it turns out that W is too big to be tracked in its entirety, then one can use sketches to keep a summary of W at the cost of introducing errors [4, 15], although we have found this unnecessary. Another way of reducing the size of W is to reduce ğ‘˜ i.e. the dimensionality of the representations computed in Stage 1, or by further sparsifying the input user representations U. Calculating streaming item representations in this manner can be implemented using frameworks such as Apache Storm/Heron/Spark/Flink.

The two top-k views R and C are stored in low-latency key-value stores. Using these two indices, it easy to retrieve nearest neighbors for any user or item â€“ we simply look up the top communities that a user or item is active in, and for each of those communities, identify the top users or items. These candidates can then be ranked by fetching their full representations and computing the similarity with the representation of the query object (either user or item). The upshot is that we neither need to brute-force scan through all users/items nor need to build specialized nearest neighbor indices.

# Deployment Details

SimClusters has been deployed in production at Twitter for more than an year so far. All the representations output by the SimClusters system are also keyed by model-version, so that we can operate multiple models in parallel to enable the trying out of new parameters or code changes without affecting existing production. The main model that is currently running in production has âˆ¼105 communities in the representations, discovered from the similarity graph of the top âˆ¼107 users by follower count. The bipartite communities discovered by the model contain nearly 70% of the edges in the input bipartite graph, suggesting that most of the structure of the graph is captured. The right member sets do not vary too much in their sizes, while the left member sets vary drastically, reflecting the variance in the original follower distribution. Within Stage 1, Step 1 (similarity calculation) is the most expensive step, taking about 2 days to run end-to-end on Hadoop MapReduce, but note that this job was in production before SimClusters and therefore is not an additional cost introduced by SimClusters. Step 2 was run from scratch for the very first time when SimClusters launched; subsequently, we update the V matrix to take into account changes to the userâ€“user similarity graph by running an abbreviated version of Neighborhood-aware MH initialized with the current V. Step 3 is also periodically run as batch application on Hadoop MapReduce using the latest version of the userâ€“user graph and the latest V from Step 2. Once we have the output from Step 3 (the U matrix), we do not directly use the V matrix anymore, which is typically too sparse for accurate modeling.

For Stage 2, we currently have four jobs â€“ two batch jobs, one for â€œuser influenceâ€ representations and one for Topic representations; and two streaming jobs, one for Tweet representations and one for Trend representations. The purpose of the user influence representations is to tell us what communities a user is influential in, as opposed to the user interest representations (the output of Stage 1) which tell us what communities a user is interested in, The user influence representations are better than the original original V matrix for this purpose as they cover many more users and are also denser for the original subset of users. Topic representations tell us which communities are the most interested in a Topic, and the input to computing these is both the user interest representations as well as a userâ€“Topic engagement graph. Tweet and Trend representations are computed and updated in a streaming job which takes as input userâ€“Tweet engagements happening in real-time. Both the user interest and user influence representations are protected using authentication to only allow authorized access, and users are provided the chance to opt out of unwanted inferences in their Privacy dashboard.

Note that we store only the non-zeros in all our representations, and in all cases we truncate entries close to zero. The user interest representations cover âˆ¼109 users while the user influence representations cover âˆ¼108 users, with both representations having on average 10âˆ’100 non-zeros. There are fewer recommendable Tweets and Trends at any given point in time (refer Table 1), but their representations are denser, having on average âˆ¼102 non-zeros. Note that for the following four representations - user influence, Topic, Tweet, and Trend - we also maintain the inverted indices, i.e. given a community, what are the top-k users/Topics/Tweets/Trends for that community (denoted by C in Section 4). Having C is essential to retrieving the items whose representation has the largest dot product or cosine similarity with another representation.

# Applications

## Similar Tweets on Tweet Details page

For users who visit a Tweet via an email or a push notification, Twitter shows a module with other recommended Tweets, alongside replies. Prior to SimClusters, this module retrieved Tweets solely based on author similarity i.e. Tweets written by users who share a lot of followers with the author of the main Tweet on the page. We ran an online A/B test where we added similar Tweets from SimClusters i.e. we retrieved Tweets whose SimClusters representation has high cosine similarity with the representation of the main Tweet on the page. We found that the engagement rate on the resulting Tweets was 25% higher.4

Subsequently, we added a second candidate source for this product based on SimClusters â€“ retrieve Tweets whose SimClusters representation have high cosine similarity with the user influence representation of the author of the main Tweet on the page. Adding this source increases the coverage further, while the overall increase in engagement rate is a more modest but still impressive 7%.

## Tweet Recommendations in Home Page

A userâ€™s Home feed on Twitter consists of both Tweets from users being directly followed as well as recommended Tweets from users not being followed (â€œOut of Network Tweetsâ€). Prior to SimClusters, the main algorithm for recommended Tweets was what is called â€œNetwork Activityâ€ - namely, use GraphJet [31] to identify which Tweets are being liked by the viewing userâ€™s followings (i.e. network).

Using SimClusters Tweet representations, we built two candidate sources to supplement Network Activity Tweets. The first candidate source identifies Tweets whose real-time representation has the highest dot-product with the viewing userâ€™s interest representation. The second candidate source is based on item-based collaborative filtering, and uses the same underlying implementation as the â€œSimilar Tweetsâ€ application described in Section 6.1 to identify Tweets similar to those Tweets which have been recently liked by the user. We ran an online A/B test by replacing existing candidates in production (in certain positions on Home) using the candidates from these two new candidate sources. The experiment showed that the engagement rate of the new candidates is 33% higher than that for candidates generated by Network Activity, and shown in similar positions. The two candidate sources together were able to increase total weighted engagements on the platform by close to 1%, which is very large considering the maturity of this product and that recommended Tweets only account for a minority of the viewed content in Home pages.

Apart from new candidates, we also use the user interest and Tweet representations to improve the ranking of candidates coming from all sources. The user and item representations are used to enrich the set of existing user features, item features, as well as user-item interaction features in the input to an engagement prediction model. A/B testing showed that the model trained with these features was able to increase engagement rate of recommended content by 4.7% relatively, which is a significant lift for a mature model.

## Ranking of Personalized Trends

Showing top trending content (e.g., Hashtags, Events, breaking news) is an important way to keep users informed about what is happening locally and globally. The implementation for Trends follows a two-stage process of Trends detection followed by ranking. Prior to SimClusters, the ranking of a Trend primarily depended on its volume and a small number of personalization features. We used Trends SimClusters representations to score Trends for a given user by using the dot-product of the userâ€™s interest representation along with the real-time representation for a Trend. A/B testing revealed that using these scores led to a 8% increase in user engagement with the Trends themselves, as well as a bigger 12% increase in engagement on the landing page subsequent to a click. These improvements are large when compared against other experiments run on this product.

## Topic Tweet Recommendations

Given a Topic in a pre-defined topic taxonomy such as â€œFashionâ€ or â€œMarvel Moviesâ€, how can we identify the best content about it? The original implementation here (before the product was launched publicly) primarily relied on custom text matching rules curated by human experts to identify topical Tweets. Once we realized that this approach surfaced a number of false positives (primarily due to a Tweetâ€™s text incidentally matching the rules for a Topic), we tested a second implementation where we first identify those Tweets whose SimClusters representation has high cosine similarity with the representation of the query Topic, and then apply the textual matching rules. Internal evaluation showed that the second approach returned much better results, therefore we launched this product publicly using this approach. Since launch, this feature has received positive press externally as well as causing higher engagement with Tweets from the broader user base.

## Ranking Who To Follow Recommendations

The candidates for Who To Follow recommendations are ranked using an engagement prediction model, to which we added new features based on the SimClusters representations of the viewing user and the candidate user. In A/B tests, we observed an impressive increase of 7% in the follow rate by using these new features.

## Applications in progress

### Notifications quality filter.

A crucial task on Twitter is to protect users from getting abusive or spammy replies or mentions. We developed new SimClusters representations for users based on the userâ€“user block graph (i.e. when one user blocks another), and used these representations as features to train a model for filtering out abusive and spammy replies. In offline tests, the model showed an impressive 4% lift in PR-AUC5 .

### 6.6.2 Supervised embeddings from feature combinations.

While SimClusters representations mostly capture information from various engagement graphs, we are also experimenting approaches to combine it with other features about users or items (for example, follower counts or geo information). One approach where we are obtaining promising early results is to train a deep neural network on an ancillary prediction task (such as engagement prediction) where the input features are both the user and item SimClusters representations along with previously developed features for the user and item. By choosing the right architecture for this neural net, for example, the two-tower DNN model [36], we are able to learn dense embeddings separately for users and items.

### 6.6.3 Real-time Event notifications.

A major application at Twitter is to notify users who may be interested when a major news event happens. Using the SimClusters representation of an Event (which is in turn derived by aggregating the representations of the human-curated Tweets about it), we can identify the communities of users who will be interested in it, and subsequently target users interested in them. We are currently evaluating such an approach.

# Related Work

Traditionally, approaches to recommender systems are categorized as either neighborhood-based (which do not involve model-fitting), or model-based (which fit a model to the input data).

In our experience of building recommendations at Twitter, we find that neighborhood-based methods are easier to scale, more accurate, more interpretable, and also more flexible in terms of accommodating new users and/or items [9, 11, 12, 31]. Recent research has also found that well-tuned neighborhood-based methods are not easy to beat in terms of accuracy [6]. However, neighborhoodbased approaches do not provide a general solution â€“ we needed to build and maintain separate systems to solve each recommendation sub-problems at Twitter in the past (see Section 1 for more discussion of our past work).

Model-based approaches, such as factorized models [18], graph embedding [10, 26] or VAE [22], fit separate parameters for each user or item. The number of model parameters that need to be learned in order to scale to a billion-user social network can easily approach 1012, necessitating unprecedentedly large systems for solving ML problems at that scale. Hybrid models, such as Factorization Machine [27] and Deep Neural Networks (DNNs) [5] have been introduced to reduce the parameter space by utilizing the side information as prior knowledge for users and items. However, they require either well-defined feature vectors or pre-trained embeddings from auxiliary tasks as the input representation of users and items. Graph Convolutional Networks (GCNs) [16, 37] can enrich pre-existing feature representations of the nodes by propagating the neighborhood information from the graph, without fitting model parameters for each node. GCNs perform well in domains where the items have a good set of pre-existing features, e.g., where the items are images [37]. Such approaches work less well in the absence of useful content features and cannot deal with the short half life of items either. We see SimClusters as an approach to scalably learn user and item representations which can be fed to hybrid models like DNNs [5] or GCNs [37].

Our problem definition bears some similarity to the cross-domain or heterogeneous recommender systems problem [2, 38], where one can use a joint objective function to simultaneously learn the representations of users and items across multiple domains [8, 39]. It is unclear how these methods can support our requirements for scale, handling dynamic items and graphs, and intepretability.

# Conclusion

We proposed a framework called SimClusters based on detecting bipartite communities from the user-user graph and use them as a representation space to solve many personalization and recommendation problems at scale. SimClusters uses a novel algorithm called Neighborhood-aware MH for solving the crucial problem of unipartite community detection with better scalability and accuracy. We also presented several diverse deployed and in-progress applications where we use SimClusters representations to improve relevance at Twitter.

# Supplement: Further Evaluation

The code for Neighborhood-aware MH and an in-memory implementation of Stage 1 are open-sourced in https://github.com/ twitter/sbf.

## Neighborhood-aware MH Empirical evaluation

### A.1.1 Comparison with RandomMH [33].

We conducted a simple empirical evaluation in which we generated synthetic graphs with 100 nodes and varying number of communities, such that the probability of an edge between nodes inside the same community was large and the probability of an edge otherwise was small. The approach from [33] which we label â€˜RandomMHâ€™, as well as our approach (â€˜Neighborhood-Awareâ€™) are implemented in the same code and use the same settings, except that the implementations for the proposal and the initialization functions are different. We compare both the approaches in terms of how many epochs they need to be to run to recover the synthetic communities, as well as the wall clock time (since the runtime for each epoch differs between the two approaches). As can be seen from the results in Table 2, the number of epochs and time required when using RandomMH grows exponentially with increasing ğ‘˜, as expected. Neighborhood-aware MH on the other hand has no such problem with increasing ğ‘˜.

### A.1.2 Comparison on real datasets.

We ran experiments on 8 real datasets (see Table 3) and compared Neighborhood-aware MH to the following algorithms from prior literature: (a) BigClam [34]: BigClam is interesting to compare to since there are many similarities, with the main difference being that itâ€™s optimized using gradient descent rather than randomized combinatorial optimization as in our case. We used the implementation in the SNAP package [21]. (b) Graclus [7]: Graclus optimizes weighted graph cuts without needing to compute eigenvectors, making it much faster than spectral algorithms without losing accuracy. 6 Note that for all 8 of these datasets, the RandomMH algorithm proposed in [33] was not able to make any progress inside the allotted time (6 hours).

We use two kinds of datasets: similarity graphs calculated for a subset of Twitter users in the way described in Section 3.1, as well as the 4 biggest undirected social networks we were able to find externally on the KONECT [19] collection. While our method (Neighborhood-Aware MH) and Graclus both work with weighted graphs, BigClam does not, so we restrict ourselves to unweighted graphs. For Neighborhood-aware MH, we run it with ğ‘™ = 1, i.e. each node gets assigned to at most one community, to keep the comparison with Graclus fair. ğ›¼ which can be used to trade precision with recall7 was set to 10, and ğ‘‡ , number of epochs, was set to 5. All experiments were run on a 16-core machine with 256GB RAM.

We evaluate all methods on Precision and Recall. A method is said to predict the existence of an edge (ğ‘¢, ğ‘£) if ğ‘¢ and ğ‘£ share at least one community per the output of the method. The Precision of a method is the proportion of actually existing predicted edges among all predicted edges for a method. The Recall of a method is the proportion of correctly predicted edges (by that method) among all actually existing edges in the graph. Given Precision and Recall, F1 is their harmonic mean. Note that Precision and Recall are not properties of a community by itself, but rather are properties of the entire output i.e. the (possibly overlapping) set of communities. Note that our evaluation measures do not need any external groundtruth; they simply measure how well the community assignments are able to reconstruct the input graph.

For all of the datasets, we generally tried to set ğ‘˜ â€“ the number of discovered communities â€“ so that the average size of a community is 100, because we see that having larger communities leads to significantly degraded Precision as unrelated pairs of nodes start to share at least one community. In the case of the Orkut and Livejournal datasets however, we used a smaller ğ‘˜ in order to get at least one of our baselines to run successfully.

For BigClam, we found that the default implementation was taking a very long time (more than 100Ã— the time for our method on our smallest dataset), so we made a modification to initialize using a random neighborhood (same as our method) instead of trying to identify the neighborhoods with the best conductance which was proving very expensive. Despite this optimization, BigClam was unable to finish execution within 6 hours for our 3 biggest datasets. For Actors and Petster, we found that BigClam finished execution successfully, but the results were completely unintelligible and seemed to have been affected by an unidentified bug.

As can be seen from the results in Table 3, our method is able to produce significantly more accurate results and is also much faster, typically 10x-100x faster. Neighborhood-aware MH is fast because each epoch requires making a single pass over all the vertices and their adjacency lists and also because the overall approach is easy to parallelize. Our approach is able to run inside 1.5 hours for a graph with 100M nodes and 5B edges (Top100M), while the largest graph either of our baselines is able to run on is at least an order of magnitude smaller.

## Bipartite Communities Empirical evaluation

A possible concern with our approach to discovering bipartite communities is whether breaking the problem up into 3 separate steps can result in a loss of accuracy, as compared to jointly learning the bipartite communities directly. To understand this empirically, we compare against NMF (Non-negative Matrix Factorization) â€“ recall that with both NMF and our approach, the end output is two low-dimensional sparse matrices. Specifically we use Scikit-Learnâ€™s implementation [3, 25] of alternating minimization using a Coordinate Descent solver, and with â€˜nndsvdâ€™ initialization, and with ğ¿1 penalty, where the ğ¿1 coefficient is adjusted to return results of comparable sparsity to our approach. For our approach, we set various parameters as follows: for the similarity graph calculation in step 1, we only include edges with cosine similarity > 0.02; for Neighborhood-aware MH in step 2, we set ğ‘™ = 4, (i.e. each rightnode can be assigned to at most 4 communities), ğ›¼ (see Eqn 1) to 10, and ğ‘‡ (max epochs) to 5; for calculating U in step 3, we assign a left-node to a community if and only if it is connected to at least 2 right-nodes that are assigned to that community. All experiments were run on commodity servers with 8 cores and 24GB RAM. Note that this evaluation is purely to benchmark the accuracy of our approach; in terms of actual applicability, neither NMF nor other variants are practically feasible at our scale.

For evaluation, we use a combination of directed graphs and document-word occurrence graphs, and evaluate on the task of link prediction. We run both the approaches on 90% of the input dataset, and make a test set consisting of 10% of the held-out edges as well as the same number of randomly generated pairs of nodes which serve as negative examples in the test set. E.g., if a graph consists of 100ğ¾ edges, this results in a â€œtraining setâ€ of 90ğ¾ edges and test-set of 20ğ¾ edges (10ğ¾ positives and 10ğ¾ negatives). In order to predict whether an edge (ğ‘–, ğ‘—) exists, we use the cosine similarity of U(ğ‘–) and V(ğ‘—) as the predicted score for the existence of the edge. (For both NMF and our approach, cosine similarity worked marginally better than dot product.) We evaluate the quality of these predicted scores in two ways - the first is we check the Correlation of the true label {0, 1} with the predicted score; and the second is to calculate the AUC (Area Under the ROC Curve).8

Details about the datasets as well as the results are in Table 4. In terms of Correlation, our approach is consistently better across all datasets, while in terms of AUC, both the approaches are comparable. We also include the timing information, where our approach is generally a little faster than NMF. However, note that the primary advantage of our approach is not that itâ€™s faster than NMF, but that itâ€™s more scalable, meaning that it is possible to extend to billionnode graphs and hundreds of thousands of latent dimensions while scaling NMF similarly is prohibitively costly.
