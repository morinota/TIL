# SimClusters: Community-Based Representations for Heterogeneous Recommendations at Twitter

published date: August 2020,
authors: Wondo Rhee, Sung Min Cho, Bongwon Suh
url(paper): https://www.kdd.org/kdd2020/accepted-papers/view/simclusters-community-based-representations-for-heterogeneous-recommendatio

(勉強会発表者: morinota)

---

## どんなもの?

table 1は、Twitterにおける各パーソナライズド・レコメンデーション問題(サービス)の多様性をまとめたもの.
それぞれ、推薦されるアイテムのカーディナリティ(=unique値の数だっけ?)と計算された推薦のshelf life(賞味期限)が異なっている.

![](https://camo.qiitausercontent.com/b7c906436ebb895d6e39309eea2985113a40be85/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f62613035656666382d653438612d376130352d353837332d6462306262383939396564622e706e67)

本論文の中心的な動機付けとなる問いは、**パーソナライズやレコメンデーションが必要なTwitter製品のすべて、あるいはほとんどの精度を高めるのに役立つ一般的なシステムを構築できるか**という事.

## 先行研究と比べて何がすごい？

本論文で提案するソリューションは、**ユーザ \* ユーザグラフ**から、コミュニティ構造に基づく汎用的な表現を構築できるという洞察に基づいており、各コミュニティは、そのコミュニティ内の多くの人々がフォローしているインフルエンサーのセットによって特徴づけられる.

**異なる種類のコンテンツ(すなわち表1のターゲット)のそれぞれは、これらのコミュニティの空間におけるベクトルとして表され**、アイテム $j$ に対する $i$ 番目のコミュニティに対応するエントリは、$i$番目のコミュニティがアイテム $j$ にどれだけ興味を持っているかを示している.

私たちのデザインには、2つの特筆すべき点がある：

- 大規模な数値最適化問題を解く必要がある従来の行列分解法を避け、代わりに、スケールアップが容易な類似性検索とコミュニティ発見の組み合わせに頼っている.

## 技術や手法の肝は？

SimClustersシステム(図1参照)は、2つのステージで構成されている：

![](https://camo.qiitausercontent.com/91062517c63bafedf6953cf10b1e3a74c62de8de/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f32623466333165372d653361632d306639642d656431392d6361316535303763623534332e706e67)

### 段階1: Communityの発見 (User Interest Representationsの取得)

この段階は、Twitterのユーザとユーザのグラフからコミュニティを発見することである. ここで言う"グラフ"とは、ユーザ間のフォロー関係を表す有向グラフ(directed graph)の事.
代表的な既存研究に習って、有向グラフを二部グラフ(bipartite graph)として再定義することが便利であることがわかっている.

#### メモ: bipartite graph(二部グラフ)ってなんだっけ?

> 数学、とくにグラフ理論における2部グラフ(にぶグラフ、英: bipartite graph)とは、**頂点集合を2つに分割**して、各部分の頂点が互いに隣接しないようにできるグラフのこと.
> 一般に**互いに隣接しない頂点からなる集合を独立集合**といい、頂点集合を n 個の独立集合に分割可能なグラフのことを n 部グラフ (n-partite graph) という.
> 完全二部グラフ: 頂点集合を2つの独立集合 V1, V2 に分割したとき、V1 と V2 の任意の頂点が隣接するグラフ.

(i.e. 独立集合V_1のある一つの頂点は、もう一つの独立集合V_2内の全ての頂点と隣接している=繋がっている状況...!)

#### 段階1における問題設定を定義:

左パーティション $L$(左の独立集合?) と右パーティション $R$ (=右の独立集合?)を持つbipartite user–user graph(=各頂点がユーザを意味するbi-partite graph)が与えられるとする.
そのグラフから $k$個の (おそらく重複する=独立な集合ではないって事?) bi-partite コミュニティを見つけ、各左ノード(=左の独立集合の各頂点)と右ノード(=右の独立集合の各頂点)にそのメンバーシップの強さ(=k個の各コミュニティに所属する度合いの強さ)を示す重みをつけてコミュニティに割り当てる.

#### 問題設定と sparse non-negative matrix factorizationの関連.

ユーザ->ユーザのフォロー関係を表す有向グラフを、二部グラフとして再定義するもう一つの利点は、ノードの右集合である $R$ をノードの左集合である $L$ と異なるように選択できること.
特に、**典型的なソーシャルネットワークの大部分の辺は少数のユーザに向けられている(=多くの一般ユーザがインフルエンサー的なユーザをフォローしている状況??)ので**、$L$ よりも小さな $R$ を選ぶことは理にかなっていると言える.
Twitterの場合、最もフォローされている上位107人のユーザーを $R$ に含め、残りの全てのユーザを $L$ に含める(約 $10^9$ 個の頂点を左の独立集合に).

また、問題定義では、コミュニティとの関連性の強さを示す非負のスコアを左右(=2つの独立集合R と L?)の各メンバー(=ノード=ユーザ)に割り当てることを求めている.
そこで、左右のメンバーシップ(=2つの独立集合R と L の各頂点の、各コミュニティとの関連度合い)を疎な非負行列 $U_{|L|\times k}$ と $V_{|R|\times k}$ で表現する. ここで、$k$ はコミュニティ数である.
したがって、**bipartite community discoveryの問題(=上述した問題設定=bi-partite user-user graphから、各ユーザとk個の各コミュニティとの関連度合いを定量化する問題)は、sparse non-negative matrix factorization (NMF)の問題と密接な類似性を持っている**のである.

#### SimClustersは問題をどんなアプローチで解く?

SimClustersでは、図2におもちゃの例で示すように、次の**3ステップのアプローチ**を採用している.

![](https://camo.qiitausercontent.com/194f3a26deb259e329da2e62437508f49aad755b/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f65613534653939372d656632322d366463372d626538302d6239333261626561376433632e706e67)

- Step 1. Similarity Graph of Right Nodes: 右の独立集合$R$の類似度グラフを取得する.
- Step 2. Communities of Right Nodes: 右の独立集合$R$の類似度グラフから、k個のコミュニティを計算する(=$R$の各ユーザと各コミュニティとの関連度合いを定量化する).
- Step 3. Communities of Left Nodes: 左の独立集合$L$の各ノード(=頂点=ユーザ)を、Step 2で発見した各コミュニティに割り当てる(=各コミュニティとの関連度合いを定量化する).

行列因数分解の観点から、この3ステップのアプローチは、$A^{T}A$の固有値分解を介した行列$A$のSVDを実行する1つの方法とよく似ている.

#### Step 1: 独立集合$R$のSimilarityグラフを作る

#### Step 2: 独立集合$R$のSimilarityグラフから、k個のコミュニティを発見する

前のステップで得られた無向の(おそらくは重み付けされた)Similarityグラフを用いて、このステップでは、密に接続されたノード集合(=右のユーザ集合)のコミュニティを発見することを目的としている.
入力されたSimilarityグラフの構造を正確に保持するためには、**ある1コミュニティ当たりのノード数($k$)が数千、数万ではなく、数百であることが重要**であることが確認されている.
つまり、ノード$∼10^7$個、エッジ$∼10^9$個の入力グラフを処理して、$∼10^5$ 個(=$∼10^7$個で$10^2$で割っている)のコミュニティを見つけることができるアルゴリズムが必要.
しかし、コミュニティ発見アルゴリズムの長い歴史にもかかわらず、これらの規模要件を満たすことができる既存のソリューションを見つけることができなかった.
以下で、今回の要求を満たすために開発した **Neighborhood-aware Metropolis Hastings(以下、Neighborhood-aware MH)**と呼ばれるアルゴリズムについて説明する.

##### Metropolis Hastings sampling approach

本アルゴリズムは、**重複するコミュニティを発見するために[33]で発表されたMetropolis Hastings sampling approach**を拡張したものであり、まず背景として説明する.

$Z_{|R|\times k}$ を疎なbinaryのコミュニティ割り当て行列(**community assignments matrix**)とし、 $Z(u)$ は頂点$u$が割り当てられたコミュニティの集合を表す(言い換えれば、$Z(u)$ は、行列 $Z$ の $u$ 行目から非ゼロの列indicesを出力するfunctionみたいな感じ?)、と仮定する.
式1は、Zに対する目的関数を指定している.(Zが推定すべきパラメータなのか...!)

$$
F(Z) = \alpha \sum_{u, v \in E}  1(|Z(u) \cap Z(v)| > 0)
+ \sum_{u, v \notin E}  1(|Z(u) \cap Z(v)| = 0)
\tag{1}
$$

ここで、

- 1 はindicator function(=特定の条件を満たす場合だけ1を返し、それ以外は0を返すfunction).
- $F(Z)$は2つの項の合計.(それぞれ 割り当てbinary行列$Z$を入力として計算できる.)
- 第一項は、グラフ内のノードの隣接するペアが少なくとも1つのコミュニティを共有する数をカウントする.
- 第二項は、グラフ内のノードの非隣接ペアがコミュニティを共有しない数をカウントする.
- **実際の大規模ネットワークのほとんどは非常にスパースなので、パラメータ$\alpha$ を用いて最初の項の寄与を重み付けすることが有用**. $\alpha$ の値が増加すると、目的関数はよりZの非ゼロ要素に基づいて最適化することになる.

また、上記の目的関数は、**全体の目的関数 $F(Z)$ が個々の頂点に対する関数 $f(u, Z)$ の和として表現できる**意味で、分解可能である. (ここで、$N(u)$ はある頂点 $u$ のneighbors集合を表す.)

$$
f(u, Z) = \alpha \sum_{v \in N(u)} 1(|Z(u) \cap Z(v)| > 0)
+ \sum_{v \notin N(u)} 1(|Z(u) \cap Z(v)| = 0)
\tag{2}
$$

以上の背景を踏まえ、まずアルゴリズム1において、重複するコミュニティを一般的な方法で発見するアプローチを説明する.

![](https://camo.qiitausercontent.com/636b650123d6050e0ad7dd598d84f8e6dab0ff32/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f33613335616133642d316333332d346239652d303463392d3531303532653430303734332e706e67)

- binary割り当て行列 $Z$ を初期化した後、最大で $T$ epoch 回 最適化を実行し、各epoch ではグラフ内のすべての頂点をシャッフルした順序(?)で反復する.
- 各頂点 $u$ について、proposal function(=コードの関数)を用いてコミュニティ割り当ての新しい集合 $Z'(u)$ をサンプリングし、新しく提案された$Z'(u)$ と現在のコミュニティ割り当てのセット $Z(u)$ の間の目的関数の違いを計算する.
- もし $Z'(u)$ の方が良ければ、$Z$を置き換える. もしそうでなくても、アルゴリズム1の6行目で示されるように、ある確率で置き換える.
  - [33]で述べられているように，決定論的な最適化手順ではなく，ランダムな最適化手順を好む理由の1つは，局所最小値にはまるのを避けることが目的.

##### 既存研究[33]の手法: ランダムMH(Metropolis-Hastings)

[33]で行われた`Initialize` function と`Proposal` function の具体的な選択方法は、アルゴリズム2に記載されている.

![](https://camo.qiitausercontent.com/d263bcb74e394587b46a57d75b88a06105c302ec/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f31656530393931352d656334662d346233332d353139652d3733353465613566343034342e706e67)

これらの機能は、純粋にランダムなサンプリングで実装されているため、この手法を"**ランダムMH(Metropolis-Hastings)**"と呼んでいる.
ランダムMHの主な実用上の欠点は、$k$ の値が適度であっても、満足のいく精度の解を得るのに非常に時間がかかるということである.(最適化戦略無しに、ランダムに、より良いパラメータを探索しているから...!)
これは、`Proposal` function が各ステップで完全にランダムなコミュニティ割り当てベクトルを生成し、現在の割り当てベクトルと照らし合わせて評価することを考えれば、驚くべきことではない. $k$ が増加すると、コミュニティ割り当ての空間(=パラメータの選択肢のイメージ)は指数関数的に増加し、`Proposal` function が許容できるtransition (パラメータ更新)を生成できる可能性は非常に低くなる.

##### 提案手法: Neighborhood-aware Metropolis Hastings

既存手法の代わりに、アルゴリズム3で規定される **Neighborhood-aware MH(Metropolis-Hastings)** を提案する.

![](https://camo.qiitausercontent.com/79362c9271640af7550a9c152c52728a49cc6559/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f37313061323631662d363433382d663137382d343864662d3462396362353639373839642e706e67)

Neighborhood-aware MH(Metropolis-Hastings) の proposal functionは、以下の2つの洞察 & 仮定に基づいている:

- 1つ目は、あるノード(=頂点=ユーザ)が、その隣接するノードが現在所属していないコミュニティに所属する可能性は極めて低いということ.
- 2つ目は、**ほとんどの実用的なアプリケーションでは、ノードを少数のコミュニティ以上に所属させる必要はない(?)**ということである.

次のような2段階のproposal function を設計している.

- 最初のステップでは、与えられたノード $u$ について、$u$ のすべてのneigborsを繰り返し、$Z$で彼らのコミュニティ割り当てを検索し、**少なくとも一度は表されるコミュニティ集合**(=uと1件以上、同じコミュニティに所属してるneigbors集合?)を識別し、それを$S$と呼ぶ.(まだ良く理解できてない.)
- 第2ステップでは、第1ステップで得られた $S$ のサイズ $\leq l$ のすべての部分集合について反復処理を行う.
  - ここで、$l$ はノードが割り当てられるコミュニティの数に対するuser-provided(=要するに開発者が指定するハイパーパラメータの意味!)の上限値である.
  - 各サブセット $s$ について、式2より関数 $f(u, s)$を計算し、最後に $e^{f(u,s)}$ に比例する確率でサブセット $s$ をサンプルする. (i.e. ソフトマックスを適用する = カテゴリカル分布に従う乱数を生成してZ'を生成してる..?)

そして、アルゴリズム1の6行目と7行目に規定されているように、サンプリングの結果が受け入れられるか、拒否されるかのどちらかになる.
$Z$の初期化については、各コミュニティにグラフ内のランダムに選ばれたノードのneigbborhoodをシードする.(?)

#### Step 3: 独立集合$L$と、k個の各コミュニティとの関連度を定量化する

前のステップの出力は、左集合に対するコミュニティ割り当て行列 $V_{|R| \times k}$ である. (i.e. $i$ 番目の行が右ノード$i$が割り当てられたコミュニティを指定する行列)
問題設定における残りの問題は、右集合に対するコミュニティ割り当て行列 $U_{|L| \times k}$ を取得する事. (i.e. i番目の行が、左ノードiが割り当てられたコミュニティを指定するような行列)

この割り当てを行う簡単な方法は、左集合の各ノードのneighbors(=全て右集合のノードであるはず!)が割り当てられているコミュニティを見ることによって、左ノードをコミュニティに割り当てること.
より正式には、$A_{|L| \times |R|}$ を**入力二部グラフの隣接行列(adjacency matrix)**(なるほど、二部グラフの入力も行列で表せば良いのか...!)とすると、$U = truncate(A\cdot V)$ とする事である.
ここで、$truncate()$ 関数は、ストレージを節約するために、各行(=左の独立集合の各頂点=各ユーザ)に対してある一定の数までのnon-zero要素だけを保持する.(=任意の1ユーザが所属するコミュニティの数を制限するfunction...!)

ちなみに、この$U$の計算式は、「$V$が正方直交行列(orthonormal matrix)であるケース(i.e. $V^T V = I$であるケース)に、$U = A \cdot V$ は $A = U \cdot V^T$ の解になる.」という事実に動機づけられている.
$V$がorthonormal matrixの場合(これは、**各右ノードを最大1つのコミュニティに割り当てることで実現できる**)(=あ、じゃあ運用する上では基本的に、$V$ は正方直交行列になるのか...!)と、そうでない場合の両方で実験を行い、いずれの場合も、結果として得られる $U$ が左nodesを正確に表現することを発見した.
(あ、じゃあ結局$V$がorthonormal matrixではなくても問題なさそうなのか...!)

上の計算式で得られた $U$を**User Interest Representations** と呼び、以降のステップの主要な入力情報を形成する.(=UがSimClustersの段階2の入力情報になる...? $V$はならない?)
このステップの計算は、Hadoop MapReduce のようなバッチ分散コンピューティングパラダイムで実装することで、我々の要求に合わせて**簡単にスケーリングすることができる**.(処理の並列化に関しては、正直あまりイメージがついていない.)

### 段階2: Item Representationsの取得

## どうやって有効だと検証した?

## 議論はある？

## 次に読むべき論文は？

## お気持ち実装
