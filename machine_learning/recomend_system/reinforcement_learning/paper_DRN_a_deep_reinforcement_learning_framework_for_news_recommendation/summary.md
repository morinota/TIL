# DRN: A Deep Reinforcement Learning Framework for News Recommendation

published date: April 2018,
authors: Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, Zhenhui Li
url(paper): https://dl.acm.org/doi/fullHtml/10.1145/3178876.3185994
(勉強会発表者: morinota)

---

## どんなもの?

## 先行研究と比べて何がすごい？

オンラインパーソナライズドニュースの推薦問題を解決するために、コンテンツベースの方法[19, 22, 33]、協調フィルタリングベースの方法[11, 28, 34]、ハイブリッド方法[12, 24, 25]など、いくつかのグループの方法が提案されている.
近年、これまでの手法の拡張・統合として、ディープラーニングモデル[8, 45, 52]が、複雑な ユーザアイテム (ニュースなど)のやり取りをモデル化できることから、新たな最先端手法となっている.

しかし、これらの手法は、**ニュース推薦における以下の3つの課題**を効果的に解決することはできない.

### 課題1 ニュース推薦がダイナミックに変化することが扱いにくい.

- ニュース推薦のダイナミックな変化は、以下の2つの側面から示すことができる.
- 第一に、**ニュースはすぐに古くなる**.
  - 我々のデータセットでは、**1つのニュースが公開されてから最後にクリックされるまでの平均時間は4.1時間である**.
- 第二に、**ユーザの様々なニュースに対する興味は、時間の経過とともに変化する**可能性がある.
  - ex. 図1は、あるユーザが10週間に読んだニュースのカテゴリを表示したもの.
  - このユーザは、最初の数週間は"政治"（図1の緑のバー）を好んで読むが、時間の経過とともに"娯楽"（図1の紫のバー）や"技術"（図1の灰色のバー）に徐々に関心が移っていく.
  - そのため、定期的にモデルを更新する必要がある.
  - **オンラインモデル更新により、ニュースの特徴やユーザの嗜好の動的変化を捉えることができるオンライン推薦手法** [11, 24] もあるが、それらは現在の報酬（例えば、クリック率）を最適化しようとするだけであり、**現在の推薦が将来にもたらすであろう影響を無視している**.

将来を考える必要性を示す例として、例1.1.を挙げる.

- 例1.1:
  - ユーザMikeがニュースを要求したとき、推薦エージェントは、雷雨警報に関するニュースと、バスケットボール選手Kobe Bryantに関するニュースの**2つをクリックする確率がほぼ同じであることを予測している**とする.
  - しかし、マイクの読書嗜好、ニュースの特徴、他のユーザーの読書パターンによると、雷雨のニュースを読んだ後、マイクはこの警報に関するニュースを読む必要はなくなったが、**コービーに関するニュースを読んだ後は、バスケットボールに関するニュースをもっと読むだろうと、我々のエージェントは推測する**のである.

この例は、**後者のニュースを推薦することで、より大きな報酬が得られることを示唆している**.
そのため、将来の報酬を考慮することで、長期的にレコメンデーションパフォーマンスを向上させることができる.

### 課題2: 現在の推薦手法[23, 35, 36, 43]は、通常、クリックを考慮するのみ

次に、現在の推薦手法[23, 35, 36, 43]は、通常、クリックを考慮するのみである.
しかし、**あるユーザがどれだけ早くこのサービスに戻ってくるか**[48]は、このユーザが推薦にどれだけ満足しているかを示すことにもなる.
とはいえ、**ユーザの帰省パターン**を取り入れて、レコメンデーションの改善に役立てようという試みは、これまでほとんど行われてこなかった.

### 課題3:

現在の推薦手法の3つ目の大きな問題は、**ユーザに似たようなものを推薦し続ける傾向があり、ユーザの類似した話題への興味を低下させる可能性があること**.
文献では、**すでにいくつかの強化学習法が、新しいアイテムを見つける判断にランダム性（＝ exploration ）を加えることを提案している**.
最新の強化学習法では、通常、単純な?グリーディ戦略 [31] またはUCB (Upper Confidence Bound) [23, 43] (主に多腕バンディット法) を適用する.
しかし、どちらの戦略も、**短期間では、推薦者のパフォーマンスをある程度損なう可能性がある**.
ϵグリーディ戦略では、全く関係のないアイテムを顧客に勧めることがあるが、UCBでは、そのアイテムが何度か試されるまでは、比較的正確な報酬推定を得ることができない.
そのため、**より効果的な探索を行うことが必要**.

### 課題1 ~ 3を踏まえて...

本論文では、オンラインパーソナライズドニュース推薦におけるこれら**3つの課題を解決するのに役立つDeep Reinforcement Learningフレームワーク**を提案する.

まず、ニュースの特性やユーザーの嗜好の動的な性質をよりよくモデル化するために、**Deep Q-Learning (DQN) [31] のフレームワークを使用することを提案**する.
このフレームワークは、**現在の報酬と将来の報酬を同時に考慮することができる**.
強化学習を推薦に用いる最近の試みは、将来の報酬を明示的にモデル化していない（MABベースの作品[23, 43]）、あるいは、状態を表すために離散的なユーザログを用いるため、大規模システムに拡張することができない（MDPベースの作品 [35, 36]）というものである.
これに対し、我々のフレームワークは**DQN構造**(??)を採用しており、容易にスケールアップすることが可能.
**第二に、各ユーザのアクティブネススコアを保持することで、ユーザリターンをユーザフィードバック情報のもう一つの形として考えている.**
第三に、現在のレコメンダーの近傍にあるアイテム候補をランダムに選んで探索する **Dueling Bandit Gradient Descent (DBGD) 法** [16, 17, 49] を適用することを提案する.

## 技術や手法の肝は？

### Deep Q-Learning (DQN)による推薦システムの概要:

私たちの深層強化レコメンダーシステムは、図2のように示すことができる.

システムを説明するために、強化学習[37]の一般的な用語に従う.

- environment: ユーザプールとニュースプールが環境を構成.
- agent: 推薦アルゴリズムがエージェントの役割を果たす.
- state: ユーザーの特徴表現.
- action: ニュースの特徴表現.

ユーザがニュースを要求するたびに、state表現(i.e.ユーザの特徴)とaction表現(i.e. すなわちニュース候補の特徴)のセットが、エージェントに渡される.
エージェントは、最適なアクション(i.e. ユーザに推薦するニュースのリスト)を選択し、報酬としてユーザのフィードバックを取得する.
具体的には、クリックラベルとユーザの activeness を推定することで報酬を構成している.
これらのレコメンデーションとフィードバックログは、すべてエージェントのメモリに保存される.
1時間ごとに、エージェントはメモリ内のログを利用して、推薦アルゴリズムを更新する.

#### Model Framework

#### 特徴量:

- news features:
- user features:
- user news features:
- context features:

強化学習推薦フレームワークの分析に集中するため、テキスト特徴[45]など、より多くの特徴量を追加することは試みなかったが、簡単に適用可能.

### 工夫1:Deep Q-Learning (DQN) [31] のフレームワークを使用:

前述したニュース推薦の動的な特徴と将来の報酬を推定する必要性を考慮し、あるユーザがある特定のニュースをクリックする確率をモデル化するために、**Deep Q-Network (DQN)** [31] を適用する.
強化学習の設定では、**ユーザがニュース(および将来の推薦ニュース)をクリックする確率**が、基本的に我々のエージェントが得ることのできる**報酬**となる.
従って、総報酬を式1のようにモデル化できる:

$$
y_{s, a} = Q(s, a) = r_{immediate} + \gamma r_{future}
\tag{1}
$$

ここで,

- $s$:
- $a$:
- $r_{immediate}$:
- $r_{future}$:
- $\gamma$: 割引係数(discount factor).

具体的には、sを現在の状態として、DDQN[41]ターゲットを用いて、式2のようにタイムスタンプtで行動aをとることによる報酬の合計を予測する.

$$
y_{s,a,t} = r_{a, t+1}  + \gamma Q(s_{a, t+1}, \argmax_{a'} Q(s_{a,t+1}, a':W_t);W_t')
\tag{2}
$$

ここで、

- $r_{a, t+1}$: 時刻tにて行動aをとることによる即時報酬.(添字t+1は、報酬が常に行動より1タイムスロット遅れるから!)(状況 sには依存しないの??)
- $W_t$ と $W_t'$: DQNの**2種類**(=切り替えて使うパラメータがある???)のパラメータセット.

この定式化では、エージェントGは、行動aが選択された場合に、次の状態$s_{a,t+1}$を推測することになる.(=つまり状態遷移関数??)
これに基づき、アクションの候補セット ${a'}$ が与えられると、パラメータ $W_t$ に従って、**将来報酬が最大となるアクション$a′$を選択する**.
この後、状態$s_{a,t+1}$が与えられた将来の推定報酬が$W'_t$に基づいて計算される.

数回繰り返すごとに、WtとW′tが入れ替わる.(??)この戦略により、Qの過大な値推定を排除できることが証明されている[41].

図4に示すように、4つのカテゴリーの特徴をネットワークに投入する.
ユーザ特徴量とコンテキスト特徴量を状態特徴量として、ユーザ\*ニュース特徴量とニュース特徴量を行動特徴量として使用する.
ある状態sで行動aをとったときの報酬(=即時報酬??)は、すべての特徴と密接に関係している.
一方、ユーザー自身の特性(例えば、このユーザーはアクティブか、このユーザーは今日十分なニュースを読んだか)によって決まる報酬(=将来報酬??)は、ユーザーのステータスと文脈のみに影響されやすいと言える.
この観察に基づき、[47]と同様に、Q関数を価値関数(value function) $V(s)$ と優位関数(advantage function) $A(s, a)$ に分け、$V(s)$は状態特徴のみ、$A(s, a)$は状態特徴と行動特徴の両方によって決定される.(つまりvalue functionは将来報酬, advantage functionは即時報酬を出力する関数??)

### 工夫2: 各ユーザのuser returnを feedbackの追加要素として考慮:

### 工夫3: Dueling Bandit Gradient Descent (DBGD) 法による探索戦略

## どうやって有効だと検証した?

本システムは、商用ニュース推薦アプリケーションでオンライン展開された. オフラインとオンラインの広範な実験により、本手法の優れた性能が示された.

### オフライン実験

まず、オフラインデータセットにおいて、我々の手法を他のベースラインと比較する.
オフラインデータセットは静的なものであり、**ユーザニュースのインタラクションの特定のペアのみが記録されている**.

#### Accuracy

精度の結果を表4に示す.

![](https://camo.qiitausercontent.com/e594ccb17585f55586b00b02dceeb517395709df/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f39396164613663632d333130362d663663372d646337372d6532336536376361623636352e706e67)

我々のアルゴリズムは全てのベースラインアルゴリズムを凌駕した.
ベースモデルのDNは、すでにベースラインと比較して非常に良い結果を出している.
これは、**dueling network構造の方が、ユーザーとニュースの相互作用をよりよくモデル化できるため**.
将来の報酬を考慮すること（DDQN）を加えることで、さらに大きな改善を実現している.
そして、ユーザactivenessや探索性を取り込んでも、オフライン環境下では必ずしも性能は向上しない、
これは、オフラインの設定では、候補となるニュースの静的な集合が限られているため、**アルゴリズムがユーザと最適なやりとりをすることができない**ためと思われる.
また、ϵ-greedyのような素朴なランダム探索は、推薦精度を低下させることになる.

#### モデルの収束処理(Model converge process)

図9に異なる手法の累積CTRを示し、収束の過程を説明する.
オフラインデータは時間順に並んでおり、ユーザが時間の経過とともにニュースリクエストを送信する過程をシミュレートしている.
比較されたすべての方法は、100リクエストセッションごとにモデルを更新する.

![](https://camo.qiitausercontent.com/78407c2c4c6e67abb4c2c0b239174ff139746ed6/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f62623137373533622d313061652d373830372d666430312d3937373238363233656534662e706e67)

予想通り、我々のアルゴリズム（DDQN + U + DBGD）は、他の方法よりも早く良いCTRに収束する.

### オンライン実験

オンライン評価段階では、商用ニュース推薦アプリケーションにモデルを導入し、アルゴリズムを比較した.
ユーザはアルゴリズムごとに均等に分かれている.
オンライン設定では、推薦の精度を測定するだけでなく、異なるアルゴリズムによる推薦の多様性を観察する事ができる.
いずれのアルゴリズムも、ニュースリクエストがあったときに、**トップ20**のニュースをユーザに推薦するように設計されている.

#### Accuracy

CTR、Precision@5、nDCGの観点から、異なるアルゴリズムを比較する.

![](https://camo.qiitausercontent.com/868976b26a1c74fbc8e732c57eabf47db42c594b/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f65393037643663392d616663302d303965392d306632612d3262363264316365656330612e706e67)

表5に示すように、我々のフルモデルDDQN + U + DBGDは、CTR、Precision@5、nDCGの点で他のすべてのモデルを大きく上回った.
**将来の報酬を追加する（DDQN）ことで、基本的なDNよりも推薦精度が向上する.**
しかし、さらにユーザの活性度を考慮したUを追加しても、推薦精度の面ではあまり意味がないように思われる. (ただし、このコンポーネントはユーザのactivenessやレコメンデーションの多様性を向上させるのに有用.)
また、DBGDを探索手法として用いることで、古典的なϵグリーディ法で引き起こされる性能低下を回避することができる.

#### Diversity

最後に、探索の効果を評価するために、ILSを用いた異なるアルゴリズムの推薦多様性を計算する.

$$
ILS(L) = \frac{\sum_{b_i \in L} \sum_{b_j \in L, b_j \neq b_i} S(b_i, b_j)}
{\sum_{b_i \in L} \sum_{b_j \in L, b_j \neq b_i} 1}
\tag{13}
$$

ここで、$S(b_i, b_j)$ は、アイテム bi と bj の間のコサイン類似度を表す.
表6に、ユーザがクリックしたニュースの多様性を示す.

![](https://camo.qiitausercontent.com/0b326a143f9cb2e1a42b7f02ac776091d14a06ec/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f39373836353265362d633135362d323436392d323336352d6265313165613035326133312e706e67)

一般的に、我々のアルゴリズムDDQN + U + DBGDのユーザーは、最高のクリック多様性を達成する.
興味深いことに、EGを追加しても推薦の多様性は改善されないらしい.
これは、ランダム探索（＝EG）を行った場合、レコメンダーが**ユーザに全く関係のないものを推薦してしまう**可能性があるためと考えられる.
これらの項目は多様性が高いが、ユーザーは読むことに興味を持たず、より自分の興味に合った項目の続きを読むために引き返してしまうかもしれない. そうすると、この探索は推薦の多様性を向上させることにはつながらない.

## 議論はある？

## 次に読むべき論文は？

## お気持ち実装
