refs: https://arxiv.org/pdf/2308.11336


## On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems  
ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹æ©Ÿä¼šã¨èª²é¡Œ  

### XIAOCONG CHEN, Data61, CSIRO, Australia  
ã‚·ã‚¢ã‚ªã‚³ãƒ³ãƒ»ãƒã‚§ãƒ³ã€Data61ã€CSIROã€ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢  
SIYU WANG, UNSW Sydney, Australia  
ã‚·ãƒ¦ãƒ»ãƒ¯ãƒ³ã€UNSWã‚·ãƒ‰ãƒ‹ãƒ¼ã€ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢  
JULIAN MCAULEY, UCSD, USA  
ã‚¸ãƒ¥ãƒªã‚¢ãƒ³ãƒ»ãƒã‚«ã‚¦ãƒªãƒ¼ã€UCSDã€ã‚¢ãƒ¡ãƒªã‚«  
DIETMAR JANNACH, University of Klagenfurt, Austria  
ãƒ‡ã‚£ãƒ¼ãƒˆãƒãƒ¼ãƒ«ãƒ»ãƒ¤ãƒ³ãƒŠãƒƒãƒã€ã‚¯ãƒ©ã‚²ãƒ³ãƒ•ãƒ«ãƒˆå¤§å­¦ã€ã‚ªãƒ¼ã‚¹ãƒˆãƒªã‚¢  
LINA YAO, Data61, CSIRO & UNSW Sydney, Australia  
ãƒªãƒŠãƒ»ãƒ¤ã‚ªã€Data61ã€CSIRO & UNSWã‚·ãƒ‰ãƒ‹ãƒ¼ã€ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢  

Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late.  
å¼·åŒ–å­¦ç¿’ã¯ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å†…ã§å‹•çš„ãªãƒ¦ãƒ¼ã‚¶ã®èˆˆå‘³ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ãŸã‚ã®å¼·åŠ›ãªãƒ„ãƒ¼ãƒ«ã¨ã—ã¦æ©Ÿèƒ½ã—ã€æœ€è¿‘ã§ã¯ç ”ç©¶ã®æ³¨ç›®ã‚’é›†ã‚ã¦ã„ã¾ã™ã€‚  
However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature.  
ã—ã‹ã—ã€é‡è¦ãªæ¬ ç‚¹ãŒæ®‹ã£ã¦ã„ã¾ã™ã€‚ãã‚Œã¯ã€**ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªæ€§è³ªã‹ã‚‰ç”Ÿã˜ã‚‹ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã®æ‚ªã•**ã§ã™ã€‚  
The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences.  
å¼·åŒ–å­¦ç¿’ã«åŸºã¥ãæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒãƒ¦ãƒ¼ã‚¶ã®å¥½ã¿ã‚’å­¦ã¶ãŸã‚ã«å¿…è¦ãªååˆ†ãªè»Œè·¡ã‚’é›†ã‚ã‚‹ãŸã‚ã«ã€é«˜ä¾¡ãªã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¿…è¦ã¨ã—ã¾ã™ã€‚  
This inefficiency renders reinforcement learning-based recommender systems a formidable undertaking, necessitating the exploration of potential solutions.  
ã“ã®éåŠ¹ç‡æ€§ã¯ã€å¼·åŒ–å­¦ç¿’ã«åŸºã¥ãæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚’å›°é›£ãªå–ã‚Šçµ„ã¿ã¨ã—ã€æ½œåœ¨çš„ãªè§£æ±ºç­–ã®æ¢æ±‚ã‚’å¿…è¦ã¨ã—ã¾ã™ã€‚  
Recent strides in offline reinforcement learning present a new perspective.  
æœ€è¿‘ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ã®é€²å±•ã¯ã€æ–°ãŸãªè¦–ç‚¹ã‚’æä¾›ã—ã¾ã™ã€‚  
Offline reinforcement learning empowers agents to glean insights from offline datasets and deploy learned policies in online settings.  
ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰æ´å¯Ÿã‚’å¾—ã¦ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç’°å¢ƒã§å­¦ç¿’ã—ãŸãƒãƒªã‚·ãƒ¼ã‚’å±•é–‹ã™ã‚‹ã“ã¨ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚  
Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlessly.  
æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¯åºƒç¯„ãªã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æŒã£ã¦ã„ã‚‹ãŸã‚ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«é©åˆã—ã¾ã™ã€‚  
Despite being a burgeoning field, works centered on recommender systems utilizing offline reinforcement learning remain limited.  
æ€¥æˆé•·ã—ã¦ã„ã‚‹åˆ†é‡ã§ã‚ã‚‹ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ã‚’åˆ©ç”¨ã—ãŸæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸç ”ç©¶ã¯é™ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚  
This survey aims to introduce and delve into offline reinforcement learning within recommender systems, offering an inclusive review of existing literature in this domain.  
ã“ã®èª¿æŸ»ã¯ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ã‚’ç´¹ä»‹ã—ã€æ˜ã‚Šä¸‹ã’ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€ã“ã®åˆ†é‡ã®æ—¢å­˜æ–‡çŒ®ã®åŒ…æ‹¬çš„ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æä¾›ã—ã¾ã™ã€‚  
Furthermore, we strive to underscore prevalent challenges, opportunities, and future pathways, poised to propel research in this evolving field.  
ã•ã‚‰ã«ã€ç§ãŸã¡ã¯ã€é€²åŒ–ã™ã‚‹ã“ã®åˆ†é‡ã®ç ”ç©¶ã‚’æ¨é€²ã™ã‚‹ãŸã‚ã®ä¸€èˆ¬çš„ãªèª²é¡Œã€æ©Ÿä¼šã€å°†æ¥ã®é“ç­‹ã‚’å¼·èª¿ã™ã‚‹ã“ã¨ã«åŠªã‚ã¾ã™ã€‚  

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## 1 INTRODUCTION ã¯ã˜ã‚ã«

In recent years, notable advancements have materialized in the realm of recommendation techniques, transcending the scope of traditional approaches (such as collaborative filtering, content-based recommendation, and matrix factorization [32]). 
è¿‘å¹´ã€æ¨è–¦æŠ€è¡“ã®åˆ†é‡ã§é¡•è‘—ãªé€²å±•ãŒè¦‹ã‚‰ã‚Œã€å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆå”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ã€è¡Œåˆ—å› å­åˆ†è§£ãªã© [32]ï¼‰ã®ç¯„å›²ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚
This evolution has led to the emergence of deep learning-based methods in the field of recommender systems (RS). 
ã“ã®é€²åŒ–ã¯ã€ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ï¼ˆRSï¼‰ã®åˆ†é‡ã«ãŠã‘ã‚‹æ·±å±¤å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã®å‡ºç¾ã‚’ã‚‚ãŸã‚‰ã—ã¾ã—ãŸã€‚
The appeal of deep learning stems from its ability to comprehend intricate non-linear relationships between users and items, making it adept at accommodating diverse data sources like images and text. 
**æ·±å±¤å­¦ç¿’ã®é­…åŠ›ã¯ã€ãƒ¦ãƒ¼ã‚¶ã¨ã‚¢ã‚¤ãƒ†ãƒ é–“ã®è¤‡é›‘ãªéç·šå½¢é–¢ä¿‚ã‚’ç†è§£ã™ã‚‹èƒ½åŠ›ã«ã‚ã‚Šã€ç”»åƒã‚„ãƒ†ã‚­ã‚¹ãƒˆãªã©ã®å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã«å¯¾å¿œã™ã‚‹ã®ã«å„ªã‚Œã¦ã„ã¾ã™**ã€‚
The adoption of deep learning in RS has proven beneficial in tackling multifaceted challenges. 
RSã«ãŠã‘ã‚‹æ·±å±¤å­¦ç¿’ã®æ¡ç”¨ã¯ã€å¤šé¢çš„ãªèª²é¡Œã«å¯¾å‡¦ã™ã‚‹ã®ã«æœ‰ç›Šã§ã‚ã‚‹ã“ã¨ãŒè¨¼æ˜ã•ã‚Œã¦ã„ã¾ã™ã€‚
Its strength lies in addressing intricate tasks and managing complex data structures [61]. 
ãã®å¼·ã¿ã¯ã€è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã«å¯¾å‡¦ã—ã€è¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç®¡ç†ã™ã‚‹ã“ã¨ã«ã‚ã‚Šã¾ã™[61]ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã  -->

Traditional recommendation systems (RS) have limitations in capturing interest dynamics, a challenge that emphasizes the distinction between usersâ€™ long-term and short-term interests [7, 61]. 
å¾“æ¥ã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ï¼ˆRSï¼‰ã¯ã€èˆˆå‘³ã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’æ‰ãˆã‚‹ä¸Šã§é™ç•ŒãŒã‚ã‚Šã€ã“ã‚Œã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é•·æœŸçš„ãŠã‚ˆã³çŸ­æœŸçš„ãªèˆˆå‘³ã®é•ã„ã‚’å¼·èª¿ã™ã‚‹èª²é¡Œã§ã™[7, 61]ã€‚
Specifically, while these traditional methods are adept at recognizing and modeling long-term interests based on historical data and patterns, they often fall short in accounting for the rapidly changing and more nuanced short-term interests. 
å…·ä½“çš„ã«ã¯ã€ã“ã‚Œã‚‰ã®å¾“æ¥ã®æ‰‹æ³•ã¯ã€æ­´å²çš„ãƒ‡ãƒ¼ã‚¿ã‚„ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ã„ã¦é•·æœŸçš„ãªèˆˆå‘³ã‚’èªè­˜ã—ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ã®ã«å„ªã‚Œã¦ã„ã¾ã™ãŒã€æ€¥é€Ÿã«å¤‰åŒ–ã™ã‚‹ã‚ˆã‚Šå¾®å¦™ãªçŸ­æœŸçš„ãªèˆˆå‘³ã‚’è€ƒæ…®ã™ã‚‹ã®ã«ã¯ã—ã°ã—ã°ä¸ååˆ†ã§ã™ã€‚
This gap in responsiveness to short-term shifts can lead to recommendations that are out-of-sync with a userâ€™s current preferences or situational needs. 
çŸ­æœŸçš„ãªå¤‰åŒ–ã«å¯¾ã™ã‚‹åå¿œã®ã‚®ãƒ£ãƒƒãƒ—ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç¾åœ¨ã®å¥½ã¿ã‚„çŠ¶æ³ã«åˆã‚ãªã„æ¨è–¦ã‚’ã‚‚ãŸã‚‰ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
In contrast, deep reinforcement learning (RL) aims to train an agent with the capacity to learn from interaction trajectories provided by the environment, achieved through the integration of deep learning and RL techniques as expounded in [11]. 
å¯¾ç…§çš„ã«ã€æ·±å±¤å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã¯ã€ç’°å¢ƒã‹ã‚‰æä¾›ã•ã‚Œã‚‹ç›¸äº’ä½œç”¨ã®è»Œè·¡ã‹ã‚‰å­¦ã¶èƒ½åŠ›ã‚’æŒã¤ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨“ç·´ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ãŠã‚Šã€ã“ã‚Œã¯[11]ã§è©³è¿°ã•ã‚Œã¦ã„ã‚‹æ·±å±¤å­¦ç¿’ã¨RLæŠ€è¡“ã®çµ±åˆã‚’é€šã˜ã¦é”æˆã•ã‚Œã¾ã™ã€‚
Notably, this approach empowers the agent to proactively glean insights from real-time user feedback, thereby enabling the discernment of evolving user preferences within the dynamic context of reinforcement learning. 
ç‰¹ã«ã€ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ç©æ¥µçš„ã«æ´å¯Ÿã‚’å¾—ã‚‹èƒ½åŠ›ã‚’ä¸ãˆã€å¼·åŒ–å­¦ç¿’ã®å‹•çš„ãªæ–‡è„ˆã®ä¸­ã§é€²åŒ–ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã‚’è­˜åˆ¥ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã  -->

RL provides a structured mathematical framework for acquiring learning-based control strategies. 
RLã¯ã€å­¦ç¿’ã«åŸºã¥ãåˆ¶å¾¡æˆ¦ç•¥ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®æ§‹é€ åŒ–ã•ã‚ŒãŸæ•°å­¦çš„æ çµ„ã¿ã‚’æä¾›ã—ã¾ã™ã€‚
By employing RL, we can systematically attain highly effective behavioral policies, which encapsulate action strategies. 
**RLã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€è¡Œå‹•æˆ¦ç•¥ã‚’åŒ…å«ã™ã‚‹éå¸¸ã«åŠ¹æœçš„ãªè¡Œå‹•ãƒãƒªã‚·ãƒ¼ã‚’ä½“ç³»çš„ã«é”æˆã§ãã¾ã™ã€‚**
These policies are engineered to optimize predefined objectives referred to as reward functions. 
ã“ã‚Œã‚‰ã®ãƒãƒªã‚·ãƒ¼ã¯ã€å ±é…¬é–¢æ•°ã¨å‘¼ã°ã‚Œã‚‹äº‹å‰ã«å®šç¾©ã•ã‚ŒãŸç›®çš„ã‚’æœ€é©åŒ–ã™ã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚
In essence, the reward function serves as a directive, guiding the RL algorithm towards desired actions, while the algorithm itself devises the means to enact these actions. 
**æœ¬è³ªçš„ã«ã€å ±é…¬é–¢æ•°ã¯æŒ‡é‡ã¨ã—ã¦æ©Ÿèƒ½ã—ã€RLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æœ›ã¾ã—ã„è¡Œå‹•ã«å°ãã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è‡ªä½“ãŒã“ã‚Œã‚‰ã®è¡Œå‹•ã‚’å®Ÿè¡Œã™ã‚‹æ‰‹æ®µã‚’è€ƒæ¡ˆã—ã¾ã™ã€‚**
Throughout its history, the field of RL has been a subject of intensive research. 
RLã®åˆ†é‡ã¯ã€ãã®æ­´å²ã‚’é€šã˜ã¦é›†ä¸­çš„ãªç ”ç©¶ã®å¯¾è±¡ã¨ãªã£ã¦ãã¾ã—ãŸã€‚
More recently, the integration of robust tools like deep neural networks into RL methodologies has yielded substantial advancements. 
æœ€è¿‘ã§ã¯ã€æ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚ˆã†ãªå¼·åŠ›ãªãƒ„ãƒ¼ãƒ«ã‚’RLã®æ–¹æ³•è«–ã«çµ±åˆã™ã‚‹ã“ã¨ã§ã€å®Ÿè³ªçš„ãªé€²å±•ãŒå¾—ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚
These neural networks act as versatile approximators, empowering RL techniques to exhibit exceptional performance across a diverse array of problem domains. 
**ã“ã‚Œã‚‰ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯å¤šç”¨é€”ã®è¿‘ä¼¼å™¨ã¨ã—ã¦æ©Ÿèƒ½ã—ã€RLæŠ€è¡“ãŒå¤šæ§˜ãªå•é¡Œé ˜åŸŸã§å“è¶Šã—ãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚**

<!-- ã“ã“ã¾ã§èª­ã‚“ã  -->

Nevertheless, a pertinent challenge to the widespread implementation of RL techniques emerges. 
ãã‚Œã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€RLæŠ€è¡“ã®åºƒç¯„ãªå®Ÿè£…ã«å¯¾ã™ã‚‹é‡è¦ãªèª²é¡ŒãŒæµ®ä¸Šã—ã¾ã™ã€‚
RL methods fundamentally follow an incremental learning approach, wherein they gather knowledge by iteratively engaging with their environment. 
RLæ‰‹æ³•ã¯åŸºæœ¬çš„ã«æ¼¸é€²çš„ãªå­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«å¾“ã„ã€ç’°å¢ƒã¨åå¾©çš„ã«é–¢ä¸ã™ã‚‹ã“ã¨ã§çŸ¥è­˜ã‚’é›†ã‚ã¾ã™ã€‚
Subsequent refinements are informed by previous experiences. 
ãã®å¾Œã®æ”¹è‰¯ã¯ã€ä»¥å‰ã®çµŒé¨“ã«åŸºã¥ã„ã¦è¡Œã‚ã‚Œã¾ã™ã€‚
While this iterative learning approach is effective in numerous scenarios, its practicality is not universal. 
**ã“ã®åå¾©å­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯å¤šãã®ã‚·ãƒŠãƒªã‚ªã§åŠ¹æœçš„ã§ã™ãŒã€ãã®å®Ÿç”¨æ€§ã¯æ™®éçš„ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚**
Consider cases such as real-world robotics, educational software pedagogy, or healthcare interventions; these situations entail potential risks and resource expenses that cannot be disregarded. 
å®Ÿä¸–ç•Œã®ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã€æ•™è‚²ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã®æ•™è‚²æ³•ã€ã¾ãŸã¯åŒ»ç™‚ä»‹å…¥ãªã©ã®ã‚±ãƒ¼ã‚¹ã‚’è€ƒãˆã¦ã¿ã¦ãã ã•ã„ã€‚ã“ã‚Œã‚‰ã®çŠ¶æ³ã¯ã€ç„¡è¦–ã§ããªã„æ½œåœ¨çš„ãªãƒªã‚¹ã‚¯ã¨ãƒªã‚½ãƒ¼ã‚¹ã®è²»ç”¨ã‚’ä¼´ã„ã¾ã™ã€‚
Moreover, even within scenarios conducive to online learning, such as in the context of RS, a preference for historical data often arises. 
ã•ã‚‰ã«ã€RSã®æ–‡è„ˆã®ã‚ˆã†ã«ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã«é©ã—ãŸã‚·ãƒŠãƒªã‚ªå†…ã§ã‚‚ã€æ­´å²çš„ãƒ‡ãƒ¼ã‚¿ã¸ã®å¥½ã¿ãŒã—ã°ã—ã°ç”Ÿã˜ã¾ã™ã€‚
This preference is particularly pronounced in intricate domains where sound decision-making hinges upon substantial data inputs. 
ã“ã®å¥½ã¿ã¯ã€å¥å…¨ãªæ„æ€æ±ºå®šãŒ substantial data inputs ã«ä¾å­˜ã™ã‚‹è¤‡é›‘ãªé ˜åŸŸã§ã¯ç‰¹ã«é¡•è‘—ã§ã™ã€‚
The rationale is that leveraging previously amassed data enables informed decisions without necessitating continuous real-world experimentation. 
ãã®ç†ç”±ã¯ã€**ä»¥å‰ã«è“„ç©ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€ç¶™ç¶šçš„ãªå®Ÿä¸–ç•Œã®å®Ÿé¨“ã‚’å¿…è¦ã¨ã›ãšã«æƒ…å ±ã«åŸºã¥ã„ãŸæ„æ€æ±ºå®šãŒå¯èƒ½ã«ãªã‚‹ã‹ã‚‰**ã§ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã  -->

The success of machine learning methods in solving real-world problems in the past decade is largely thanks to new ways of learning from large amounts of data. 
éå»10å¹´é–“ã«ãŠã‘ã‚‹æ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•ã®å®Ÿä¸–ç•Œã®å•é¡Œè§£æ±ºã«ãŠã‘ã‚‹æˆåŠŸã¯ã€å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ã¶æ–°ã—ã„æ–¹æ³•ã®ãŠã‹ã’ã§ã™ã€‚
These methods get better as theyâ€™re trained with more data. 
ã“ã‚Œã‚‰ã®æ‰‹æ³•ã¯ã€ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã•ã‚Œã‚‹ã“ã¨ã§æ”¹å–„ã•ã‚Œã¾ã™ã€‚
However, applying this approach to online Reinforcement Learning (RL) doesnâ€™t fit well. 
**ã—ã‹ã—ã€ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã«é©ç”¨ã™ã‚‹ã“ã¨ã¯ã†ã¾ãã„ãã¾ã›ã‚“ã€‚**
While this wasnâ€™t a big problem when RL methods were simpler and used small datasets for easy problems, adding complex neural networks to RL makes us wonder if we can use the same data-driven approach for RL goals. 
RLæ‰‹æ³•ãŒã‚ˆã‚Šå˜ç´”ã§ç°¡å˜ãªå•é¡Œã«å°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ã„ãŸã¨ãã«ã¯å¤§ããªå•é¡Œã§ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸãŒã€è¤‡é›‘ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’RLã«è¿½åŠ ã™ã‚‹ã¨ã€RLã®ç›®æ¨™ã«å¯¾ã—ã¦åŒã˜ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä½¿ç”¨ã§ãã‚‹ã‹ã©ã†ã‹ç–‘å•ã«æ€ã„ã¾ã™ã€‚
This would mean creating a system where RL learns from existing data without needing more data collected in real-time [27]. 
ã“ã‚Œã¯ã€RLãŒãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§åé›†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’å¿…è¦ã¨ã›ãšã«æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ã¶ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™[27]ã€‚

However, this idea of using existing data for RL brings its own challenges. 
ã—ã‹ã—ã€RLã«æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã„ã†ã“ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã¯ç‹¬è‡ªã®èª²é¡Œã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚
As we discuss in this article, many common RL methods can learn from data collected differently from how the policy behaves. 
**ã“ã®è¨˜äº‹ã§è­°è«–ã™ã‚‹ã‚ˆã†ã«ã€å¤šãã®ä¸€èˆ¬çš„ãªRLæ‰‹æ³•ã¯ã€ãƒãƒªã‚·ãƒ¼ã®æŒ¯ã‚‹èˆã„ã¨ã¯ç•°ãªã‚‹æ–¹æ³•ã§åé›†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ã¶ã“ã¨ãŒã§ãã¾ã™ã€‚** (ãƒ‡ãƒ¼ã‚¿åé›†æ–¹ç­–ã¨å­¦ç¿’æ–¹ç­–ã­...!:thinking:)
But these methods often struggle when trying to learn effectively from a whole set of data collected in advance, without more data being collected as the policy improves. 
ã—ã‹ã—ã€ã“ã‚Œã‚‰ã®æ‰‹æ³•ã¯ã€ãƒãƒªã‚·ãƒ¼ãŒæ”¹å–„ã•ã‚Œã‚‹ã«ã¤ã‚Œã¦æ–°ãŸãªãƒ‡ãƒ¼ã‚¿ãŒåé›†ã•ã‚Œã‚‹ã“ã¨ãªãã€äº‹å‰ã«åé›†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å…¨ä½“ã‹ã‚‰åŠ¹æœçš„ã«å­¦ã¼ã†ã¨ã™ã‚‹ã¨ã—ã°ã—ã°è‹¦åŠ´ã—ã¾ã™ã€‚
Making things more complicated with high-dimensional neural networks can make this problem worse. 
é«˜æ¬¡å…ƒã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§äº‹æ…‹ã‚’è¤‡é›‘ã«ã™ã‚‹ã“ã¨ã¯ã€ã“ã®å•é¡Œã‚’æ‚ªåŒ–ã•ã›ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
A big issue with using pre-existing data for RL is that the dataâ€™s distribution may not match real-world conditions [27]. 
RLã«æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹éš›ã®å¤§ããªå•é¡Œã¯ã€ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒãŒå®Ÿä¸–ç•Œã®æ¡ä»¶ã¨ä¸€è‡´ã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã§ã™[27]ã€‚
Still, the potential of a fully offline RL system is exciting. 
ãã‚Œã§ã‚‚ã€å®Œå…¨ãªã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã‚·ã‚¹ãƒ†ãƒ ã®å¯èƒ½æ€§ã¯é­…åŠ›çš„ã§ã™ã€‚
Just like how machine learning can turn data into useful tools like image recognition or speech understanding, an offline RL system, using strong function approximators, might turn data into smart decision-makers. 
æ©Ÿæ¢°å­¦ç¿’ãŒãƒ‡ãƒ¼ã‚¿ã‚’ç”»åƒèªè­˜ã‚„éŸ³å£°ç†è§£ã®ã‚ˆã†ãªæœ‰ç”¨ãªãƒ„ãƒ¼ãƒ«ã«å¤‰ãˆã‚‹ã®ã¨åŒæ§˜ã«ã€å¼·åŠ›ãªé–¢æ•°è¿‘ä¼¼å™¨ã‚’ä½¿ç”¨ã—ãŸã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã‚·ã‚¹ãƒ†ãƒ ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒãƒ¼ãƒˆãªæ„æ€æ±ºå®šè€…ã«å¤‰ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
This could mean that people with lots of data could make policies that help them make better choices for what they want to achieve [35]. 
ã“ã‚Œã¯ã€å¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’æŒã¤äººã€…ãŒã€é”æˆã—ãŸã„ç›®æ¨™ã«å¯¾ã—ã¦ã‚ˆã‚Šè‰¯ã„é¸æŠã‚’ã™ã‚‹ã®ã«å½¹ç«‹ã¤ãƒãƒªã‚·ãƒ¼ã‚’ä½œæˆã§ãã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™[35]ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã  -->

RS and advertising are particularly well-suited areas for applying offline RL. 
**RSã¨åºƒå‘Šã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã‚’é©ç”¨ã™ã‚‹ã®ã«ç‰¹ã«é©ã—ãŸåˆ†é‡**ã§ã™ã€‚
This is because collecting data is straightforward and efficient, often done by recording user actions. 
ã“ã‚Œã¯ã€**ãƒ‡ãƒ¼ã‚¿ã®åé›†ãŒç°¡å˜ã§åŠ¹ç‡çš„ã§ã‚ã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã‚’è¨˜éŒ²ã™ã‚‹ã“ã¨ã§è¡Œã‚ã‚Œã‚‹ã“ã¨ãŒå¤šã„ãŸã‚**ã§ã™ã€‚
Moreover, the existing RS literature provides sufficient datasets which can be used for training offline RL. 
ã•ã‚‰ã«ã€æ—¢å­˜ã®RSæ–‡çŒ®ã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®è¨“ç·´ã«ä½¿ç”¨ã§ãã‚‹ååˆ†ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚
However, these domains are also critical in terms of safety. 
ã—ã‹ã—ã€ã“ã‚Œã‚‰ã®é ˜åŸŸã¯å®‰å…¨æ€§ã®è¦³ç‚¹ã‹ã‚‰ã‚‚é‡è¦ã§ã™ã€‚
Making a very poor decision could lead to significant financial losses. 
**éå¸¸ã«æ‚ªã„æ±ºå®šã‚’ä¸‹ã™ã“ã¨ã¯ã€é‡å¤§ãªè²¡å‹™æå¤±ã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™**ã€‚(ã†ã‚“ã†ã‚“ã€‚ã ã‹ã‚‰RSã§ã¯ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚¼ãƒ­ã‹ã‚‰å§‹ã‚ã‚‹ã“ã¨ã¯ã§ããªã„...!:thinking:)
Therefore, traditional online exploration methods are not practical here. 
ã—ãŸãŒã£ã¦ã€å¾“æ¥ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¢ç´¢æ‰‹æ³•ã¯ã“ã“ã§ã¯å®Ÿç”¨çš„ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
This is why offline RL methods have a history of being used in these fields. 
ã“ã‚ŒãŒã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLæ‰‹æ³•ãŒã“ã‚Œã‚‰ã®åˆ†é‡ã§ä½¿ç”¨ã•ã‚Œã¦ããŸç†ç”±ã§ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã  -->

One technique commonly employed is called off-policy evaluation. 
ä¸€èˆ¬çš„ã«ä½¿ç”¨ã•ã‚Œã‚‹æ‰‹æ³•ã®1ã¤ã¯ã€ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è©•ä¾¡ã¨å‘¼ã°ã‚Œã¾ã™ã€‚
This approach is useful for running A/B tests and estimating the effectiveness of advertising and RS methods without needing to interact with the environment further. 
ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€A/Bãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã€åºƒå‘Šã‚„RSæ‰‹æ³•ã®åŠ¹æœã‚’æ¨å®šã™ã‚‹ã®ã«å½¹ç«‹ã¡ã€ã•ã‚‰ã«ç’°å¢ƒã¨ç›¸äº’ä½œç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã›ã‚“ã€‚

In the case of RS, things are a bit different compared to other applications. 
RSã®å ´åˆã€ä»–ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨æ¯”è¼ƒã—ã¦çŠ¶æ³ã¯å°‘ã—ç•°ãªã‚Šã¾ã™ã€‚
RS policy evaluation is often set up as a contextual bandit problem. 
**RSã®ãƒãƒªã‚·ãƒ¼è©•ä¾¡ã¯ã€ã—ã°ã—ã°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã¨ã—ã¦è¨­å®šã•ã‚Œã¾ã™**ã€‚
Here, "states" might represent a userâ€™s past behavior, and "actions" are the recommendations made to them. 
ã“ã“ã§ã€ã€ŒçŠ¶æ…‹ã€ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éå»ã®è¡Œå‹•ã‚’è¡¨ã—ã€ã€Œè¡Œå‹•ã€ã¯å½¼ã‚‰ã«å¯¾ã—ã¦è¡Œã‚ã‚ŒãŸæ¨è–¦ã§ã™ã€‚
This simplification avoids the complexity of sequential decision making, which is useful. 
ã“ã®å˜ç´”åŒ–ã¯ã€é€£ç¶šçš„ãªæ„æ€æ±ºå®šã®è¤‡é›‘ã•ã‚’å›é¿ã—ã€æœ‰ç”¨ã§ã™ã€‚
However, it can lead to inaccuracies if actions are connected over time, like in robotics or healthcare scenarios. 
ã—ã‹ã—ã€ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã‚„åŒ»ç™‚ã‚·ãƒŠãƒªã‚ªã®ã‚ˆã†ã«ã€è¡Œå‹•ãŒæ™‚é–“çš„ã«é–¢é€£ã—ã¦ã„ã‚‹å ´åˆã€èª¤å·®ã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

Using offline RL for RS has practical applications such as optimizing recommendations presented together on a page, improving entire web pages, and estimating website visits with the help of doubly robust estimation. 
RSã«ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã«ã¯ã€ãƒšãƒ¼ã‚¸ä¸Šã§ä¸€ç·’ã«æç¤ºã•ã‚Œã‚‹æ¨è–¦ã®æœ€é©åŒ–ã€å…¨ä½“ã®ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã®æ”¹å–„ã€ãƒ€ãƒ–ãƒªãƒ¼ãƒ­ãƒã‚¹ãƒˆæ¨å®šã‚’ç”¨ã„ãŸã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆè¨ªå•ã®æ¨å®šãªã©ã®å®Ÿç”¨çš„ãªå¿œç”¨ãŒã‚ã‚Šã¾ã™ã€‚
Another use is A/B testing to fine-tune click rates for optimization. 
åˆ¥ã®ç”¨é€”ã¯ã€æœ€é©åŒ–ã®ãŸã‚ã«ã‚¯ãƒªãƒƒã‚¯ç‡ã‚’å¾®èª¿æ•´ã™ã‚‹A/Bãƒ†ã‚¹ãƒˆã§ã™ã€‚
Researchers have also used offline data to learn policies. 
ç ”ç©¶è€…ãŸã¡ã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ãƒãƒªã‚·ãƒ¼ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ã€‚
This includes efforts like improving clickthrough rates for newspaper articles, ranking advertisements on search pages, and tailoring ad recommendations for digital marketing. 
ã“ã‚Œã«ã¯ã€æ–°èè¨˜äº‹ã®ã‚¯ãƒªãƒƒã‚¯ç‡ã‚’æ”¹å–„ã™ã‚‹ã“ã¨ã€æ¤œç´¢ãƒšãƒ¼ã‚¸ã§ã®åºƒå‘Šã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€ãƒ‡ã‚¸ã‚¿ãƒ«ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã®ãŸã‚ã®åºƒå‘Šæ¨è–¦ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ãªã©ã®å–ã‚Šçµ„ã¿ãŒå«ã¾ã‚Œã¾ã™ã€‚

In this survey, our main focus will be on offline RL in RS (offline RL4RS). 
ã“ã®èª¿æŸ»ã§ã¯ã€ç§ãŸã¡ã®ä¸»ãªç„¦ç‚¹ã¯RSã«ãŠã‘ã‚‹ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLï¼ˆoffline RL4RSï¼‰ã«ãªã‚Šã¾ã™ã€‚
We aim to provide a comprehensive overview of existing works, along with discussing open challenges and future directions. 
ç§ãŸã¡ã¯ã€æ—¢å­˜ã®ç ”ç©¶ã®åŒ…æ‹¬çš„ãªæ¦‚è¦ã‚’æä¾›ã—ã€ã‚ªãƒ¼ãƒ—ãƒ³ãªèª²é¡Œã¨å°†æ¥ã®æ–¹å‘æ€§ã«ã¤ã„ã¦è­°è«–ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

**1.1** **Relations to existing surveys** **1.1 æ—¢å­˜ã®èª¿æŸ»ã¨ã®é–¢ä¿‚**

Two existing surveys have centered on the topic of RL in RS [1, 11]. 
æ—¢å­˜ã®2ã¤ã®èª¿æŸ»ã¯ã€RSã«ãŠã‘ã‚‹RLã®ãƒˆãƒ”ãƒƒã‚¯ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™[1, 11]ã€‚
While Afsar et al. [1] provides an overview of RL in RS, it does not comprehensively explore the expanding realm of deep RL. 
Afsar et al. [1]ã¯RSã«ãŠã‘ã‚‹RLã®æ¦‚è¦ã‚’æä¾›ã—ã¦ã„ã¾ã™ãŒã€æ·±å±¤RLã®æ‹¡å¤§ã™ã‚‹é ˜åŸŸã‚’åŒ…æ‹¬çš„ã«æ¢æ±‚ã—ã¦ã„ã¾ã›ã‚“ã€‚
In contrast, [11] delves more deeply into the analysis and discussion of RL in RS, but predominantly focuses on online RL and its RS applications. 
å¯¾ç…§çš„ã«ã€[11]ã¯RSã«ãŠã‘ã‚‹RLã®åˆ†æã¨è­°è«–ã«ã‚ˆã‚Šæ·±ãæ˜ã‚Šä¸‹ã’ã¦ã„ã¾ã™ãŒã€ä¸»ã«ã‚ªãƒ³ãƒ©ã‚¤ãƒ³RLã¨ãã®RSã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚
Itâ€™s noteworthy that [11] identifies offline RL in RS as a potential future direction but does not offer an all-encompassing review of this area. 
[11]ã¯RSã«ãŠã‘ã‚‹ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã‚’æ½œåœ¨çš„ãªå°†æ¥ã®æ–¹å‘æ€§ã¨ã—ã¦ç‰¹å®šã—ã¦ã„ã¾ã™ãŒã€ã“ã®åˆ†é‡ã®åŒ…æ‹¬çš„ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æä¾›ã—ã¦ã„ã¾ã›ã‚“ã€‚
The limited coverage of offline RL in RS can be attributed to its emergence around the same time as these two surveys. 
RSã«ãŠã‘ã‚‹ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®é™ã‚‰ã‚ŒãŸã‚«ãƒãƒ¬ãƒƒã‚¸ã¯ã€ã“ã‚Œã‚‰ã®2ã¤ã®èª¿æŸ»ã¨ã»ã¼åŒæ™‚ã«å‡ºç¾ã—ãŸã“ã¨ã«èµ·å› ã—ã¦ã„ã¾ã™ã€‚
Furthermore, due to the recent establishment of the offline RL concept, certain works examined in these two existing surveys are classified as special cases of policy-based methods. 
ã•ã‚‰ã«ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®æ¦‚å¿µãŒæœ€è¿‘ç¢ºç«‹ã•ã‚ŒãŸãŸã‚ã€ã“ã‚Œã‚‰ã®2ã¤ã®æ—¢å­˜ã®èª¿æŸ»ã§æ¤œè¨ã•ã‚ŒãŸç‰¹å®šã®ç ”ç©¶ã¯ã€ãƒãƒªã‚·ãƒ¼ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã®ç‰¹åˆ¥ãªã‚±ãƒ¼ã‚¹ã¨ã—ã¦åˆ†é¡ã•ã‚Œã¦ã„ã¾ã™ã€‚
Differently, this survey endeavors to refine these categorizations by reclassifying prior works into the domain of offline RL in RS. 
ç•°ãªã‚Šã€ã“ã®èª¿æŸ»ã¯ã€ä»¥å‰ã®ç ”ç©¶ã‚’RSã«ãŠã‘ã‚‹ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®é ˜åŸŸã«å†åˆ†é¡ã™ã‚‹ã“ã¨ã§ã€ã“ã‚Œã‚‰ã®åˆ†é¡ã‚’æ´—ç·´ã•ã›ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚
Additionally, we extend the literature to encompass the most recent developments in offline RL for RS, thereby augmenting our understanding of recent progress in this field. 
ã•ã‚‰ã«ã€RSã®ãŸã‚ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã«ãŠã‘ã‚‹æœ€è¿‘ã®ç™ºå±•ã‚’å«ã‚€ã‚ˆã†ã«æ–‡çŒ®ã‚’æ‹¡å¼µã—ã€ã“ã®åˆ†é‡ã®æœ€è¿‘ã®é€²å±•ã«å¯¾ã™ã‚‹ç†è§£ã‚’æ·±ã‚ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã  -->

**1.2** **Structure of this Survey**
**1.2 ã“ã®èª¿æŸ»ã®æ§‹æˆ**

This survey is structured into four distinct sections. 
ã“ã®èª¿æŸ»ã¯4ã¤ã®ç•°ãªã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚
Firstly, we offer an introduction to RL basics, providing readers with a foundational understanding of various RL algorithms, including Q-Learning, Policy-based Methods, Actor-Critic Methods, and Model-based RL. 
ã¾ãšã€RLã®åŸºæœ¬ã«ã¤ã„ã¦ã®ç´¹ä»‹ã‚’æä¾›ã—ã€Q-Learningã€ãƒãƒªã‚·ãƒ¼ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã€ã‚¢ã‚¯ã‚¿ãƒ¼-ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯æ‰‹æ³•ã€ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®RLãªã©ã€ã•ã¾ã–ã¾ãªRLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®åŸºç¤çš„ãªç†è§£ã‚’èª­è€…ã«æä¾›ã—ã¾ã™ã€‚
Subsequently, we delve into the concept of offline RL and present a problem formulation that explores how to integrate recommender systems (RS) into the offline RL framework. 
æ¬¡ã«ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®æ¦‚å¿µã«æ·±ãå…¥ã‚Šè¾¼ã¿ã€**æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ï¼ˆRSï¼‰ã‚’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«çµ±åˆã™ã‚‹æ–¹æ³•ã‚’æ¢ã‚‹å•é¡Œã®å®šå¼åŒ–**ã‚’æç¤ºã—ã¾ã™ã€‚
Continuing, we conduct a comprehensive review of existing works from two main perspectives: off-policy evaluation using logged data and the realm of offline RL in RS. 
ç¶šã„ã¦ã€ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è©•ä¾¡ã¨RSã«ãŠã‘ã‚‹ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®é ˜åŸŸã¨ã„ã†2ã¤ã®ä¸»è¦ãªè¦–ç‚¹ã‹ã‚‰ã€æ—¢å­˜ã®ç ”ç©¶ã®åŒ…æ‹¬çš„ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡Œã„ã¾ã™ã€‚
This examination highlights current research trends and insights. 
ã“ã®èª¿æŸ»ã¯ã€ç¾åœ¨ã®ç ”ç©¶å‹•å‘ã¨æ´å¯Ÿã‚’å¼·èª¿ã—ã¾ã™ã€‚
Following the review, we outline the open challenges and promising opportunities that warrant in-depth exploration. 
ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®å¾Œã€ç§ãŸã¡ã¯è©³ç´°ãªæ¢æ±‚ã«å€¤ã™ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ãªèª²é¡Œã¨æœ‰æœ›ãªæ©Ÿä¼šã‚’æ¦‚èª¬ã—ã¾ã™ã€‚
Finally, building upon the identified challenges and opportunities, we propose potential future directions that could serve as solutions to these challenges. 
æœ€å¾Œã«ã€ç‰¹å®šã•ã‚ŒãŸèª²é¡Œã¨æ©Ÿä¼šã«åŸºã¥ã„ã¦ã€ã“ã‚Œã‚‰ã®èª²é¡Œã«å¯¾ã™ã‚‹è§£æ±ºç­–ã¨ãªã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹å°†æ¥ã®æ–¹å‘æ€§ã‚’ææ¡ˆã—ã¾ã™ã€‚
This forward-looking section aims to guide future research endeavors in the field, by suggesting pathways to address the outstanding issues and capitalize on the untapped opportunities. 
ã“ã®å°†æ¥ã‚’è¦‹æ®ãˆãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ã€æœªè§£æ±ºã®å•é¡Œã«å¯¾å‡¦ã—ã€æœªé–‹æ‹“ã®æ©Ÿä¼šã‚’æ´»ç”¨ã™ã‚‹ãŸã‚ã®é“ç­‹ã‚’ææ¡ˆã™ã‚‹ã“ã¨ã§ã€ã“ã®åˆ†é‡ã®å°†æ¥ã®ç ”ç©¶åŠªåŠ›ã‚’å°ãã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## 2 OFFLINE RL OVERVIEW AND PROBLEM STATEMENT
## 2 ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®æ¦‚è¦ã¨å•é¡Œå®šç¾©

In this section, we delve into fundamental concepts essential to understanding the field of RL. 
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰åˆ†é‡ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ä¸å¯æ¬ ãªåŸºæœ¬æ¦‚å¿µã«ã¤ã„ã¦æ˜ã‚Šä¸‹ã’ã¾ã™ã€‚
We initiate with RL preliminaries, encompassing Markov Decision Processes, On-Policy and Off-Policy Learning, and Typical RL algorithms. 
ã¾ãšã€ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ã€ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼å­¦ç¿’ã¨ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å­¦ç¿’ã€å…¸å‹çš„ãªRLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å«ã‚€RLã®å‰æçŸ¥è­˜ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚
In doing so, we establish the foundational understanding by clarifying key principles and terminologies employed throughout this survey. 
ãã®éç¨‹ã§ã€**æœ¬èª¿æŸ»å…¨ä½“ã§ä½¿ç”¨ã•ã‚Œã‚‹ä¸»è¦ãªåŸå‰‡ã¨ç”¨èªã‚’æ˜ç¢ºã«ã™ã‚‹**ã“ã¨ã«ã‚ˆã‚Šã€åŸºç¤çš„ãªç†è§£ã‚’ç¢ºç«‹ã—ã¾ã™ã€‚
Subsequently, we shift our focus toward the concept of offline RL and how it can be used to formulate RS. 
ãã®å¾Œã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®æ¦‚å¿µã¨ã€ãã‚ŒãŒæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ï¼ˆRSï¼‰ã‚’å½¢æˆã™ã‚‹ãŸã‚ã«ã©ã®ã‚ˆã†ã«ä½¿ç”¨ã§ãã‚‹ã‹ã«ç„¦ç‚¹ã‚’ç§»ã—ã¾ã™ã€‚
For the sake of clarity, we have summarized the common notations used in this survey in Table 1. 
æ˜ç¢ºã•ã®ãŸã‚ã«ã€æœ¬èª¿æŸ»ã§ä½¿ç”¨ã•ã‚Œã‚‹ä¸€èˆ¬çš„ãªè¡¨è¨˜ã‚’è¡¨1ã«ã¾ã¨ã‚ã¾ã—ãŸã€‚

- è¡¨1: æœ¬èª¿æŸ»ã§ä½¿ç”¨ã•ã‚Œã‚‹ä¸€èˆ¬çš„ãªè¡¨è¨˜
  - $M$: ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹
  - $\pi_{\beta}$: behavior policy(ãƒ‡ãƒ¼ã‚¿åé›†æ–¹ç­–)
  - $\gamma$: å‰²å¼•å› å­(discount factor)
  - $E$: æœŸå¾…å€¤(expected value)
  - $\theta$: æ–¹ç­–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿(policy parameter)
  - $s$: çŠ¶æ…‹(state) - ãƒ¦ãƒ¼ã‚¶é–¢é€£æƒ…å ±
  - $a$: è¡Œå‹•(action) - æ¨è–¦ã‚¢ã‚¤ãƒ†ãƒ 
  - $\pi$: æ–¹ç­–(policy) - æ¨è–¦æ–¹ç­–, target policy
  - $R(\cdot, \cdot)$: å ±é…¬é–¢æ•°(reward function) - ä¾‹ãˆã°ãƒ¦ãƒ¼ã‚¶ã®ã‚¯ãƒªãƒƒã‚¯è¡Œå‹•
  - $D$: ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(offline dataset) - $\{(s_t, a_t, s_{t+1}, r_t)\}$ ã®é›†åˆ

**2.1** **Markov Decision Process**  
**2.1** **ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹**

In this section, we shall expound upon fundamental concepts within the realm of RL, adhering closely to established standard definitions as outlined in[44]. 
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€RLã®é ˜åŸŸå†…ã®åŸºæœ¬æ¦‚å¿µã«ã¤ã„ã¦è©³è¿°ã—ã€[44]ã§æ¦‚èª¬ã•ã‚ŒãŸç¢ºç«‹ã•ã‚ŒãŸæ¨™æº–å®šç¾©ã«å¯†æ¥ã«å¾“ã„ã¾ã™ã€‚
RL deals with the challenge of learning how to control dynamic systems in a broad context. 
RLã¯ã€åºƒã„æ–‡è„ˆã§å‹•çš„ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆ¶å¾¡ã™ã‚‹æ–¹æ³•ã‚’å­¦ã¶ã¨ã„ã†èª²é¡Œã«å–ã‚Šçµ„ã¿ã¾ã™ã€‚
RL4RS are typically described by fully observed Markov decision processes (MDP) or partially observed ones known as Partially Observable Markov Decision Processes (POMDP). 
**RL4RSã¯é€šå¸¸ã€å®Œå…¨ã«è¦³æ¸¬ã•ã‚ŒãŸãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆMDPï¼‰ã¾ãŸã¯éƒ¨åˆ†çš„ã«è¦³æ¸¬ã•ã‚ŒãŸãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆPOMDPï¼‰ã¨ã—ã¦èª¬æ˜ã•ã‚Œã¾ã™ã€‚**
Moreover, we will also provide  
ã•ã‚‰ã«ã€ç§ãŸã¡ã¯ä»¥ä¸‹ã®å®šç¾©ã‚’æä¾›ã—ã¾ã™ã€‚

---

Definition 1 (Markov decision process). The Markov decision process is formalized as the tuple M = {S, A, P, R,ğ›¾ }. 
å®šç¾©1ï¼ˆãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼‰ã€‚ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ã¯ã€ã‚¿ãƒ—ãƒ« $M = {S, A, P, R, \gamma}$ ã¨ã—ã¦å½¢å¼åŒ–ã•ã‚Œã¾ã™ã€‚
Within this structure, each component serves a distinct purpose: S encompasses the set of states ğ‘ _ âˆˆS, capable of adopting discrete or continuous values, potentially even multi-dimensional vectors. 
ã“ã®æ§‹é€ å†…ã§ã€å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ç•°ãªã‚‹ç›®çš„ã‚’æœãŸã—ã¾ã™ã€‚Sã¯çŠ¶æ…‹ã®é›†åˆ $s \in S$ ã‚’åŒ…å«ã—ã€é›¢æ•£ã¾ãŸã¯é€£ç¶šã®å€¤ã‚’æ¡ç”¨ã§ãã€å ´åˆã«ã‚ˆã£ã¦ã¯å¤šæ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã§ã•ãˆã‚ã‚Šã¾ã™ã€‚
A characterizes the set of actions ğ‘_ âˆˆA, which may be discrete or continuous in _nature. 
$A$ ã¯ã€æ€§è³ªä¸Šé›¢æ•£ã¾ãŸã¯é€£ç¶šã§ã‚ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®é›†åˆ $a \in A$ ã‚’ç‰¹å¾´ä»˜ã‘ã¾ã™ã€‚
P defines a conditional probability distribution, P(ğ‘ ğ‘¡_ +1|ğ‘ ğ‘¡,ğ‘ğ‘¡ ), delineating the progression of _the systemâ€™s dynamics over time. 
$P$ ã¯æ¡ä»¶ä»˜ãç¢ºç‡åˆ†å¸ƒ $P(s_{t+1} | s_t, a_t)$ ã‚’å®šç¾©ã—ã€ã‚·ã‚¹ãƒ†ãƒ ã®å‹•æ…‹ãŒæ™‚é–“ã¨ã¨ã‚‚ã«é€²è¡Œã™ã‚‹æ§˜å­ã‚’ç¤ºã—ã¾ã™ã€‚(çŠ¶æ…‹é·ç§»ã®ç¢ºç‡åˆ†å¸ƒã ã‚ˆã­...!:thinking:)
R : S Ã— A â†’_ R serves as the reward function, linking states and _actions to real-valued rewards. 
$R : S Ã— A â†’ \mathbb{R}$ ã¯å ±é…¬é–¢æ•°ã¨ã—ã¦æ©Ÿèƒ½ã—ã€çŠ¶æ…‹ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿæ•°å€¤ã®å ±é…¬ã«çµã³ä»˜ã‘ã¾ã™ã€‚
ğ›¾_ âˆˆ[0, 1] assumes the role of a scalar discount factor, influencing the _extent to which future rewards are taken into consideration._ 
$\gamma \in [0, 1]$ ã¯ã‚¹ã‚«ãƒ©ãƒ¼ã®å‰²å¼•å› å­ã®å½¹å‰²ã‚’æœãŸã—ã€å°†æ¥ã®å ±é…¬ãŒè€ƒæ…®ã•ã‚Œã‚‹ç¨‹åº¦ã«å½±éŸ¿ã‚’ä¸ãˆã¾ã™ã€‚

---

Throughout most of this article, we will primarily employ fully-observed formalism. 
ã“ã®è¨˜äº‹ã®ã»ã¨ã‚“ã©ã®éƒ¨åˆ†ã§ã¯ã€ä¸»ã«å®Œå…¨è¦³æ¸¬å½¢å¼ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
However, we also include the definition of the partially observed Markov decision process (POMDP) to ensure comprehensiveness. 
ãŸã ã—ã€åŒ…æ‹¬æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã«ã€**éƒ¨åˆ†çš„ã«è¦³æ¸¬ã•ã‚ŒãŸãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆPOMDP, Partially Observed Markov Decision Processï¼‰**ã®å®šç¾©ã‚‚å«ã‚ã¾ã™ã€‚
The MDP definition can be extended to the partially observed setting in the following manner:  
MDPã®å®šç¾©ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«éƒ¨åˆ†çš„ã«è¦³æ¸¬ã•ã‚ŒãŸè¨­å®šã«æ‹¡å¼µã§ãã¾ã™ã€‚

---

Definition 2 (Partially observed Markov decision process). The partially observed Markov _decision process is defined as a tuple M = {S, A, O, P, R,ğ›¾_ }, where S, A, P, R,ğ›¾ _are defined as_ _before, O is a set of observations, where each observation is given by ğ‘œ_ âˆˆO.  
å®šç¾©2ï¼ˆéƒ¨åˆ†çš„ã«è¦³æ¸¬ã•ã‚ŒãŸãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼‰ã€‚éƒ¨åˆ†çš„ã«è¦³æ¸¬ã•ã‚ŒãŸãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ã¯ã€ã‚¿ãƒ—ãƒ« $M = {S, A, O, P, R, \gamma}$ ã¨ã—ã¦å®šç¾©ã•ã‚Œã¾ã™ã€‚ã“ã“ã§ã€$S, A, P, R, \gamma$ ã¯å‰è¿°ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã€$O$ ã¯è¦³æ¸¬ã®é›†åˆã§ã‚ã‚Šã€å„è¦³æ¸¬ã¯ $o \in O$ ã«ã‚ˆã£ã¦ä¸ãˆã‚‰ã‚Œã¾ã™ã€‚

---

The ultimate objective within a RL problem is to acquire a policy, denoted as ğœ‹, which establishes a probability distribution over actions conditioned upon states, ğœ‹ (ğ‘ğ‘¡ |ğ‘ ğ‘¡ ), or alternatively conditioned upon observations within the context of partially observed scenarios, ğœ‹ (ğ‘ğ‘¡ |ğ‘œğ‘¡ ).  
RLå•é¡Œã«ãŠã‘ã‚‹æœ€çµ‚çš„ãªç›®çš„ã¯ã€ãƒãƒªã‚·ãƒ¼ $\pi$ ã‚’å–å¾—ã™ã‚‹ã“ã¨ã§ã‚ã‚Šã€ã“ã‚Œã¯çŠ¶æ…‹ã«æ¡ä»¶ä»˜ã‘ã‚‰ã‚ŒãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«é–¢ã™ã‚‹ç¢ºç‡åˆ†å¸ƒ $\pi (a_t | s_t)$ ã¾ãŸã¯éƒ¨åˆ†çš„ã«è¦³æ¸¬ã•ã‚ŒãŸã‚·ãƒŠãƒªã‚ªã®æ–‡è„ˆã§è¦³æ¸¬ã«æ¡ä»¶ä»˜ã‘ã‚‰ã‚ŒãŸ $\pi (a_t | o_t)$ ã‚’ç¢ºç«‹ã—ã¾ã™ã€‚
From these definitions, we can derive the trajectory distribution. 
ã“ã‚Œã‚‰ã®å®šç¾©ã‹ã‚‰ã€è»Œé“åˆ†å¸ƒã‚’å°å‡ºã§ãã¾ã™ã€‚
A trajectory in this context refers to a sequence encompassing both states and actions, spanning a length of _ğ‘‡_, represented as ğœ = {ğ‘ 0,ğ‘0, Â· Â· Â·,ğ‘ ğ‘‡ _,ğ‘ğ‘‡_ }.  
ã“ã®æ–‡è„ˆã«ãŠã‘ã‚‹è»Œé“ã¯ã€çŠ¶æ…‹ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ä¸¡æ–¹ã‚’å«ã‚€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æŒ‡ã—ã€é•·ã• $T$ ã«ã‚ãŸã‚Šã€$\tau = {s_0, a_0, \cdots, s_T, a_T}$ ã¨è¡¨ã•ã‚Œã¾ã™ã€‚
It is noteworthy that the parameter ğ‘‡ can be an infinite value, implying the consideration of scenarios with an indefinite time horizon, as seen in infinite horizon MDP [44].  
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $T$ ã¯ç„¡é™ã®å€¤ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€ç„¡é™ãƒ›ãƒ©ã‚¤ã‚ºãƒ³MDP [44] ã«è¦‹ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ã€ç„¡æœŸé™ã®æ™‚é–“ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã‚’æŒã¤ã‚·ãƒŠãƒªã‚ªã‚’è€ƒæ…®ã™ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚
The trajectory distribution ğ‘ğœ‹ for a given MDP tuple M and policy ğœ‹ is given by  
ä¸ãˆã‚‰ã‚ŒãŸMDPã‚¿ãƒ—ãƒ«Mã¨ãƒãƒªã‚·ãƒ¼ $\pi$ ã®è»Œé“åˆ†å¸ƒ $p_{\pi}$ ã¯æ¬¡ã®ã‚ˆã†ã«ä¸ãˆã‚‰ã‚Œã¾ã™ã€‚

$$
p_\pi(\tau) = d_0(s_0) \prod_{t=0}^{T} \pi(a_t | s_t) P(s_{t+1} | s_t, a_t),
\tag{1}
$$  

(ãƒ¡ãƒ¢: $p(\tau)$ ã¯2*Tå€‹ã®ç¢ºç‡å¤‰æ•°ã®åŒæ™‚åˆ†å¸ƒã¿ãŸã„ãªã‚¤ãƒ¡ãƒ¼ã‚¸...!! ãªã®ã§ ç©ã®å½¢ã§è¡¨ã•ã‚Œã¦ã‚‹...!!:thinking:)

where ğ‘‘_0(ğ‘ _0) represents the initial state distribution. 
ã“ã“ã§ã€ $d_0(s_0)$ ã¯åˆæœŸçŠ¶æ…‹åˆ†å¸ƒã‚’è¡¨ã—ã¾ã™ã€‚
This definition can easily be extended into the partially observed setting by including the observations ğ‘œğ‘¡ . 
ã“ã®å®šç¾©ã¯ã€è¦³æ¸¬ $o_t$ ã‚’å«ã‚ã‚‹ã“ã¨ã§ã€éƒ¨åˆ†çš„ã«è¦³æ¸¬ã•ã‚ŒãŸè¨­å®šã«ç°¡å˜ã«æ‹¡å¼µã§ãã¾ã™ã€‚
The RL objective ğ½ (ğœ‹), can then be written as an expectation under this trajectory distribution:  
RLã®ç›®çš„ $J(\pi)$ ã¯ã€ã“ã®è»Œé“åˆ†å¸ƒã®ä¸‹ã§ã®æœŸå¾…å€¤ã¨ã—ã¦æ¬¡ã®ã‚ˆã†ã«æ›¸ãã“ã¨ãŒã§ãã¾ã™ã€‚

$$
J(\pi) = E_{\tau \sim p_\pi(\tau)} \left[ \sum_{t=0}^{T} \gamma^t R(s_t, a_t) \right].
\tag{2}
$$

(ãƒ¡ãƒ¢: æœŸå¾…å€¤ã®ä¸­èº«ã¯ã€å‰²å¼•å ±é…¬ã®å’Œ! $R(s_t, a_t)$ ã¯å³æ™‚å ±é…¬...!!:thinking:)

<!-- ã“ã“ã¾ã§èª­ã‚“ã  -->

**2.2** **On-Policy and Off-Policy Learning**  
**2.2** **ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼ã¨ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å­¦ç¿’**

While the process of interaction unfolds, gathering additional episodes enhances the precision of the function estimates. 
**ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒé€²è¡Œã™ã‚‹é–“ã€è¿½åŠ ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’åé›†ã™ã‚‹ã“ã¨ã§é–¢æ•°æ¨å®šã®ç²¾åº¦ãŒå‘ä¸Šã—**ã¾ã™ã€‚
Nevertheless, a challenge arises. 
ã—ã‹ã—ã€èª²é¡ŒãŒç”Ÿã˜ã¾ã™ã€‚
If the policy improvement algorithm consistently adjusts the policy in a greedy mannerâ€”prioritizing actions with immediate rewardsâ€” then actions and states lying outside this advantageous route might not be adequately sampled. 
ãƒãƒªã‚·ãƒ¼æ”¹å–„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒå¸¸ã«è²ªæ¬²ãªæ–¹æ³•ã§ãƒãƒªã‚·ãƒ¼ã‚’èª¿æ•´ã—ã€å³æ™‚å ±é…¬ã®ã‚ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å„ªå…ˆã™ã‚‹å ´åˆã€ã“ã®æœ‰åˆ©ãªãƒ«ãƒ¼ãƒˆã®å¤–ã«ã‚ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚„çŠ¶æ…‹ã¯ååˆ†ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚(æ´»ç”¨ã°ã£ã‹ã‚Šã ã¨å›°ã‚‹ã­ã£ã¦è©±ã­!!:thinking:)
Consequently, superior rewards that could exist in these unexplored areas remain concealed from the learning process. 
ãã®çµæœã€ã“ã‚Œã‚‰ã®æœªæ¢ç´¢ã®é ˜åŸŸã«å­˜åœ¨ã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹å„ªã‚ŒãŸå ±é…¬ã¯ã€å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰éš ã•ã‚ŒãŸã¾ã¾ã«ãªã‚Šã¾ã™ã€‚
Fundamentally, we confront a decision between opting for the optimal choice based on existing data or delving deeper into exploration to collect more data. 
åŸºæœ¬çš„ã«ã€æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦æœ€é©ãªé¸æŠã‚’ã™ã‚‹ã‹ã€ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã™ã‚‹ãŸã‚ã«æ¢ç´¢ã‚’æ·±ã‚ã‚‹ã‹ã®æ±ºå®šã«ç›´é¢ã—ã¾ã™ã€‚
This predicament is commonly recognized as the Exploration vs. Exploitation Dilemma. 
ã“ã®ã‚¸ãƒ¬ãƒ³ãƒã¯ã€**æ¢ç´¢ã¨æ´»ç”¨ã®ã‚¸ãƒ¬ãƒ³ãƒ**ã¨ã—ã¦ä¸€èˆ¬çš„ã«èªè­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚

What we need is a middle ground between these two extremes. 
ç§ãŸã¡ãŒå¿…è¦ã¨ã™ã‚‹ã®ã¯ã€ã“ã‚Œã‚‰äºŒã¤ã®æ¥µç«¯ã®é–“ã®ä¸­é–“ç‚¹ã§ã™ã€‚
Pure exploration would require a significant amount of time to collect the necessary information, while pure exploitation could trap the agent in a local reward maximum. 
ç´”ç²‹ãªæ¢ç´¢ã¯ã€å¿…è¦ãªæƒ…å ±ã‚’åé›†ã™ã‚‹ãŸã‚ã«å¤šãã®æ™‚é–“ã‚’è¦ã—ã€ç´”ç²‹ãªæ´»ç”¨ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å±€æ‰€çš„ãªå ±é…¬ã®æœ€å¤§å€¤ã«é–‰ã˜è¾¼ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
To address this, there are two approaches that ensure all actions are adequately sampled, known as on-policy and off-policy methods. 
ã“ã‚Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€**ã™ã¹ã¦ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒé©åˆ‡ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ã“ã¨ã‚’ä¿è¨¼ã™ã‚‹äºŒã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€ã™ãªã‚ã¡ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼ã¨ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼æ‰‹æ³•**ãŒã‚ã‚Šã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã  -->

On-policy methods resolve the exploration vs. exploitation dilemma by incorporating randomness_ through a soft policy. 
ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼æ‰‹æ³•ã¯ã€ã‚½ãƒ•ãƒˆãƒãƒªã‚·ãƒ¼ã‚’é€šã˜ã¦ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’å–ã‚Šå…¥ã‚Œã‚‹ã“ã¨ã«ã‚ˆã‚Šã€æ¢ç´¢ã¨æ´»ç”¨ã®ã‚¸ãƒ¬ãƒ³ãƒã‚’è§£æ±ºã—ã¾ã™ã€‚(soft policy ã£ã¦ã®ã¯ã€æ¢ç´¢è¦ç´ ã‚’å–ã‚Šå…¥ã‚ŒãŸç¢ºç‡çš„æ–¹ç­–ã£ã¦æ„å‘³ãªã®ã‹ãª??åˆã‚ã¦èã„ãŸã€‚æ¬¡ã®è¡Œã§ã¾ã•ã«ãã†ã£ã¦æ›¸ã„ã¦ãŸ:thinking:)
This means that non-greedy actions are chosen with some probability. 
ã“ã‚Œã¯ã€**éè²ªæ¬²ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒä¸€å®šã®ç¢ºç‡ã§é¸æŠã•ã‚Œã‚‹ã“ã¨**ã‚’æ„å‘³ã—ã¾ã™ã€‚
These policies are referred to as ğœ–-greedy policies because they select random actions with a probability of ğœ– and follow the optimal action with a probability of 1-ğœ–. 
ã“ã‚Œã‚‰ã®ãƒãƒªã‚·ãƒ¼ã¯ã€ç¢ºç‡ğœ–ã§ãƒ©ãƒ³ãƒ€ãƒ ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã—ã€ç¢ºç‡1-ğœ–ã§æœ€é©ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«å¾“ã†ãŸã‚ã€ğœ–-è²ªæ¬²ãƒãƒªã‚·ãƒ¼ã¨å‘¼ã°ã‚Œã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã  -->

Since the probability of randomly selecting an action from the action space is ğœ–, the probability of choosing any specific non-optimal action is ğœ–/|A(ğ‘ )|. 
ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã™ã‚‹ç¢ºç‡ãŒğœ–ã§ã‚ã‚‹ãŸã‚ã€ç‰¹å®šã®éæœ€é©ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã™ã‚‹ç¢ºç‡ã¯ $\epsilon / |A(s)|$ ã§ã™ã€‚
On the other hand, the probability of following the optimal action will always be slightly higher due to the 1 - ğœ– probability of selecting it outright and the ğœ–/|A(ğ‘ )| probability of choosing it through sampling the action space:  
ä¸€æ–¹ã§ã€æœ€é©ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«å¾“ã†ç¢ºç‡ã¯ã€ç›´æ¥é¸æŠã™ã‚‹ç¢ºç‡1 - ğœ–ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦é¸æŠã™ã‚‹ç¢ºç‡ğœ–/|A(ğ‘ )|ã®ãŸã‚ã€å¸¸ã«ã‚ãšã‹ã«é«˜ããªã‚Šã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã  -->

_Off-policy methods offer a different solution to the exploration vs. exploitation problem. 
ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼æ‰‹æ³•ã¯ã€æ¢ç´¢ã¨æ´»ç”¨ã®å•é¡Œã«å¯¾ã™ã‚‹ç•°ãªã‚‹è§£æ±ºç­–ã‚’æä¾›ã—ã¾ã™ã€‚
While_ on-policy algorithms attempt to improve the same ğœ–-greedy policy used for exploration, off-policy approaches utilize two distinct policies: a behavior policy and a target policy. 
ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒæ¢ç´¢ã«ä½¿ç”¨ã•ã‚Œã‚‹åŒã˜ğœ–-è²ªæ¬²ãƒãƒªã‚·ãƒ¼ã‚’æ”¹å–„ã—ã‚ˆã†ã¨ã™ã‚‹ä¸€æ–¹ã§ã€ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€**è¡Œå‹•ãƒãƒªã‚·ãƒ¼ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒªã‚·ãƒ¼ã¨ã„ã†äºŒã¤ã®ç•°ãªã‚‹ãƒãƒªã‚·ãƒ¼ã‚’åˆ©ç”¨**ã—ã¾ã™ã€‚
The behavioral policy (denoted as ğœ‹ğ›½ ) is employed for exploration and episode generation, while the target or goal policy (denoted as ğœ‹) is used for function estimation and improvement. 
è¡Œå‹•ãƒãƒªã‚·ãƒ¼ï¼ˆğœ‹ğ›½ã¨è¡¨è¨˜ï¼‰ã¯æ¢ç´¢ã¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆã«ä½¿ç”¨ã•ã‚Œã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¾ãŸã¯ç›®æ¨™ãƒãƒªã‚·ãƒ¼ï¼ˆğœ‹ã¨è¡¨è¨˜ï¼‰ã¯é–¢æ•°æ¨å®šã¨æ”¹å–„ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

The efficacy of this approach lies in the capacity of the target policy ğœ‹ to attain a balanced perspective of the environment, enabling it to assimilate insights from the behavioral policy _ğ‘, while_ concurrently capturing advantageous actions and seeking further improvements. 
ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æœ‰åŠ¹æ€§ã¯ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒªã‚·ãƒ¼ $\pi$ ãŒç’°å¢ƒã®ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸè¦–ç‚¹ã‚’å¾—ã‚‹èƒ½åŠ›ã«ã‚ã‚Šã€è¡Œå‹•ãƒãƒªã‚·ãƒ¼ $\pi_{\beta}$ ã‹ã‚‰ã®æ´å¯Ÿã‚’å–ã‚Šå…¥ã‚Œã¤ã¤ã€æœ‰åˆ©ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ‰ãˆã€ã•ã‚‰ãªã‚‹æ”¹å–„ã‚’è¿½æ±‚ã—ã¾ã™ã€‚
Nevertheless, it is imperative to acknowledge that in off-policy learning, a distributional discrepancy arises between the target policy estimation and the sampled policy. 
ã—ã‹ã—ã€**ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å­¦ç¿’ã«ãŠã„ã¦ã¯ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒªã‚·ãƒ¼ã®æ¨å®šã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒãƒªã‚·ãƒ¼ã®é–“ã«åˆ†å¸ƒã®ä¸ä¸€è‡´ãŒç”Ÿã˜ã‚‹ã“ã¨ã‚’èªè­˜ã™ã‚‹ã“ã¨ãŒé‡è¦**ã§ã™ã€‚(ã†ã‚“ã†ã‚“:thinking:)
Consequently, a widely employed technique known as importance sampling is applied to address this disparity [28]. 
ãã®çµæœã€ã“ã®ä¸ä¸€è‡´ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨å‘¼ã°ã‚Œã‚‹åºƒãä½¿ç”¨ã•ã‚Œã‚‹æ‰‹æ³•ãŒé©ç”¨ã•ã‚Œã¾ã™[28]ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

**2.3** **Typical RL algorithms**  
**2.3** **å…¸å‹çš„ãªRLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **

Letâ€™s briefly outline various types of RL algorithms and present their definitions. 
ã•ã¾ã–ã¾ãªã‚¿ã‚¤ãƒ—ã®RLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç°¡å˜ã«æ¦‚èª¬ã—ã€ãã‚Œã‚‰ã®å®šç¾©ã‚’ç¤ºã—ã¾ã™ã€‚
At a high level, all standard RL algorithms follow a common learning loop: the agent engages with the MDP using M a behavior policy ğœ‹ğ›½ . 
**é«˜ã„ãƒ¬ãƒ™ãƒ«ã§ã¯ã€ã™ã¹ã¦ã®æ¨™æº–RLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯å…±é€šã®å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã«å¾“ã„ã¾ã™**ï¼šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯è¡Œå‹•ãƒãƒªã‚·ãƒ¼ $\pi_{\beta}$ ã‚’ä½¿ç”¨ã—ã¦MDPã¨é–¢ä¸ã—ã¾ã™ã€‚
This behavior policy, which could or could not align with ğœ‹ (ğ‘|ğ‘ ), leads the agent to observe the current state ğ‘ ğ‘¡, choose an action ğ‘ğ‘¡, and then witness the subsequent state _ğ‘ ğ‘¡_ +1 and the reward value ğ‘Ÿğ‘¡ = R(ğ‘ ğ‘¡,ğ‘ğ‘¡ ). 
ã“ã®è¡Œå‹•ãƒãƒªã‚·ãƒ¼ã¯ã€$\pi (a|s)$ ã¨ä¸€è‡´ã™ã‚‹å ´åˆã‚‚ã‚ã‚Œã°ã—ãªã„å ´åˆã‚‚ã‚ã‚Šã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ç¾åœ¨ã®çŠ¶æ…‹ $s_t$ ã‚’è¦³å¯Ÿã—ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ $a_t$ ã‚’é¸æŠã—ã€ãã®å¾Œã®çŠ¶æ…‹ $s_{t+1}$ ã¨å ±é…¬å€¤ $r_t = R(s_t, a_t)$ ã‚’ç›®æ’ƒã—ã¾ã™ã€‚
This sequence can repeat over multiple steps, allowing the agent to gather transitions {ğ‘ ğ‘¡,ğ‘ğ‘¡,ğ‘ ğ‘¡ +1,ğ‘Ÿğ‘¡ }. 
ã“ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¯è¤‡æ•°ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ç¹°ã‚Šè¿”ã™ã“ã¨ãŒã§ãã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯é·ç§» { $s_t, a_t, s_{t+1}, r_t$ } ã‚’åé›†ã§ãã¾ã™ã€‚
These observed transitions are then used by the agent to adjust its policy, and this update process might incorporate earlier observed transitions as well. 
ã“ã‚Œã‚‰ã®è¦³å¯Ÿã•ã‚ŒãŸé·ç§»ã¯ã€**ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã£ã¦ãƒãƒªã‚·ãƒ¼ã‚’èª¿æ•´ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œ**ã€ã“ã®æ›´æ–°ãƒ—ãƒ­ã‚»ã‚¹ã«ã¯ä»¥å‰ã«è¦³å¯Ÿã•ã‚ŒãŸé·ç§»ã‚‚å«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
Weâ€™ll denote the set of available transitions for policy updating as D = {(ğ‘ ğ‘¡,ğ‘ğ‘¡,ğ‘ ğ‘¡ +1,ğ‘Ÿğ‘¡ )}. 
ãƒãƒªã‚·ãƒ¼æ›´æ–°ã®ãŸã‚ã®åˆ©ç”¨å¯èƒ½ãªé·ç§»ã®é›†åˆã‚’ $D = {(s_t, a_t, s_{t+1}, r_t)}$ ã¨è¡¨ã—ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã  -->

**Q-learning [51] is an off-policy value-based learning scheme aimed at finding a target policy** that selects the best action:  
Q-learning [51]ã¯ã€æœ€é©ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã™ã‚‹ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒªã‚·ãƒ¼ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ãŸã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ã®value-basedãªå­¦ç¿’ã‚¹ã‚­ãƒ¼ãƒ ã§ã™ã€‚
(value-basedã£ã¦ãªã‚“ã ã‚??:thinking:)

$$
\pi(s) = \arg \max_a Q_\pi(s, a)
\tag{3}
$$  


Here, ğ‘„ğ‘¢ (ğ‘ ,ğ‘) represents the Q-value and applies to a discrete action space. 
ã“ã“ã§ã€ $Q_{u}(s, a)$ ã¯Q-valueã‚’è¡¨ã—ã€é›¢æ•£ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“ã«é©ç”¨ã•ã‚Œã¾ã™ã€‚
For deterministic policies, the Q-value can be computed as:  
æ±ºå®šè«–çš„ãƒãƒªã‚·ãƒ¼ã®å ´åˆã€Qå€¤ã¯æ¬¡ã®ã‚ˆã†ã«è¨ˆç®—ã§ãã¾ã™ã€‚

$$
Q(s_t, a_t) = E_{\tau \sim \pi} \left[ r(s_t, a_t) + \gamma Q(s_{t+1}, a_{t+1}) \right].
\tag{4}
$$  

Deep Q learning (DQN) [38] employs deep learning to approximate a nonlinear Q function parameterized by ğœƒğ‘: ğ‘„ğœƒğ‘ (ğ‘ ,ğ‘). 
**Deep Qå­¦ç¿’ï¼ˆDQNï¼‰[38]ã¯ã€æ·±å±¤å­¦ç¿’ã‚’ç”¨ã„ã¦ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta_q$ ã«ã‚ˆã£ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚ŒãŸéç·šå½¢Qé–¢æ•° $Q_{\theta_q} (s, a)$ ã‚’è¿‘ä¼¼**ã—ã¾ã™ã€‚
(policy-valueæœŸå¾…å€¤ã®äºˆæ¸¬å€¤ãŒä¸€ç•ªé«˜ããªã‚‹ã‚ˆã†ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸ã¶ = Qå­¦ç¿’?? :thinking:)
DQN involves a network ğ‘„ğœƒğ‘ thatâ€™s updated asynchronously by minimizing the Mean Squared Error (MSE) as defined by:  
DQNã¯ã€å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMSEï¼‰ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦éåŒæœŸã«æ›´æ–°ã•ã‚Œã‚‹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ $Q_{\theta_q}$ ã‚’å«ã¿ã¾ã™ã€‚

$$
L(\theta_q) = E_{\tau \sim \pi} \left[ Q_{\theta_q}(s_t, a_t) - (r(s_t, a_t) + \gamma Q_{\theta_q}(s_{t+1}, a_{t+1})) \right]^2.
\tag{5}
$$

In this equation, ğœ signifies a sampled trajectory including (ğ‘ ,ğ‘,ğ‘ [â€²],ğ‘Ÿ (ğ‘ ,ğ‘)). 
ã“ã®æ–¹ç¨‹å¼ã§ã¯ã€ $\tau$ ã¯ ( $s, a, s', r(s, a)$ ) ã‚’å«ã‚€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸè»Œé“ã‚’ç¤ºã—ã¾ã™ã€‚
Notably, ğ‘ ğ‘¡[â€²] [and][ ğ‘]ğ‘¡[â€²] originate from the behavior policy ğœ‹ğ‘, while ğ‘ ,ğ‘ come from the target policy ğœ‹. 
ç‰¹ã«ã€ $s_{t}'$ ã¨ $a_{t}'$ ã¯è¡Œå‹•ãƒãƒªã‚·ãƒ¼ $\pi_{b}$ ã‹ã‚‰æ´¾ç”Ÿã—ã€ $s, a$ ã¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒªã‚·ãƒ¼ $\pi$ ã‹ã‚‰æ¥ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã  -->

Furthermore, the concept of **value functions** plays a role. 
ã•ã‚‰ã«ã€ä¾¡å€¤é–¢æ•°ã®æ¦‚å¿µãŒå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚
(value function = æ–¹ç­–æ€§èƒ½ã®äºˆæ¸¬é–¢æ•° = Qé–¢æ•°ã€ã¿ãŸã„ãªã‚¤ãƒ¡ãƒ¼ã‚¸??:thinking:)
These assess states and actions. 
ã“ã‚Œã‚‰ã¯çŠ¶æ…‹ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
The value function ğ‘‰ğœ‹ (ğ‘ ) evaluates states, and ğ‘„ğœ‹ (ğ‘ ğ‘¡,ğ‘ğ‘¡ ) evaluates actions. 
ä¾¡å€¤é–¢æ•° $V_{\pi} (s)$ ã¯çŠ¶æ…‹ã‚’è©•ä¾¡ã—ã€ $Q_{\pi} (s_t, a_t)$ ã¯ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
The relationship between them is given by:  
ãã‚Œã‚‰ã®é–¢ä¿‚ã¯æ¬¡ã®ã‚ˆã†ã«ç¤ºã•ã‚Œã¾ã™ã€‚

$$
V_\pi(s) = E_{a \sim \pi} [Q_\pi(s, a)].
\tag{6}
$$  

The value function gets updated via the Temporal Difference (TD) method:  
ä¾¡å€¤é–¢æ•°ã¯æ™‚é–“å·®ï¼ˆTDï¼‰æ³•ã«ã‚ˆã£ã¦æ›´æ–°ã•ã‚Œã¾ã™ã€‚

$$
V_\pi(s_t) \leftarrow V_\pi(s_t) + \alpha \left[ r(s_{t+1}, a_{t+1}) + \gamma V_\pi(s_{t+1}) - V_\pi(s_t) \right].
\tag{7}
$$  

where ğ›¼ represents the learning rate.  
ã“ã“ã§ã€ğ›¼ã¯å­¦ç¿’ç‡ã‚’è¡¨ã—ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã  -->

**Policy gradient [52] is a technique used in reinforcement learning that tackles scenarios** where actions are high-dimensional or continuousâ€”something not easily managed by Q-learning.  
ãƒãƒªã‚·ãƒ¼å‹¾é…[52]ã¯ã€**ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒé«˜æ¬¡å…ƒã¾ãŸã¯é€£ç¶šã§ã‚ã‚‹ã‚·ãƒŠãƒªã‚ªã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«å¼·åŒ–å­¦ç¿’ã§ä½¿ç”¨ã•ã‚Œã‚‹æ‰‹æ³•**ã§ã™ã€‚ã“ã‚Œã¯Qå­¦ç¿’ã§ã¯ç°¡å˜ã«ç®¡ç†ã§ããªã„ã‚‚ã®ã§ã™ã€‚(ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒé«˜æ¬¡å…ƒã£ã¦ã€ãƒ™ã‚¯ãƒˆãƒ«ã£ã½ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã£ã¦ã“ã¨??:thinking:)
Unlike Q-learning, which focuses on finding optimal actions, policy gradient aims to find optimal parameters ğœƒ for a policy ğœ‹ğœƒ in order to maximize the total reward.  
æœ€é©ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹Qå­¦ç¿’ã¨ã¯ç•°ãªã‚Šã€ãƒãƒªã‚·ãƒ¼å‹¾é…ã¯ã€**ç·å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã«ãƒãƒªã‚·ãƒ¼ $\pi_{\theta}$ ã®æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã‚’ç›®çš„**ã¨ã—ã¦ã„ã¾ã™ã€‚(ã†ã‚“ã†ã‚“...!:thinking:)
The central goal of policy gradient is to maximize the expected return, or accumulated reward, starting from the initial state.  
ãƒãƒªã‚·ãƒ¼å‹¾é…ã®ä¸­å¿ƒçš„ãªç›®æ¨™ã¯ã€åˆæœŸçŠ¶æ…‹ã‹ã‚‰å§‹ã¾ã‚‹æœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³ã¾ãŸã¯ç´¯ç©å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã§ã™ã€‚
This is captured by the equation:  
ã“ã‚Œã¯æ¬¡ã®æ–¹ç¨‹å¼ã§è¡¨ã•ã‚Œã¾ã™ã€‚

$$
J(\pi_{\theta}) = E_{\tau \sim \pi_{\theta}} [r(\tau)]
= \inf \pi_{\theta}(\tau) r(\tau) d\tau.
\tag{8}
$$

Here, ğœ‹ğœƒ (ğœ) signifies the probability of observing trajectory ğœ.  
ã“ã“ã§ã€ $\pi_{\theta} (\tau)$ ã¯è»Œé“ $\tau$ ã‚’è¦³å¯Ÿã™ã‚‹ç¢ºç‡ã‚’æ„å‘³ã—ã¾ã™ã€‚(=è»Œè·¡ã¤ã¾ã‚Šã‚¢ã‚¯ã‚·ãƒ§ãƒ³é…åˆ—ã®é¸æŠç¢ºç‡åˆ†å¸ƒã¿ãŸã„ãªã‚¤ãƒ¡ãƒ¼ã‚¸...!:thinking:)
The technique learns the optimal parameter ğœƒ by computing the gradient âˆ‡ğœƒ _ğ½_ (ğœ‹ğœƒ ) as follows:  
ã“ã®æ‰‹æ³•ã¯ã€æ¬¡ã®ã‚ˆã†ã«å‹¾é… $\nabla_\theta J(\pi_\theta)$ ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’å­¦ç¿’ã—ã¾ã™ã€‚

$$
\nabla_\theta J(\pi_\theta) = E_{\tau \sim d_\pi} [\sum_{t=1}^{T} r(s_t, a_t) \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(s_t, a_t)].
\tag{9}
$$

In the above equation, ğ‘‘ğœ‹ğœƒ is the distribution of trajectories generated by policy ğœ‹ğœƒ.  
ä¸Šè¨˜ã®æ–¹ç¨‹å¼ã§ã¯ã€ $d_{\pi_\theta}$ ã¯ãƒãƒªã‚·ãƒ¼ $\pi_\theta$ ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸè»Œé“ã®åˆ†å¸ƒã§ã™ã€‚

The derivation involves the substitution:  
å°å‡ºã«ã¯æ¬¡ã®ç½®æ›ãŒå«ã¾ã‚Œã¾ã™ã€‚

$$
\pi_\theta(\tau) = p(s_1) \prod_{t=1}^{T} \pi_\theta(s_t, a_t) p(s_{t+1}|s_t, a_t).
$$
(è»Œè·¡ $tau$ ã¯çŠ¶æ…‹sã¨é¸ã°ã‚Œã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³aã®åŒæ™‚ç¢ºç‡ã ã‹ã‚‰ã€prodè¨˜å·ã‚’ç”¨ã„ãŸç¢ºç‡è³ªé‡ã®ç©ã§è¡¨ã•ã‚Œã‚‹!:thinking:)

Here, ğ‘ (Â·) is independent of the policy parameter ğœƒ, and for simplicity, itâ€™s not explicitly included in the derivation.  
ã“ã“ã§ã€$p(Â·)$ ã¯ãƒãƒªã‚·ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã¨ã¯ç‹¬ç«‹ã—ãŸç¢ºç‡åˆ†å¸ƒã§ã‚ã‚Šã€ç°¡å˜ã®ãŸã‚ã«å°å‡ºã«ã¯æ˜ç¤ºçš„ã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚

Prior policy gradient algorithms, like REINFORCE, have often used Monte-Carlo sampling to estimate ğœ from ğ‘‘ğœ‹ğœƒ.  
ä»¥å‰ã®ãƒãƒªã‚·ãƒ¼å‹¾é…ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ä¾‹ãˆã°REINFORCEã¯ã€ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ $d_{\pi_\theta}$ ã‹ã‚‰ $\tau$ ã‚’æ¨å®šã™ã‚‹ã“ã¨ãŒã‚ˆãã‚ã‚Šã¾ã—ãŸã€‚(REINFORCEã¯å‹¾é…ãƒ™ãƒ¼ã‚¹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãªã®ã‹...!:thinking:)

<!-- ã“ã“ã¾ã§èª­ã‚“ã  -->

**Actor-critic networks bring together the strengths of both Q-learning and policy gradient** techniques.  
**ã‚¢ã‚¯ã‚¿ãƒ¼-ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯**ã¯ã€Qå­¦ç¿’ã¨ãƒãƒªã‚·ãƒ¼å‹¾é…ã®ä¸¡æ–¹ã®å¼·ã¿ã‚’çµ„ã¿åˆã‚ã›ã¾ã™ã€‚(actor-criticãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆã‚ã¦èã„ãŸ...!:thinking:)
They can function as either on-policy methods [26] or off-policy methods [12].  
ã“ã‚Œã‚‰ã¯ã€ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼æ‰‹æ³•[26]ã¾ãŸã¯ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼æ‰‹æ³•[12]ã¨ã—ã¦æ©Ÿèƒ½ã§ãã¾ã™ã€‚
An actor-critic network is composed of two key components:  
ã‚¢ã‚¯ã‚¿ãƒ¼-ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€**äºŒã¤ã®ä¸»è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ**ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚

- The actor: This component optimizes the policy ğœ‹ğœƒ based on the guidance provided by âˆ‡ğœƒ _ğ½_ (ğœ‹ğœƒ ).  
  - ã‚¢ã‚¯ã‚¿ãƒ¼ï¼šã“ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ã€$\nabla_\theta J(\pi_\theta)$ ã«ã‚ˆã£ã¦æä¾›ã•ã‚Œã‚‹ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã«åŸºã¥ã„ã¦ãƒãƒªã‚·ãƒ¼ $\pi_\theta$ ã‚’æœ€é©åŒ–ã—ã¾ã™ã€‚
- The critic: The critic evaluates the learned policy ğœ‹ğœƒ using the Q-value function ğ‘„ğœƒğ‘ (ğ‘ ,ğ‘).  
  - ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ï¼šã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã¯ã€Qå€¤é–¢æ•° $Q_{\theta_q} (s, a)$ ã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ã•ã‚ŒãŸãƒãƒªã‚·ãƒ¼ $\pi_\theta$ ã‚’è©•ä¾¡ã—ã¾ã™ã€‚

The overall gradient expression for an actor-critic network is as follows:  
ã‚¢ã‚¯ã‚¿ãƒ¼-ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¨ä½“çš„ãªå‹¾é…è¡¨ç¾ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

$$
E_{s \sim d_\pi_{\theta}} [Q_{\theta_q}(s, a) \nabla_\theta \log \pi_\theta(s, a)].
$$  

In the case of off-policy learning, the value function for ğœ‹ğœƒ (ğ‘|ğ‘ ) can be further defined using deterministic policy gradient (DPG):  
ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å­¦ç¿’ã®å ´åˆã€ $\pi_\theta (a|s)$ ã®ä¾¡å€¤é–¢æ•°ã¯ã€æ±ºå®šè«–çš„ãƒãƒªã‚·ãƒ¼å‹¾é…ï¼ˆDPGï¼‰ã‚’ä½¿ç”¨ã—ã¦ã•ã‚‰ã«å®šç¾©ã§ãã¾ã™ã€‚

$$
E_{s \sim d_\pi_{\theta}} [\nabla_{a} Q_{\theta_q}(s, a) | a= \pi \theta(s) \nabla_\theta \pi_\theta(s, a)].
\tag{12}
$$

Itâ€™s worth noting that while traditional policy gradient calculations involve integrating over both the state space and the action space, DPG only requires integrating over the state space.  
ä¼çµ±çš„ãªãƒãƒªã‚·ãƒ¼å‹¾é…è¨ˆç®—ãŒçŠ¶æ…‹ç©ºé–“ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“ã®ä¸¡æ–¹ã«ã‚ãŸã£ã¦ç©åˆ†ã™ã‚‹ã®ã«å¯¾ã—ã€DPGã¯çŠ¶æ…‹ç©ºé–“ã«ã‚ãŸã£ã¦ã®ã¿ç©åˆ†ã™ã‚‹ã“ã¨ãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã™ã‚‹ä¾¡å€¤ãŒã‚ã‚Šã¾ã™ã€‚
In S A S DPG, given a state ğ‘  âˆˆS, there corresponds only one action ğ‘ âˆˆA such that ğœ‡ğœƒ (ğ‘ ) = ğ‘.  
S A S DPGã«ãŠã„ã¦ã€çŠ¶æ…‹ğ‘  âˆˆSãŒä¸ãˆã‚‰ã‚Œã‚‹ã¨ã€å¯¾å¿œã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ğ‘ âˆˆAã¯ãŸã ä¸€ã¤ã§ã‚ã‚Šã€ğœ‡ğœƒ (ğ‘ ) = ğ‘ã¨ãªã‚Šã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! ã‚ˆãã‚ã‹ã£ã¦ãªã„ -->

**Model-based RL is a broad term encompassing methods that rely on explicit estimates of the** transition or dynamics function.  
**ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®RL**ã¯ã€é·ç§»ã¾ãŸã¯å‹•çš„é–¢æ•°ã®æ˜ç¤ºçš„ãªæ¨å®šã«ä¾å­˜ã™ã‚‹æ‰‹æ³•ã‚’å«ã‚€åºƒç¯„ãªç”¨èªã§ã™ã€‚
The distinguishing feature of model-based RL is that it assumes P the dynamics model is known and can be learned.  
ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®RLã®ç‰¹å¾´ã¯ã€å‹•çš„ãƒ¢ãƒ‡ãƒ«PãŒçŸ¥ã‚‰ã‚Œã¦ãŠã‚Šã€å­¦ç¿’å¯èƒ½ã§ã‚ã‚‹ã¨ä»®å®šã™ã‚‹ã“ã¨ã§ã™ã€‚
This is in contrast to other forms of RL where P such a dynamics model is neither known nor learnable.  
ã“ã‚Œã¯ã€ä»–ã®å½¢å¼ã®RLã«ãŠã„ã¦ã€Pã®ã‚ˆã†ãªå‹•çš„ãƒ¢ãƒ‡ãƒ«ãŒçŸ¥ã‚‰ã‚Œã¦ãŠã‚‰ãšã€å­¦ç¿’å¯èƒ½ã§ãªã„ã“ã¨ã¨å¯¾ç…§çš„ã§ã™ã€‚
å¼·åŒ–å­¦ç¿’
**2.4** **Offline RL**  
**2.4** **ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL**

The offline RL problem can be defined as a data-driven formulation of the RL problem [27].  
ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®å•é¡Œã¯ã€RLå•é¡Œã®ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹å®šå¼åŒ–ã¨ã—ã¦å®šç¾©ã§ãã¾ã™[27]ã€‚

The ultimate objective remains centered on optimizing the goal presented in Equation (2).  
æœ€çµ‚çš„ãªç›®çš„ã¯ã€æ–¹ç¨‹å¼ï¼ˆ2ï¼‰ã§ç¤ºã•ã‚ŒãŸç›®æ¨™ã‚’æœ€é©åŒ–ã™ã‚‹ã“ã¨ã«ã‚ã‚Šã¾ã™ã€‚

Notably, the agentâ€™s capacity to engage with the environment and amass supplementary transitions using the behavior policy is nullified.  
ç‰¹ã«ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç’°å¢ƒã¨é–¢ã‚ã‚Šã€è¡Œå‹•ãƒãƒªã‚·ãƒ¼ã‚’ä½¿ç”¨ã—ã¦è¿½åŠ ã®é·ç§»ã‚’è“„ç©ã™ã‚‹èƒ½åŠ›ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¾ã™ã€‚

Instead, the learning algorithm receives a fixed collection of transitions denoted as D = {ğ‘ ğ‘¡[ğ‘–][,ğ‘][ğ‘–]ğ‘¡[,ğ‘ ]ğ‘¡[ğ‘–] +1[,ğ‘Ÿ]ğ‘¡[ğ‘–] [}][, and its task is to acquire the most optimal policy using this provided] dataset.  
ä»£ã‚ã‚Šã«ã€å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯D = {ğ‘ ğ‘¡[ğ‘–][,ğ‘][ğ‘–]ğ‘¡[,ğ‘ ]ğ‘¡[ğ‘–] +1[,ğ‘Ÿ]ğ‘¡[ğ‘–]}ã¨ã—ã¦ç¤ºã•ã‚Œã‚‹å›ºå®šã•ã‚ŒãŸé·ç§»ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å—ã‘å–ã‚Šã€ã“ã®æä¾›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦æœ€é©ãªãƒãƒªã‚·ãƒ¼ã‚’å–å¾—ã™ã‚‹ã“ã¨ãŒãã®ã‚¿ã‚¹ã‚¯ã§ã™ã€‚

This approach aligns more closely with the supervised learning paradigm, and we can view as the training dataset for the policy.  
ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ç›£è¦–å­¦ç¿’ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã«ã‚ˆã‚Šå¯†æ¥ã«ä¸€è‡´ã—ã€ãƒãƒªã‚·ãƒ¼ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦è¦‹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

Fundamentally, offline RL necessitates that the learning algorithm comprehends the underlying dynamics of the MDP M solely from a fixed dataset.  
åŸºæœ¬çš„ã«ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã¯ã€å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒå›ºå®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ã®ã¿MDP Mã®åŸºç¤ã¨ãªã‚‹å‹•çš„ã‚’ç†è§£ã™ã‚‹ã“ã¨ã‚’å¿…è¦ã¨ã—ã¾ã™ã€‚

Subsequently, it must create a policy ğœ‹ (ğ‘|ğ‘ ) that achieves the highest cumulative reward during the interaction with the MDP.  
ãã®å¾Œã€MDPã¨ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ä¸­ã«æœ€é«˜ã®ç´¯ç©å ±é…¬ã‚’é”æˆã™ã‚‹ãƒãƒªã‚·ãƒ¼ğœ‹ (ğ‘|ğ‘ )ã‚’ä½œæˆã—ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚

We will denote the distribution over states and actions in D as ğœ‹ğ›½ (also referred to as the behavior policy).  
Dã«ãŠã‘ã‚‹çŠ¶æ…‹ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®åˆ†å¸ƒã‚’ğœ‹ğ›½ï¼ˆè¡Œå‹•ãƒãƒªã‚·ãƒ¼ã¨ã‚‚å‘¼ã°ã‚Œã¾ã™ï¼‰ã¨ã—ã¦è¡¨è¨˜ã—ã¾ã™ã€‚

Here, we assume that state-action pairs (ğ‘ ,ğ‘) âˆˆD are drawn from ğ‘  âˆ¼ _ğ‘‘[ğœ‹][ğ›½]_ (ğ‘ ), and actions are sampled according to the behavior policy, i.e., ğ‘ âˆ¼ _ğœ‹ğ›½_ (ğ‘|ğ‘ ).  
ã“ã“ã§ã¯ã€çŠ¶æ…‹-ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒšã‚¢(ğ‘ ,ğ‘) âˆˆDãŒğ‘  âˆ¼ _ğ‘‘[ğœ‹][ğ›½]_ (ğ‘ )ã‹ã‚‰å¼•ãå‡ºã•ã‚Œã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯è¡Œå‹•ãƒãƒªã‚·ãƒ¼ã«å¾“ã£ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ã¨ä»®å®šã—ã¾ã™ã€‚ã™ãªã‚ã¡ã€ğ‘ âˆ¼ _ğœ‹ğ›½_ (ğ‘|ğ‘ )ã§ã™ã€‚

This problem formulation has been expressed using a range of terminologies.  
ã“ã®å•é¡Œå®šå¼åŒ–ã¯ã€ã•ã¾ã–ã¾ãªç”¨èªã‚’ä½¿ç”¨ã—ã¦è¡¨ç¾ã•ã‚Œã¦ã„ã¾ã™ã€‚

Within the field of RS, the term that frequently induces confusion is â€œoff-policy RLâ€.  
RSã®åˆ†é‡ã«ãŠã„ã¦ã€ã—ã°ã—ã°æ··ä¹±ã‚’å¼•ãèµ·ã“ã™ç”¨èªã¯ã€Œã‚ªãƒ•ãƒãƒªã‚·ãƒ¼RLã€ã§ã™ã€‚

This phrase is commonly employed as a broad label encompassing all RL algorithms that can leverage datasets of transitions, wherein the actions in each transition were acquired using policies distinct from the current D policy ğœ‹ (ğ‘|ğ‘ ).  
ã“ã®ãƒ•ãƒ¬ãƒ¼ã‚ºã¯ã€é·ç§»ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ´»ç”¨ã§ãã‚‹ã™ã¹ã¦ã®RLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’åŒ…æ‹¬ã™ã‚‹åºƒç¯„ãªãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä¸€èˆ¬çš„ã«ä½¿ç”¨ã•ã‚Œã€å„é·ç§»ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯ç¾åœ¨ã®Dãƒãƒªã‚·ãƒ¼ğœ‹ (ğ‘|ğ‘ )ã¨ã¯ç•°ãªã‚‹ãƒãƒªã‚·ãƒ¼ã‚’ä½¿ç”¨ã—ã¦å–å¾—ã•ã‚Œã¾ã—ãŸã€‚

However, itâ€™s important to note that the term â€œoff-policyâ€ typically signifies an RL algorithm where the behavior policy ğœ‹ğ›½ differs from the target policy ğœ‹, as discussed earlier.  
ãŸã ã—ã€ã€Œã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ã€ã¨ã„ã†ç”¨èªã¯ã€é€šå¸¸ã€è¡Œå‹•ãƒãƒªã‚·ãƒ¼ğœ‹ğ›½ãŒã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒªã‚·ãƒ¼ğœ‹ã¨ç•°ãªã‚‹RLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ„å‘³ã™ã‚‹ã“ã¨ã«æ³¨æ„ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚

This distinction can sometimes cause confusion.  
ã“ã®åŒºåˆ¥ã¯æ™‚ã«æ··ä¹±ã‚’å¼•ãèµ·ã“ã™ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

Hence, the terms â€œfully off-policy RLâ€ or â€œoffline RLâ€ are recently introduced to indicate situations where no additional online data collection takes place.  
ã—ãŸãŒã£ã¦ã€ã€Œå®Œå…¨ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼RLã€ã¾ãŸã¯ã€Œã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã€ã¨ã„ã†ç”¨èªãŒæœ€è¿‘å°å…¥ã•ã‚Œã€è¿½åŠ ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†ãŒè¡Œã‚ã‚Œãªã„çŠ¶æ³ã‚’ç¤ºã—ã¾ã™ã€‚

We have presented various illustrations of distinct RL approaches to emphasize the disparities between them in Figure 2.  
ç§ãŸã¡ã¯ã€å›³2ã«ãŠã„ã¦ãã‚Œã‚‰ã®é–“ã®é•ã„ã‚’å¼·èª¿ã™ã‚‹ãŸã‚ã«ã€ç•°ãªã‚‹RLã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ã•ã¾ã–ã¾ãªä¾‹ã‚’ç¤ºã—ã¾ã—ãŸã€‚

rollout data {(st, at, st+1, rt )}  
ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆãƒ‡ãƒ¼ã‚¿ {(st, at, st+1, rt )}

Buffer  
ãƒãƒƒãƒ•ã‚¡

_Ï€t+1_  
_Ï€t+1_

**(a) on-policy RL4RS**  
**(a) ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼RL4RS**

**(b) off-policy RL4RS**  
**(b) ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼RL4RS**

rollout data {(st, at, st+1, rt )}  
ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆãƒ‡ãƒ¼ã‚¿ {(st, at, st+1, rt )}

_Ï€t+1_  
_Ï€t+1_

|Col1|Col2|Col3|  
|---|---|---|  
|Reward r t â€¦ State s t Policy Ï€t â€¦ Reward implies User u Action a t|  
|Update Policy Ï€t+1|  
|Offline Dataset|  
|Reward implies User u Action at|  
|Data Collection: data collected once with any policy|  

|Col2|Col3|  
|---|---|  
|Offline Dataset Learn Policy Ï€|  
|Reward r t â€¦ State s t Policy Ï€ â€¦ Reward implies User u Action a t|  
|Reward r t â€¦ State s t Policy Ï€ â€¦ Reward implies User u Action a t|  

Training phase  
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚§ãƒ¼ã‚º

**(c) offline RL4RS**  
**(c) ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RS**

User u Action at  
ãƒ¦ãƒ¼ã‚¶ãƒ¼u ã‚¢ã‚¯ã‚·ãƒ§ãƒ³at

Deployment/Fine-tune  
ãƒ‡ãƒ—ãƒ­ã‚¤/ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

Fig. 2. Illustration of classic on-policy RL (a), classic off-policy RL (b), and offline RL (c).  
å›³2. å¤å…¸çš„ãªã‚ªãƒ³ãƒãƒªã‚·ãƒ¼RL (a)ã€å¤å…¸çš„ãªã‚ªãƒ•ãƒãƒªã‚·ãƒ¼RL (b)ã€ãŠã‚ˆã³ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL (c)ã®å›³ç¤ºã€‚

The challenge of offline RL can be tackled through algorithms belonging to any of the four main categories in RL: Q-learning, policy gradient, actor-critic, and model-based RL.  
ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®èª²é¡Œã¯ã€RLã®å››ã¤ã®ä¸»è¦ã‚«ãƒ†ã‚´ãƒªã®ã„ãšã‚Œã‹ã«å±ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é€šã˜ã¦å¯¾å‡¦ã§ãã¾ã™ï¼šQå­¦ç¿’ã€ãƒãƒªã‚·ãƒ¼å‹¾é…ã€ã‚¢ã‚¯ã‚¿ãƒ¼-ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã€ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®RLã€‚

In principle, any off-policy RL algorithm could function as an offline RL approach when the online interaction process is excluded.  
åŸå‰‡ã¨ã—ã¦ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ãŒé™¤å¤–ã•ã‚Œã‚‹ã¨ã€ä»»æ„ã®ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼RLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

For instance, a straightforward offline RL technique can be created by employing Q-learning without requiring supplementary online exploration.  
ä¾‹ãˆã°ã€è¿½åŠ ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¢ç´¢ã‚’å¿…è¦ã¨ã›ãšã«Qå­¦ç¿’ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ç°¡å˜ãªã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLæ‰‹æ³•ã‚’ä½œæˆã§ãã¾ã™ã€‚

This method utilizes the dataset D to pre-fill the data buffer.  
ã“ã®æ‰‹æ³•ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆDã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ•ã‚¡ã‚’äº‹å‰ã«åŸ‹ã‚ã¾ã™ã€‚

**2.5** **Offline RL4RS - Problem Formulation**  
**2.5** **ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RS - å•é¡Œå®šå¼åŒ–**

In this section, we establish a problem formulation for Offline RL4RS.  
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã®å•é¡Œå®šå¼åŒ–ã‚’ç¢ºç«‹ã—ã¾ã™ã€‚

We begin with a standard MDP framework, commonly used in RS.  
RSã§ä¸€èˆ¬çš„ã«ä½¿ç”¨ã•ã‚Œã‚‹æ¨™æº–MDPãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚

The setup involves a set of users denoted as U = ğ‘¢,ğ‘¢1,ğ‘¢2,ğ‘¢3, ...  
ã“ã®è¨­å®šã«ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é›†åˆU = ğ‘¢,ğ‘¢1,ğ‘¢2,ğ‘¢3, ...ãŒå«ã¾ã‚Œã¾ã™ã€‚

and a set of items denoted as I = ğ‘–,ğ‘–1,ğ‘–2,ğ‘–3, ....  
ã‚¢ã‚¤ãƒ†ãƒ ã®é›†åˆI = ğ‘–,ğ‘–1,ğ‘–2,ğ‘–3, ...ãŒå«ã¾ã‚Œã¾ã™ã€‚

The process begins with the system recommending item ğ‘– to user ğ‘¢ and receiving feedback ğ‘“ğ‘–[ğ‘¢][.  
ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€ã‚·ã‚¹ãƒ†ãƒ ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ğ‘¢ã«ã‚¢ã‚¤ãƒ†ãƒ ğ‘–ã‚’æ¨è–¦ã—ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ğ‘“ğ‘–[ğ‘¢]ã‚’å—ã‘å–ã‚‹ã“ã¨ã‹ã‚‰å§‹ã¾ã‚Šã¾ã™ã€‚

This feedback is then utilized to enhance future recommendations, leading to the identification of an optimal policy ğœ‹ [âˆ—] that guides the selection of items to recommend in order to achieve positive feedback.  
ã“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯ã€å°†æ¥ã®æ¨è–¦ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«åˆ©ç”¨ã•ã‚Œã€ãƒã‚¸ãƒ†ã‚£ãƒ–ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å¾—ã‚‹ãŸã‚ã«æ¨è–¦ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®é¸æŠã‚’å°ãæœ€é©ãƒãƒªã‚·ãƒ¼ğœ‹ [âˆ—]ã®ç‰¹å®šã«ã¤ãªãŒã‚Šã¾ã™ã€‚

The MDP framework treats the user as the environment while the system acts as the agent.  
MDPãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ç’°å¢ƒã¨ã—ã¦æ‰±ã„ã€ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã—ã¦æ©Ÿèƒ½ã•ã›ã¾ã™ã€‚

The fundamental components within the MDP context, especially in Deep Reinforcement Learning (DRL)-based RS, include:  
MDPã®æ–‡è„ˆå†…ã®åŸºæœ¬çš„ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã€ç‰¹ã«æ·±å±¤å¼·åŒ–å­¦ç¿’ï¼ˆDRLï¼‰ãƒ™ãƒ¼ã‚¹ã®RSã«ã¯ä»¥ä¸‹ãŒå«ã¾ã‚Œã¾ã™ã€‚

- State S: At a given time ğ‘¡, the state ğ‘ ğ‘¡ âˆˆS is defined by a combination of the userâ€™s characteristics and the recent ğ‘™ items that the user has shown interest in prior to time ğ‘¡.  
- çŠ¶æ…‹Sï¼šã‚ã‚‹æ™‚ç‚¹ğ‘¡ã«ãŠã„ã¦ã€çŠ¶æ…‹ğ‘ ğ‘¡ âˆˆSã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç‰¹æ€§ã¨ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ™‚ç‚¹ğ‘¡ã®å‰ã«èˆˆå‘³ã‚’ç¤ºã—ãŸæœ€è¿‘ã®ğ‘™ã‚¢ã‚¤ãƒ†ãƒ ã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã£ã¦å®šç¾©ã•ã‚Œã¾ã™ã€‚

This may also include demographic information if relevant.  
ã“ã‚Œã«ã¯ã€é–¢é€£ã™ã‚‹å ´åˆã€äººå£çµ±è¨ˆæƒ…å ±ã‚‚å«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

- Action A: The action ğ‘ğ‘¡ âˆˆA represents the agentâ€™s prediction of the userâ€™s evolving preferences at time ğ‘¡.  
- ã‚¢ã‚¯ã‚·ãƒ§ãƒ³Aï¼šã‚¢ã‚¯ã‚·ãƒ§ãƒ³ğ‘ğ‘¡ âˆˆAã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæ™‚ç‚¹ğ‘¡ã«ãŠã‘ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é€²åŒ–ã™ã‚‹å¥½ã¿ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã‚’è¡¨ã—ã¾ã™ã€‚

Here, A encompasses the entire set of potential candidate items, which could be vast, potentially numbering in the millions.  
ã“ã“ã§ã€Aã¯ã€æ•°ç™¾ä¸‡ã«åŠã¶å¯èƒ½æ€§ã®ã‚ã‚‹ã™ã¹ã¦ã®å€™è£œã‚¢ã‚¤ãƒ†ãƒ ã®é›†åˆã‚’å«ã¿ã¾ã™ã€‚

- Transition Probability P: The transition probability ğ‘ (ğ‘ ğ‘¡ +1|ğ‘ ğ‘¡,ğ‘ğ‘¡ ) quantifies the likelihood of transitioning from state ğ‘ ğ‘¡ to ğ‘ ğ‘¡ +1 when the agent performs action ğ‘ğ‘¡ .  
- é·ç§»ç¢ºç‡Pï¼šé·ç§»ç¢ºç‡ğ‘ (ğ‘ ğ‘¡ +1|ğ‘ ğ‘¡,ğ‘ğ‘¡)ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚¢ã‚¯ã‚·ãƒ§ãƒ³ğ‘ğ‘¡ã‚’å®Ÿè¡Œã—ãŸã¨ãã«çŠ¶æ…‹ğ‘ ğ‘¡ã‹ã‚‰ğ‘ ğ‘¡ +1ã«é·ç§»ã™ã‚‹å¯èƒ½æ€§ã‚’å®šé‡åŒ–ã—ã¾ã™ã€‚

In the context of a recommender system, this probability corresponds to the likelihood of user behavior changes.  
æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®æ–‡è„ˆã«ãŠã„ã¦ã€ã“ã®ç¢ºç‡ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•å¤‰åŒ–ã®å¯èƒ½æ€§ã«å¯¾å¿œã—ã¾ã™ã€‚

- Reward function R: After the agent selects an appropriate action ğ‘ğ‘¡ based on the current state ğ‘ ğ‘¡ at time ğ‘¡, the user receives the item recommended by the agent.  
- å ±é…¬é–¢æ•°Rï¼šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæ™‚ç‚¹ğ‘¡ã«ãŠã‘ã‚‹ç¾åœ¨ã®çŠ¶æ…‹ğ‘ ğ‘¡ã«åŸºã¥ã„ã¦é©åˆ‡ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ğ‘ğ‘¡ã‚’é¸æŠã—ãŸå¾Œã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã£ã¦æ¨è–¦ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚

The feedback from the user regarding the recommended item contributes to the reward ğ‘Ÿğ‘¡ = R(ğ‘ ğ‘¡,ğ‘ğ‘¡ ).  
æ¨è–¦ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã«é–¢ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯ã€å ±é…¬ğ‘Ÿğ‘¡ = R(ğ‘ ğ‘¡,ğ‘ğ‘¡)ã«å¯„ä¸ã—ã¾ã™ã€‚

This reward reflects the userâ€™s response and guides the enhancement of the learned policy ğœ‹ by the recommendation agent.  
ã“ã®å ±é…¬ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åå¿œã‚’åæ˜ ã—ã€æ¨è–¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹å­¦ç¿’ã—ãŸãƒãƒªã‚·ãƒ¼ğœ‹ã®å‘ä¸Šã‚’å°ãã¾ã™ã€‚

- Discount Factor ğ›¾: The discount factor ğ›¾ âˆˆ[0, 1] is employed to balance the consideration of future and immediate rewards.  
- å‰²å¼•å› å­ğ›¾ï¼šå‰²å¼•å› å­ğ›¾ âˆˆ[0, 1]ã¯ã€å°†æ¥ã®å ±é…¬ã¨å³æ™‚ã®å ±é…¬ã®è€ƒæ…®ã‚’ãƒãƒ©ãƒ³ã‚¹ã•ã›ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

A value of ğ›¾ = 0 indicates the agent prioritizes immediate rewards, while a non-zero value implies a blend of both immediate and future rewards.  
ğ›¾ = 0ã®å€¤ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå³æ™‚ã®å ±é…¬ã‚’å„ªå…ˆã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã€éã‚¼ãƒ­ã®å€¤ã¯å³æ™‚ã®å ±é…¬ã¨å°†æ¥ã®å ±é…¬ã®ä¸¡æ–¹ã®ãƒ–ãƒ¬ãƒ³ãƒ‰ã‚’ç¤ºå”†ã—ã¾ã™ã€‚

- Offline Dataset D: The offline dataset D is amassed by an unknown behavior policy ğœ‹ğ›½ .  
- ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆDï¼šã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆDã¯ã€æœªçŸ¥ã®è¡Œå‹•ãƒãƒªã‚·ãƒ¼ğœ‹ğ›½ã«ã‚ˆã£ã¦åé›†ã•ã‚Œã¾ã™ã€‚

This dataset serves as the historical records of user interactions and is utilized to improve the recommendation policy.  
ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã®å±¥æ­´è¨˜éŒ²ã¨ã—ã¦æ©Ÿèƒ½ã—ã€æ¨è–¦ãƒãƒªã‚·ãƒ¼ã®æ”¹å–„ã«åˆ©ç”¨ã•ã‚Œã¾ã™ã€‚

This MDP-based framework lays the groundwork for Offline RL4RS, where the aim is to devise effective recommendation policies using historical interaction data, even when the data is collected under an unknown or different behavior policy.  
ã“ã®MDPãƒ™ãƒ¼ã‚¹ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã®åŸºç›¤ã‚’ç¯‰ãã€ç›®æ¨™ã¯ã€æœªçŸ¥ã¾ãŸã¯ç•°ãªã‚‹è¡Œå‹•ãƒãƒªã‚·ãƒ¼ã®ä¸‹ã§åé›†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§ã‚ã£ã¦ã‚‚ã€æ­´å²çš„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦åŠ¹æœçš„ãªæ¨è–¦ãƒãƒªã‚·ãƒ¼ã‚’è€ƒæ¡ˆã™ã‚‹ã“ã¨ã§ã™ã€‚

If a POMDP is used, we just need to add the observation O which is the partial information from users and ğ‘™ items in which the user was interested before time ğ‘¡.  
POMDPãŒä½¿ç”¨ã•ã‚Œã‚‹å ´åˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®éƒ¨åˆ†çš„ãªæƒ…å ±ã¨ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ™‚ç‚¹ğ‘¡ã®å‰ã«èˆˆå‘³ã‚’æŒã£ã¦ã„ãŸğ‘™ã‚¢ã‚¤ãƒ†ãƒ ã«é–¢ã™ã‚‹è¦³æ¸¬Oã‚’è¿½åŠ ã™ã‚‹ã ã‘ã§ã™ã€‚

Definition 3. Given an offline dataset D, which contains the trajectories when user ğ‘¢ âˆˆU _interacts with the system for a certain period with an unknown behavior policy ğœ‹ğ›½_ _, the RL agent aims_ _to learn a policy ğœ‹_ _from the offline dataset D.  
å®šç¾©3. ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆDãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€ã“ã‚Œã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ğ‘¢ âˆˆUãŒæœªçŸ¥ã®è¡Œå‹•ãƒãƒªã‚·ãƒ¼ğœ‹ğ›½ã§ã‚·ã‚¹ãƒ†ãƒ ã¨ä¸€å®šæœŸé–“ç›¸äº’ä½œç”¨ã™ã‚‹éš›ã®è»Œé“ã‚’å«ã¿ã€RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆDã‹ã‚‰ãƒãƒªã‚·ãƒ¼ğœ‹ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚

After that, the trained policy ğœ‹_ _will be deployed/tested_ _on a production or evaluation environment with a similar scenario with the collected dataset D._  
ãã®å¾Œã€å­¦ç¿’ã—ãŸãƒãƒªã‚·ãƒ¼ğœ‹ã¯ã€åé›†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆDã‚’ç”¨ã„ãŸé¡ä¼¼ã®ã‚·ãƒŠãƒªã‚ªã§æœ¬ç•ªã¾ãŸã¯è©•ä¾¡ç’°å¢ƒã«ãƒ‡ãƒ—ãƒ­ã‚¤/ãƒ†ã‚¹ãƒˆã•ã‚Œã¾ã™ã€‚



## 3 CURRENT PROGRESS IN OFFLINE RL4RS
## 3 ç¾åœ¨ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã®é€²å±•

In this section, we survey offline RL-based RS. 
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã«åŸºã¥ãæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ï¼ˆRSï¼‰ã‚’èª¿æŸ»ã—ã¾ã™ã€‚

Generally speaking, it can be divided into two categories: off-policy with logged data (i.e., â€œfullâ€ off-policy) and offline RL. 
ä¸€èˆ¬çš„ã«ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã¯ã€ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ï¼ˆã™ãªã‚ã¡ã€Œå®Œå…¨ã€ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ï¼‰ã¨ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®2ã¤ã®ã‚«ãƒ†ã‚´ãƒªã«åˆ†ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

These two concepts are generally the same except for some specific settings in off-policy methods such as assuming bandit conditions. 
ã“ã‚Œã‚‰2ã¤ã®æ¦‚å¿µã¯ã€ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆæ¡ä»¶ã‚’ä»®å®šã™ã‚‹ãªã©ã€ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼æ‰‹æ³•ã®ç‰¹å®šã®è¨­å®šã‚’é™¤ã„ã¦ã€ä¸€èˆ¬çš„ã«ã¯åŒã˜ã§ã™ã€‚

Due to the recent introduction of offline RL, we have opted to distinguish and separate these for clarity and to prevent potential confusion. 
æœ€è¿‘ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®å°å…¥ã«ã‚ˆã‚Šã€æ˜ç¢ºã•ã‚’ä¿ã¡ã€æ½œåœ¨çš„ãªæ··ä¹±ã‚’é˜²ããŸã‚ã«ã€ã“ã‚Œã‚‰ã‚’åŒºåˆ¥ã—ã¦åˆ†ã‘ã‚‹ã“ã¨ã«ã—ã¾ã—ãŸã€‚

**3.1 Off-policy with Logged Data**
**3.1 ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸã‚ªãƒ•ãƒãƒªã‚·ãƒ¼**

_3.1.1 Off-policy Evaluation. The typical method in this domain is known as off-policy evaluation._ 
_3.1.1 ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è©•ä¾¡ã€‚ã“ã®åˆ†é‡ã®å…¸å‹çš„ãªæ‰‹æ³•ã¯ã€ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è©•ä¾¡ã¨ã—ã¦çŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚_

Off-policy evaluation methods are rooted in the direct estimation of policy returns. 
ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è©•ä¾¡æ‰‹æ³•ã¯ã€ãƒãƒªã‚·ãƒ¼ãƒªã‚¿ãƒ¼ãƒ³ã®ç›´æ¥çš„ãªæ¨å®šã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚

These methods often utilize a technique known as importance sampling, which involves estimating the return of a given policy or approximating the corresponding policy gradient. 
ã“ã‚Œã‚‰ã®æ‰‹æ³•ã¯ã€ç‰¹å®šã®ãƒãƒªã‚·ãƒ¼ã®ãƒªã‚¿ãƒ¼ãƒ³ã‚’æ¨å®šã™ã‚‹ã‹ã€å¯¾å¿œã™ã‚‹ãƒãƒªã‚·ãƒ¼å‹¾é…ã‚’è¿‘ä¼¼ã™ã‚‹é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨å‘¼ã°ã‚Œã‚‹æ‰‹æ³•ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒã‚ˆãã‚ã‚Šã¾ã™ã€‚

A straightforward application of importance sampling involves using trajectories sampled from ğœ‹ğ›½ (ğœ) to derive an unbiased estimator of ğ½ (ğœ‹):  
é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ç°¡å˜ãªé©ç”¨ã¯ã€ğœ‹ğ›½ (ğœ) ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸè»Œè·¡ã‚’ä½¿ç”¨ã—ã¦ã€ğ½ (ğœ‹) ã®ç„¡åæ¨å®šé‡ã‚’å°å‡ºã™ã‚‹ã“ã¨ã‚’å«ã¿ã¾ã™ï¼š

$$
\hat{J}(\pi) = \sum_{t=0}^{T} \gamma[t] R(s, a) = E_{\tau \sim \pi_\beta} \left[ \sum_{t=0}^{T} \frac{\pi_\theta(a_t | s_t)}{\pi_\beta(s_t | a_t)} R(s, a) \right]
$$

However, this estimator often exhibits high variance, particularly if _ğ‘‡_ (the time horizon) is large, due to the product of importance weights. 
ã—ã‹ã—ã€ã“ã®æ¨å®šé‡ã¯ã€é‡è¦åº¦é‡ã¿ã®ç©ã®ãŸã‚ã«ã€ç‰¹ã« _ğ‘‡_ï¼ˆæ™‚é–“ã®åœ°å¹³ç·šï¼‰ãŒå¤§ãã„å ´åˆã«é«˜ã„åˆ†æ•£ã‚’ç¤ºã™ã“ã¨ãŒã‚ˆãã‚ã‚Šã¾ã™ã€‚

To address this, a weighted importance sampling estimator can be used. 
ã“ã‚Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€é‡ã¿ä»˜ãé‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¨å®šé‡ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

This involves dividing the weights by $n$ to normalize them, resulting in a biased estimator with significantly lower variance, while still maintaining strong consistency. 
ã“ã‚Œã¯ã€é‡ã¿ã‚’ $n$ ã§å‰²ã£ã¦æ­£è¦åŒ–ã™ã‚‹ã“ã¨ã‚’å«ã¿ã€ãƒã‚¤ã‚¢ã‚¹ã®ã‚ã‚‹æ¨å®šé‡ã‚’ç”Ÿæˆã—ã¾ã™ãŒã€åˆ†æ•£ã¯å¤§å¹…ã«ä½ãã€å¼·ã„ä¸€è²«æ€§ã‚’ç¶­æŒã—ã¾ã™ã€‚

When considering the estimation of Q-values for each state-action pair (ğ‘ ğ‘¡,ğ‘ğ‘¡ ), denoted as _ğ‘„Ë†_ _[ğœ‹]_ (ğ‘ ğ‘¡,ğ‘ğ‘¡ ), an approximate model comes into play. 
å„çŠ¶æ…‹-è¡Œå‹•ãƒšã‚¢ (ğ‘ ğ‘¡,ğ‘ğ‘¡) ã®Qå€¤ã®æ¨å®šã‚’è€ƒæ…®ã™ã‚‹éš›ã€_ğ‘„Ë†_ _[ğœ‹]_ (ğ‘ ğ‘¡,ğ‘ğ‘¡) ã¨ã—ã¦ç¤ºã•ã‚Œã‚‹è¿‘ä¼¼ãƒ¢ãƒ‡ãƒ«ãŒç™»å ´ã—ã¾ã™ã€‚

This model could be derived from estimating the transition probability P(ğ‘ ğ‘¡ +1|ğ‘ ğ‘¡,ğ‘ğ‘¡ ) of the Markov Decision Process (MDP) and subsequently solving for the corresponding Q-function. 
ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆMDPï¼‰ã®é·ç§»ç¢ºç‡ P(ğ‘ ğ‘¡ +1|ğ‘ ğ‘¡,ğ‘ğ‘¡) ã‚’æ¨å®šã—ã€ãã®å¾Œã€å¯¾å¿œã™ã‚‹Qé–¢æ•°ã‚’è§£ãã“ã¨ã‹ã‚‰å°å‡ºã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

Alternatively, other methods for approximating Q-values could be employed. 
ã‚ã‚‹ã„ã¯ã€Qå€¤ã‚’è¿‘ä¼¼ã™ã‚‹ãŸã‚ã®ä»–ã®æ‰‹æ³•ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

The integration of these approximated Q-values as control variates within the importance sampled estimator leads to an enhanced approach:  
ã“ã‚Œã‚‰ã®è¿‘ä¼¼Qå€¤ã‚’é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¨å®šé‡å†…ã®åˆ¶å¾¡å¤‰æ•°ã¨ã—ã¦çµ±åˆã™ã‚‹ã“ã¨ã§ã€å¼·åŒ–ã•ã‚ŒãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¾—ã‚‰ã‚Œã¾ã™ï¼š

$$
\hat{J}(\pi_\theta) = \sum_{t=0}^{T} \gamma[t] \left[ r_t[i] - \hat{Q}[\pi](s_t, a_t) \right] - w_t[i]^{-1} E_a \sim \pi_\theta \left[ \hat{Q}[\pi](s_t, a_t) \right]
$$

This method, referred to as a doubly robust estimator, exhibits unbiasedness either when _ğœ‹ğ›½_ is known or when the model is accurate. 
ã“ã®æ‰‹æ³•ã¯ã€ãƒ€ãƒ–ãƒªãƒ¼ãƒ­ãƒã‚¹ãƒˆæ¨å®šé‡ã¨å‘¼ã°ã‚Œã€_ğœ‹ğ›½_ ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆã‚„ãƒ¢ãƒ‡ãƒ«ãŒæ­£ç¢ºãªå ´åˆã«ç„¡åæ€§ã‚’ç¤ºã—ã¾ã™ã€‚

In essence, it leverages both the unbiasedness of the importance sampling method and the approximated Q-values to produce an estimator with lower variance and strong consistency. 
æœ¬è³ªçš„ã«ã¯ã€é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•ã®ç„¡åæ€§ã¨è¿‘ä¼¼Qå€¤ã®ä¸¡æ–¹ã‚’æ´»ç”¨ã—ã¦ã€åˆ†æ•£ãŒä½ãã€ä¸€è²«æ€§ã®å¼·ã„æ¨å®šé‡ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

_3.1.2 Recent works. The recent advancements in off-policy using logged data method can be_ broadly categorized into three distinct domains: estimator improvement (focus on the discrepancy between the offline data and online data), algorithmic improvement (focus on the recommendation algorithm itself), and miscellaneous application domains. 
_3.1.2 æœ€è¿‘ã®ç ”ç©¶ã€‚ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ã®æœ€è¿‘ã®é€²å±•ã¯ã€_æ¨å®šé‡ã®æ”¹å–„ï¼ˆã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ä¸ä¸€è‡´ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ï¼‰ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ”¹å–„ï¼ˆæ¨è–¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è‡ªä½“ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ï¼‰ã€ãŠã‚ˆã³ãã®ä»–ã®å¿œç”¨åˆ†é‡_ ã®3ã¤ã®ç•°ãªã‚‹é ˜åŸŸã«å¤§åˆ¥ã§ãã¾ã™ã€‚

We have compiled a summary of these works in Figure 3. 
ã“ã‚Œã‚‰ã®ç ”ç©¶ã®æ¦‚è¦ã‚’å›³3ã«ã¾ã¨ã‚ã¾ã—ãŸã€‚

**Estimator Improvement Hoiles and Schaar [18] focus on the problem of student course scheduling and curriculum design.** 
**æ¨å®šé‡ã®æ”¹å–„ Hoilesã¨Schaar [18] ã¯ã€å­¦ç”Ÿã®ã‚³ãƒ¼ã‚¹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã¨ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ è¨­è¨ˆã®å•é¡Œã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚**

It proposes an algorithm for personalized course recommendation and curriculum design based on logged student data. 
ã“ã‚Œã¯ã€ãƒ­ã‚°ã•ã‚ŒãŸå­¦ç”Ÿãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸã‚³ãƒ¼ã‚¹æ¨è–¦ã¨ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ è¨­è¨ˆã®ãŸã‚ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆã—ã¾ã™ã€‚

The algorithm uses a regression estimator for contextual multi-armed bandits and provides guarantees on their predictive performance. 
ã“ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€æ–‡è„ˆçš„ãƒãƒ«ãƒã‚¢ãƒ¼ãƒ ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®ãŸã‚ã®å›å¸°æ¨å®šé‡ã‚’ä½¿ç”¨ã—ã€ãã®äºˆæ¸¬æ€§èƒ½ã«é–¢ã™ã‚‹ä¿è¨¼ã‚’æä¾›ã—ã¾ã™ã€‚

The paper also addresses the issue of missing data and provides guidelines for including expert domain knowledge in the recommendations. 
ã“ã®è«–æ–‡ã¯ã€æ¬ æãƒ‡ãƒ¼ã‚¿ã®å•é¡Œã«ã‚‚å¯¾å‡¦ã—ã€æ¨è–¦ã«å°‚é–€å®¶ã®ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã‚’å«ã‚ã‚‹ãŸã‚ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’æä¾›ã—ã¾ã™ã€‚

The algorithms can be used to identify curriculum gaps and provide recommendations for course schedules. 
ã“ã‚Œã‚‰ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’ç‰¹å®šã—ã€ã‚³ãƒ¼ã‚¹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æ¨å¥¨ã‚’æä¾›ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã¾ã™ã€‚

The paper also discusses off-policy evaluation techniques and the use of the regression estimator for estimating the expected reward of a new policy. 
ã“ã®è«–æ–‡ã§ã¯ã€ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è©•ä¾¡æ‰‹æ³•ã¨æ–°ã—ã„ãƒãƒªã‚·ãƒ¼ã®æœŸå¾…å ±é…¬ã‚’æ¨å®šã™ã‚‹ãŸã‚ã®å›å¸°æ¨å®šé‡ã®ä½¿ç”¨ã«ã¤ã„ã¦ã‚‚è«–ã˜ã¦ã„ã¾ã™ã€‚

One drawback is that the proposed approach assumes a fixed set of courses and does not consider the dynamic nature of course offerings and student preferences. 
1ã¤ã®æ¬ ç‚¹ã¯ã€ææ¡ˆã•ã‚ŒãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå›ºå®šã•ã‚ŒãŸã‚³ãƒ¼ã‚¹ã‚»ãƒƒãƒˆã‚’ä»®å®šã—ã¦ãŠã‚Šã€ã‚³ãƒ¼ã‚¹ã®æä¾›ã‚„å­¦ç”Ÿã®å¥½ã¿ã®å‹•çš„ãªæ€§è³ªã‚’è€ƒæ…®ã—ã¦ã„ãªã„ã“ã¨ã§ã™ã€‚

Swaminathan et al. [45] address the problem of off-policy evaluation and optimization in combinatorial contextual bandits. 
Swaminathanã‚‰ [45] ã¯ã€çµ„åˆã›æ–‡è„ˆãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«ãŠã‘ã‚‹ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è©•ä¾¡ã¨æœ€é©åŒ–ã®å•é¡Œã«å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™ã€‚

The motivation behind this research is the need to estimate the reward of a new target policy using data collected by a different logging policy. 
ã“ã®ç ”ç©¶ã®å‹•æ©Ÿã¯ã€ç•°ãªã‚‹ãƒ­ã‚°ãƒãƒªã‚·ãƒ¼ã«ã‚ˆã£ã¦åé›†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã€æ–°ã—ã„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒªã‚·ãƒ¼ã®å ±é…¬ã‚’æ¨å®šã™ã‚‹å¿…è¦æ€§ã§ã™ã€‚

The authors propose a pseudoinverse (PI) estimator that makes a linearity assumption about the evaluated metric, allowing for more efficient estimation compared to importance sampling. 
è‘—è€…ã‚‰ã¯ã€è©•ä¾¡ã•ã‚ŒãŸãƒ¡ãƒˆãƒªãƒƒã‚¯ã«é–¢ã—ã¦ç·šå½¢æ€§ã®ä»®å®šã‚’è¡Œã†æ“¬ä¼¼é€†ï¼ˆPIï¼‰æ¨å®šé‡ã‚’ææ¡ˆã—ã€é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨æ¯”è¼ƒã—ã¦ã‚ˆã‚ŠåŠ¹ç‡çš„ãªæ¨å®šã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚

The PI estimator requires exponentially fewer samples to achieve a given error bound and can be used for off-policy optimization as well. 
PIæ¨å®šé‡ã¯ã€æ‰€å®šã®èª¤å·®å¢ƒç•Œã‚’é”æˆã™ã‚‹ãŸã‚ã«æŒ‡æ•°çš„ã«å°‘ãªã„ã‚µãƒ³ãƒ—ãƒ«ã‚’å¿…è¦ã¨ã—ã€ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼æœ€é©åŒ–ã«ã‚‚ä½¿ç”¨ã§ãã¾ã™ã€‚

The methodology involves using the PI estimator to impute action-level rewards for each context, enabling direct optimization of whole-page metrics through pointwise learning to rank algorithms. 
ã“ã®æ–¹æ³•è«–ã¯ã€PIæ¨å®šé‡ã‚’ä½¿ç”¨ã—ã¦å„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«ã®å ±é…¬ã‚’è£œå®Œã—ã€ãƒã‚¤ãƒ³ãƒˆãƒ¯ã‚¤ã‚ºå­¦ç¿’ã‚’é€šã˜ã¦å…¨ãƒšãƒ¼ã‚¸ãƒ¡ãƒˆãƒªãƒƒã‚¯ã®ç›´æ¥æœ€é©åŒ–ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚

The authors demonstrate the effectiveness of their approach on real-world search ranking datasets, showing that the PI estimator outperforms prior baselines in terms of off-policy evaluation of whole-page metrics. 
è‘—è€…ã‚‰ã¯ã€å®Ÿä¸–ç•Œã®æ¤œç´¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã‘ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æœ‰åŠ¹æ€§ã‚’ç¤ºã—ã€PIæ¨å®šé‡ãŒå…¨ãƒšãƒ¼ã‚¸ãƒ¡ãƒˆãƒªãƒƒã‚¯ã®ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è©•ä¾¡ã«ãŠã„ã¦ä»¥å‰ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

This method has several limitations. 
ã“ã®æ–¹æ³•ã«ã¯ã„ãã¤ã‹ã®åˆ¶é™ãŒã‚ã‚Šã¾ã™ã€‚

One drawback of this method is that it relies on the linearity assumption, which may not always hold in practice. 
ã“ã®æ–¹æ³•ã®1ã¤ã®æ¬ ç‚¹ã¯ã€å®Ÿéš›ã«ã¯å¸¸ã«æˆã‚Šç«‹ãŸãªã„å¯èƒ½æ€§ã®ã‚ã‚‹ç·šå½¢æ€§ã®ä»®å®šã«ä¾å­˜ã—ã¦ã„ã‚‹ã“ã¨ã§ã™ã€‚

Moreover, there is a bias-variance tradeoff between the weighted pseudoinverse (wPI) method and the direct method, with wPI showing bias for the Expected Reciprocal Rank metric. 
ã•ã‚‰ã«ã€é‡ã¿ä»˜ãæ“¬ä¼¼é€†ï¼ˆwPIï¼‰æ³•ã¨ç›´æ¥æ³•ã®é–“ã«ã¯ãƒã‚¤ã‚¢ã‚¹-åˆ†æ•£ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒã‚ã‚Šã€wPIã¯æœŸå¾…é€†é †ä½ãƒ¡ãƒˆãƒªãƒƒã‚¯ã«å¯¾ã—ã¦ãƒã‚¤ã‚¢ã‚¹ã‚’ç¤ºã—ã¾ã™ã€‚

The wPI method also deteriorates for larger slate spaces and is sensitive to linearity assumptions. 
wPIæ³•ã¯ã€ã‚ˆã‚Šå¤§ããªã‚¹ãƒ¬ãƒ¼ãƒˆç©ºé–“ã«å¯¾ã—ã¦ã‚‚åŠ£åŒ–ã—ã€ç·šå½¢æ€§ã®ä»®å®šã«æ•æ„Ÿã§ã™ã€‚

These drawbacks highlight areas where further refinement and research are needed to enhance the robustness and effectiveness of the approach. 
ã“ã‚Œã‚‰ã®æ¬ ç‚¹ã¯ã€ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å …ç‰¢æ€§ã¨åŠ¹æœã‚’é«˜ã‚ã‚‹ãŸã‚ã«ã•ã‚‰ãªã‚‹æ´—ç·´ã¨ç ”ç©¶ãŒå¿…è¦ãªé ˜åŸŸã‚’æµ®ãå½«ã‚Šã«ã—ã¦ã„ã¾ã™ã€‚

Jeunen and Goethals [21, 22] focus on improving the recommendation performance of policies that rely on value-based models (i.e., Q-learning) of expected reward. 
Jeunenã¨Goethals [21, 22] ã¯ã€æœŸå¾…å ±é…¬ã®ä¾¡å€¤ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆã™ãªã‚ã¡Qå­¦ç¿’ï¼‰ã«ä¾å­˜ã™ã‚‹ãƒãƒªã‚·ãƒ¼ã®æ¨è–¦æ€§èƒ½ã‚’æ”¹å–„ã™ã‚‹ã“ã¨ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚

The authors propose a pessimistic reward modeling framework that incorporates Bayesian uncertainty estimates to express skepticism about the reward model. 
è‘—è€…ã‚‰ã¯ã€å ±é…¬ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹æ‡ç–‘å¿ƒã‚’è¡¨ç¾ã™ã‚‹ãŸã‚ã«ãƒ™ã‚¤ã‚ºçš„ä¸ç¢ºå®Ÿæ€§æ¨å®šã‚’çµ„ã¿è¾¼ã‚“ã æ‚²è¦³çš„å ±é…¬ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã¾ã™ã€‚

This allows for the generation of conservative decision rules based on lower-confidence-bound estimates, rather than the usual maximum likelihood or maximum PI estimates. 
ã“ã‚Œã«ã‚ˆã‚Šã€é€šå¸¸ã®æœ€å°¤æ¨å®šã‚„æœ€å¤§PIæ¨å®šã§ã¯ãªãã€ä¸‹é™ä¿¡é ¼åŒºé–“æ¨å®šã«åŸºã¥ãä¿å®ˆçš„ãªæ„æ€æ±ºå®šãƒ«ãƒ¼ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

The approach is agnostic to the logging policy and does not require propensity scores, making it more flexible and avoiding the limitations of inverse propensity score weighting. 
ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ãƒ­ã‚°ãƒãƒªã‚·ãƒ¼ã«ä¾å­˜ã›ãšã€å‚¾å‘ã‚¹ã‚³ã‚¢ã‚’å¿…è¦ã¨ã—ãªã„ãŸã‚ã€ã‚ˆã‚ŠæŸ”è»Ÿã§ã‚ã‚Šã€é€†å‚¾å‘ã‚¹ã‚³ã‚¢é‡ã¿ä»˜ã‘ã®åˆ¶é™ã‚’å›é¿ã—ã¾ã™ã€‚

The methodology involves training reward models using a range of datasets generated under different environmental conditions. 
ã“ã®æ–¹æ³•è«–ã¯ã€ç•°ãªã‚‹ç’°å¢ƒæ¡ä»¶ä¸‹ã§ç”Ÿæˆã•ã‚ŒãŸã•ã¾ã–ã¾ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹ã“ã¨ã‚’å«ã¿ã¾ã™ã€‚

The authors compare the performance of policies that act based on reward models using maximum likelihood or maximum PI estimates, with policies that use lower confidence bounds based on tuned parameters. 
è‘—è€…ã‚‰ã¯ã€æœ€å¤§å°¤åº¦ã¾ãŸã¯æœ€å¤§PIæ¨å®šã‚’ä½¿ç”¨ã—ã¦å ±é…¬ãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ã„ã¦è¡Œå‹•ã™ã‚‹ãƒãƒªã‚·ãƒ¼ã®æ€§èƒ½ã‚’ã€èª¿æ•´ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«åŸºã¥ãä¸‹é™ä¿¡é ¼åŒºé–“ã‚’ä½¿ç”¨ã™ã‚‹ãƒãƒªã‚·ãƒ¼ã¨æ¯”è¼ƒã—ã¾ã™ã€‚

The evaluation is done through simulated A/B tests, with the resulting click-through-rate (CTR) estimates compared to the logging policy and an unattainable skyline policy. 
è©•ä¾¡ã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚ŒãŸA/Bãƒ†ã‚¹ãƒˆã‚’é€šã˜ã¦è¡Œã‚ã‚Œã€å¾—ã‚‰ã‚ŒãŸã‚¯ãƒªãƒƒã‚¯ç‡ï¼ˆCTRï¼‰æ¨å®šã¯ãƒ­ã‚°ãƒãƒªã‚·ãƒ¼ãŠã‚ˆã³é”æˆä¸å¯èƒ½ãªã‚¹ã‚«ã‚¤ãƒ©ã‚¤ãƒ³ãƒãƒªã‚·ãƒ¼ã¨æ¯”è¼ƒã•ã‚Œã¾ã™ã€‚

The experiments show that the pessimistic decision-making approach consistently decreases post-decision disappointment and can significantly increase the policyâ€™s attained CTR. 
å®Ÿé¨“ã¯ã€æ‚²è¦³çš„ãªæ„æ€æ±ºå®šã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒä¸€è²«ã—ã¦æ±ºå®šå¾Œã®å¤±æœ›ã‚’æ¸›å°‘ã•ã›ã€ãƒãƒªã‚·ãƒ¼ã®é”æˆã—ãŸCTRã‚’å¤§å¹…ã«å¢—åŠ ã•ã›ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

One drawback of this approach is that it relies on the assumption that the reward estimates are conditionally unbiased, which may not always hold in practice. 
ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®1ã¤ã®æ¬ ç‚¹ã¯ã€å ±é…¬æ¨å®šãŒæ¡ä»¶ä»˜ãã§ç„¡åã§ã‚ã‚‹ã¨ã„ã†ä»®å®šã«ä¾å­˜ã—ã¦ã„ã‚‹ã“ã¨ã§ã‚ã‚Šã€ã“ã‚Œã¯å®Ÿéš›ã«ã¯å¸¸ã«æˆã‚Šç«‹ãŸãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

The authors acknowledge that underfitting and model misspecification can make this assumption unrealistic. 
è‘—è€…ã‚‰ã¯ã€ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚„ãƒ¢ãƒ‡ãƒ«ã®èª¤æŒ‡å®šãŒã“ã®ä»®å®šã‚’éç¾å®Ÿçš„ã«ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’èªã‚ã¦ã„ã¾ã™ã€‚

Additionally, the approach requires tuning the hyperparameter alpha, which determines the lower confidence bound, and finding the optimal value may not always be straightforward. 
ã•ã‚‰ã«ã€ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ä¸‹é™ä¿¡é ¼åŒºé–“ã‚’æ±ºå®šã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Î±ã®èª¿æ•´ã‚’å¿…è¦ã¨ã—ã€æœ€é©ãªå€¤ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã¯å¸¸ã«ç°¡å˜ã§ã¯ãªã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚

Narita et al. [39] proposes a new off-policy evaluation method for RL4RS. 
Naritaã‚‰ [39] ã¯ã€RL4RSã®ãŸã‚ã®æ–°ã—ã„ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è©•ä¾¡æ‰‹æ³•ã‚’ææ¡ˆã—ã¾ã™ã€‚

The motivation behind this work is to address the limitations of existing estimators, such as inverse propensity weighting and doubly robust estimators, which suffer from bias and overfitting issues. 
ã“ã®ç ”ç©¶ã®å‹•æ©Ÿã¯ã€é€†å‚¾å‘é‡ã¿ä»˜ã‘ã‚„ãƒ€ãƒ–ãƒªãƒ¼ãƒ­ãƒã‚¹ãƒˆæ¨å®šé‡ãªã©ã®æ—¢å­˜ã®æ¨å®šé‡ã®é™ç•Œã«å¯¾å‡¦ã™ã‚‹ã“ã¨ã§ã‚ã‚Šã€ã“ã‚Œã‚‰ã¯ãƒã‚¤ã‚¢ã‚¹ã‚„éå‰°é©åˆã®å•é¡Œã«æ‚©ã¾ã•ã‚Œã¦ã„ã¾ã™ã€‚

The authors introduce a new estimator that combines the doubly robust estimator with double/debiased machine learning techniques. 
è‘—è€…ã‚‰ã¯ã€ãƒ€ãƒ–ãƒªãƒ¼ãƒ­ãƒã‚¹ãƒˆæ¨å®šé‡ã¨ãƒ€ãƒ–ãƒ«/ãƒ‡ãƒã‚¤ã‚¢ã‚¹æ©Ÿæ¢°å­¦ç¿’æŠ€è¡“ã‚’çµ„ã¿åˆã‚ã›ãŸæ–°ã—ã„æ¨å®šé‡ã‚’å°å…¥ã—ã¾ã™ã€‚

The key features of this estimator are its robustness to bias in behavior policy and state-action value function estimates, as well as the use of a sample-splitting procedure called cross-fitting to remove overfitting bias. 
ã“ã®æ¨å®šé‡ã®ä¸»ãªç‰¹å¾´ã¯ã€è¡Œå‹•ãƒãƒªã‚·ãƒ¼ã¨çŠ¶æ…‹-è¡Œå‹•ä¾¡å€¤é–¢æ•°æ¨å®šã«ãŠã‘ã‚‹ãƒã‚¤ã‚¢ã‚¹ã«å¯¾ã™ã‚‹å …ç‰¢æ€§ã€ãŠã‚ˆã³éå‰°é©åˆãƒã‚¤ã‚¢ã‚¹ã‚’é™¤å»ã™ã‚‹ãŸã‚ã®ã‚¯ãƒ­ã‚¹ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã¨å‘¼ã°ã‚Œã‚‹ã‚µãƒ³ãƒ—ãƒ«åˆ†å‰²æ‰‹æ³•ã®ä½¿ç”¨ã§ã™ã€‚

However, the experiments are limited to specific domains, such as the CartPole-v0 environment and online ads, and it is unclear how the estimator would perform in other tasks in RS. 
ã—ã‹ã—ã€å®Ÿé¨“ã¯CartPole-v0ç’°å¢ƒã‚„ã‚ªãƒ³ãƒ©ã‚¤ãƒ³åºƒå‘Šãªã©ã®ç‰¹å®šã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã«åˆ¶é™ã•ã‚Œã¦ãŠã‚Šã€æ¨å®šé‡ãŒRSã®ä»–ã®ã‚¿ã‚¹ã‚¯ã§ã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã‹ã¯ä¸æ˜ã§ã™ã€‚

Jagerman et al. [20] address the problem of off-policy evaluation in non-stationary environments, where user preferences change over time. 
Jagermanã‚‰ [20] ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ãŒæ™‚é–“ã¨ã¨ã‚‚ã«å¤‰åŒ–ã™ã‚‹éå®šå¸¸ç’°å¢ƒã«ãŠã‘ã‚‹ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è©•ä¾¡ã®å•é¡Œã«å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™ã€‚

Existing off-policy evaluation techniques fail to work in such environments. 
æ—¢å­˜ã®ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è©•ä¾¡æ‰‹æ³•ã¯ã€ãã®ã‚ˆã†ãªç’°å¢ƒã§ã¯æ©Ÿèƒ½ã—ã¾ã›ã‚“ã€‚

It proposes several off-policy estimators that operate well in non-stationary environments. 
ã“ã®ç ”ç©¶ã¯ã€éå®šå¸¸ç’°å¢ƒã§ã†ã¾ãæ©Ÿèƒ½ã™ã‚‹ã„ãã¤ã‹ã®ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼æ¨å®šé‡ã‚’ææ¡ˆã—ã¾ã™ã€‚

These estimators rely more on recent bandit feedback and accurately capture changes in user preferences. 
ã“ã‚Œã‚‰ã®æ¨å®šé‡ã¯ã€æœ€è¿‘ã®ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«ã‚ˆã‚Šä¾å­˜ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã®å¤‰åŒ–ã‚’æ­£ç¢ºã«æ‰ãˆã¾ã™ã€‚

They provide a rigorous analysis of the proposed estimatorsâ€™ bias and show that the bias does not grow over time, unlike the standard Inverse Propensity Scoring (IPS) estimator. 
å½¼ã‚‰ã¯ã€ææ¡ˆã•ã‚ŒãŸæ¨å®šé‡ã®ãƒã‚¤ã‚¢ã‚¹ã«é–¢ã™ã‚‹å³å¯†ãªåˆ†æã‚’æä¾›ã—ã€ãƒã‚¤ã‚¢ã‚¹ãŒæ™‚é–“ã¨ã¨ã‚‚ã«å¢—åŠ ã—ãªã„ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚ã“ã‚Œã¯ã€æ¨™æº–ã®é€†å‚¾å‘ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆIPSï¼‰æ¨å®šé‡ã¨ã¯ç•°ãªã‚Šã¾ã™ã€‚

They also create adaptive variants of the estimators that change their parameters in real-time to improve estimation performance. 
å½¼ã‚‰ã¯ã¾ãŸã€æ¨å®šæ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰æ›´ã™ã‚‹æ¨å®šé‡ã®é©å¿œå‹ãƒãƒªã‚¢ãƒ³ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

Extensive empirical evaluation on recommendation datasets shows that the proposed estimators significantly outperform the regular IPS estimator and provide a more accurate estimation of a policyâ€™s true performance. 
æ¨è–¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹åºƒç¯„ãªå®Ÿè¨¼è©•ä¾¡ã¯ã€ææ¡ˆã•ã‚ŒãŸæ¨å®šé‡ãŒé€šå¸¸ã®IPSæ¨å®šé‡ã‚’å¤§å¹…ã«ä¸Šå›ã‚Šã€ãƒãƒªã‚·ãƒ¼ã®çœŸã®æ€§èƒ½ã®ã‚ˆã‚Šæ­£ç¢ºãªæ¨å®šã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

One drawback of the work is the trade-off between bias and variance. 
ã“ã®ç ”ç©¶ã®1ã¤ã®æ¬ ç‚¹ã¯ã€ãƒã‚¤ã‚¢ã‚¹ã¨åˆ†æ•£ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã§ã™ã€‚

While the estimators avoid a bias term that grows with time, they introduce variance that scales with the window size or decay factor. 
æ¨å®šé‡ã¯ã€æ™‚é–“ã¨ã¨ã‚‚ã«å¢—åŠ ã™ã‚‹ãƒã‚¤ã‚¢ã‚¹é …ã‚’å›é¿ã—ã¾ã™ãŒã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã‚„æ¸›è¡°ä¿‚æ•°ã«å¿œã˜ã¦ã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹åˆ†æ•£ã‚’å°å…¥ã—ã¾ã™ã€‚

Choosing a smaller window size or larger decay factor reduces bias but increases variance, and vice versa. 
å°ã•ãªã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã‚„å¤§ããªæ¸›è¡°ä¿‚æ•°ã‚’é¸æŠã™ã‚‹ã¨ãƒã‚¤ã‚¢ã‚¹ãŒæ¸›å°‘ã—ã¾ã™ãŒã€åˆ†æ•£ãŒå¢—åŠ ã—ã€ãã®é€†ã‚‚ã¾ãŸç„¶ã‚Šã§ã™ã€‚

Finding the optimal balance between bias and variance is a challenge. 
ãƒã‚¤ã‚¢ã‚¹ã¨åˆ†æ•£ã®æœ€é©ãªãƒãƒ©ãƒ³ã‚¹ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã¯èª²é¡Œã§ã™ã€‚

**Algorithmic Improvement Wang et al. [46] address the problem of designing a stable off-policy RL method for RS.** 
**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ”¹å–„ Wangã‚‰ [46] ã¯ã€RSã®ãŸã‚ã®å®‰å®šã—ãŸã‚ªãƒ•ãƒãƒªã‚·ãƒ¼RLæ‰‹æ³•ã®è¨­è¨ˆã®å•é¡Œã«å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™ã€‚**

Moreover, the exploration error is also highlighted, which arises from the mismatch between the recommendation policy and the distribution of customersâ€™ feedback in the training data. 
ã•ã‚‰ã«ã€æ¢ç´¢ã‚¨ãƒ©ãƒ¼ã‚‚å¼·èª¿ã•ã‚Œã¦ãŠã‚Šã€ã“ã‚Œã¯æ¨è–¦ãƒãƒªã‚·ãƒ¼ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«ãŠã‘ã‚‹é¡§å®¢ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®åˆ†å¸ƒã¨ã®ä¸ä¸€è‡´ã‹ã‚‰ç”Ÿã˜ã¾ã™ã€‚

This exploration error can lead to unstable training processes and potentially diverging results. 
ã“ã®æ¢ç´¢ã‚¨ãƒ©ãƒ¼ã¯ã€ä¸å®‰å®šãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã‚„æ½œåœ¨çš„ã«ç™ºæ•£ã™ã‚‹çµæœã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

To mitigate this problem, the authors propose an off-policy logged data method called Generator Constrained deep Q-learning (GCQ). 
ã“ã®å•é¡Œã‚’è»½æ¸›ã™ã‚‹ãŸã‚ã«ã€è‘—è€…ã‚‰ã¯Generator Constrained deep Q-learningï¼ˆGCQï¼‰ã¨å‘¼ã°ã‚Œã‚‹ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ã®ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿æ‰‹æ³•ã‚’ææ¡ˆã—ã¾ã™ã€‚

GCQ combines a neural generator that simulates customersâ€™ possible feedback with a Q-network that selects the highest valued action to form the recommendation policy. 
GCQã¯ã€é¡§å®¢ã®å¯èƒ½ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã¨ã€æ¨è–¦ãƒãƒªã‚·ãƒ¼ã‚’å½¢æˆã™ã‚‹ãŸã‚ã«æœ€ã‚‚ä¾¡å€¤ã®ã‚ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã™ã‚‹Qãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’çµ„ã¿åˆã‚ã›ã¦ã„ã¾ã™ã€‚

The authors also design the generatorâ€™s architecture based on Huffman Trees to reduce decision time. 
è‘—è€…ã‚‰ã¯ã€æ±ºå®šæ™‚é–“ã‚’çŸ­ç¸®ã™ã‚‹ãŸã‚ã«ã€Huffman Treesã«åŸºã¥ã„ã¦ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è¨­è¨ˆã—ã¦ã„ã¾ã™ã€‚

One drawback of this work is the limited capability to handle long sequences of user behavior. 
ã“ã®ç ”ç©¶ã®1ã¤ã®æ¬ ç‚¹ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã®é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å‡¦ç†ã™ã‚‹èƒ½åŠ›ãŒé™ã‚‰ã‚Œã¦ã„ã‚‹ã“ã¨ã§ã™ã€‚

Chen et al. [5] address the problem of data biases that arise when applying policy gradient methods in a recommendation system. 
Chenã‚‰ [5] ã¯ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹ãƒãƒªã‚·ãƒ¼å‹¾é…æ³•ã‚’é©ç”¨ã™ã‚‹éš›ã«ç”Ÿã˜ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ã‚¢ã‚¹ã®å•é¡Œã«å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™ã€‚

The primary goal is to address the distribution mismatch from the behavior policy ğœ‹ğ›½ and the learned policy ğœ‹. 
ä¸»ãªç›®æ¨™ã¯ã€è¡Œå‹•ãƒãƒªã‚·ãƒ¼ ğœ‹ğ›½ ã¨å­¦ç¿’ã•ã‚ŒãŸãƒãƒªã‚·ãƒ¼ ğœ‹ ã‹ã‚‰ã®åˆ†å¸ƒã®ä¸ä¸€è‡´ã«å¯¾å‡¦ã™ã‚‹ã“ã¨ã§ã™ã€‚

As a result, an off-policy-corrected gradient estimator is introduced to reduce the variance of each gradient term while still correcting for the bias of a non-corrected policy gradient. 
ãã®çµæœã€ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ä¿®æ­£å‹¾é…æ¨å®šé‡ãŒå°å…¥ã•ã‚Œã€å„å‹¾é…é …ã®åˆ†æ•£ã‚’æ¸›å°‘ã•ã›ã¤ã¤ã€ä¿®æ­£ã•ã‚Œã¦ã„ãªã„ãƒãƒªã‚·ãƒ¼å‹¾é…ã®ãƒã‚¤ã‚¢ã‚¹ã‚’ä¿®æ­£ã—ã¾ã™ã€‚

A recurrent neural network (RNN) is adopted to model the user state at each time step. 
å†å¸°ç¥çµŒãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆRNNï¼‰ãŒã€å„æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã§ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼çŠ¶æ…‹ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ãŸã‚ã«æ¡ç”¨ã•ã‚Œã¾ã™ã€‚

To estimate the behavior policy ğœ‹ğ›½, which is a mixture of the policies of multiple agents in the system, the authors use a context-dependent neural estimator which is a contextual bandit based method. 
ã‚·ã‚¹ãƒ†ãƒ å†…ã®è¤‡æ•°ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒãƒªã‚·ãƒ¼ã®æ··åˆã§ã‚ã‚‹è¡Œå‹•ãƒãƒªã‚·ãƒ¼ ğœ‹ğ›½ ã‚’æ¨å®šã™ã‚‹ãŸã‚ã«ã€è‘—è€…ã‚‰ã¯æ–‡è„ˆä¾å­˜ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ¨å®šé‡ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã¯æ–‡è„ˆçš„ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«åŸºã¥ãæ‰‹æ³•ã§ã™ã€‚

One drawback of the proposed method is the variance of the estimator, which can be large when there are very low or high values of the importance weights. 
ææ¡ˆã•ã‚ŒãŸæ–¹æ³•ã®1ã¤ã®æ¬ ç‚¹ã¯ã€æ¨å®šé‡ã®åˆ†æ•£ã§ã‚ã‚Šã€é‡è¦åº¦é‡ã¿ã®å€¤ãŒéå¸¸ã«ä½ã„ã¾ãŸã¯é«˜ã„å ´åˆã«å¤§ãããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

To reduce this variance, the authors take a first-order approximation and ignore the state visitation differences under the two policies. 
ã“ã®åˆ†æ•£ã‚’æ¸›å°‘ã•ã›ã‚‹ãŸã‚ã«ã€è‘—è€…ã‚‰ã¯ä¸€æ¬¡è¿‘ä¼¼ã‚’è¡Œã„ã€2ã¤ã®ãƒãƒªã‚·ãƒ¼ã®ä¸‹ã§ã®çŠ¶æ…‹è¨ªå•ã®é•ã„ã‚’ç„¡è¦–ã—ã¾ã™ã€‚

This results in a slightly biased estimator with a lower variance. 
ã“ã‚Œã«ã‚ˆã‚Šã€åˆ†æ•£ãŒä½ã„ãŒã‚ãšã‹ã«ãƒã‚¤ã‚¢ã‚¹ã®ã‚ã‚‹æ¨å®šé‡ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚

Another drawback is the difficulty in estimating the behavior policy ğœ‹ğ‘ğ‘’ğ‘¡ğ‘, especially when there are multiple agents in the system and the collected trajectories are generated by a mixture of deterministic policies and stochastic policies. 
ã‚‚ã†1ã¤ã®æ¬ ç‚¹ã¯ã€è¡Œå‹•ãƒãƒªã‚·ãƒ¼ ğœ‹ğ‘ğ‘’ğ‘¡ğ‘ ã‚’æ¨å®šã™ã‚‹ã“ã¨ã®é›£ã—ã•ã§ã‚ã‚Šã€ç‰¹ã«ã‚·ã‚¹ãƒ†ãƒ å†…ã«è¤‡æ•°ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå­˜åœ¨ã—ã€åé›†ã•ã‚ŒãŸè»Œè·¡ãŒæ±ºå®šè«–çš„ãƒãƒªã‚·ãƒ¼ã¨ç¢ºç‡çš„ãƒãƒªã‚·ãƒ¼ã®æ··åˆã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚Œã‚‹å ´åˆã§ã™ã€‚

Jeunen et al. [23] propose a new approach called the Dual Bandit, which combines value-based and policy-based methods to improve performance in recommendation settings. 
Jeunenã‚‰ [23] ã¯ã€æ¨è–¦è¨­å®šã«ãŠã‘ã‚‹æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€ä¾¡å€¤ãƒ™ãƒ¼ã‚¹ã¨ãƒãƒªã‚·ãƒ¼ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ãŸDual Banditã¨å‘¼ã°ã‚Œã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¾ã™ã€‚

It highlights that existing offline evaluation results are often even contradictory over different runs and datasets, or extremely hard to reproduce in a robust manner. 
æ—¢å­˜ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡çµæœã¯ã€ç•°ãªã‚‹å®Ÿè¡Œã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–“ã§çŸ›ç›¾ã—ã¦ã„ã‚‹ã“ã¨ãŒå¤šãã€å …ç‰¢ãªæ–¹æ³•ã§å†ç¾ã™ã‚‹ã“ã¨ãŒéå¸¸ã«é›£ã—ã„ã“ã¨ã‚’å¼·èª¿ã—ã¦ã„ã¾ã™ã€‚

Hence, they introduce simulation environments as an alternative and reproducible evaluation approach. 
ã—ãŸãŒã£ã¦ã€å½¼ã‚‰ã¯ä»£æ›¿ã®å†ç¾å¯èƒ½ãªè©•ä¾¡ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã—ã¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒã‚’å°å…¥ã—ã¾ã™ã€‚

**Others Sakhi et al. [41] introduce a probabilistic model known as BLOB (Bayesian Latent Organic Bandit) designed for bandit-based RS.** 
**ãã®ä»– Sakhiã‚‰ [41] ã¯ã€ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒ™ãƒ¼ã‚¹ã®RSã®ãŸã‚ã«è¨­è¨ˆã•ã‚ŒãŸBLOBï¼ˆãƒ™ã‚¤ã‚ºæ½œåœ¨æœ‰æ©Ÿãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆï¼‰ã¨å‘¼ã°ã‚Œã‚‹ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ã‚’å°å…¥ã—ã¾ã™ã€‚**

BLOB aims to enhance recommendation quality by combining organic user behavior (items viewed without intervention) with bandit signals (recommendations and their outcomes). 
BLOBã¯ã€ä»‹å…¥ãªã—ã§è¡¨ç¤ºã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ï¼ˆæœ‰æ©Ÿãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ï¼‰ã¨ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆä¿¡å·ï¼ˆæ¨è–¦ã¨ãã®çµæœï¼‰ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€æ¨è–¦ã®è³ªã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚

Traditional recommendation algorithms often focus on either organic-based or bandit-based approaches, but the authors recognize the potential to enhance recommendation quality by integrating both aspects. 
å¾“æ¥ã®æ¨è–¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€æœ‰æ©Ÿãƒ™ãƒ¼ã‚¹ã¾ãŸã¯ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ã„ãšã‚Œã‹ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ã“ã¨ãŒå¤šã„ã§ã™ãŒã€è‘—è€…ã‚‰ã¯ä¸¡æ–¹ã®å´é¢ã‚’çµ±åˆã™ã‚‹ã“ã¨ã§æ¨è–¦ã®è³ªã‚’å‘ä¸Šã•ã›ã‚‹å¯èƒ½æ€§ã‚’èªè­˜ã—ã¦ã„ã¾ã™ã€‚

The goal is to create a model that leverages the relationship between organic and bandit behaviors to provide more accurate and personalized recommendations. 
ç›®æ¨™ã¯ã€æœ‰æ©Ÿçš„è¡Œå‹•ã¨ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆè¡Œå‹•ã®é–¢ä¿‚ã‚’æ´»ç”¨ã—ã¦ã€ã‚ˆã‚Šæ­£ç¢ºã§ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸæ¨è–¦ã‚’æä¾›ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹ã“ã¨ã§ã™ã€‚

The proposed model uses a matrix variate prior distribution to relate these two types of behaviors, and variational autoencoders are employed for training. 
ææ¡ˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã€ã“ã‚Œã‚‰2ã¤ã®è¡Œå‹•ã‚¿ã‚¤ãƒ—ã‚’é–¢é€£ä»˜ã‘ã‚‹ãŸã‚ã«è¡Œåˆ—å¤‰é‡äº‹å‰åˆ†å¸ƒã‚’ä½¿ç”¨ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯å¤‰åˆ†ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

However, the proposed model requires a two-state training process which needs to train the model for organic behavior and bandit signals separately instead of training simultaneously. 
ã—ã‹ã—ã€ææ¡ˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã€åŒæ™‚ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã®ã§ã¯ãªãã€æœ‰æ©Ÿçš„è¡Œå‹•ã¨ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆä¿¡å·ã®ãŸã‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’åˆ¥ã€…ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å¿…è¦ãŒã‚ã‚‹2çŠ¶æ…‹ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã‚’å¿…è¦ã¨ã—ã¾ã™ã€‚

Xiao and Wang [55] present a value ranking algorithm that combines RL and ranking metrics to improve the effectiveness of ranking algorithms. 
Xiaoã¨Wang [55] ã¯ã€RLã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®åŠ¹æœã‚’å‘ä¸Šã•ã›ã‚‹ä¾¡å€¤ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆã—ã¾ã™ã€‚

The proposed method uses the concept of extrapolation and regularization to address the challenges of partial and sparse rewards. 
ææ¡ˆã•ã‚ŒãŸæ–¹æ³•ã¯ã€éƒ¨åˆ†çš„ãŠã‚ˆã³ã‚¹ãƒ‘ãƒ¼ã‚¹ãªå ±é…¬ã®èª²é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€å¤–æŒ¿ã¨æ­£å‰‡åŒ–ã®æ¦‚å¿µã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

Extrapolation is used to estimate rewards from logged feedback, while regularization is used to incorporate ranking signals into the RL policy. 
å¤–æŒ¿ã¯ãƒ­ã‚°ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰å ±é…¬ã‚’æ¨å®šã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã€æ­£å‰‡åŒ–ã¯ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä¿¡å·ã‚’RLãƒãƒªã‚·ãƒ¼ã«çµ„ã¿è¾¼ã‚€ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

The authors propose a sequential Expectation-Maximization (EM) framework that alternates between the E-step, which estimates rewards and ranking signals, and the M-step, which optimizes the RL policy. 
è‘—è€…ã‚‰ã¯ã€å ±é…¬ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä¿¡å·ã‚’æ¨å®šã™ã‚‹Eã‚¹ãƒ†ãƒƒãƒ—ã¨ã€RLãƒãƒªã‚·ãƒ¼ã‚’æœ€é©åŒ–ã™ã‚‹Mã‚¹ãƒ†ãƒƒãƒ—ã®é–“ã§äº¤äº’ã«é€²ã‚€é€æ¬¡çš„æœŸå¾…æœ€å¤§åŒ–ï¼ˆEMï¼‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã¾ã™ã€‚

They show that this framework can effectively learn from rewards and ranking signals. 
å½¼ã‚‰ã¯ã€ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒå ±é…¬ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä¿¡å·ã‹ã‚‰åŠ¹æœçš„ã«å­¦ç¿’ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

This proposed algorithmâ€™s drawback lies in the bandit setting, as it doesnâ€™t account for future rewards. 
ã“ã®ææ¡ˆã•ã‚ŒãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¬ ç‚¹ã¯ã€ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆè¨­å®šã«ã‚ã‚Šã€å°†æ¥ã®å ±é…¬ã‚’è€ƒæ…®ã—ã¦ã„ãªã„ã“ã¨ã§ã™ã€‚

Additionally, in the full RL setting, it might suffer from the curse of dimensionality. 
ã•ã‚‰ã«ã€å®Œå…¨ãªRLè¨­å®šã§ã¯ã€æ¬¡å…ƒã®å‘ªã„ã«æ‚©ã¾ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

Hong et al. [19] address the complex issue of multi-task off-policy learning from bandit feedback, a challenge that has significant implications for various applications, including RS. 
Hongã‚‰ [19] ã¯ã€ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å­¦ç¿’ã®è¤‡é›‘ãªå•é¡Œã«å–ã‚Šçµ„ã‚“ã§ãŠã‚Šã€ã“ã‚Œã¯RSã‚’å«ã‚€ã•ã¾ã–ã¾ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«é‡è¦ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹èª²é¡Œã§ã™ã€‚

It is motivated to develop a solution that can efficiently handle multiple tasks simultaneously, leveraging the relationships between tasks to enhance performance. 
ã“ã‚Œã¯ã€ã‚¿ã‚¹ã‚¯é–“ã®é–¢ä¿‚ã‚’æ´»ç”¨ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ã‚’åŒæ™‚ã«åŠ¹ç‡çš„ã«å‡¦ç†ã§ãã‚‹ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹ç™ºã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚

It proposes a hierarchical off-policy optimization algorithm (HierOPO) to tackle this problem. 
ã“ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€éšå±¤çš„ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆHierOPOï¼‰ã‚’ææ¡ˆã—ã¾ã™ã€‚

The problem is formulated as a contextual off-policy optimization within a hierarchical graphical model, specifically focusing on linear Gaussian models. 
ã“ã®å•é¡Œã¯ã€éšå±¤çš„ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«å†…ã®æ–‡è„ˆçš„ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼æœ€é©åŒ–ã¨ã—ã¦å®šå¼åŒ–ã•ã‚Œã€ç‰¹ã«ç·šå½¢ã‚¬ã‚¦ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚

The authors provide an efficient implementation and analysis, proving per-task bounds on the sub-optimality of the learned policies. 
è‘—è€…ã‚‰ã¯ã€åŠ¹ç‡çš„ãªå®Ÿè£…ã¨åˆ†æã‚’æä¾›ã—ã€å­¦ç¿’ã•ã‚ŒãŸãƒãƒªã‚·ãƒ¼ã®ã‚µãƒ–æœ€é©æ€§ã«é–¢ã™ã‚‹ã‚¿ã‚¹ã‚¯ã”ã¨ã®å¢ƒç•Œã‚’è¨¼æ˜ã—ã¾ã™ã€‚

They demonstrate that using the hierarchy improves performance compared to solving each task independently. 
å½¼ã‚‰ã¯ã€éšå±¤ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€å„ã‚¿ã‚¹ã‚¯ã‚’ç‹¬ç«‹ã«è§£æ±ºã™ã‚‹ã‚ˆã‚Šã‚‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

The algorithm is evaluated on synthetic problems and applied to a multi-user recommendation system. 
ã“ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€åˆæˆå•é¡Œã§è©•ä¾¡ã•ã‚Œã€ãƒãƒ«ãƒãƒ¦ãƒ¼ã‚¶ãƒ¼æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«é©ç”¨ã•ã‚Œã¾ã™ã€‚

However, the proposed method is a model-based off-policy approach, the model-based approaches tend to be biased, due to using a potentially misspecified model. 
ã—ã‹ã—ã€ææ¡ˆã•ã‚ŒãŸæ–¹æ³•ã¯ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€æ½œåœ¨çš„ã«èª¤æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€ãƒã‚¤ã‚¢ã‚¹ãŒã‹ã‹ã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚

**3.2 Offline RL4RS**
**3.2 ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RS**

In this section, we will provide reviews of existing offline RL4RS methods. 
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€æ—¢å­˜ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSæ‰‹æ³•ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æä¾›ã—ã¾ã™ã€‚

Different from off-policy evaluation, offline RL4RS does not limit the setting to bandit-based methods. 
ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è©•ä¾¡ã¨ã¯ç•°ãªã‚Šã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã¯è¨­å®šã‚’ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã«åˆ¶é™ã—ã¾ã›ã‚“ã€‚

Moreover, in this part, we have included the off-policy learning based methods as offline RL. 
ã•ã‚‰ã«ã€ã“ã®éƒ¨åˆ†ã§ã¯ã€ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å­¦ç¿’ã«åŸºã¥ãæ‰‹æ³•ã‚’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã¨ã—ã¦å«ã‚ã¦ã„ã¾ã™ã€‚

However, the existing works in this field lack organization, with no apparent interconnection among the various works that often emphasize different aspects. 
ã—ã‹ã—ã€ã“ã®åˆ†é‡ã®æ—¢å­˜ã®ç ”ç©¶ã¯çµ„ç¹”åŒ–ã•ã‚Œã¦ãŠã‚‰ãšã€ã•ã¾ã–ã¾ãªç ”ç©¶ã®é–“ã«æ˜ã‚‰ã‹ãªç›¸äº’æ¥ç¶šãŒãªãã€ã—ã°ã—ã°ç•°ãªã‚‹å´é¢ã‚’å¼·èª¿ã—ã¦ã„ã¾ã™ã€‚

Currently, we lack a systematic approach to review these works, resorting to a sequential examination of each one individually. 
ç¾åœ¨ã€ã“ã‚Œã‚‰ã®ç ”ç©¶ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹ãŸã‚ã®ä½“ç³»çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒæ¬ ã‘ã¦ãŠã‚Šã€å„ç ”ç©¶ã‚’å€‹åˆ¥ã«é †æ¬¡æ¤œè¨ã™ã‚‹ã“ã¨ã«é ¼ã£ã¦ã„ã¾ã™ã€‚

Ma et al. [33] discuss off-policy learning in two-stage RS. 
Maã‚‰ [33] ã¯ã€2æ®µéšã®RSã«ãŠã‘ã‚‹ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å­¦ç¿’ã«ã¤ã„ã¦è«–ã˜ã¦ã„ã¾ã™ã€‚

The proposed method consists of a candidate generation model in the first stage and a ranking model in the second stage. 
ææ¡ˆã•ã‚ŒãŸæ–¹æ³•ã¯ã€ç¬¬ä¸€æ®µéšã§å€™è£œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã€ç¬¬äºŒæ®µéšã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚

The authors propose a two-stage off-policy policy gradient method that takes into account the ranking model when training the candidate generation model. 
è‘—è€…ã‚‰ã¯ã€å€™è£œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹éš›ã«ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’è€ƒæ…®ã«å…¥ã‚ŒãŸ2æ®µéšã®ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼å‹¾é…æ³•ã‚’ææ¡ˆã—ã¾ã™ã€‚

The proposed method employs IPS to correct the bias and design variance reduction tricks to reduce the variance. 
ææ¡ˆã•ã‚ŒãŸæ–¹æ³•ã¯ã€ãƒã‚¤ã‚¢ã‚¹ã‚’ä¿®æ­£ã™ã‚‹ãŸã‚ã«IPSã‚’ä½¿ç”¨ã—ã€åˆ†æ•£ã‚’æ¸›å°‘ã•ã›ã‚‹ãŸã‚ã®åˆ†æ•£å‰Šæ¸›ãƒˆãƒªãƒƒã‚¯ã‚’è¨­è¨ˆã—ã¾ã™ã€‚

However, the proposed method does not provide a comprehensive experiment about how the ranking model and the candidate generation model affect the final performance. 
ã—ã‹ã—ã€ææ¡ˆã•ã‚ŒãŸæ–¹æ³•ã¯ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¨å€™è£œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒæœ€çµ‚çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«ã©ã®ã‚ˆã†ã«å½±éŸ¿ã™ã‚‹ã‹ã«ã¤ã„ã¦ã®åŒ…æ‹¬çš„ãªå®Ÿé¨“ã‚’æä¾›ã—ã¦ã„ã¾ã›ã‚“ã€‚

Chen et al. [6] focus on scaling an off-policy actor-critic algorithm for industrial recommendation systems. 
Chenã‚‰ [6] ã¯ã€ç”£æ¥­ç”¨æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ãŸã‚ã®ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ã‚¢ã‚¯ã‚¿ãƒ¼-ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚

The motivation behind their research is to address the challenges of offline evaluation and learning in RS, where only partial feedback is available. 
å½¼ã‚‰ã®ç ”ç©¶ã®å‹•æ©Ÿã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡ã¨RSã«ãŠã‘ã‚‹å­¦ç¿’ã®èª²é¡Œã«å¯¾å‡¦ã™ã‚‹ã“ã¨ã§ã‚ã‚Šã€ã“ã“ã§ã¯éƒ¨åˆ†çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã—ã‹åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚

The authors propose an approach that combines off-policy learning with importance weighting to estimate the value of state-action pairs under the target policy. 
è‘—è€…ã‚‰ã¯ã€ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å­¦ç¿’ã¨é‡è¦åº¦é‡ã¿ä»˜ã‘ã‚’çµ„ã¿åˆã‚ã›ã¦ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒªã‚·ãƒ¼ã®ä¸‹ã§ã®çŠ¶æ…‹-è¡Œå‹•ãƒšã‚¢ã®ä¾¡å€¤ã‚’æ¨å®šã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¾ã™ã€‚

They use a critic network to estimate the value function and update the policy network accordingly. 
å½¼ã‚‰ã¯ã€ä¾¡å€¤é–¢æ•°ã‚’æ¨å®šã™ã‚‹ãŸã‚ã«ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã€ãã‚Œã«å¿œã˜ã¦ãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ›´æ–°ã—ã¾ã™ã€‚

The methodology involves minimizing the temporal difference loss and using a Huber loss to handle outliers. 
ã“ã®æ–¹æ³•è«–ã¯ã€æ™‚é–“å·®æå¤±ã‚’æœ€å°åŒ–ã—ã€å¤–ã‚Œå€¤ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã«ãƒã‚¤ãƒãƒ¼ãƒ­ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’å«ã¿ã¾ã™ã€‚

The authors also investigate the impact of different estimation methods for the target value function. 
è‘—è€…ã‚‰ã¯ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¾¡å€¤é–¢æ•°ã®ç•°ãªã‚‹æ¨å®šæ‰‹æ³•ã®å½±éŸ¿ã‚‚èª¿æŸ»ã—ã¾ã™ã€‚

However, the proposed methods have several limitations. 
ã—ã‹ã—ã€ææ¡ˆã•ã‚ŒãŸæ–¹æ³•ã«ã¯ã„ãã¤ã‹ã®åˆ¶é™ãŒã‚ã‚Šã¾ã™ã€‚

One drawback is the potential bias introduced by using the cumulative future return on the behavior trajectory while ignoring the importance weighting on future trajectories. 
1ã¤ã®æ¬ ç‚¹ã¯ã€å°†æ¥ã®è»Œè·¡ã«å¯¾ã™ã‚‹é‡è¦åº¦é‡ã¿ä»˜ã‘ã‚’ç„¡è¦–ã—ãªãŒã‚‰ã€è¡Œå‹•è»Œè·¡ã®ç´¯ç©æœªæ¥ãƒªã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦å°å…¥ã•ã‚Œã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ãƒã‚¤ã‚¢ã‚¹ã§ã™ã€‚

Another drawback is the conservative nature of the learned policy when using sampling from the learned policy. 
ã‚‚ã†1ã¤ã®æ¬ ç‚¹ã¯ã€å­¦ç¿’ã•ã‚ŒãŸãƒãƒªã‚·ãƒ¼ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹éš›ã®å­¦ç¿’ã•ã‚ŒãŸãƒãƒªã‚·ãƒ¼ã®ä¿å®ˆçš„ãªæ€§è³ªã§ã™ã€‚

The softmax policy parameterization used in the approach leads to a more myopic policy, recommending more popular and longer content and less novel content. 
ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ä½¿ç”¨ã•ã‚Œã‚‹ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ãƒãƒªã‚·ãƒ¼ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã¯ã€ã‚ˆã‚Šäººæ°—ã®ã‚ã‚‹é•·ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¨å¥¨ã—ã€ã‚ˆã‚Šæ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å°‘ãªãã™ã‚‹ã‚ˆã‚Šè¿‘è¦–çš„ãªãƒãƒªã‚·ãƒ¼ã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚

Gao et al. [15] centre around the problem of the Matthew effect in offline RL based RS. 
Gaoã‚‰ [15] ã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã«åŸºã¥ãRSã«ãŠã‘ã‚‹ãƒã‚·ãƒ¥ãƒ¼åŠ¹æœã®å•é¡Œã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚

The Matthew effect describes a phenomenon where popular items or categories are recommended more frequently, leading to the neglect of less popular ones. 
ãƒã‚·ãƒ¥ãƒ¼åŠ¹æœã¯ã€äººæ°—ã®ã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã‚„ã‚«ãƒ†ã‚´ãƒªãŒã‚ˆã‚Šé »ç¹ã«æ¨è–¦ã•ã‚Œã‚‹ç¾è±¡ã‚’èª¬æ˜ã—ã€çµæœã¨ã—ã¦äººæ°—ã®ãªã„ã‚¢ã‚¤ãƒ†ãƒ ãŒç„¡è¦–ã•ã‚Œã‚‹ã“ã¨ã«ã¤ãªãŒã‚Šã¾ã™ã€‚

This bias towards popular items can reduce the diversity in recommendations and decrease user satisfaction. 
äººæ°—ã®ã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã™ã‚‹ã“ã®ãƒã‚¤ã‚¢ã‚¹ã¯ã€æ¨è–¦ã®å¤šæ§˜æ€§ã‚’æ¸›å°‘ã•ã›ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æº€è¶³åº¦ã‚’ä½ä¸‹ã•ã›ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

To address the Matthew effect, the authors propose a Debiased model-based Offline RL (DORL) method. 
ãƒã‚·ãƒ¥ãƒ¼åŠ¹æœã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€è‘—è€…ã‚‰ã¯ãƒ‡ãƒã‚¤ã‚¢ã‚¹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLï¼ˆDORLï¼‰æ‰‹æ³•ã‚’ææ¡ˆã—ã¾ã™ã€‚

DORL introduces a penalty term to the RL algorithm, encouraging exploration and diversity in recommendations. 
DORLã¯ã€RLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ãƒšãƒŠãƒ«ãƒ†ã‚£é …ã‚’å°å…¥ã—ã€æ¨è–¦ã«ãŠã‘ã‚‹æ¢ç´¢ã¨å¤šæ§˜æ€§ã‚’ä¿ƒé€²ã—ã¾ã™ã€‚

By adding this penalty, the method aims to reduce the bias towards popular items and promote a more varied selection. 
ã“ã®ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€æ–¹æ³•ã¯äººæ°—ã®ã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã™ã‚‹ãƒã‚¤ã‚¢ã‚¹ã‚’æ¸›å°‘ã•ã›ã€ã‚ˆã‚Šå¤šæ§˜ãªé¸æŠã‚’ä¿ƒé€²ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚

Wang et al. [47] address the challenges inherent in designing reward functions and handling large-scale datasets within RL4RS. 
Wangã‚‰ [47] ã¯ã€RL4RSå†…ã§å ±é…¬é–¢æ•°ã‚’è¨­è¨ˆã—ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ‰±ã†éš›ã®å›ºæœ‰ã®èª²é¡Œã«å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™ã€‚

Traditional RL4RS approaches may fall short in accurately estimating rewards only based on limited observations. 
å¾“æ¥ã®RL4RSã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€é™ã‚‰ã‚ŒãŸè¦³å¯Ÿã«åŸºã¥ã„ã¦å ±é…¬ã‚’æ­£ç¢ºã«æ¨å®šã™ã‚‹ã“ã¨ã«å¤±æ•—ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

To address this problem, a Causal Decision Transformer for RS (CDT4Rec) is proposed, a novel model that integrates offline RL and transformer architecture. 
ã“ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã¨ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’çµ±åˆã—ãŸæ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹RSã®ãŸã‚ã®å› æœæ±ºå®šãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ï¼ˆCDT4Recï¼‰ãŒææ¡ˆã•ã‚Œã¾ã™ã€‚

CDT4Rec employs a causal mechanism to estimate rewards based on user behavior, allowing for a more accurate understanding of user preferences. 
CDT4Recã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã«åŸºã¥ã„ã¦å ±é…¬ã‚’æ¨å®šã™ã‚‹å› æœãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æ¡ç”¨ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã‚’ã‚ˆã‚Šæ­£ç¢ºã«ç†è§£ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

The transformer architecture is used to process large datasets and capture dependencies, enabling the model to handle complex data structures. 
ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‡¦ç†ã—ã€ä¾å­˜é–¢ä¿‚ã‚’æ‰ãˆã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ãŒè¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’æ‰±ãˆã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

Yuan et al. [57] is motivated by the challenges associated with optimizing mobile notification systems. 
Yuanã‚‰ [57] ã¯ã€ãƒ¢ãƒã‚¤ãƒ«é€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã®æœ€é©åŒ–ã«é–¢é€£ã™ã‚‹èª²é¡Œã«å‹•æ©Ÿã¥ã‘ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

Traditional response-prediction models often struggle to accurately attribute the impact to a single notification, leading to inefficiencies in managing and delivering notifications. 
å¾“æ¥ã®å¿œç­”äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã¯ã€å˜ä¸€ã®é€šçŸ¥ã«å½±éŸ¿ã‚’æ­£ç¢ºã«å¸°å±ã•ã›ã‚‹ã“ã¨ã«è‹¦åŠ´ã—ã€é€šçŸ¥ã®ç®¡ç†ã¨é…ä¿¡ã«ãŠã‘ã‚‹éåŠ¹ç‡ã‚’å¼•ãèµ·ã“ã—ã¾ã™ã€‚

Recognizing this limitation, the authors aim to explore the application of RL to enhance the decision-making process for sequential notifications, seeking to provide a more effective and targeted approach to mobile notification systems. 
ã“ã®åˆ¶é™ã‚’èªè­˜ã—ã€è‘—è€…ã‚‰ã¯ã€é€£ç¶šé€šçŸ¥ã®æ„æ€æ±ºå®šãƒ—ãƒ­ã‚»ã‚¹ã‚’å¼·åŒ–ã™ã‚‹ãŸã‚ã«RLã®é©ç”¨ã‚’æ¢æ±‚ã—ã€ãƒ¢ãƒã‚¤ãƒ«é€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã«å¯¾ã—ã¦ã‚ˆã‚ŠåŠ¹æœçš„ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’çµã£ãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚

Hence, an offline RL framework specifically designed for sequential notification decisions is proposed. 
ã—ãŸãŒã£ã¦ã€é€£ç¶šé€šçŸ¥æ±ºå®šã®ãŸã‚ã«ç‰¹åˆ¥ã«è¨­è¨ˆã•ã‚ŒãŸã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒææ¡ˆã•ã‚Œã¾ã™ã€‚

They introduce a state-marginalized importance sampling policy evaluation approach, which is a novel method to assess the effectiveness of different notification strategies. 
å½¼ã‚‰ã¯ã€ç•°ãªã‚‹é€šçŸ¥æˆ¦ç•¥ã®åŠ¹æœã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®æ–°ã—ã„æ–¹æ³•ã§ã‚ã‚‹çŠ¶æ…‹å‘¨è¾ºåŒ–é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒãƒªã‚·ãƒ¼è©•ä¾¡ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å°å…¥ã—ã¾ã™ã€‚

Through simulations, the authors demonstrate the performance of the approach, and they also present a real-world application of the framework, detailing the practical considerations and results. 
ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é€šã˜ã¦ã€è‘—è€…ã‚‰ã¯ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ€§èƒ½ã‚’ç¤ºã—ã€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®å®Ÿä¸–ç•Œã§ã®é©ç”¨ã‚’æç¤ºã—ã€å®Ÿéš›ã®è€ƒæ…®äº‹é …ã¨çµæœã‚’è©³è¿°ã—ã¾ã™ã€‚

Wang et al. [50] are motivated by the challenge of adapting to new users in recommendation systems, particularly when there are limited interactions to understand user preferences. 
Wangã‚‰ [50] ã¯ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹æ–°ã—ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®é©å¿œã®èª²é¡Œã«å‹•æ©Ÿã¥ã‘ã‚‰ã‚Œã¦ãŠã‚Šã€ç‰¹ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®ç›¸äº’ä½œç”¨ãŒé™ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆã§ã™ã€‚

This situation, often referred to as the â€œcold-startâ€ problem, can hinder the ability to provide personalized recommendations that align with long-term user interests. 
ã“ã®çŠ¶æ³ã¯ã€ã—ã°ã—ã°ã€Œã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆã€å•é¡Œã¨å‘¼ã°ã‚Œã€é•·æœŸçš„ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ã«æ²¿ã£ãŸãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸæ¨è–¦ã‚’æä¾›ã™ã‚‹èƒ½åŠ›ã‚’å¦¨ã’ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

The proposed approach introduces a user context variable to represent user preferences, employing a meta-level model-based RL method for rapid user adaptation. 
ææ¡ˆã•ã‚ŒãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã‚’è¡¨ã™ãŸã‚ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ•°ã‚’å°å…¥ã—ã€è¿…é€Ÿãªãƒ¦ãƒ¼ã‚¶ãƒ¼é©å¿œã®ãŸã‚ã®ãƒ¡ã‚¿ãƒ¬ãƒ™ãƒ«ã®ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹RLæ‰‹æ³•ã‚’æ¡ç”¨ã—ã¾ã™ã€‚

The user model and recommendation agent interact alternately, with the interaction relationship modeled from an information-theoretic perspective. 
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¢ãƒ‡ãƒ«ã¨æ¨è–¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯äº¤äº’ã«ç›¸äº’ä½œç”¨ã—ã€ç›¸äº’ä½œç”¨é–¢ä¿‚ã¯æƒ…å ±ç†è«–çš„ãªè¦–ç‚¹ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«åŒ–ã•ã‚Œã¾ã™ã€‚

Zhang et al. [59] discuss the problem of interactive recommendation with natural-language feedback and proposes an offline RL framework to address the challenges of collecting experience through user interaction. 
Zhangã‚‰ [59] ã¯ã€è‡ªç„¶è¨€èªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä¼´ã†ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ¨è–¦ã®å•é¡Œã«ã¤ã„ã¦è«–ã˜ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦çµŒé¨“ã‚’åé›†ã™ã‚‹èª²é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã¾ã™ã€‚

The authors develop a behavior-agnostic off-policy correction framework that leverages the conservative Q-function for off-policy evaluation. 
è‘—è€…ã‚‰ã¯ã€ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è©•ä¾¡ã®ãŸã‚ã«ä¿å®ˆçš„Qé–¢æ•°ã‚’æ´»ç”¨ã™ã‚‹è¡Œå‹•ç„¡é–¢ä¿‚ãªã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ä¿®æ­£ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’é–‹ç™ºã—ã¾ã™ã€‚

This allows for learning effective policies from fixed datasets without further interactions. 
ã“ã‚Œã«ã‚ˆã‚Šã€ã•ã‚‰ãªã‚‹ç›¸äº’ä½œç”¨ãªã—ã«å›ºå®šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰åŠ¹æœçš„ãªãƒãƒªã‚·ãƒ¼ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

Xiao and Wang [54] propose a general offline RL framework for the interactive recommendation. 
Xiaoã¨Wang [54] ã¯ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ¨è–¦ã®ãŸã‚ã®ä¸€èˆ¬çš„ãªã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã¾ã™ã€‚

The proposed method introduces different techniques such as support constraints, supervised regularization, policy constraints, dual constraints, and reward extrapolation. 
ææ¡ˆã•ã‚ŒãŸæ–¹æ³•ã¯ã€ã‚µãƒãƒ¼ãƒˆåˆ¶ç´„ã€ç›£è¦–æ­£å‰‡åŒ–ã€ãƒãƒªã‚·ãƒ¼åˆ¶ç´„ã€äºŒé‡åˆ¶ç´„ã€å ±é…¬å¤–æŒ¿ãªã©ã®ã•ã¾ã–ã¾ãªæŠ€è¡“ã‚’å°å…¥ã—ã¾ã™ã€‚

These methods aim to minimize the mismatch between the recommendation policy and logging policy and to balance the supervised signal and task reward. 
ã“ã‚Œã‚‰ã®æ–¹æ³•ã¯ã€æ¨è–¦ãƒãƒªã‚·ãƒ¼ã¨ãƒ­ã‚°ãƒãƒªã‚·ãƒ¼ã®ä¸ä¸€è‡´ã‚’æœ€å°é™ã«æŠ‘ãˆã€ç›£è¦–ä¿¡å·ã¨ã‚¿ã‚¹ã‚¯å ±é…¬ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚



## 4 CHALLENGES AND OPPORTUNITIES èª²é¡Œã¨æ©Ÿä¼š

Offline RL4RS is an emerging domain that introduces multiple challenges demanding comprehensive exploration. 
ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã¯ã€åŒ…æ‹¬çš„ãªæ¢ç´¢ã‚’è¦æ±‚ã™ã‚‹è¤‡æ•°ã®èª²é¡Œã‚’å°å…¥ã™ã‚‹æ–°èˆˆåˆ†é‡ã§ã™ã€‚
In this section, we aim to outline the open challenges in offline RL4RS. 
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã«ãŠã‘ã‚‹æœªè§£æ±ºã®èª²é¡Œã‚’æ¦‚èª¬ã—ã¾ã™ã€‚
Given that RS fall under the application scope of offline RL, several shared challenges naturally arise. 
RSã¯ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®å¿œç”¨ç¯„å›²ã«å«ã¾ã‚Œã‚‹ãŸã‚ã€ã„ãã¤ã‹ã®å…±é€šã®èª²é¡ŒãŒè‡ªç„¶ã«ç”Ÿã˜ã¾ã™ã€‚
We will begin by addressing some common challenges before delving into the specific challenges unique to RS when utilizing offline RL techniques. 
ä¸€èˆ¬çš„ãªèª²é¡Œã«å–ã‚Šçµ„ã‚“ã å¾Œã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLæŠ€è¡“ã‚’åˆ©ç”¨ã™ã‚‹éš›ã®RSç‰¹æœ‰ã®èª²é¡Œã«æ·±ãæ˜ã‚Šä¸‹ã’ã¦ã„ãã¾ã™ã€‚

**4.1** **High-quality Offline Data and Cold-Start Problems é«˜å“è³ªãªã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã¨ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆå•é¡Œ** 
One of the most prominent challenges in offline Reinforcement Learning (RL) lies in the fact that the learning process hinges solely on the provided static dataset. 
ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã«ãŠã‘ã‚‹æœ€ã‚‚é¡•è‘—ãªèª²é¡Œã®ä¸€ã¤ã¯ã€å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ãŒæä¾›ã•ã‚ŒãŸé™çš„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã¿ã«ä¾å­˜ã—ã¦ã„ã‚‹ã¨ã„ã†äº‹å®Ÿã§ã™ã€‚
This limitation results in a significant obstacle to enhancing exploration, as exploration falls outside the algorithmâ€™s purview. 
ã“ã®åˆ¶é™ã¯ã€æ¢ç´¢ãŒã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç¯„å›²å¤–ã«ã‚ã‚‹ãŸã‚ã€æ¢ç´¢ã‚’å¼·åŒ–ã™ã‚‹ä¸Šã§ã®é‡è¦ãªéšœå®³ã¨ãªã‚Šã¾ã™ã€‚
Consequently, if the dataset lacks transitions that demonstrate regions of the state space yielding high rewards, the algorithm may be fundamentally incapable of uncovering these rewarding regions. 
ã—ãŸãŒã£ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é«˜ã„å ±é…¬ã‚’ç”Ÿã¿å‡ºã™çŠ¶æ…‹ç©ºé–“ã®é ˜åŸŸã‚’ç¤ºã™é·ç§»ãŒæ¬ ã‘ã¦ã„ã‚‹å ´åˆã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã“ã‚Œã‚‰ã®å ±é…¬ã‚’å¾—ã‚‰ã‚Œã‚‹é ˜åŸŸã‚’ç™ºè¦‹ã™ã‚‹ã“ã¨ãŒæ ¹æœ¬çš„ã«ä¸å¯èƒ½ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
In contrast to control tasks, which are common in offline RL applications and often face challenges in gathering comprehensive data to facilitate effective learning from high-reward scenarios, the landscape changes when it comes to RS. 
ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ä¸€èˆ¬çš„ãªåˆ¶å¾¡ã‚¿ã‚¹ã‚¯ã¨ã¯å¯¾ç…§çš„ã«ã€é«˜å ±é…¬ã‚·ãƒŠãƒªã‚ªã‹ã‚‰åŠ¹æœçš„ã«å­¦ç¿’ã™ã‚‹ãŸã‚ã®åŒ…æ‹¬çš„ãªãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã™ã‚‹ã“ã¨ã«èª²é¡ŒãŒã‚ã‚‹å ´åˆãŒå¤šã„ã§ã™ãŒã€RSã«é–¢ã—ã¦ã¯çŠ¶æ³ãŒå¤‰ã‚ã‚Šã¾ã™ã€‚
In this domain, a plethora of offline datasets, such as those from MovieLens, GoodReads, and Amazon, are publicly available. 
ã“ã®åˆ†é‡ã§ã¯ã€MovieLensã€GoodReadsã€Amazonãªã©ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå¤šæ•°å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚
These datasets stem from real-world interactions and adeptly capture usersâ€™ preferences. 
ã“ã‚Œã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€å®Ÿä¸–ç•Œã®ç›¸äº’ä½œç”¨ã‹ã‚‰æ´¾ç”Ÿã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã‚’å·§ã¿ã«æ‰ãˆã¦ã„ã¾ã™ã€‚
However, RS diverge from traditional offline RL application domains due to their distinct characteristics. 
ã—ã‹ã—ã€RSã¯ãã®ç‹¬è‡ªã®ç‰¹æ€§ã«ã‚ˆã‚Šã€å¾“æ¥ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLå¿œç”¨åˆ†é‡ã‹ã‚‰é€¸è„±ã—ã¦ã„ã¾ã™ã€‚
To illustrate, letâ€™s consider implicit feedback, particularly review data. 
ä¾‹ã‚’æŒ™ã’ã‚‹ã¨ã€æš—é»™ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã€ç‰¹ã«ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚
This kind of data poses a challenge when attempting to embed it within the state space due to its reliance on text. 
ã“ã®ç¨®ã®ãƒ‡ãƒ¼ã‚¿ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã«ä¾å­˜ã—ã¦ã„ã‚‹ãŸã‚ã€çŠ¶æ…‹ç©ºé–“ã«åŸ‹ã‚è¾¼ã‚€éš›ã«èª²é¡Œã‚’å¼•ãèµ·ã“ã—ã¾ã™ã€‚
Although techniques like word2vec exist to transform textual data into vectors that might potentially be integrated into the state space, the question of how to effectively guide the agent in utilizing such data in RS remains unexplored. 
word2vecã®ã‚ˆã†ãªæŠ€è¡“ãŒå­˜åœ¨ã—ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’çŠ¶æ…‹ç©ºé–“ã«çµ±åˆã§ãã‚‹ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ãŒã€RSã«ãŠã„ã¦ãã®ã‚ˆã†ãªãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹æœçš„ã«åˆ©ç”¨ã™ã‚‹ãŸã‚ã«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ã©ã®ã‚ˆã†ã«å°ãã‹ã¨ã„ã†å•é¡Œã¯æœªè§£æ±ºã®ã¾ã¾ã§ã™ã€‚
Another intriguing aspect is the presence of graph data, extensively used in RS to represent social connections, item relationships, and more. 
ã‚‚ã†ä¸€ã¤ã®èˆˆå‘³æ·±ã„å´é¢ã¯ã€RSã§åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ã§ã€ç¤¾ä¼šçš„ãªã¤ãªãŒã‚Šã‚„ã‚¢ã‚¤ãƒ†ãƒ ã®é–¢ä¿‚ãªã©ã‚’è¡¨ç¾ã—ã¾ã™ã€‚
The prevalent form of representation is a knowledge graph, which can be transformed into embeddings through the application of Graph Neural Networks (GNN). 
ä¸€èˆ¬çš„ãªè¡¨ç¾å½¢å¼ã¯çŸ¥è­˜ã‚°ãƒ©ãƒ•ã§ã‚ã‚Šã€Graph Neural Networks (GNN)ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§åŸ‹ã‚è¾¼ã¿ã«å¤‰æ›ã§ãã¾ã™ã€‚
Nonetheless, it faces a similar challenge as textual data: how to empower the agent to effectively utilize this information. 
ãã‚Œã§ã‚‚ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨åŒæ§˜ã®èª²é¡Œã«ç›´é¢ã—ã¦ã„ã¾ã™ï¼šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã“ã®æƒ…å ±ã‚’åŠ¹æœçš„ã«åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹æ–¹æ³•ã§ã™ã€‚
There are some works investigating graph RL which may be able to provide some directions to offline RL4RS. 
ã‚°ãƒ©ãƒ•RLã‚’èª¿æŸ»ã—ã¦ã„ã‚‹ã„ãã¤ã‹ã®ç ”ç©¶ãŒã‚ã‚Šã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã«å¯¾ã™ã‚‹ã„ãã¤ã‹ã®æ–¹å‘æ€§ã‚’æä¾›ã§ãã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚
However, a challenge surfaces due to whatâ€™s known as the â€œdata sparsity problemâ€. 
ã—ã‹ã—ã€ã€Œãƒ‡ãƒ¼ã‚¿ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§å•é¡Œã€ã¨ã—ã¦çŸ¥ã‚‰ã‚Œã‚‹èª²é¡ŒãŒæµ®ä¸Šã—ã¾ã™ã€‚
This means that despite having ample data, thereâ€™s no assurance that the collected user interactions or behaviors cover all the situations where users have expressed positive feedback, like giving high ratings. 
ã“ã‚Œã¯ã€ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã£ã¦ã‚‚ã€åé›†ã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç›¸äº’ä½œç”¨ã‚„è¡Œå‹•ãŒã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé«˜è©•ä¾¡ã‚’ä¸ãˆã‚‹ãªã©ã®ãƒã‚¸ãƒ†ã‚£ãƒ–ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç¤ºã—ãŸã™ã¹ã¦ã®çŠ¶æ³ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹ä¿è¨¼ãŒãªã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚
In other words, there might be important scenarios where users found something valuable, but the data doesnâ€™t reflect those instances well. 
è¨€ã„æ›ãˆã‚Œã°ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä½•ã‹ä¾¡å€¤ã®ã‚ã‚‹ã‚‚ã®ã‚’è¦‹ã¤ã‘ãŸé‡è¦ãªã‚·ãƒŠãƒªã‚ªãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€ãƒ‡ãƒ¼ã‚¿ã¯ãã‚Œã‚‰ã®äº‹ä¾‹ã‚’ã†ã¾ãåæ˜ ã—ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
On the other hand, there is another widely recognized hurdle in RS that also applies to Offline RL4RS: the cold-start problem. 
ä¸€æ–¹ã§ã€RSã«ãŠã„ã¦åºƒãèªè­˜ã•ã‚Œã¦ã„ã‚‹ã‚‚ã†ä¸€ã¤ã®éšœå®³ãŒã‚ã‚Šã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã«ã‚‚é©ç”¨ã•ã‚Œã¾ã™ï¼šã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆå•é¡Œã§ã™ã€‚
Unlike data sparsity, cold-start challenges emerge when the agent aims to provide recommendations to a new user. 
ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã¨ã¯ç•°ãªã‚Šã€ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆã®èª²é¡Œã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæ–°ã—ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æ¨å¥¨ã‚’æä¾›ã—ã‚ˆã†ã¨ã™ã‚‹éš›ã«ç™ºç”Ÿã—ã¾ã™ã€‚
This issue arises due to the absence of adequate historical data or interactions, which in turn hampers the understanding of preferences and traits related to these new users or items. 
ã“ã®å•é¡Œã¯ã€ååˆ†ãªå±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚„ç›¸äº’ä½œç”¨ãŒæ¬ å¦‚ã—ã¦ã„ã‚‹ãŸã‚ã«ç™ºç”Ÿã—ã€ã“ã‚Œã«ã‚ˆã‚Šæ–°ã—ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚„ã‚¢ã‚¤ãƒ†ãƒ ã«é–¢é€£ã™ã‚‹å¥½ã¿ã‚„ç‰¹æ€§ã®ç†è§£ãŒå¦¨ã’ã‚‰ã‚Œã¾ã™ã€‚
While addressing the cold-start problem is an ongoing research avenue in conventional RS tasks, it hasnâ€™t received sufficient attention in the context of RL4RS. 
ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆå•é¡Œã«å¯¾å‡¦ã™ã‚‹ã“ã¨ã¯ã€å¾“æ¥ã®RSã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹ç¶™ç¶šçš„ãªç ”ç©¶ã®é“ç­‹ã§ã™ãŒã€RL4RSã®æ–‡è„ˆã§ã¯ååˆ†ãªæ³¨æ„ã‚’æ‰•ã‚ã‚Œã¦ã„ã¾ã›ã‚“ã€‚
Considering the interactive procedure of the RL4RS, new users have limited contextual information that they can use to formulate the state representation; this contributes to the difficulty of making recommendations. 
RL4RSã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªæ‰‹ç¶šãã‚’è€ƒæ…®ã™ã‚‹ã¨ã€æ–°ã—ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯çŠ¶æ…‹è¡¨ç¾ã‚’å½¢æˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã‚‹é™ã‚‰ã‚ŒãŸæ–‡è„ˆæƒ…å ±ã—ã‹æŒã£ã¦ãŠã‚‰ãšã€ã“ã‚ŒãŒæ¨å¥¨ã‚’è¡Œã†ä¸Šã§ã®é›£ã—ã•ã«å¯„ä¸ã—ã¦ã„ã¾ã™ã€‚
This predicament continues to remain an unsolved puzzle within the realm of offline RL4RS. 
ã“ã®å›°é›£ã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã®é ˜åŸŸã«ãŠã„ã¦æœªè§£æ±ºã®ãƒ‘ã‚ºãƒ«ã¨ã—ã¦æ®‹ã‚Šç¶šã‘ã¦ã„ã¾ã™ã€‚

**4.2** **Distribution Shift åˆ†å¸ƒã®ã‚·ãƒ•ãƒˆ**
A challenge of significant intricacy within the context of offline RL pertains to the effective formulation and addressing of counterfactual queriesâ€”a task that might not be readily apparent but is of great importance. 
ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®æ–‡è„ˆã«ãŠã‘ã‚‹é‡è¦ãªè¤‡é›‘ã•ã®ã‚ã‚‹èª²é¡Œã¯ã€åäº‹å®Ÿçš„ã‚¯ã‚¨ãƒªã®åŠ¹æœçš„ãªå®šå¼åŒ–ã¨å¯¾å‡¦ã«é–¢ã™ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€ã“ã‚Œã¯æ˜ç™½ã§ã¯ãªã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€éå¸¸ã«é‡è¦ã§ã™ã€‚
Counterfactual queries, in essence, are defined as hypothetical â€œwhat ifâ€ scenarios. 
åäº‹å®Ÿçš„ã‚¯ã‚¨ãƒªã¯ã€æœ¬è³ªçš„ã«ä»®æƒ³çš„ãªã€Œã‚‚ã—ã‚‚ã€ã®ã‚·ãƒŠãƒªã‚ªã¨ã—ã¦å®šç¾©ã•ã‚Œã¾ã™ã€‚
These queries involve creating educated guesses about potential outcomes if the agent were to undertake actions different from those observed in the data. 
ã“ã‚Œã‚‰ã®ã‚¯ã‚¨ãƒªã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒãƒ‡ãƒ¼ã‚¿ã§è¦³å¯Ÿã•ã‚ŒãŸè¡Œå‹•ã¨ã¯ç•°ãªã‚‹è¡Œå‹•ã‚’å–ã£ãŸå ´åˆã®æ½œåœ¨çš„ãªçµæœã«ã¤ã„ã¦ã®æ¨æ¸¬ã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚’å«ã¿ã¾ã™ã€‚
It is the core behind offline RL, as our objective is to learn a policy that can perform better than the behavior recorded in the dataset. 
ã“ã‚Œã¯ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®æ ¸å¿ƒã§ã‚ã‚Šã€ç§ãŸã¡ã®ç›®çš„ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«è¨˜éŒ²ã•ã‚ŒãŸè¡Œå‹•ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã§ãã‚‹ãƒãƒªã‚·ãƒ¼ã‚’å­¦ã¶ã“ã¨ã§ã™ã€‚
Hence, the agent must execute an action that is different from the learned policy. 
ã—ãŸãŒã£ã¦ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯å­¦ç¿’ã—ãŸãƒãƒªã‚·ãƒ¼ã¨ã¯ç•°ãªã‚‹è¡Œå‹•ã‚’å®Ÿè¡Œã—ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚
This situation, unfortunately, places a substantial strain on the capabilities of several prevailing deep-learning methods. 
æ®‹å¿µãªãŒã‚‰ã€ã“ã®çŠ¶æ³ã¯ã€ã„ãã¤ã‹ã®æ—¢å­˜ã®æ·±å±¤å­¦ç¿’æ‰‹æ³•ã®èƒ½åŠ›ã«å¤§ããªè² æ‹…ã‚’ã‹ã‘ã¾ã™ã€‚
Existing methods have been methodically fashioned under the assumption that the data is independence and identical distribution (i.i.d.). 
æ—¢å­˜ã®æ‰‹æ³•ã¯ã€ãƒ‡ãƒ¼ã‚¿ãŒç‹¬ç«‹åŒä¸€åˆ†å¸ƒï¼ˆi.i.d.ï¼‰ã§ã‚ã‚‹ã¨ã„ã†ä»®å®šã®ä¸‹ã§ä½“ç³»çš„ã«æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚
In traditional supervised learning based RS, the goal is to train a model to achieve superior performance, such as higher accuracy, recall or precision. 
å¾“æ¥ã®æ•™å¸«ã‚ã‚Šå­¦ç¿’ã«åŸºã¥ãRSã§ã¯ã€ç›®æ¨™ã¯ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦ã€ã‚ˆã‚Šé«˜ã„ç²¾åº¦ã€å†ç¾ç‡ã€ã¾ãŸã¯é©åˆç‡ã‚’é”æˆã™ã‚‹ã“ã¨ã§ã™ã€‚
The evaluation dataset follows the same distribution as the training dataset. 
è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨åŒã˜åˆ†å¸ƒã«å¾“ã„ã¾ã™ã€‚
Hence, in offline RL4RS, the key point is to learn a policy that can recommend different items (ideally with better feedback) from the behavior recorded in the dataset. 
ã—ãŸãŒã£ã¦ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã«ãŠã„ã¦é‡è¦ãªç‚¹ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«è¨˜éŒ²ã•ã‚ŒãŸè¡Œå‹•ã‹ã‚‰ç•°ãªã‚‹ã‚¢ã‚¤ãƒ†ãƒ ï¼ˆç†æƒ³çš„ã«ã¯ã‚ˆã‚Šè‰¯ã„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä¼´ã†ï¼‰ã‚’æ¨å¥¨ã§ãã‚‹ãƒãƒªã‚·ãƒ¼ã‚’å­¦ã¶ã“ã¨ã§ã™ã€‚
The challenge behind counterfactual queries is that of distribution shift. 
åäº‹å®Ÿçš„ã‚¯ã‚¨ãƒªã®èƒŒå¾Œã«ã‚ã‚‹èª²é¡Œã¯ã€åˆ†å¸ƒã®ã‚·ãƒ•ãƒˆã§ã™ã€‚
The policy is trained under one distribution, but it will be evaluated on a different distribution. 
ãƒãƒªã‚·ãƒ¼ã¯ã‚ã‚‹åˆ†å¸ƒã®ä¸‹ã§è¨“ç·´ã•ã‚Œã¾ã™ãŒã€ç•°ãªã‚‹åˆ†å¸ƒã§è©•ä¾¡ã•ã‚Œã¾ã™ã€‚
Given that such a problem is not widely discussed in the RS literature, we will provide some algorithmic insights from the offline RL perspective to help address this in offline RL4RS. 
ã“ã®ã‚ˆã†ãªå•é¡ŒãŒRSæ–‡çŒ®ã§åºƒãè­°è«–ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®è¦³ç‚¹ã‹ã‚‰ã„ãã¤ã‹ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ çš„ãªæ´å¯Ÿã‚’æä¾›ã—ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã§ã®è§£æ±ºã«å½¹ç«‹ã¦ã¾ã™ã€‚
Distribution shift issues can be addressed in several ways, with the simplest one being to constrain something about the learning process such that the distribution shift is bounded. 
åˆ†å¸ƒã®ã‚·ãƒ•ãƒˆã®å•é¡Œã¯ã€ã„ãã¤ã‹ã®æ–¹æ³•ã§å¯¾å‡¦ã§ãã€æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ã€å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã«é–¢ã—ã¦ä½•ã‹ã‚’åˆ¶ç´„ã—ã€åˆ†å¸ƒã®ã‚·ãƒ•ãƒˆã‚’åˆ¶é™ã™ã‚‹ã“ã¨ã§ã™ã€‚
For example, we can constrain how much the learned policy $\pi(a|s)$ differs from behavior policy $\pi_\beta(a|s)$ by using some techniques like Trust Region Policy Optimization (TRPO). 
ä¾‹ãˆã°ã€Trust Region Policy Optimization (TRPO)ã®ã‚ˆã†ãªæŠ€è¡“ã‚’ä½¿ç”¨ã—ã¦ã€å­¦ç¿’ã—ãŸãƒãƒªã‚·ãƒ¼ $\pi(a|s)$ ãŒè¡Œå‹•ãƒãƒªã‚·ãƒ¼ $\pi_\beta(a|s)$ ã‹ã‚‰ã©ã‚Œã ã‘ç•°ãªã‚‹ã‹ã‚’åˆ¶ç´„ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
However, if there is a significant disparity between the distribution of the training dataset and that of the evaluation environment, it might lead to the emergence of out-of-distribution (OOD) behavior. 
ã—ã‹ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å¸ƒã¨è©•ä¾¡ç’°å¢ƒã®åˆ†å¸ƒã®é–“ã«å¤§ããªä¸ä¸€è‡´ãŒã‚ã‚‹å ´åˆã€åˆ†å¸ƒå¤–ï¼ˆOODï¼‰è¡Œå‹•ã®å‡ºç¾ã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
Several recent studies have delved into OOD recommendation, taking into account shifts in user features. 
æœ€è¿‘ã®ã„ãã¤ã‹ã®ç ”ç©¶ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç‰¹å¾´ã®ã‚·ãƒ•ãƒˆã‚’è€ƒæ…®ã—ãŸOODæ¨è–¦ã«å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™ã€‚
These efforts can be categorized into two main groups: OOD generalization and OOD adaptation. 
ã“ã‚Œã‚‰ã®å–ã‚Šçµ„ã¿ã¯ã€ä¸»ã«2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†é¡ã§ãã¾ã™ï¼šOODä¸€èˆ¬åŒ–ã¨OODé©å¿œã€‚
The underlying notion here is to acquire a causal representation of usersâ€™ preferences by leveraging their most recent behaviors. 
ã“ã“ã§ã®åŸºæœ¬çš„ãªè€ƒãˆæ–¹ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æœ€è¿‘ã®è¡Œå‹•ã‚’æ´»ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã®å› æœè¡¨ç¾ã‚’å–å¾—ã™ã‚‹ã“ã¨ã§ã™ã€‚
This representation is then utilized within a causal graph framework to comprehend how shifts in features could impact usersâ€™ preferences. 
ã“ã®è¡¨ç¾ã¯ã€å› æœã‚°ãƒ©ãƒ•ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å†…ã§åˆ©ç”¨ã•ã‚Œã€ç‰¹å¾´ã®ã‚·ãƒ•ãƒˆãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã«ã©ã®ã‚ˆã†ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
Furthermore, the current methodologies primarily target sequential recommendation systems, which share certain properties with MDPs, rendering them relevant to offline RL4RS. 
ã•ã‚‰ã«ã€ç¾åœ¨ã®æ–¹æ³•è«–ã¯ä¸»ã«ã€MDPã¨å…±é€šã®ç‰¹æ€§ã‚’æŒã¤é€æ¬¡æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚’å¯¾è±¡ã¨ã—ã¦ãŠã‚Šã€ã“ã‚Œã«ã‚ˆã‚Šã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã«é–¢é€£æ€§ã‚’æŒãŸã›ã¦ã„ã¾ã™ã€‚
However, this domain is still in its exploratory phase, and it has not garnered substantial attention. 
ã—ã‹ã—ã€ã“ã®åˆ†é‡ã¯ã¾ã æ¢ç´¢æ®µéšã«ã‚ã‚Šã€ååˆ†ãªæ³¨ç›®ã‚’é›†ã‚ã¦ã„ã¾ã›ã‚“ã€‚
As a result, this presents an open challenge with significant potential for further exploration. 
ãã®çµæœã€ã“ã‚Œã¯ã•ã‚‰ãªã‚‹æ¢æ±‚ã®ãŸã‚ã®é‡è¦ãªå¯èƒ½æ€§ã‚’æŒã¤æœªè§£æ±ºã®èª²é¡Œã‚’æç¤ºã—ã¾ã™ã€‚

**4.3** **Bias and Variance Trade-off ãƒã‚¤ã‚¢ã‚¹ã¨åˆ†æ•£ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**
Another prevalent issue within offline RL4RS pertains to the bias inherited from RS, a topic that has recently gained increasing research attention. 
ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã«ãŠã‘ã‚‹ã‚‚ã†ä¸€ã¤ã®ä¸€èˆ¬çš„ãªå•é¡Œã¯ã€RSã‹ã‚‰å¼•ãç¶™ãŒã‚Œã‚‹ãƒã‚¤ã‚¢ã‚¹ã«é–¢ã™ã‚‹ã‚‚ã®ã§ã€ã“ã‚Œã¯æœ€è¿‘ã¾ã™ã¾ã™ç ”ç©¶ã®æ³¨ç›®ã‚’é›†ã‚ã¦ã„ã¾ã™ã€‚
This bias stems from the nature of offline data, with recent studies revealing that user behavior data are not experimental but rather observational, introducing bias-related challenges. 
ã“ã®ãƒã‚¤ã‚¢ã‚¹ã¯ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã®æ€§è³ªã«èµ·å› ã—ã€æœ€è¿‘ã®ç ”ç©¶ã§ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ãŒå®Ÿé¨“çš„ã§ã¯ãªãè¦³å¯Ÿçš„ã§ã‚ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã€ãƒã‚¤ã‚¢ã‚¹ã«é–¢é€£ã™ã‚‹èª²é¡Œã‚’å¼•ãèµ·ã“ã—ã¦ã„ã¾ã™ã€‚
The prevalence of bias can be attributed to two primary factors. 
ãƒã‚¤ã‚¢ã‚¹ã®è”“å»¶ã¯ã€ä¸»ã«2ã¤ã®è¦å› ã«èµ·å› ã—ã¾ã™ã€‚
Firstly, the inherent character of user behavior data is observational rather than experimental. 
ç¬¬ä¸€ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã®æœ¬è³ªçš„ãªæ€§è³ªã¯ã€å®Ÿé¨“çš„ã§ã¯ãªãè¦³å¯Ÿçš„ã§ã™ã€‚
In simpler terms, the data fed into RS are susceptible to selection bias. 
ç°¡å˜ã«è¨€ãˆã°ã€RSã«ä¾›çµ¦ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã¯é¸æŠãƒã‚¤ã‚¢ã‚¹ã«å½±éŸ¿ã•ã‚Œã‚„ã™ã„ã§ã™ã€‚
For instance, in a video recommendation system, users tend to engage with, rate, and comment on movies that align with their personal interests. 
ä¾‹ãˆã°ã€ãƒ“ãƒ‡ã‚ªæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯è‡ªåˆ†ã®èˆˆå‘³ã«åˆã£ãŸæ˜ ç”»ã«å¯¾ã—ã¦é–¢ä¸ã—ã€è©•ä¾¡ã—ã€ã‚³ãƒ¡ãƒ³ãƒˆã™ã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚
Secondly, a discrepancy in distribution exists, signifying that the distributions of users and items within the recommender system are uneven. 
ç¬¬äºŒã«ã€åˆ†å¸ƒã®ä¸ä¸€è‡´ãŒå­˜åœ¨ã—ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å†…ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®åˆ†å¸ƒãŒä¸å‡ä¸€ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
This imbalance can lead to a â€œpopularity biasâ€, where popular items receive disproportionately frequent recommendations compared to others. 
ã“ã®ä¸å‡è¡¡ã¯ã€Œäººæ°—ãƒã‚¤ã‚¢ã‚¹ã€ã‚’å¼•ãèµ·ã“ã—ã€äººæ°—ã®ã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ãŒä»–ã®ã‚¢ã‚¤ãƒ†ãƒ ã«æ¯”ã¹ã¦ä¸å‡è¡¡ã«é »ç¹ã«æ¨è–¦ã•ã‚Œã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚
Nonetheless, disregarding products within the "long tail" of less popular items can have adverse effects on businesses, given that these items are equally essential, albeit less likely to be discovered by chance. 
ãã‚Œã§ã‚‚ã€ã‚ã¾ã‚Šäººæ°—ã®ãªã„ã‚¢ã‚¤ãƒ†ãƒ ã®ã€Œãƒ­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ«ã€ã«ã‚ã‚‹è£½å“ã‚’ç„¡è¦–ã™ã‚‹ã“ã¨ã¯ã€ã“ã‚Œã‚‰ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒåŒæ§˜ã«é‡è¦ã§ã‚ã‚‹ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€å¶ç„¶ã«ç™ºè¦‹ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒä½ã„ãŸã‚ã€ãƒ“ã‚¸ãƒã‚¹ã«æ‚ªå½±éŸ¿ã‚’åŠã¼ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
As mentioned earlier, a substantial portion of existing offline off-policy with logged data methods primarily focus on off-policy evaluation. 
å‰è¿°ã®ã‚ˆã†ã«ã€æ—¢å­˜ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ã®ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸæ‰‹æ³•ã®å¤§éƒ¨åˆ†ã¯ã€ä¸»ã«ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è©•ä¾¡ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚
This approach employs importance sampling to tackle the bias issue. 
ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ãƒã‚¤ã‚¢ã‚¹ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
However, the importance sampling gives rise to another hurdleâ€”high variance. 
ã—ã‹ã—ã€é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯åˆ¥ã®éšœå®³ã€é«˜ã„åˆ†æ•£ã‚’å¼•ãèµ·ã“ã—ã¾ã™ã€‚
While importance sampling already contends with high variance, this issue is further exacerbated in the context of sequential scenarios. 
é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯ã™ã§ã«é«˜ã„åˆ†æ•£ã«å¯¾å‡¦ã—ã¦ã„ã¾ã™ãŒã€ã“ã®å•é¡Œã¯é€æ¬¡ã‚·ãƒŠãƒªã‚ªã®æ–‡è„ˆã§ã•ã‚‰ã«æ‚ªåŒ–ã—ã¾ã™ã€‚
In this setting, the importance weights at consecutive time steps are multiplied together, leading to an exponential amplification of variance. 
ã“ã®è¨­å®šã§ã¯ã€é€£ç¶šã™ã‚‹æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã§ã®é‡è¦åº¦é‡ã¿ãŒæ›ã‘åˆã‚ã•ã‚Œã€åˆ†æ•£ãŒæŒ‡æ•°çš„ã«å¢—å¹…ã•ã‚Œã¾ã™ã€‚
Approximate and marginalized importance sampling methods mitigate this concern to some extent by circumventing the multiplication of importance weights across multiple time steps. 
è¿‘ä¼¼çš„ãŠã‚ˆã³å‘¨è¾ºçš„ãªé‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•ã¯ã€è¤‡æ•°ã®æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã«ã‚ãŸã‚‹é‡è¦åº¦é‡ã¿ã®æ›ã‘ç®—ã‚’å›é¿ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€ã“ã®æ‡¸å¿µã‚’ã‚ã‚‹ç¨‹åº¦è»½æ¸›ã—ã¾ã™ã€‚
Yet, the fundamental challenge persists: when the behavior policy $\pi_\beta$ substantially diverges from the current learned policy $\pi_\theta$, the importance weights degenerate. 
ãã‚Œã§ã‚‚ã€æ ¹æœ¬çš„ãªèª²é¡Œã¯æ®‹ã‚Šã¾ã™ï¼šè¡Œå‹•ãƒãƒªã‚·ãƒ¼ $\pi_\beta$ ãŒç¾åœ¨ã®å­¦ç¿’ãƒãƒªã‚·ãƒ¼ $\pi_\theta$ ã‹ã‚‰å¤§ããé€¸è„±ã™ã‚‹ã¨ã€é‡è¦åº¦é‡ã¿ãŒåŠ£åŒ–ã—ã¾ã™ã€‚
Consequently, any estimations of the return or gradient encounter excessive variance, particularly in scenarios characterized by high-dimensional state and action spaces or extended time horizons. 
ãã®çµæœã€ãƒªã‚¿ãƒ¼ãƒ³ã‚„å‹¾é…ã®æ¨å®šã¯éå‰°ãªåˆ†æ•£ã«ç›´é¢ã—ã€ç‰¹ã«é«˜æ¬¡å…ƒã®çŠ¶æ…‹ãŠã‚ˆã³è¡Œå‹•ç©ºé–“ã‚„é•·ã„æ™‚é–“ã®ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã‚’ç‰¹å¾´ã¨ã™ã‚‹ã‚·ãƒŠãƒªã‚ªã§ã¯é¡•è‘—ã§ã™ã€‚
For this reason, importance-sampled estimators are most effective when the policyâ€™s deviation from the behavior policy remains within a reasonable limit. 
ã“ã®ç†ç”±ã‹ã‚‰ã€é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸæ¨å®šé‡ã¯ã€ãƒãƒªã‚·ãƒ¼ã®è¡Œå‹•ãƒãƒªã‚·ãƒ¼ã‹ã‚‰ã®é€¸è„±ãŒåˆç†çš„ãªç¯„å›²å†…ã«ã‚ã‚‹ã¨ãã«æœ€ã‚‚åŠ¹æœçš„ã§ã™ã€‚
In the general off-policy setting, this condition generally holds true, as new trajectories are frequently amassed and integrated into the dataset using the latest policy. 
ä¸€èˆ¬çš„ãªã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è¨­å®šã§ã¯ã€ã“ã®æ¡ä»¶ã¯ä¸€èˆ¬çš„ã«çœŸã§ã‚ã‚Šã€æ–°ã—ã„è»Œé“ãŒé »ç¹ã«é›†ã‚ã‚‰ã‚Œã€æœ€æ–°ã®ãƒãƒªã‚·ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«çµ±åˆã•ã‚Œã¾ã™ã€‚
However, in the offline context, this is not typically the case. 
ã—ã‹ã—ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã®æ–‡è„ˆã§ã¯ã€é€šå¸¸ã¯ãã†ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
Consequently, the extent of enhancement achievable through importance sampling is confined by several factors: (i) the relative suboptimality of the behavior policy; (ii) the dimensionality of the state and action space; (iii) the effective task horizon. 
ãã®çµæœã€é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’é€šã˜ã¦é”æˆå¯èƒ½ãªæ”¹å–„ã®ç¨‹åº¦ã¯ã€ã„ãã¤ã‹ã®è¦å› ã«ã‚ˆã£ã¦åˆ¶é™ã•ã‚Œã¾ã™ï¼šï¼ˆiï¼‰è¡Œå‹•ãƒãƒªã‚·ãƒ¼ã®ç›¸å¯¾çš„ãªæœ€é©æ€§ã®ä½ã•ï¼›ï¼ˆiiï¼‰çŠ¶æ…‹ãŠã‚ˆã³è¡Œå‹•ç©ºé–“ã®æ¬¡å…ƒï¼›ï¼ˆiiiï¼‰åŠ¹æœçš„ãªã‚¿ã‚¹ã‚¯ã®ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã€‚
Hence, the tradeoff between bias and variance in offline RL4RS presents an intriguing potential avenue for advancement. 
ã—ãŸãŒã£ã¦ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã«ãŠã‘ã‚‹ãƒã‚¤ã‚¢ã‚¹ã¨åˆ†æ•£ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã¯ã€é€²å±•ã®ãŸã‚ã®èˆˆå‘³æ·±ã„å¯èƒ½æ€§ã®ã‚ã‚‹é“ã‚’æç¤ºã—ã¾ã™ã€‚

**4.4** **Explainability èª¬æ˜å¯èƒ½æ€§**
While deep learning-based models can significantly enhance the performance of RS, they often lack interpretability. 
æ·±å±¤å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã¯RSã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ãŒã€ã—ã°ã—ã°è§£é‡ˆå¯èƒ½æ€§ã«æ¬ ã‘ã¾ã™ã€‚
Consequently, the task of rendering recommender outputs understandable becomes vital, all while maintaining high-quality recommendations. 
ãã®çµæœã€æ¨è–¦å‡ºåŠ›ã‚’ç†è§£å¯èƒ½ã«ã™ã‚‹ä½œæ¥­ãŒé‡è¦ã«ãªã‚Šã€é«˜å“è³ªãªæ¨å¥¨ã‚’ç¶­æŒã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
Elevating explainability in RS carries benefits beyond aiding end-users in comprehending suggested items. 
RSã«ãŠã‘ã‚‹èª¬æ˜å¯èƒ½æ€§ã‚’é«˜ã‚ã‚‹ã“ã¨ã¯ã€ææ¡ˆã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’ç†è§£ã™ã‚‹ã®ã‚’åŠ©ã‘ã‚‹ã ã‘ã§ãªãã€ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆè€…ãŒRSã®å†…éƒ¨å‹•ä½œã‚’ç²¾æŸ»ã™ã‚‹ã“ã¨ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚
Additionally, the realm of explainability in RL has been garnering attention, although the current focus primarily revolves around visualizing learned representations. 
ã•ã‚‰ã«ã€RLã«ãŠã‘ã‚‹èª¬æ˜å¯èƒ½æ€§ã®é ˜åŸŸã¯æ³¨ç›®ã‚’é›†ã‚ã¦ã„ã¾ã™ãŒã€ç¾åœ¨ã®ç„¦ç‚¹ã¯ä¸»ã«å­¦ç¿’ã•ã‚ŒãŸè¡¨ç¾ã®è¦–è¦šåŒ–ã«ã‚ã‚Šã¾ã™ã€‚
What remains is an explanation of how the learned policy translates into actionable decisions. 
æ®‹ã‚‹ã®ã¯ã€å­¦ç¿’ã•ã‚ŒãŸãƒãƒªã‚·ãƒ¼ãŒã©ã®ã‚ˆã†ã«å®Ÿè¡Œå¯èƒ½ãªæ±ºå®šã«å¤‰æ›ã•ã‚Œã‚‹ã‹ã®èª¬æ˜ã§ã™ã€‚
In the transition to RL4RS, the emphasis on explainability will shift towards elucidating how the agent justifies its recommended items. 
RL4RSã¸ã®ç§»è¡Œã«ãŠã„ã¦ã€èª¬æ˜å¯èƒ½æ€§ã«å¯¾ã™ã‚‹é‡ç‚¹ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã©ã®ã‚ˆã†ã«æ¨å¥¨ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ­£å½“åŒ–ã™ã‚‹ã‹ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ã“ã¨ã«ç§»ã‚Šã¾ã™ã€‚
Hence, explainability becomes a relatively easy task compared with interpreting the learning process or decision process. 
ã—ãŸãŒã£ã¦ã€èª¬æ˜å¯èƒ½æ€§ã¯ã€å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚„æ„æ€æ±ºå®šãƒ—ãƒ­ã‚»ã‚¹ã‚’è§£é‡ˆã™ã‚‹ã“ã¨ã¨æ¯”è¼ƒã—ã¦ã€æ¯”è¼ƒçš„å®¹æ˜“ãªä½œæ¥­ã¨ãªã‚Šã¾ã™ã€‚
Attention models have emerged as powerful tools that not only bolster predictive performance but also enhance explainability. 
ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã¯ã€äºˆæ¸¬æ€§èƒ½ã‚’é«˜ã‚ã‚‹ã ã‘ã§ãªãã€èª¬æ˜å¯èƒ½æ€§ã‚’å‘ä¸Šã•ã›ã‚‹å¼·åŠ›ãªãƒ„ãƒ¼ãƒ«ã¨ã—ã¦ç™»å ´ã—ã¾ã—ãŸã€‚
For instance, Wang et al. introduce an RL framework coupled with an attention model for explainable recommendations. 
ä¾‹ãˆã°ã€Wangã‚‰ã¯ã€èª¬æ˜å¯èƒ½ãªæ¨è–¦ã®ãŸã‚ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨çµ„ã¿åˆã‚ã›ãŸRLãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ç´¹ä»‹ã—ã¦ã„ã¾ã™ã€‚
This approach ensures model-agnostic by segregating the recommendation model from the explanation generator. 
ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã‚’èª¬æ˜ç”Ÿæˆå™¨ã‹ã‚‰åˆ†é›¢ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€ãƒ¢ãƒ‡ãƒ«ã«ä¾å­˜ã—ãªã„ã“ã¨ã‚’ä¿è¨¼ã—ã¾ã™ã€‚
Agents instantiated through attention-based neural networks facilitate the generation of sentence-level explanations. 
ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’é€šã˜ã¦ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€æ–‡ãƒ¬ãƒ™ãƒ«ã®èª¬æ˜ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚’å®¹æ˜“ã«ã—ã¾ã™ã€‚
This approach could prove promising given the close connection between offline RL4RS and online RL4RS. 
ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³RL4RSã®å¯†æ¥ãªé–¢ä¿‚ã‚’è€ƒãˆã‚‹ã¨ã€æœ‰æœ›ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
Moreover, with access to offline datasets in offline RL4RS, more solutions become feasible. 
ã•ã‚‰ã«ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã«ãŠã„ã¦ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šå¤šãã®è§£æ±ºç­–ãŒå®Ÿç¾å¯èƒ½ã«ãªã‚Šã¾ã™ã€‚
Knowledge graphs, for instance, contain abundant user and item information, enabling the creation of more personalized, intuitive explanations for recommendation systems. 
ä¾‹ãˆã°ã€çŸ¥è­˜ã‚°ãƒ©ãƒ•ã¯è±Šå¯Œãªãƒ¦ãƒ¼ã‚¶ãƒ¼ãŠã‚ˆã³ã‚¢ã‚¤ãƒ†ãƒ æƒ…å ±ã‚’å«ã‚“ã§ãŠã‚Šã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ãŸã‚ã®ã‚ˆã‚Šãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸç›´æ„Ÿçš„ãªèª¬æ˜ã®ä½œæˆã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚
However, the processing of graph data presents challenges. 
ã—ã‹ã—ã€ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã«ã¯èª²é¡ŒãŒã‚ã‚Šã¾ã™ã€‚
One potential strategy involves embedding a prelearned knowledge graph from the offline dataset into the environment. 
1ã¤ã®æ½œåœ¨çš„ãªæˆ¦ç•¥ã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰äº‹å‰å­¦ç¿’ã•ã‚ŒãŸçŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚’ç’°å¢ƒã«åŸ‹ã‚è¾¼ã‚€ã“ã¨ã§ã™ã€‚
The final objective then shifts from recommending items to navigating the knowledge graph. 
æœ€çµ‚çš„ãªç›®æ¨™ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ¨è–¦ã™ã‚‹ã“ã¨ã‹ã‚‰çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚’ãƒŠãƒ“ã‚²ãƒ¼ãƒˆã™ã‚‹ã“ã¨ã«ç§»ã‚Šã¾ã™ã€‚
As an example, Zhao et al. extract informative path demonstrations with minimal labeling effort. 
ä¾‹ã¨ã—ã¦ã€Zhaoã‚‰ã¯æœ€å°é™ã®ãƒ©ãƒ™ãƒªãƒ³ã‚°ä½œæ¥­ã§æƒ…å ±è±Šå¯Œãªãƒ‘ã‚¹ã®ãƒ‡ãƒ¢ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
Then an adversarial actor-critic model for demonstration-guided pathfinding is proposed. 
æ¬¡ã«ã€ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«åŸºã¥ããƒ‘ã‚¹ãƒ•ã‚¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã®ãŸã‚ã®æ•µå¯¾çš„ãªã‚¢ã‚¯ã‚¿ãƒ¼-ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ãŒææ¡ˆã•ã‚Œã¾ã™ã€‚
This approach enhances recommendation accuracy and explainability through RL and knowledge graph reasoning and can be further expanded by integrating offline RL features. 
ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€RLã¨çŸ¥è­˜ã‚°ãƒ©ãƒ•æ¨è«–ã‚’é€šã˜ã¦æ¨è–¦ã®ç²¾åº¦ã¨èª¬æ˜å¯èƒ½æ€§ã‚’å‘ä¸Šã•ã›ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLæ©Ÿèƒ½ã‚’çµ±åˆã™ã‚‹ã“ã¨ã§ã•ã‚‰ã«æ‹¡å¼µã§ãã¾ã™ã€‚



## 5 FUTURE DIRECTIONS ä»Šå¾Œã®æ–¹å‘æ€§

In offline RL4RS, several key areas emerge as promising avenues. 
ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã«ãŠã„ã¦ã€ã„ãã¤ã‹ã®é‡è¦ãªåˆ†é‡ãŒæœ‰æœ›ãªé“ç­‹ã¨ã—ã¦æµ®ã‹ã³ä¸ŠãŒã£ã¦ã„ã¾ã™ã€‚

Cross-domain recommendation systems offer the potential in transferring insights between diverse domains, enhancing recommendation effectiveness. 
ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¯ã€ç•°ãªã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³é–“ã§ã®çŸ¥è¦‹ã®ç§»è»¢ã‚’å¯èƒ½ã«ã—ã€æ¨è–¦ã®åŠ¹æœã‚’é«˜ã‚ã‚‹å¯èƒ½æ€§ã‚’æä¾›ã—ã¾ã™ã€‚

The integration of large language models holds the prospect of enriching contextual understanding and refining user-item interactions. 
å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®çµ±åˆã¯ã€æ–‡è„ˆç†è§£ã‚’è±Šã‹ã«ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®ç›¸äº’ä½œç”¨ã‚’æ´—ç·´ã•ã›ã‚‹å¯èƒ½æ€§ã‚’ç§˜ã‚ã¦ã„ã¾ã™ã€‚

Incorporating causality into offline RL4RS can deepen comprehension of user behaviors, leading to more accurate and interpretable recommendations. 
ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã«å› æœé–¢ä¿‚ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã®ç†è§£ãŒæ·±ã¾ã‚Šã€ã‚ˆã‚Šæ­£ç¢ºã§è§£é‡ˆå¯èƒ½ãªæ¨è–¦ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

The exploration of self-supervised learning and graph-based techniques presents innovative possibilities for capturing intricate user-item relationships. 
è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚„ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹ã®æŠ€è¡“ã®æ¢æ±‚ã¯ã€è¤‡é›‘ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®é–¢ä¿‚ã‚’æ‰ãˆã‚‹ãŸã‚ã®é©æ–°çš„ãªå¯èƒ½æ€§ã‚’æç¤ºã—ã¾ã™ã€‚

Moreover, addressing uncertainty and fortifying the robustness of RL4RS against noise and adversarial inputs stand out as essential directions for ensuring dependable and consistent recommendation outcomes. 
ã•ã‚‰ã«ã€ä¸ç¢ºå®Ÿæ€§ã«å¯¾å‡¦ã—ã€ãƒã‚¤ã‚ºã‚„æ•µå¯¾çš„å…¥åŠ›ã«å¯¾ã™ã‚‹RL4RSã®å …ç‰¢æ€§ã‚’å¼·åŒ–ã™ã‚‹ã“ã¨ã¯ã€ä¿¡é ¼æ€§ãŒé«˜ãä¸€è²«ã—ãŸæ¨è–¦çµæœã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã®é‡è¦ãªæ–¹å‘æ€§ã¨ã—ã¦éš›ç«‹ã£ã¦ã„ã¾ã™ã€‚

**5.1** **Cross-Domain Recommendation ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨è–¦**

Cross-domain recommendation refers to the task of providing recommendations to users by leveraging data and knowledge from multiple distinct domains. 
ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨è–¦ã¨ã¯ã€è¤‡æ•°ã®ç•°ãªã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã¨çŸ¥è­˜ã‚’æ´»ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æ¨è–¦ã‚’æä¾›ã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚’æŒ‡ã—ã¾ã™ã€‚

Cross-domain recommendation systems can be particularly useful in scenarios where user data is sparse within a single domain but might be enriched when multiple domains are combined. 
ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¯ã€å˜ä¸€ã®ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒå¸Œè–„ãªã‚·ãƒŠãƒªã‚ªã§ç‰¹ã«æœ‰ç”¨ã§ã™ãŒã€è¤‡æ•°ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§è±Šã‹ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

Additionally, they enable more comprehensive and diverse recommendations by tapping into different aspects of usersâ€™ interests. 
ã•ã‚‰ã«ã€ç•°ãªã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ã®å´é¢ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚ŠåŒ…æ‹¬çš„ã§å¤šæ§˜ãªæ¨è–¦ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚

From this viewpoint, we may be able to treat offline RL4RS as a type of cross-domain recommendation in certain situations. 
ã“ã®è¦³ç‚¹ã‹ã‚‰ã€ç‰¹å®šã®çŠ¶æ³ã«ãŠã„ã¦ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã‚’ä¸€ç¨®ã®ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨è–¦ã¨ã—ã¦æ‰±ã†ã“ã¨ãŒã§ãã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚

For example, when the evaluation environment is significantly different from the offline dataset, we may treat the evaluation platform as a new domain and we would like to transfer D those learned knowledge from into such a platform. 
ä¾‹ãˆã°ã€è©•ä¾¡ç’°å¢ƒãŒã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨å¤§ããç•°ãªã‚‹å ´åˆã€è©•ä¾¡ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚’æ–°ã—ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨ã—ã¦æ‰±ã„ã€ãã“ã«å­¦ç¿’ã—ãŸçŸ¥è­˜ã‚’ç§»è»¢ã—ãŸã„ã¨è€ƒãˆã¾ã™ã€‚

The challenge in cross-domain recommendation lies in effectively transferring knowledge and patterns across domains while accounting for variations in user behaviors and item characteristics. 
ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨è–¦ã®èª²é¡Œã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã‚„ã‚¢ã‚¤ãƒ†ãƒ ã®ç‰¹æ€§ã®å¤‰å‹•ã‚’è€ƒæ…®ã—ãªãŒã‚‰ã€ãƒ‰ãƒ¡ã‚¤ãƒ³é–“ã§çŸ¥è­˜ã‚„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åŠ¹æœçš„ã«ç§»è»¢ã™ã‚‹ã“ã¨ã«ã‚ã‚Šã¾ã™ã€‚

Techniques such as domain adaptation, transfer learning, and hybrid models are often employed to bridge the gaps between different domains and optimize recommendation performance. 
ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œã€è»¢ç§»å­¦ç¿’ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ãªã©ã®æŠ€è¡“ãŒã€ç•°ãªã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³é–“ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’åŸ‹ã‚ã€æ¨è–¦æ€§èƒ½ã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã«ã—ã°ã—ã°ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

Moreover, recent work in cross-domain offline RL would be beneficial. 
ã•ã‚‰ã«ã€ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã«ãŠã‘ã‚‹æœ€è¿‘ã®ç ”ç©¶ã¯æœ‰ç›Šã§ã™ã€‚

Liu et al. [31] present BOSA (Beyond OOD State Actions), a method for cross-domain offline RL (RL). 
Liuã‚‰ã¯ã€ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLï¼ˆRLï¼‰ã®ãŸã‚ã®æ‰‹æ³•BOSAï¼ˆBeyond OOD State Actionsï¼‰ã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚

BOSA tackles the challenges of out-of-distribution (OOD) state actions and data inefficiency by incorporating additional source domain data. 
BOSAã¯ã€è¿½åŠ ã®ã‚½ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šå…¥ã‚Œã‚‹ã“ã¨ã§ã€åˆ†å¸ƒå¤–ï¼ˆOODï¼‰çŠ¶æ…‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¨ãƒ‡ãƒ¼ã‚¿ã®éåŠ¹ç‡æ€§ã®èª²é¡Œã«å–ã‚Šçµ„ã¿ã¾ã™ã€‚

The authors propose specific objectives to address OOD transition dynamics and demonstrate that BOSA improves data efficiency and outperforms existing methods. 
è‘—è€…ãŸã¡ã¯ã€OODé·ç§»ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã®å…·ä½“çš„ãªç›®æ¨™ã‚’ææ¡ˆã—ã€BOSAãŒãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã‚’æ”¹å–„ã—ã€æ—¢å­˜ã®æ‰‹æ³•ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

The method is also applicable to model-based RL and data augmentation techniques. 
ã“ã®æ‰‹æ³•ã¯ã€ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®RLã‚„ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µæŠ€è¡“ã«ã‚‚é©ç”¨å¯èƒ½ã§ã™ã€‚

However, in offline RL4RS, this problem is still open for investigation as the techniques mentioned have not yet been explored in offline RL4RS. 
ã—ã‹ã—ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã«ãŠã„ã¦ã¯ã€ã“ã‚Œã‚‰ã®æŠ€è¡“ãŒã¾ã ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã§æ¢æ±‚ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ã“ã®å•é¡Œã¯ä¾ç„¶ã¨ã—ã¦èª¿æŸ»ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚

**5.2** **Implicit Feedback and Large Language Models æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¨å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«**

Implicit feedback serves as a commonly employed feedback mechanism for learning recommendation policies in RS. 
æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ï¼ˆRSï¼‰ã«ãŠã‘ã‚‹æ¨è–¦ãƒãƒªã‚·ãƒ¼ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã®ä¸€èˆ¬çš„ã«ä½¿ç”¨ã•ã‚Œã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã§ã™ã€‚

Implicit feedback encompasses user actions like clicks, views, purchases, time spent, and dwell time during interactions with platforms or systems, signifying user preferences and interests. 
æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯ã€ã‚¯ãƒªãƒƒã‚¯ã€ãƒ“ãƒ¥ãƒ¼ã€è³¼å…¥ã€è²»ã‚„ã—ãŸæ™‚é–“ã€ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚„ã‚·ã‚¹ãƒ†ãƒ ã¨ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ä¸­ã®æ»åœ¨æ™‚é–“ãªã©ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å«ã¿ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã‚„èˆˆå‘³ã‚’ç¤ºã—ã¾ã™ã€‚

Although not as explicit as ratings or reviews, these behaviors offer valuable insights. 
è©•ä¾¡ã‚„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã»ã©æ˜ç¤ºçš„ã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€ã“ã‚Œã‚‰ã®è¡Œå‹•ã¯è²´é‡ãªæ´å¯Ÿã‚’æä¾›ã—ã¾ã™ã€‚

In the context of RL4RS, the reward mechanism evaluates recommended items. 
RL4RSã®æ–‡è„ˆã«ãŠã„ã¦ã€å ±é…¬ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯æ¨è–¦ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’è©•ä¾¡ã—ã¾ã™ã€‚

Typically, this involves binary rewards based on click behavior, with some efforts, like Zheng et al. [66], incorporating dwell times for a more comprehensive reward signal. 
é€šå¸¸ã€ã“ã‚Œã¯ã‚¯ãƒªãƒƒã‚¯è¡Œå‹•ã«åŸºã¥ããƒã‚¤ãƒŠãƒªå ±é…¬ã‚’å«ã¿ã€Zhengã‚‰[66]ã®ã‚ˆã†ã«ã€ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªå ±é…¬ä¿¡å·ã®ãŸã‚ã«æ»åœ¨æ™‚é–“ã‚’çµ„ã¿è¾¼ã‚€åŠªåŠ›ã‚‚ã‚ã‚Šã¾ã™ã€‚

However, accommodating multiple implicit feedback sources concurrently in RL4RS poses challenges due to limited relevant datasets or simulations. 
ã—ã‹ã—ã€RL4RSã«ãŠã„ã¦è¤‡æ•°ã®æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚½ãƒ¼ã‚¹ã‚’åŒæ™‚ã«å–ã‚Šå…¥ã‚Œã‚‹ã“ã¨ã¯ã€é–¢é€£ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚„ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒé™ã‚‰ã‚Œã¦ã„ã‚‹ãŸã‚ã€èª²é¡Œã‚’å‘ˆã—ã¾ã™ã€‚

Additionally, harnessing review comments, a common type of implicit feedback in RS, within RL4RS remains a subject of exploration. 
ã•ã‚‰ã«ã€RSã«ãŠã‘ã‚‹ä¸€èˆ¬çš„ãªæš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ä¸€ç¨®ã§ã‚ã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆã‚’RL4RSå†…ã§æ´»ç”¨ã™ã‚‹ã“ã¨ã¯ã€æ¢æ±‚ã®å¯¾è±¡ã¨ãªã£ã¦ã„ã¾ã™ã€‚

Zhang et al. [60] propose a text encoder solution, albeit relying on a manually gathered generator to produce review texts, which primarily validate feature learning rather than directly influencing the final reward. 
Zhangã‚‰[60]ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆã—ã¦ã„ã¾ã™ãŒã€ã“ã‚Œã¯æ‰‹å‹•ã§åé›†ã•ã‚ŒãŸã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã«ä¾å­˜ã—ã¦ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã€ä¸»ã«ç‰¹å¾´å­¦ç¿’ã‚’æ¤œè¨¼ã™ã‚‹ã‚‚ã®ã§ã€æœ€çµ‚çš„ãªå ±é…¬ã«ç›´æ¥å½±éŸ¿ã‚’ä¸ãˆã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

Transitioning this approach to offline RL4RS presents difficulties. 
ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã«ç§»è¡Œã™ã‚‹ã“ã¨ã¯å›°é›£ã§ã™ã€‚

Firstly, integrating review comments into the reward function requires careful study. 
ã¾ãšã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆã‚’å ±é…¬é–¢æ•°ã«çµ±åˆã™ã‚‹ã«ã¯æ…é‡ãªç ”ç©¶ãŒå¿…è¦ã§ã™ã€‚

Secondly, textual data introduces high-dimensional state representations, potentially necessitating novel algorithms tailored to this scenario. 
ç¬¬äºŒã«ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯é«˜æ¬¡å…ƒã®çŠ¶æ…‹è¡¨ç¾ã‚’å°å…¥ã—ã€ã“ã®ã‚·ãƒŠãƒªã‚ªã«ç‰¹åŒ–ã—ãŸæ–°ã—ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒå¿…è¦ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

Recently, Large Language Models (LLMs) have received increasing research interest in RS. 
æœ€è¿‘ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ãŒRSã«ãŠã„ã¦ã¾ã™ã¾ã™ç ”ç©¶ã®é–¢å¿ƒã‚’é›†ã‚ã¦ã„ã¾ã™ã€‚

LLM demonstrates a superior capability in handling textual data from multiple tasks such as natural language understanding, contextual understanding and sentiment analysis [65]. 
LLMã¯ã€è‡ªç„¶è¨€èªç†è§£ã€æ–‡è„ˆç†è§£ã€æ„Ÿæƒ…åˆ†æãªã©ã®è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹å„ªã‚ŒãŸèƒ½åŠ›ã‚’ç¤ºã—ã¾ã™[65]ã€‚

Existing RS works provides some insights about how LLMs can be adopted in RS such as prompt engineering to instruct the LLM to make recommendations [58], utilizing the Generative Pre-trained Transformer (GPT) as the backbone to process features [43] etc. 
æ—¢å­˜ã®RSã®ç ”ç©¶ã¯ã€LLMã‚’RSã«ã©ã®ã‚ˆã†ã«æ¡ç”¨ã§ãã‚‹ã‹ã«ã¤ã„ã¦ã®ã„ãã¤ã‹ã®æ´å¯Ÿã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚ä¾‹ãˆã°ã€LLMã«æ¨è–¦ã‚’è¡Œã†ã‚ˆã†æŒ‡ç¤ºã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°[58]ã‚„ã€ç‰¹å¾´ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã®ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦Generative Pre-trained Transformerï¼ˆGPTï¼‰ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨[43]ãªã©ã§ã™ã€‚

Moreover, some attempts have been undertaken about how LLM can be used in RL. 
ã•ã‚‰ã«ã€LLMã‚’RLã§ã©ã®ã‚ˆã†ã«ä½¿ç”¨ã§ãã‚‹ã‹ã«ã¤ã„ã¦ã®ã„ãã¤ã‹ã®è©¦ã¿ãŒè¡Œã‚ã‚Œã¦ã„ã¾ã™ã€‚

Du et al. [14] introduce a method called ELLM (Exploring with Large Language Models) that aims to enhance pretraining in RL by using LLM. 
Duã‚‰[14]ã¯ã€LLMã‚’ä½¿ç”¨ã—ã¦RLã®äº‹å‰å­¦ç¿’ã‚’å¼·åŒ–ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ãŸæ‰‹æ³•ELLMï¼ˆExploring with Large Language Modelsï¼‰ã‚’ç´¹ä»‹ã—ã¦ã„ã¾ã™ã€‚

ELLM works by prompting an LLM with a description of the agentâ€™s current state and then rewarding the agent for achieving goals suggested by the LLM. 
ELLMã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç¾åœ¨ã®çŠ¶æ…‹ã®èª¬æ˜ã‚’ç”¨ã„ã¦LLMã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸ãˆã€ãã®å¾ŒLLMãŒææ¡ˆã—ãŸç›®æ¨™ã‚’é”æˆã™ã‚‹ã“ã¨ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«å ±é…¬ã‚’ä¸ãˆã¾ã™ã€‚

This method biases exploration towards behaviors that are meaningful and potentially useful from a human perspective, without needing human intervention. 
ã“ã®æ‰‹æ³•ã¯ã€äººé–“ã®è¦–ç‚¹ã‹ã‚‰æ„å‘³ãŒã‚ã‚Šã€æ½œåœ¨çš„ã«æœ‰ç”¨ãªè¡Œå‹•ã¸ã®æ¢ç´¢ã‚’åã‚‰ã›ã€äººé–“ã®ä»‹å…¥ã‚’å¿…è¦ã¨ã—ã¾ã›ã‚“ã€‚

Meanwhile, Carta et al. [3] explore the use of LLM in interactive environments through an approach called GLAM (Grounding Language Models). 
ä¸€æ–¹ã€Cartaã‚‰[3]ã¯ã€GLAMï¼ˆGrounding Language Modelsï¼‰ã¨å‘¼ã°ã‚Œã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’é€šã˜ã¦ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªç’°å¢ƒã«ãŠã‘ã‚‹LLMã®ä½¿ç”¨ã‚’æ¢æ±‚ã—ã¦ã„ã¾ã™ã€‚

This method aligns the knowledge of LLMs with the environment, focusing on aspects like sample efficiency, generalization to new tasks, and the impact of online RL. 
ã“ã®æ‰‹æ³•ã¯ã€LLMã®çŸ¥è­˜ã‚’ç’°å¢ƒã¨æ•´åˆã•ã›ã€ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã€æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã¸ã®ä¸€èˆ¬åŒ–ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³RLã®å½±éŸ¿ãªã©ã®å´é¢ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚

**5.3** **Causality å› æœé–¢ä¿‚**

In the previous section, we mentioned that offline RL can be formulated as answering counterfactual queries. 
å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã¯åå®Ÿä»®æƒ³ã‚¯ã‚¨ãƒªã«ç­”ãˆã‚‹å½¢ã§å®šå¼åŒ–ã§ãã‚‹ã“ã¨ã«è¨€åŠã—ã¾ã—ãŸã€‚

It is an intuitive choice to integrate the causality into offline RL from this perspective. 
ã“ã®è¦³ç‚¹ã‹ã‚‰ã€å› æœé–¢ä¿‚ã‚’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã«çµ±åˆã™ã‚‹ã“ã¨ã¯ç›´æ„Ÿçš„ãªé¸æŠã§ã™ã€‚

Moreover, causality is widely used in RS and receiving increasing interest in offline RL. 
ã•ã‚‰ã«ã€å› æœé–¢ä¿‚ã¯RSã§åºƒãä½¿ç”¨ã•ã‚Œã¦ãŠã‚Šã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã«ãŠã„ã¦ã‚‚é–¢å¿ƒãŒé«˜ã¾ã£ã¦ã„ã¾ã™ã€‚

We believe it would be a promising topic in offline RL4RS. 
ç§ãŸã¡ã¯ã€ã“ã‚Œã¯ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã«ãŠã‘ã‚‹æœ‰æœ›ãªãƒˆãƒ”ãƒƒã‚¯ã«ãªã‚‹ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚

In the work by Zhu et al. [67], an exploration is undertaken regarding the integration of causal world models into the domain of model-based offline RL. 
Zhuã‚‰[67]ã®ç ”ç©¶ã§ã¯ã€å› æœä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®é ˜åŸŸã«çµ±åˆã™ã‚‹æ¢æ±‚ãŒè¡Œã‚ã‚Œã¦ã„ã¾ã™ã€‚

The theoretical underpinning of their study accentuates the superiority of causal world models over ordinary world models in the context of offline RL. 
å½¼ã‚‰ã®ç ”ç©¶ã®ç†è«–çš„åŸºç›¤ã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®æ–‡è„ˆã«ãŠã„ã¦å› æœä¸–ç•Œãƒ¢ãƒ‡ãƒ«ãŒé€šå¸¸ã®ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å¼·èª¿ã—ã¦ã„ã¾ã™ã€‚

This advantage is attributed to the incorporation of causal structure within the generalization error bound. 
ã“ã®åˆ©ç‚¹ã¯ã€ä¸€èˆ¬åŒ–èª¤å·®å¢ƒç•Œå†…ã«å› æœæ§‹é€ ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã«èµ·å› ã—ã¦ã„ã¾ã™ã€‚

The authors introduce an operational algorithm termed FOCUS (Offline Model-based RL with Causal Structure) to exemplify the potential value derived from comprehending and effectively utilizing causal structure in the domain of offline RL. 
è‘—è€…ãŸã¡ã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®é ˜åŸŸã«ãŠã‘ã‚‹å› æœæ§‹é€ ã‚’ç†è§£ã—ã€åŠ¹æœçš„ã«æ´»ç”¨ã™ã‚‹ã“ã¨ã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹æ½œåœ¨çš„ãªä¾¡å€¤ã‚’ç¤ºã™ãŸã‚ã«ã€FOCUSï¼ˆå› æœæ§‹é€ ã‚’æŒã¤ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹RLï¼‰ã¨ã„ã†æ“ä½œã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç´¹ä»‹ã—ã¦ã„ã¾ã™ã€‚

Additionally, Liao et al. [29] introduce the notion of instrumental variable value iteration for causal offline RL. 
ã•ã‚‰ã«ã€Liaoã‚‰[29]ã¯ã€å› æœã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®ãŸã‚ã®è¨ˆé‡çµŒæ¸ˆå­¦çš„å¤‰æ•°ä¾¡å€¤åå¾©ã®æ¦‚å¿µã‚’å°å…¥ã—ã¦ã„ã¾ã™ã€‚

The presentation of their work introduces IV-aided Value Iteration (IVVI), an algorithm designed with efficiency in mind, aimed at extracting optimal policies from observational data in the presence of unobserved variables. 
å½¼ã‚‰ã®ç ”ç©¶ã®ç™ºè¡¨ã§ã¯ã€è¦³å¯Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ€é©ãªãƒãƒªã‚·ãƒ¼ã‚’æŠ½å‡ºã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ãŸåŠ¹ç‡æ€§ã‚’è€ƒæ…®ã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ IVæ”¯æ´ä¾¡å€¤åå¾©ï¼ˆIVVIï¼‰ãŒç´¹ä»‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

The utilization of instrumental variables (IVs) forms the foundation, with the authors devising a framework named Confounded Markov Decision Process with Instrumental Variables (CMDP-IV) to contextualize the problem. 
è¨ˆé‡çµŒæ¸ˆå­¦çš„å¤‰æ•°ï¼ˆIVï¼‰ã®åˆ©ç”¨ãŒåŸºç›¤ã‚’å½¢æˆã—ã€è‘—è€…ãŸã¡ã¯å•é¡Œã‚’æ–‡è„ˆåŒ–ã™ã‚‹ãŸã‚ã«IVã‚’æŒã¤æ··ä¹±ã—ãŸãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆCMDP-IVï¼‰ã¨ã„ã†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’è€ƒæ¡ˆã—ã¦ã„ã¾ã™ã€‚

Notably, the IVVI algorithm, established upon a primal-dual reformulation of a conditional moment restriction, emerges as the first demonstrably efficient solution for instrument-aided offline RL. 
ç‰¹ã«ã€æ¡ä»¶ä»˜ããƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆåˆ¶ç´„ã®ãƒ—ãƒ©ã‚¤ãƒãƒ«-åŒå¯¾å†å®šå¼åŒ–ã«åŸºã¥ã„ã¦ç¢ºç«‹ã•ã‚ŒãŸIVVIã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€è¨ˆé‡çµŒæ¸ˆå­¦çš„æ”¯æ´ã‚’å—ã‘ãŸã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã«å¯¾ã™ã‚‹åˆã®å®Ÿè¨¼çš„ã«åŠ¹ç‡çš„ãªè§£æ±ºç­–ã¨ã—ã¦æµ®ä¸Šã—ã¾ã™ã€‚

One of the most common applications of integrating causality into the RL4RS is counterfactual augmentation. 
å› æœé–¢ä¿‚ã‚’RL4RSã«çµ±åˆã™ã‚‹æœ€ã‚‚ä¸€èˆ¬çš„ãªå¿œç”¨ã®ä¸€ã¤ã¯ã€åå®Ÿä»®æƒ³æ‹¡å¼µã§ã™ã€‚

Chen et al. [8, 9] develop a data augmentation technique that employs counterfactual reasoning to produce more informative interaction trajectories for RL4RS. 
Chenã‚‰[8, 9]ã¯ã€åå®Ÿä»®æƒ³æ¨è«–ã‚’ç”¨ã„ã¦RL4RSã®ãŸã‚ã«ã‚ˆã‚Šæƒ…å ±é‡ã®å¤šã„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒªã‚’ç”Ÿæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µæŠ€è¡“ã‚’é–‹ç™ºã—ã¦ã„ã¾ã™ã€‚

Wang et al. [48] introduces the Causal Decision Transformer for RS (CDT4Rec), a model that merges offline RL with the transformer architecture. 
Wangã‚‰[48]ã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã¨ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’çµ±åˆã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹RSã®ãŸã‚ã®å› æœæ±ºå®šãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ï¼ˆCDT4Recï¼‰ã‚’ç´¹ä»‹ã—ã¦ã„ã¾ã™ã€‚

CDT4Rec is designed to tackle the challenges of crafting reward functions and leveraging large-scale datasets in RS. 
CDT4Recã¯ã€RSã«ãŠã‘ã‚‹å ±é…¬é–¢æ•°ã®ä½œæˆã¨å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ´»ç”¨ã«é–¢ã™ã‚‹èª²é¡Œã«å¯¾å‡¦ã™ã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚

It employs a causal mechanism to deduce rewards from user behavior and uses the transformer architecture to handle vast datasets and identify dependencies. 
ãã‚Œã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã‹ã‚‰å ±é…¬ã‚’æ¨å®šã™ã‚‹ãŸã‚ã«å› æœãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æ¡ç”¨ã—ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ç”¨ã—ã¦è†¨å¤§ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‡¦ç†ã—ã€ä¾å­˜é–¢ä¿‚ã‚’ç‰¹å®šã—ã¾ã™ã€‚

Drawing inspiration from the works mentioned above, exploring causality in offline RL4RS emerges as a promising avenue for future research. 
ä¸Šè¨˜ã®ç ”ç©¶ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¾—ã¦ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã«ãŠã‘ã‚‹å› æœé–¢ä¿‚ã®æ¢æ±‚ã¯ã€ä»Šå¾Œã®ç ”ç©¶ã®æœ‰æœ›ãªé“ç­‹ã¨ã—ã¦æµ®ä¸Šã—ã¾ã™ã€‚

Particularly, as causal offline RL4RS advances, its primary emphasis on counterfactual augmentation highlights an exciting direction. 
ç‰¹ã«ã€å› æœã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSãŒé€²å±•ã™ã‚‹ã«ã¤ã‚Œã¦ã€åå®Ÿä»®æƒ³æ‹¡å¼µã«å¯¾ã™ã‚‹ä¸»ãªå¼·èª¿ã¯ã€åˆºæ¿€çš„ãªæ–¹å‘æ€§ã‚’æµ®ãå½«ã‚Šã«ã—ã¾ã™ã€‚

However, it is important to recognize the need for additional endeavors in different domains, including but not limited to distribution shifts and the presence of biases. 
ã—ã‹ã—ã€åˆ†å¸ƒã®å¤‰åŒ–ã‚„ãƒã‚¤ã‚¢ã‚¹ã®å­˜åœ¨ã‚’å«ã‚€ãŒãã‚Œã«é™å®šã•ã‚Œãªã„ç•°ãªã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã«ãŠã‘ã‚‹è¿½åŠ ã®å–ã‚Šçµ„ã¿ã®å¿…è¦æ€§ã‚’èªè­˜ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚

**5.4** **Robustness å …ç‰¢æ€§**

The vulnerability of deep learning-based methods is evident through adversarial samples, underscoring the pressing concern of robustness in both RS and RL. 
æ·±å±¤å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã®è„†å¼±æ€§ã¯ã€æ•µå¯¾çš„ã‚µãƒ³ãƒ—ãƒ«ã‚’é€šã˜ã¦æ˜ã‚‰ã‹ã§ã‚ã‚Šã€RSã¨RLã®ä¸¡æ–¹ã«ãŠã‘ã‚‹å …ç‰¢æ€§ã®é‡è¦ãªæ‡¸å¿µã‚’å¼·èª¿ã—ã¦ã„ã¾ã™ã€‚

Particularly, the exploration of adversarial attacks and defense strategies within the domain of RS has garnered significant attention in recent times, as emphasized by the comprehensive survey conducted by [13]. 
ç‰¹ã«ã€RSã®é ˜åŸŸã«ãŠã‘ã‚‹æ•µå¯¾çš„æ”»æ’ƒã¨é˜²å¾¡æˆ¦ç•¥ã®æ¢æ±‚ã¯ã€æœ€è¿‘å¤§ããªæ³¨ç›®ã‚’é›†ã‚ã¦ãŠã‚Šã€[13]ã«ã‚ˆã£ã¦å®Ÿæ–½ã•ã‚ŒãŸåŒ…æ‹¬çš„ãªèª¿æŸ»ã«ã‚ˆã£ã¦å¼·èª¿ã•ã‚Œã¦ã„ã¾ã™ã€‚

This attention is fueled by the critical importance of security within the realm of RS operations. 
ã“ã®æ³¨ç›®ã¯ã€RSã®é‹ç”¨ã«ãŠã‘ã‚‹ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®é‡è¦æ€§ã«ã‚ˆã£ã¦ä¿ƒé€²ã•ã‚Œã¦ã„ã¾ã™ã€‚

Furthermore, the vulnerability of RL policies to adversarial perturbations in agentsâ€™ observations has been established by [30]. 
ã•ã‚‰ã«ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¦³å¯Ÿã«ãŠã‘ã‚‹æ•µå¯¾çš„æ‘‚å‹•ã«å¯¾ã™ã‚‹RLãƒãƒªã‚·ãƒ¼ã®è„†å¼±æ€§ã¯[30]ã«ã‚ˆã£ã¦ç¢ºç«‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

In the context of RL4RS, Cao et al. [2] introduce an adversarial attack detection approach. 
RL4RSã®æ–‡è„ˆã«ãŠã„ã¦ã€Caoã‚‰[2]ã¯æ•µå¯¾çš„æ”»æ’ƒæ¤œå‡ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç´¹ä»‹ã—ã¦ã„ã¾ã™ã€‚

This method leverages the utilization of a Gated Recurrent Unit (GRU) to encode the action space into a lower-dimensional representation, alongside the design of decoders to identify potential attacks. 
ã“ã®æ‰‹æ³•ã¯ã€Gated Recurrent Unitï¼ˆGRUï¼‰ã‚’åˆ©ç”¨ã—ã¦ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“ã‚’ä½æ¬¡å…ƒã®è¡¨ç¾ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€æ½œåœ¨çš„ãªæ”»æ’ƒã‚’ç‰¹å®šã™ã‚‹ãŸã‚ã®ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®è¨­è¨ˆã‚’è¡Œã„ã¾ã™ã€‚

However, itâ€™s important to note that this method exclusively addresses attacks rooted in the Fast Gradient Sign Method (FGSM) and strategically-timed maneuvers. 
ã—ã‹ã—ã€ã“ã®æ‰‹æ³•ã¯ã€Fast Gradient Sign Methodï¼ˆFGSMï¼‰ãŠã‚ˆã³æˆ¦ç•¥çš„ã«ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’åˆã‚ã›ãŸæ“ä½œã«åŸºã¥ãæ”»æ’ƒã«ã®ã¿å¯¾å‡¦ã—ã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚

As a result, its ability to detect other forms of attacks is limited. 
ãã®çµæœã€ä»–ã®å½¢å¼ã®æ”»æ’ƒã‚’æ¤œå‡ºã™ã‚‹èƒ½åŠ›ã¯é™ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

Within the arena of offline RL, recent advancements provide a promising direction. 
ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®é ˜åŸŸã«ãŠã„ã¦ã€æœ€è¿‘ã®é€²å±•ã¯æœ‰æœ›ãªæ–¹å‘æ€§ã‚’æä¾›ã—ã¾ã™ã€‚

Panaganti et al. [40] address the challenge of robust offline RL, centering on the learning of policies that can withstand uncertainties in model parameters. 
Panagantiã‚‰[40]ã¯ã€ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸ç¢ºå®Ÿæ€§ã«è€ãˆã‚‹ãƒãƒªã‚·ãƒ¼ã®å­¦ç¿’ã«ç„¦ç‚¹ã‚’å½“ã¦ã€å …ç‰¢ãªã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLã®èª²é¡Œã«å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™ã€‚

The authors introduce the Robust Fitted Q-Iteration (RFQI) algorithm, which relies solely on offline data to determine the optimal robust policy. 
è‘—è€…ãŸã¡ã¯ã€æœ€é©ãªå …ç‰¢ãƒãƒªã‚·ãƒ¼ã‚’æ±ºå®šã™ã‚‹ãŸã‚ã«ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ã¿ã«ä¾å­˜ã™ã‚‹å …ç‰¢ãªãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°Qåå¾©ï¼ˆRFQIï¼‰ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç´¹ä»‹ã—ã¦ã„ã¾ã™ã€‚

This algorithm adeptly tackles concerns such as offline data collection, model optimization, and unbiased estimation. 
ã“ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†ã€ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ã€ãƒã‚¤ã‚¢ã‚¹ã®ãªã„æ¨å®šãªã©ã®æ‡¸å¿µã«å·§ã¿ã«å¯¾å‡¦ã—ã¾ã™ã€‚

Additionally, Zhang et al. [62] concentrate on a scenario involving a batch dataset of state-action-reward-next state tuples, susceptible to potential corruption by adversaries. 
ã•ã‚‰ã«ã€Zhangã‚‰[62]ã¯ã€æ•µã«ã‚ˆã‚‹æ½œåœ¨çš„ãªç ´æã®å½±éŸ¿ã‚’å—ã‘ã‚„ã™ã„çŠ¶æ…‹-ã‚¢ã‚¯ã‚·ãƒ§ãƒ³-å ±é…¬-æ¬¡çŠ¶æ…‹ã®ã‚¿ãƒ—ãƒ«ã®ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å«ã‚€ã‚·ãƒŠãƒªã‚ªã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚

Their objective is to extract a near-optimal policy from this compromised dataset. 
å½¼ã‚‰ã®ç›®çš„ã¯ã€ã“ã®æãªã‚ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰è¿‘ä¼¼æœ€é©ãƒãƒªã‚·ãƒ¼ã‚’æŠ½å‡ºã™ã‚‹ã“ã¨ã§ã™ã€‚



## 6 CONCLUSION çµè«–

The recent advancements in RL4RS pave the way for efficiently capturing usersâ€™ dynamic interests. 
æœ€è¿‘ã®RL4RSã®é€²å±•ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å‹•çš„ãªèˆˆå‘³ã‚’åŠ¹ç‡çš„ã«æ‰ãˆã‚‹é“ã‚’é–‹ãã¾ã™ã€‚

However, the nature of online interactions necessitates costly trajectory collection procedures, posing a significant hurdle for researchers interested in this field. 
ã—ã‹ã—ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã®æ€§è³ªã¯ã€é«˜ä¾¡ãªè»Œé“åé›†æ‰‹ç¶šãã‚’å¿…è¦ã¨ã—ã€ã“ã®åˆ†é‡ã«èˆˆå‘³ã‚’æŒã¤ç ”ç©¶è€…ã«ã¨ã£ã¦å¤§ããªéšœå®³ã¨ãªã£ã¦ã„ã¾ã™ã€‚

In this survey, our goal is to provide a comprehensive overview of offline RL4RS, a novel paradigm that eliminates the need for an expensive data collection process. 
ã“ã®èª¿æŸ»ã®ç›®çš„ã¯ã€é«˜ä¾¡ãªãƒ‡ãƒ¼ã‚¿åé›†ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ’é™¤ã™ã‚‹æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§ã‚ã‚‹ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã®åŒ…æ‹¬çš„ãªæ¦‚è¦ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã™ã€‚

Alongside reviewing recent works, we also offer insights into potential future opportunities. 
æœ€è¿‘ã®ç ”ç©¶ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹ã ã‘ã§ãªãã€å°†æ¥ã®å¯èƒ½æ€§ã«ã¤ã„ã¦ã®æ´å¯Ÿã‚‚æä¾›ã—ã¾ã™ã€‚

Specifically, weâ€™ve compiled and analyzed recent progress in offline RL4RS, organized into two distinct categories: off-policy learning utilizing logged data and offline RL4RS techniques. 
å…·ä½“çš„ã«ã¯ã€ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å­¦ç¿’ï¼ˆãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’åˆ©ç”¨ï¼‰ã¨ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSæŠ€è¡“ã¨ã„ã†2ã¤ã®ç•°ãªã‚‹ã‚«ãƒ†ã‚´ãƒªã«æ•´ç†ã•ã‚ŒãŸã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã®æœ€è¿‘ã®é€²å±•ã‚’ã¾ã¨ã‚ã€åˆ†æã—ã¾ã—ãŸã€‚

Furthermore, we address several prevailing challenges in this domain: offline data quality, distribution shift, bias and variance, and explainability. 
ã•ã‚‰ã«ã€ã“ã®åˆ†é‡ã«ãŠã‘ã‚‹ã„ãã¤ã‹ã®ä¸€èˆ¬çš„ãªèª²é¡Œã€ã™ãªã‚ã¡ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã®è³ªã€åˆ†å¸ƒã®å¤‰åŒ–ã€ãƒã‚¤ã‚¢ã‚¹ã¨åˆ†æ•£ã€èª¬æ˜å¯èƒ½æ€§ã«ã¤ã„ã¦ã‚‚å–ã‚Šä¸Šã’ã¾ã™ã€‚

Additionally, we present potential avenues for future exploration in this rapidly evolving field, such as cross-domain recommendation, LLMs, causality, and robustness. 
åŠ ãˆã¦ã€ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨è–¦ã€LLMsã€å› æœé–¢ä¿‚ã€ãƒ­ãƒã‚¹ãƒˆæ€§ãªã©ã€ã“ã®æ€¥é€Ÿã«é€²åŒ–ã™ã‚‹åˆ†é‡ã«ãŠã‘ã‚‹å°†æ¥ã®æ¢æ±‚ã®ãŸã‚ã®æ½œåœ¨çš„ãªé“ç­‹ã‚’æç¤ºã—ã¾ã™ã€‚

Being an emerging topic, offline RL4RS introduces fresh possibilities for integrating pre-existing offline datasets into the realm of RL4RS. 
æ–°ãŸã«ç™»å ´ã—ãŸãƒˆãƒ”ãƒƒã‚¯ã§ã‚ã‚‹ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RL4RSã¯ã€æ—¢å­˜ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’RL4RSã®é ˜åŸŸã«çµ±åˆã™ã‚‹ãŸã‚ã®æ–°ã—ã„å¯èƒ½æ€§ã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚

This survey can also be perceived as a visionary paper, offering potential benefits to researchers who are newcomers to this field. 
ã“ã®èª¿æŸ»ã¯ã€ã¾ãŸã€ã“ã®åˆ†é‡ã«æ–°ãŸã«å‚å…¥ã™ã‚‹ç ”ç©¶è€…ã«æ½œåœ¨çš„ãªåˆ©ç›Šã‚’æä¾›ã™ã‚‹ãƒ“ã‚¸ãƒ§ãƒŠãƒªãƒ¼ãªè«–æ–‡ã¨è¦‹ãªã™ã“ã¨ã‚‚ã§ãã¾ã™ã€‚
