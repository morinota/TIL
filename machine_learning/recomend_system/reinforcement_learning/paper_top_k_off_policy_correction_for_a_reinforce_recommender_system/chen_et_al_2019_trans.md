## 0.1. link

- https://arxiv.org/abs/1812.02353 httpsã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

## 0.2. title ã‚¿ã‚¤ãƒˆãƒ«

Top-K Off-Policy Correction for a REINFORCE Recommender System
REINFORCEæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ãŸã‚ã®Top-Kã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£æ³•

## 0.3. abstract ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆ

Industrial recommender systems deal with extremely large action spaces -- many millions of items to recommend.
ç”£æ¥­ç”¨æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¯ã€ä½•ç™¾ä¸‡ã‚‚ã®å•†å“ã‚’æ¨è–¦ã™ã‚‹ã¨ã„ã†éå¸¸ã«å¤§ããªè¡Œå‹•ç©ºé–“ã‚’æ‰±ã£ã¦ã„ã‚‹.
Moreover, they need to serve billions of users, who are unique at any point in time, making a complex user state space.
ã•ã‚‰ã«ã€ä½•åå„„äººã‚‚ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å¯¾è±¡ã¨ã™ã‚‹ãŸã‚ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®çŠ¶æ…‹ç©ºé–“ã‚‚è¤‡é›‘ã§ã‚ã‚‹.
Luckily, huge quantities of logged implicit feedback (e.g., user clicks, dwell time) are available for learning.
å¹¸ã„ãªã“ã¨ã«ã€è†¨å¤§ãªé‡ã®æš—é»™çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ãƒªãƒƒã‚¯æ•°ã€æ»åœ¨æ™‚é–“ãªã©ï¼‰ãŒå­¦ç¿’ç”¨ã«åˆ©ç”¨ã§ãã‚‹.
Learning from the logged feedback is however subject to biases caused by only observing feedback on recommendations selected by the previous versions of the recommender.
ã—ã‹ã—ã€**ãƒ­ã‚°ã«è¨˜éŒ²ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ã®å­¦ç¿’ã¯ã€ä»¥å‰ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã«ã‚ˆã£ã¦é¸æŠã•ã‚ŒãŸæ¨è–¦ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆã«å¯¾ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ã¿ã‚’è¦³æ¸¬ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦å¼•ãèµ·ã“ã•ã‚Œã‚‹ãƒã‚¤ã‚¢ã‚¹ã«å·¦å³ã•ã‚Œã‚‹**.
In this work, we present a general recipe of addressing such biases in a production top-K recommender system at Youtube, built with a policy-gradient-based algorithm, i.e. REINFORCE.
æœ¬è«–æ–‡ã§ã¯ã€Youtubeã®ãƒˆãƒƒãƒ—Kæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã„ã¦ã€**policy-gradient-based algorithm**(i.e. REINFORCE)ã‚’ç”¨ã„ã¦ã€ã“ã®ã‚ˆã†ãªãƒã‚¤ã‚¢ã‚¹ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã®ä¸€èˆ¬çš„ãªãƒ¬ã‚·ãƒ”ã‚’ç´¹ä»‹ã™ã‚‹.
The contributions of the paper are:
æœ¬è«–æ–‡ã®è²¢çŒ®ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã‚ã‚‹.
(1) scaling REINFORCE to a production recommender system with an action space on the orders of millions; (2) applying off-policy correction to address data biases in learning from logged feedback collected from multiple behavior policies; (3) proposing a novel top-K off-policy correction to account for our policy recommending multiple items at a time; (4) showcasing the value of exploration.
(1)REINFORCEã‚’æ•°ç™¾ä¸‡ã®è¡Œå‹•ç©ºé–“ã‚’æŒã¤æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«æ‹¡å¼µã™ã‚‹ã“ã¨ã€(2)è¤‡æ•°ã®behavior policy ã‹ã‚‰åé›†ã—ãŸãƒ­ã‚°feedbackã‹ã‚‰å­¦ç¿’ã™ã‚‹éš›ã® data bias ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã« off-policy ä¿®æ­£ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã€(3)ä¸€åº¦ã«è¤‡æ•°ã®itemã‚’æ¨è–¦ã™ã‚‹ãƒãƒªã‚·ãƒ¼ã‚’èª¬æ˜ã™ã‚‹ãŸã‚ã«æ–°ã—ã„ãƒˆãƒƒãƒ—Kã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ä¿®æ­£ã‚’ææ¡ˆã™ã‚‹ã“ã¨ã€(4)æ¢æŸ»ã®ä¾¡å€¤ã‚’ç´¹ä»‹ã™ã‚‹ã“ã¨ã€ã§ã‚ã‚‹.
We demonstrate the efficacy of our approaches through a series of simulations and multiple live experiments on Youtube.
ç§ãŸã¡ã¯ã€ä¸€é€£ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨Youtubeã§ã®è¤‡æ•°ã®ãƒ©ã‚¤ãƒ–å®Ÿé¨“ã‚’é€šã—ã¦ã€ç§ãŸã¡ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æœ‰åŠ¹æ€§ã‚’å®Ÿè¨¼ã—ã¦ã„ã‚‹.

# 1. Introduction ã¯ã˜ã‚ã«

Recommender systems are relied on, throughout industry, to help users sort through huge corpuses of content and discover the small fraction of content they would be interested in.
æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¯ã€ãƒ¦ãƒ¼ã‚¶ãŒè†¨å¤§ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ä¸­ã‹ã‚‰èˆˆå‘³ã®ã‚ã‚Šãã†ãªã”ãä¸€éƒ¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¢ã—å‡ºã™ã“ã¨ã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã«ã€ç”£æ¥­ç•Œå…¨ä½“ã§åˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹.
This problem is challenging because of the huge number of items that could be recommended.
ã“ã®å•é¡Œã¯ã€æ¨è–¦ã•ã‚Œã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ãŒè†¨å¤§ã§ã‚ã‚‹ãŸã‚ã€å›°é›£ãªã‚‚ã®ã¨ãªã£ã¦ã„ã‚‹.
Furthermore, surfacing the right item to the right user at the right time requires the recommender system to constantly adapt to usersâ€™ shifting interest (state) based on their historical interaction with the system [6].
ã•ã‚‰ã«ã€**é©åˆ‡ãªãƒ¦ãƒ¼ã‚¶ã«é©åˆ‡ãªã‚¢ã‚¤ãƒ†ãƒ ã‚’é©åˆ‡ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§**æç¤ºã™ã‚‹ãŸã‚ã«ã¯ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¯ã€ãƒ¦ãƒ¼ã‚¶ã®éå»ã®ã‚·ã‚¹ãƒ†ãƒ ã¨ã®ç›¸äº’ä½œç”¨ã«åŸºã¥ã„ã¦**å¤‰åŒ–ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ã®èˆˆå‘³(çŠ¶æ…‹ = state)ã«å¸¸ã«é©å¿œã™ã‚‹**å¿…è¦ãŒã‚ã‚‹[6].
Unfortunately, we observe relatively little data for such a large state and action space, with most users only having been exposed to a small fraction of items and providing explicit feedback to an even smaller fraction.
ã—ã‹ã—ã€ã“ã®ã‚ˆã†ãªå¤§ããªçŠ¶æ…‹ãƒ»è¡Œå‹•ç©ºé–“ã«å¯¾ã—ã¦ã€æˆ‘ã€…ã¯æ¯”è¼ƒçš„å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã—ã‹è¦³æ¸¬ã—ã¦ãŠã‚‰ãšã€ã»ã¨ã‚“ã©ã®ãƒ¦ãƒ¼ã‚¶ã¯ã”ãä¸€éƒ¨ã®ã‚¢ã‚¤ãƒ†ãƒ ã«ã—ã‹è§¦ã‚Œã¦ãŠã‚‰ãšã€ã•ã‚‰ã«ã”ãä¸€éƒ¨ã®ãƒ¦ãƒ¼ã‚¶ã«ã—ã‹explicit feedback ã‚’ä¸ãˆã¦ã„ãªã„.
That is, recommender systems receive extremely sparse data for training in general, e.g., the Netflix Prize dataset was only 0.1% dense [5].
ä¾‹ãˆã°ã€Netflix Prizeã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯0.1%ã®å¯†åº¦ã«éããªã„[5].
As a result, a good amount of research in recommender systems explores different mechanisms for treating this extreme sparsity.
ãã®çµæœã€ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®ç ”ç©¶ã®ã‹ãªã‚Šã®éƒ¨åˆ†ã¯ã€**ã“ã®æ¥µç«¯ãªã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã®ã•ã¾ã–ã¾ãªãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æ¢æ±‚**ã—ã¦ã„ã‚‹.
Learning from implicit user feedback, such as clicks and dwell-time, as well as filling in unobserved interactions, has been an important step in improving recommenders [19] but the problem remains an open one.
ã‚¯ãƒªãƒƒã‚¯ã‚„æ»åœ¨æ™‚é–“ãªã©ã®ãƒ¦ãƒ¼ã‚¶ã®implicit feedbackã‹ã‚‰ã®å­¦ç¿’ã‚„ã€è¦³æ¸¬ã•ã‚Œãª ã„ Interaction ã®å……å¡«ã¯ã€ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚’æ”¹å–„ã™ã‚‹ä¸Šã§é‡è¦ãªã‚¹ãƒ†ãƒƒãƒ—ã¨ãªã£ã¦ã„ã¾ã™ãŒ [19]ã€ã“ã®å•é¡Œã¯ã¾ã æœªè§£æ±ºã®ã‚‚ã®ã§ã‚ã‚‹.

In a mostly separate line of research, reinforcement learning (RL) has recently achieved impressive advances in games [38, 46] as well as robotics [22, 25].
å¼·åŒ–å­¦ç¿’(RL)ã¯ã€ã‚²ãƒ¼ãƒ [38, 46]ã‚„ãƒ­ãƒœãƒƒãƒˆå·¥å­¦[22, 25]ã«ãŠã„ã¦ã€æœ€è¿‘ç›®è¦šã—ã„ç™ºå±•ã‚’é‚ã’ã¦ã„ã‚‹.
RL in general focuses on building agents that take actions in an environment so as to maximize some notion of long term reward.
RLã¯ä¸€èˆ¬ã«ã€**é•·æœŸçš„ãªå ±é…¬(reward)ã®æ¦‚å¿µã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ã«ç’°å¢ƒä¸­ã§è¡Œå‹•ã™ã‚‹agentã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨**ã«ç„¦ç‚¹ã‚’åˆã‚ã›ã¦ã„ã‚‹.
Here we explore framing recommendation as building RL agents to maximize each userâ€™s long term satisfaction with the system.
ã“ã“ã§ã¯ã€æ¨è–¦ã‚’ã€**å„ãƒ¦ãƒ¼ã‚¶ã®ã‚·ã‚¹ãƒ†ãƒ ã«å¯¾ã™ã‚‹é•·æœŸçš„ãªæº€è¶³åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹RL agent** ã®æ§‹ç¯‰ã¨ã—ã¦ã¨ã‚‰ãˆã‚‹ã“ã¨ã‚’æ¤œè¨ã™ã‚‹.
This offers us new perspectives on recommendation problems as well as opportunities to build on top of the recent RL advancement.
ã“ã‚Œã¯ã€æ¨è–¦å•é¡Œã«å¯¾ã™ã‚‹æ–°ã—ã„è¦–ç‚¹ã¨ã€æœ€è¿‘ã®RLã®é€²æ­©ã®ä¸Šã«æ§‹ç¯‰ã•ã‚Œã‚‹æ©Ÿä¼šã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã‚ã‚‹.
However, there are significant challenges to put this perspective into practice.
ã—ã‹ã—ã€ã“ã®è¦–ç‚¹ã‚’å®Ÿè·µã™ã‚‹ãŸã‚ã«ã¯ã€å¤§ããªèª²é¡ŒãŒã‚ã‚‹.

As introduced above, recommender systems deal with large state and action spaces, and this is particularly exacerbated in industrial settings.
å‰è¿°ã—ãŸã‚ˆã†ã«ã€**æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¯å¤§ããªçŠ¶æ…‹ç©ºé–“(ãƒ¦ãƒ¼ã‚¶ã®stateãŒå¤‰ã‚ã‚Šå¾—ã‚‹ã£ã¦è©±?)ã¨è¡Œå‹•ç©ºé–“(=ãƒ¦ãƒ¼ã‚¶ã®é¸æŠè‚¢=ã‚¢ã‚¤ãƒ†ãƒ ãŒå¤šã„ã£ã¦è©±?)**ã‚’æ‰±ã†ãŒã€ã“ã‚Œã¯ç”£æ¥­ç’°å¢ƒã«ãŠã„ã¦ç‰¹ã«æ‚ªåŒ–ã—ã¦ã„ã‚‹.
The set of items available to recommend is non-stationary and new items are brought into the system constantly, resulting in an ever-growing action space with new items having even sparser feedback.
æ¨è–¦å¯èƒ½ãªã‚¢ã‚¤ãƒ†ãƒ ã®ã‚»ãƒƒãƒˆã¯éå®šå¸¸(non-stationary)ã§ã‚ã‚Šã€æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ãŒå¸¸ã«ã‚·ã‚¹ãƒ†ãƒ ã«æŒã¡è¾¼ã¾ã‚Œã‚‹ãŸã‚ã€**æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ã®feedbackãŒã•ã‚‰ã«ç–ã«ãªã‚Šã€action space ãŒå¸¸ã«å¤§ãããªã‚‹**(=æ–°ã‚¢ã‚¤ãƒ†ãƒ ãŒè¿½åŠ ã•ã‚ŒãŸäº‹ã§??)(ã“ã‚Œã¯action spaceã®æ‹¡å¤§ã®è©±!).
Further, user preferences over these items are shifting all the time, resulting in continuously-evolving user states.
ã•ã‚‰ã«ã€ã“ã‚Œã‚‰ã®ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å—œå¥½ã¯å¸¸ã«å¤‰åŒ–ã—ã¦ãŠã‚Šã€çµæœã¨ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®çŠ¶æ…‹ã¯å¸¸ã«å¤‰åŒ–ã—ã¦ã„ã‚‹. (ã“ã‚Œã¯ state spaceãŒlargeã£ã¦è©±!)
Being able to reason through these large number of actions in such a complex environment poses unique challenges in applying existing RL algorithms.
ã“ã®ã‚ˆã†ãªè¤‡é›‘ãªç’°å¢ƒã«ãŠã„ã¦ã€å¤šæ•°ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¨è«–ã™ã‚‹ã“ã¨ã¯ã€æ—¢å­˜ã®RLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é©ç”¨ã™ã‚‹ä¸Šã§ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªèª²é¡Œã‚’ã‚‚ãŸã‚‰ã™.
Here we share our experience adapting the REINFORCE algorithm [48] to a neural candidate generator (a top-ğ¾ recommender system) with extremely large action and state spaces.
ã“ã“ã§ã¯ã€REINFORCEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ [48]ã‚’ã€**éå¸¸ã«å¤§ããªaction space ã¨ state space ã‚’æŒã¤neural candidate generator(top-Kæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ )ã«é©å¿œã•ã›ãŸçµŒé¨“**ã‚’ç´¹ä»‹ã™ã‚‹.

In addition to the massive action and state spaces, RL for recommendation is distinct in its limited availability of data.
ã¾ãŸã€æ¨è–¦ã®ãŸã‚ã® RL ã¯ã€è†¨å¤§ãªè¡Œå‹•ç©ºé–“(action space)ã¨çŠ¶æ…‹ç©ºé–“(state space)ã«åŠ ãˆã¦ã€**åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒé™ã‚‰ã‚Œã¦ã„ã‚‹ã“ã¨ãŒç‰¹å¾´**ã§ã‚ã‚‹.
Classic RL applications have overcome data inefficiencies by collecting large quantities of training data with self-play and simulation [38].
å¤å…¸çš„ãª RL ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€è‡ªå·±å†ç”Ÿã¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã£ã¦å¤§é‡ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã™ã‚‹ã“ã¨ã§ã€ãƒ‡ãƒ¼ã‚¿ã®éåŠ¹ç‡æ€§ã‚’å…‹æœã—ã¦ããŸ [38].
In contrast, the complex dynamics of the recommender system has made simulation for generating realistic recommendation data nonviable.
ã“ã‚Œã«å¯¾ã—ã€ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®è¤‡é›‘ãªãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã¯ã€ç¾å®Ÿçš„ãªæ¨è–¦ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’éå®Ÿç¾çš„ãªã‚‚ã®ã«ã—ã¦ã„ã‚‹.
As a result, we cannot easily probe for reward in previously unexplored areas of the state and action space, since observing reward requires giving a real recommendation to a real user.
ãã®çµæœã€**å ±é…¬(reward)ã‚’è¦³æ¸¬ã™ã‚‹ã«ã¯å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ã«æ¨è–¦ã™ã‚‹å¿…è¦ãŒã‚ã‚‹**ãŸã‚ã€çŠ¶æ…‹ç©ºé–“ã¨è¡Œå‹•ç©ºé–“ã®æœªè¸ã®é ˜åŸŸã«ãŠã‘ã‚‹å ±é…¬ã‚’å®¹æ˜“ã«æ¢ç´¢ã™ã‚‹ã“ã¨ãŒã§ããªã„.
Instead, the model relies mostly on data made available from the previous recommendation models (policies), most of which we cannot control or can no longer control.
ãã®ãŸã‚ï¼Œã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ï¼Œã“ã‚Œã¾ã§ã®æ¨è–¦ãƒ¢ãƒ‡ãƒ«(=æ¨è–¦ã‚¢ã‚¤ãƒ†ãƒ ã®é¸æŠæˆ¦ç•¥=policy)ã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã«ä¾å­˜ã™ã‚‹ã“ã¨ã«ãªã‚‹ãŒï¼Œãã®ã»ã¨ã‚“ã©ã¯æˆ‘ã€…ãŒã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã§ããªã„ï¼Œã‚ã‚‹ã„ã¯ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã§ããªããªã£ãŸãƒ‡ãƒ¼ã‚¿ã§ã‚ã‚‹.
To most effectively utilize logged-feedback from other policies, we take an off-policy learning approach, in which we simultaneously learn a model of the previous policies and incorporate it in correcting the data biases when training our new policy.
ãã“ã§ã€ä»–ã®æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åŠ¹æœçš„ã«åˆ©ç”¨ã™ã‚‹ãŸã‚ã«ã€éå»ã®æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã‚’åŒæ™‚ã«å­¦ç¿’ã—ã€æ–°ã—ã„æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’æ™‚ã«ãƒ‡ãƒ¼ã‚¿ã®biasã‚’è£œæ­£ã™ã‚‹**off-policyå­¦ç¿’**ã¨ã„ã†ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ã¨ã‚‹.
We also experimentally demonstrate the value in exploratory data.
ã¾ãŸã€æ¢ç´¢çš„(exploratory)ãªãƒ‡ãƒ¼ã‚¿ã«ãŠã‘ã‚‹ä¾¡å€¤ã‚’å®Ÿé¨“çš„ã«ç¤ºã—ã¦ã„ã‚‹.

Finally, most of the research in RL focuses on producing a policy that chooses a single item.
æœ€å¾Œã«ï¼Œ**RL ã®ç ”ç©¶ã®ã»ã¨ã‚“ã©ã¯ï¼Œå˜ä¸€ã®itemã‚’é¸æŠã™ã‚‹policyã®ä½œæˆã«ç„¦ç‚¹ã‚’ã‚ã¦ã¦ã„ã‚‹**ï¼
Real-world recommenders, on the other hand, typically offer the user multiple recommendations at a time [44].
ä¸€æ–¹ã€å®Ÿä¸–ç•Œã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã¯é€šå¸¸ã€ä¸€åº¦ã«è¤‡æ•°ã®æ¨è–¦ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æä¾›ã™ã‚‹ [44].
Therefore, we define a novel top-ğ¾ off-policy correction for our top-ğ¾ recommender system.
ãã“ã§ã€æˆ‘ã€…ã¯top-K æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ãŸã‚ã«ã€æ–°ã—ã„**top-K off-policy correction(è£œæ­£)**ã‚’å®šç¾©ã™ã‚‹.
We find that while the standard off-policy correction results in a policy that is optimal for top-1 recommendation, this top-ğ¾ off-policy correction leads to significant better top-ğ¾ recommendations in both simulations and live experiments.
ãã®çµæœã€æ¨™æº–çš„ãªoff-policyè£œæ­£ã¯top-1æ¨è–¦ã«æœ€é©ãªpolicyã¨ãªã‚‹ãŒã€ã“ã®top-K off-policyè£œæ­£ã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ãƒ©ã‚¤ãƒ–å®Ÿé¨“ã®ä¸¡æ–¹ã§æœ‰æ„ã«å„ªã‚ŒãŸtop-Kæ¨è–¦ã«ã¤ãªãŒã‚‹ã“ã¨ã‚’è¦‹å‡ºã—ãŸ.
Together, we offer the following contributions:
ä»¥ä¸Šã«ã‚ˆã‚Šã€æˆ‘ã€…ã¯ä»¥ä¸‹ã®è²¢çŒ®ã‚’è¡Œã†.

- REINFORCE Recommender: We scale a REINFORCE policy-gradient-based approach to learn a neural recommendation policy in a extremely large action space. REINFORCEãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã€‚ éå¸¸ã«å¤§ããªè¡Œå‹•ç©ºé–“ã«ãŠã‘ã‚‹neural recommendation policy ã‚’å­¦ç¿’ã™ã‚‹ç‚ºã«ã€REINFORCEã®policy-gradient-basedãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’scaleã•ã›ã‚‹.
- Off-Policy Candidate Generation: We apply off-policy correction to learn from logged feedback, collected from an ensemble of prior model policies. We incorporate a learned neural model of the behavior policies to correct data biases. ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å€™è£œã®ç”Ÿæˆã€‚ äº‹å‰ãƒ¢ãƒ‡ãƒ«policy ã® ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‹ã‚‰åé›†ã•ã‚ŒãŸãƒ­ã‚°feedbackã‹ã‚‰å­¦ç¿’ã™ã‚‹ off-policy è£œæ­£ã‚’é©ç”¨ã™ã‚‹. å­¦ç¿’ã—ãŸbehavior policy ã® neural model ã‚’çµ„ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ã®biasã‚’è£œæ­£ã™ã‚‹.
- Top-ğ¾ Off-Policy Correction: We offer a novel top-ğ¾ offpolicy correction to account for the fact that our recommender outputs multiple items at a time. Top-K Off-Policy Correction: ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ãŒä¸€åº¦ã«è¤‡æ•°ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨ã‚’è€ƒæ…®ã—ã€æ–°ã—ã„top-K off-policyè£œæ­£ã‚’æä¾›ã™ã‚‹.
- Benefits in Live Experiments: We demonstrate in live experiments, which was rarely done in existing RL literature, the value of these approaches to improve user long term satisfaction ãƒ©ã‚¤ãƒ–å®Ÿé¨“ã§ã®ãƒ¡ãƒªãƒƒãƒˆ. æ—¢å­˜ã®RLã®æ–‡çŒ®ã§ã¯ã»ã¨ã‚“ã©è¡Œã‚ã‚Œã¦ã„ãªã‹ã£ãŸã€**ãƒ¦ãƒ¼ã‚¶ã®é•·æœŸçš„ãªæº€è¶³åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã‚Œã‚‰ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ä¾¡å€¤**ã‚’ãƒ©ã‚¤ãƒ–å®Ÿé¨“ã§å®Ÿè¨¼ã™ã‚‹.

We find this combination of approaches valuable for increasing user enjoyment and believe it frames many of the practical challenges going forward for using RL in recommendations.
ç§ãŸã¡ã¯ã€ã“ã®ã‚ˆã†ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®çµ„ã¿åˆã‚ã›ãŒã€ãƒ¦ãƒ¼ã‚¶ã®æ¥½ã—ã¿ã‚’å¢—ã‚„ã™ãŸã‚ã«æœ‰åŠ¹ã§ã‚ã‚‹ã¨è€ƒãˆã€RLã‚’æ¨è–¦ã«åˆ©ç”¨ã™ã‚‹éš›ã®ä»Šå¾Œã®å®Ÿç”¨çš„ãªèª²é¡Œã®å¤šãã‚’è§£æ±ºã™ã‚‹ã‚‚ã®ã§ã‚ã‚‹ã¨è€ƒãˆã¦ã„ã‚‹.

# 2. Related work é–¢é€£ä½œå“

## 2.1. Reinforcement Learning: å¼·åŒ–å­¦ç¿’

Value-based approaches such as Q-learning, and policy-based ones such as policy gradients constitute classical approaches to solve RL problems [40].
Q-learningã®ã‚ˆã†ãª**Value-basedã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**ã‚„ã€ ãƒãƒªã‚·ãƒ¼å‹¾é…ã®ã‚ˆã†ãª**policy-basedã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**ã¯ã€ RLå•é¡Œã‚’è§£ããŸã‚ã®å¤å…¸çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã‚ã‚‹ [40].
A general comparison of modern RL approaches can be found in [29] with a focus on asynchronous learning which is key to scaling up to large problems.
ç¾ä»£ã®RLæ‰‹æ³•ã®ä¸€èˆ¬çš„ãªæ¯”è¼ƒã¯[29]ã«è¨˜è¼‰ã•ã‚Œã¦ãŠã‚Šï¼Œå¤§è¦æ¨¡å•é¡Œã¸ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã®éµã¨ãªã‚‹**éåŒæœŸå­¦ç¿’(asynchronous learning)**ã«ç„¦ç‚¹ãŒå½“ã¦ã‚‰ã‚Œã¦ã„ã‚‹.
Although value-based methods present many advantages such as seamless off-policy learning, they are known to be prone to instability with function approximation [41].
value-basedã®æ–¹æ³•ã¯ã€seamless(?)ãªoff-policyå­¦ç¿’ãªã©å¤šãã®åˆ©ç‚¹ã‚’æŒã¤ãŒã€é–¢æ•°è¿‘ä¼¼ã«ã‚ˆã‚Šä¸å®‰å®šã«ãªã‚Šã‚„ã™ã„ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã‚‹[41].
Often, extensive hyper-parameter tuning is required to achieve stable behavior for these approaches.
å¤šãã®å ´åˆï¼Œã“ã‚Œã‚‰ã®æ‰‹æ³•ã§å®‰å®šã—ãŸå‹•ä½œã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«ã¯ï¼Œå¤§è¦æ¨¡ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¿…è¦ã¨ãªã‚‹.
Despite the practical success of many value-based approaches such as deep Q-learning [30], policy convergence of these algorithms are not well-studied.
æ·±å±¤Qå­¦ç¿’[30]ã®ã‚ˆã†ãªå¤šãã®value-basedã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å®Ÿç”¨çš„ãªæˆåŠŸã«ã‚‚ã‹ã‹ã‚ã‚‰ãšï¼Œã“ã‚Œã‚‰ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®policyåæŸ(policy convergence)ã¯ååˆ†ã«ç ”ç©¶ã•ã‚Œã¦ã„ãªã„.
Policy-based approaches on the other hand, remain rather stable w.r.t. function approximations given a sufficiently small learning rate.
ä¸€æ–¹ã€policy-basedã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ååˆ†ã«å°ã•ã„å­¦ç¿’ç‡ãŒä¸ãˆã‚‰ã‚Œã‚‹ã¨ã€é–¢æ•°è¿‘ä¼¼ã«å¯¾ã—ã¦ã‚€ã—ã‚å®‰å®šãªçŠ¶æ…‹ã‚’ä¿ã¤.
We therefore choose to rely on a policy-gradient-based approach, in particular REINFORCE [48], and to adapt this on-policy method to provide reliable policy gradient estimates when training off-policy.
ãã“ã§æˆ‘ã€…ã¯ï¼Œpolicy-gradient-basedã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼Œç‰¹ã«REINFORCE [48]ã«ä¾å­˜ã—ï¼Œoff-policy(=policyå¤–ã£ã¦æ„å‘³?)ã®è¨“ç·´æ™‚ã«ä¿¡é ¼ã§ãã‚‹policy gradientã®æ¨å®šã‚’æä¾›ã™ã‚‹ãŸã‚ã«ï¼Œã“ã®on-policy(=policyä¸Š?)ã®æ–¹æ³•ã‚’é©å¿œã•ã›ã‚‹ã“ã¨ã‚’é¸æŠã™ã‚‹.

## 2.2. Neural Recommenders: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼

Another line of work that is closely related to ours is the growing body of literature on applying deep neural networks to recommender systems [11, 16, 37], in particular using recurrent neural networks to incorporate temporal information and historical events for recommendation [6, 17, 20, 45, 49].
ç‰¹ã«ã€recurrent neural network ã‚’ä½¿ç”¨ã—ã¦ã€æ¨è–¦ã®ãŸã‚ã®æ™‚é–“çš„æƒ…å ±ã¨å±¥æ­´çš„ã‚¤ãƒ™ãƒ³ãƒˆã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã‚‹.[6, 17, 20, 45, 49].
We employed similar network architectures to model the evolving of user states through interactions with the recommender system.
æˆ‘ã€…ã¯ã€**æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¨ã®Interactionã‚’é€šã˜ã¦ãƒ¦ãƒ¼ã‚¶ã®çŠ¶æ…‹(state)ãŒé€²åŒ–ã™ã‚‹ã“ã¨ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ãŸã‚ã«**ã€åŒæ§˜ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨ã—ãŸ.
As neural architecture design is not the main focus of our work, we refer interested readers to these prior works for more detailed discussions.
ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨­è¨ˆã¯æˆ‘ã€…ã®ç ”ç©¶ã®ä¸»è¦ãªç„¦ç‚¹ã§ã¯ãªã„ãŸã‚ã€ã‚ˆã‚Šè©³ç´°ãªè­°è«–ã«ã¤ã„ã¦ã¯ã“ã‚Œã‚‰ã®å…ˆè¡Œç ”ç©¶ã‚’å‚ç…§ã™ã‚‹ã‚ˆã†èª­è€…ã«å‹§ã‚ã‚‹.

## 2.3. Bandit Problems in recommender systems:

On-line learning methods are also popular to quickly adapt recommendation systems as new user feedback becomes available.
ã¾ãŸã€æ–°ã—ã„ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®feedbackãŒåˆ©ç”¨å¯èƒ½ã«ãªã£ãŸã¨ãã«ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚’è¿…é€Ÿã«é©å¿œã•ã›ã‚‹ãŸã‚ã«ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’æ³•ãŒæ™®åŠã—ã¦ã„ã‚‹.
Bandit algorithms such as Upper Confidence Bound (UCB) [3] trade off exploration and exploitation in an analytically tractable way that provides strong guarantees on the regret.
UCB (Upper Confidence Bound) [3]ãªã©ã®ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒ»ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€è§£æçš„ã«æ‰±ã„ã‚„ã™ã„æ–¹æ³•ã§æ¢ç´¢(exploration)ã¨æ´»ç”¨(exploitation)ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã—ã€regret(å¾Œæ‚”?)ã‚’å¼·ãä¿è¨¼ã—ã¦ã„ã‚‹.
Different algorithms such as Thomson sampling [9], have been successfully applied to news recommendations and display advertising.
**Thomson sampling[9]ã®ã‚ˆã†ãªç•°ãªã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã‚„ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤åºƒå‘Šã«ã†ã¾ãé©ç”¨ã•ã‚Œã¦ã„ã‚‹**.(ã»ã†ã»ã†...!!)
Contextual bandits offer a contextaware refinement of the basic on-line learning approaches and tailor the recommendation toward user interests [27].
Contextual bandits ã¯ã€åŸºæœ¬çš„ãªã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’**context-awareã«æ”¹è‰¯**ã—ã€ãƒ¦ãƒ¼ã‚¶ã®èˆˆå‘³ã«åˆã‚ã›ãŸæ¨è–¦ã‚’è¡Œã† [27].
Agarwal et al. [2] aimed to make contextual bandits tractable and easy to implement.
Agarwal ã‚‰ [2] ã¯ Contextual bandits ã‚’æ‰±ã„ã‚„ã™ãï¼Œç°¡å˜ã«å®Ÿè£…ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ãŸ.
Hybrid methods that rely on matrix factorization and bandits have also been developed to solve cold-start problems in recommender systems [28].
ã¾ãŸã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆå•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€**è¡Œåˆ—åˆ†è§£ã¨ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«ã‚ˆã‚‹ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•**ã‚‚é–‹ç™ºã•ã‚Œã¦ã„ã‚‹[28].

## 2.4. Propensity Scoring and Reinforcement Learning in Recommender Systems: Propensity Scoring and Reinforcement Learning in Recommender Systems:

The problem of learning off-policy [31, 33, 34] is pervasive in RL and affects policy gradient generally.
off-policyå­¦ç¿’ã®å•é¡Œ[31, 33, 34]ã¯RLã«åºƒãå­˜åœ¨ã—ï¼Œä¸€èˆ¬ã«policy gradientã«å½±éŸ¿ã™ã‚‹.
As a policy evolves so does the distribution under which gradient expectations are computed.
policy ãŒé€²åŒ–ã™ã‚‹ã«ã¤ã‚Œã¦ï¼Œ"**gradientã®æœŸå¾…å€¤ãŒè¨ˆç®—ã•ã‚Œã‚‹åˆ†å¸ƒ**"(ãªã«ãã‚Œ...?)ã‚‚é€²åŒ–ã™ã‚‹.
Standard approaches in robotics [1, 36] circumvent this issue by constraining policy updates so that they do not change the policy too substantially before new data is collected under an updated policy, which in return provides monotonic improvement guarantees of the RL objective.
ãƒ­ãƒœãƒƒãƒˆå·¥å­¦ã«ãŠã‘ã‚‹æ¨™æº–çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ[1, 36]ã¯ï¼Œæ›´æ–°ã•ã‚ŒãŸpolicyã®ä¸‹ã§æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒåé›†ã•ã‚Œã‚‹å‰ã«ï¼Œpolicyã‚’ã‚ã¾ã‚Šå¤§ããå¤‰æ›´ã—ãªã„ã‚ˆã†ã«policyæ›´æ–°ã‚’åˆ¶ç´„ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã“ã®å•é¡Œã‚’å›é¿ã—ï¼Œãã®ä»£ã‚ã‚Šã«RLç›®çš„ã®å˜èª¿æ”¹å–„ä¿è¨¼(? monotonic improvement guarantees)ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã‚ã‚‹.
Such proximal methods are unfortunately not applicable in the recommendations setting where item catalogues and user behaviors change rapidly, and therefore substantial policy changes are required.
ã“ã®ã‚ˆã†ãªè¿‘æ¥æ‰‹æ³•ã¯ï¼Œæ®‹å¿µãªãŒã‚‰ï¼Œ**ã‚¢ã‚¤ãƒ†ãƒ ã‚«ã‚¿ãƒ­ã‚°ã‚„ãƒ¦ãƒ¼ã‚¶ã®è¡Œå‹•ãŒæ€¥é€Ÿã«å¤‰åŒ–ã™ã‚‹ãŸã‚ï¼Œå¤§å¹…ãªpolicyå¤‰æ›´ãŒå¿…è¦ã¨ãªã‚‹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®šã«ã¯é©ç”¨ã§ããªã„**.
Meanwhile feedback is slow to collect at scale w.r.t. the large state and action space.
ã¾ãŸã€state space ã‚„ action space ãŒå¤§ãã„ãŸã‚ã€feedbackã¯ãã®åé›†ã«æ™‚é–“ãŒã‹ã‹ã‚‹.
As a matter of fact, offline evaluation of a given policy is already a challenge in the recommender system setting.
å®Ÿéš›ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€**ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§policyã‚’è©•ä¾¡ã™ã‚‹ã“ã¨ã¯æ—¢ã«èª²é¡Œã¨ãªã£ã¦ã„ã‚‹**.(ã“ã‚Œã¯æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ç²¾åº¦çš„ãªã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡ã®èª²é¡Œã¨é–¢é€£ã™ã‚‹ã‚“ã ã‚ã†ã‹...?)
Multiple off-policy estimators leveraging inverse-propensity scores, capped inverse-propensity scores and various variance control measures have been developed [13, 42, 43, 47].
inverse-propensity scoresï¼Œcapped inverse-propensity scoresï¼Œæ§˜ã€…ãªåˆ†æ•£åˆ¶å¾¡æ‰‹æ®µã‚’æ´»ç”¨ã—ãŸè¤‡æ•°ã®**off-policyæ¨å®šå™¨(estimators)**ãŒé–‹ç™ºã•ã‚Œã¦ããŸ [13, 42, 43, 47].
Off-policy evaluation corrects for a similar data skew as off-policy RL and similar methods are applied on both problems.
off-policy evaluationã¯off-policy RLã¨åŒæ§˜ã«ãƒ‡ãƒ¼ã‚¿ã®æ­ªã¿ã‚’è£œæ­£ã—ã€åŒæ§˜ã®æ‰‹æ³•ãŒä¸¡å•é¡Œã«é©ç”¨ã•ã‚Œã‚‹.
Inverse propensity scoring has also been employed to improve a serving policy at scale in [39].
ã¾ãŸï¼ŒInverse propensity scoring(é€†å‚¾å‘ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°)ã¯[39]ã«ãŠã„ã¦ï¼Œè¦æ¨¡ã«å¿œã˜ãŸæä¾›policyã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«æ¡ç”¨ã•ã‚Œã¦ã„ã‚‹.
Joachims et al. [21] learns a model of logged feedback for an unbiased ranking model; we take a similar perspective but use a DNN to model the logged behavior policy required for the off-policy learning.
Joachimsã‚‰[21]ã¯unbiasedãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã«ã€è¨˜éŒ²ã•ã‚ŒãŸfeedbackã®ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹. æˆ‘ã€…ã¯åŒæ§˜ã®è¦³ç‚¹ã‹ã‚‰ã€**off-policyå­¦ç¿’ã«å¿…è¦ãªã€è¨˜éŒ²ã•ã‚ŒãŸbehavior policy ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ãŸã‚ã«DNNã‚’ä½¿ç”¨ã™ã‚‹**.
More recently an off-policy approach has been adapted to the more complex problem of slate recommendation [44] where a pseudo-inverse estimator assuming a structural prior on the slate and reward is applied in conjunction with inverse propensity scoring.
æœ€è¿‘ã€off-policyãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã‚ˆã‚Šè¤‡é›‘ãªslateæ¨è–¦(??)ã®å•é¡Œã«é©å¿œã•ã‚Œ [44]ã€slate(å¼·åŒ–å­¦ç¿’ã®domain languageã£ã½ã„.rewardã¨åŒæ§˜)ã¨å ±é…¬(reward)ã®æ§‹é€ çš„äº‹å‰åˆ†å¸ƒ(structural prior)ã‚’ä»®å®šã—ãŸç–‘ä¼¼é€†æ¨å®šæ³•(pseudo-inverse estimator)ãŒé€†å‚¾å‘ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°(inverse propensity scoring)ã¨ä¸€ç·’ã«é©ç”¨ã•ã‚Œã¦ã„ã‚‹.

# 3. Reinforce Recommender

We begin with describing the setup of our recommender system, and our approach to RL-based recommendation.
ã¾ãšã€æˆ‘ã€…ã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹æˆã¨ã€RLã«åŸºã¥ãæ¨è–¦ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã¤ã„ã¦èª¬æ˜ã™ã‚‹.

For each user, we consider a sequence of user historical interactions with the system, recording the actions taken by the recommender, i.e., videos recommended, as well as user feedback, such as clicks and watch time.
å„ãƒ¦ãƒ¼ã‚¶ã«ã¤ã„ã¦ã€ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ãŒè¡Œã£ãŸactionã€ã™ãªã‚ã¡ã€æ¨è–¦ã™ã‚‹ãƒ“ãƒ‡ã‚ª(éå»ã«æ¨è–¦ã—ãŸã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆ)ã€ãŠã‚ˆã³ã€ã‚¯ãƒªãƒƒã‚¯ã‚„è¦–è´æ™‚é–“ãªã©ã®user feedbackã‚’è¨˜éŒ²ã—ãŸã€**ã‚·ã‚¹ãƒ†ãƒ (æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ?)ã¨ã®ãƒ¦ãƒ¼ã‚¶ã®éå»ã®Interaction(=æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ãŒã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã‚’ã‚ªã‚¹ã‚¹ãƒ¡ã—ã¦ã€ãã‚Œã«å¯¾ã—ã¦ãƒ¦ãƒ¼ã‚¶ãŒã©ã®ã‚ˆã†ãªreactionã‚’ç¤ºã—ãŸã‹!)ã® sequence(é †åº)**ã‚’è€ƒæ…®ã™ã‚‹.
Given such a sequence, we predict the next action to take, i.e., videos to recommend, so that user satisfaction metrics, e.g., indicated by clicks or watch time, improve.
ã“ã®ã‚ˆã†ãªä¸€é€£ã®å‹•ä½œãŒä¸ãˆã‚‰ã‚Œã‚‹ã¨ã€æ¬¡ã«ã¨ã‚‹ã¹ãå‹•ä½œã€ã™ãªã‚ã¡ã€æ¨å¥¨ã™ã‚‹å‹•ç”»ã‚’äºˆæ¸¬ã—ã€ã‚¯ãƒªãƒƒã‚¯æ•°ã‚„è¦–è´æ™‚é–“ãªã©ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦ã®æŒ‡æ¨™ã‚’å‘ä¸Šã•ã›ã‚‹.

We translate this setup into a Markov Decision Process (MDP) (S, A, P, ğ‘…, ğœŒ0,ğ›¾) where
ã“ã®è¨­å®šã‚’ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹(Markov Decision Process) $(S, A, P, R, \rho_0,\gamma)$ã«å¤‰æ›ã™ã‚‹.

- S: a continuous state space describing the user states; S: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®çŠ¶æ…‹ã‚’è¨˜è¿°ã™ã‚‹é€£ç¶šçš„ãªçŠ¶æ…‹ç©ºé–“ã€‚
- A: a discrete action space, containing items available for recommendation; A: æ¨è–¦å¯èƒ½ãªã‚¢ã‚¤ãƒ†ãƒ ã‚’å«ã‚€ã€é›¢æ•£çš„ãªè¡Œå‹•ç©ºé–“ã€‚
- $P: S \times A \times S \rightarrow \mathbb{R}$ is the state transition probability; P : S Ã— A Ã— S â†’ R ã¯çŠ¶æ…‹é·ç§»ç¢ºç‡ã§ã‚ã‚‹ã€‚
- $R: S \times A \rightarrow \mathbb{R}$ is the reward function, where ğ‘Ÿ(ğ‘ , ğ‘) is the immediate reward obtained by performing action ğ‘ at user state ğ‘ ; ğ‘… : S Ã— A â†’ R ã¯å ±é…¬é–¢æ•°ã§ã€$r(s, a)$ ã¯ãƒ¦ãƒ¼ã‚¶ã®çŠ¶æ…‹ğ‘ ã§è¡Œå‹•ğ‘ã‚’è¡Œã†ã“ã¨ã«ã‚ˆã£ã¦å¾—ã‚‰ã‚Œã‚‹å³æ™‚å ±é…¬ã§ã‚ã‚‹.

- ğœŒ0 is the initial state distribution; áœŒã¯åˆæœŸçŠ¶æ…‹åˆ†å¸ƒã§ã‚ã‚‹ã€‚

- ğ›¾ is the discount factor for future rewards. Ç¾ã¯ã€å°†æ¥ã®å ±é…¬ã«å¯¾ã™ã‚‹å‰²å¼•ä¿‚æ•°ã§ã‚ã‚‹ã€‚

We seek a policy ğœ‹ (ğ‘|ğ‘ ) that casts a distribution over the item to recommend ğ‘ âˆˆ A conditional to the user state ğ‘  âˆˆ S, so as to maximize the expected cumulative reward obtained by the recommender system,
æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ãŒå¾—ã‚‹æœŸå¾…ç´¯ç©å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ã«ã€ãƒ¦ãƒ¼ã‚¶ã®çŠ¶æ…‹áµ„âˆˆSã‚’æ¡ä»¶ã¨ã—ã¦ã€æ¨è–¦ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ áµâˆˆAã«å¯¾ã—ã¦åˆ†å¸ƒã‚’æŠ•ã’ã‹ã‘ã‚‹æ–¹é‡ğ‘ ã‚’æ¨¡ç´¢ã—ã¾ã™,

$$
\max_{\pi} J(\pi) = E_{\tau \sim \pi}[R(\tau)],
\\
\text{where } R(\tau) = \sum_{t=0}^{|\tau|} r(s_t, a_t)
$$

Here the expectation is taken over the trajectories $\tau = (s_0, ğ‘_0, s_1, \cdots)$ obtained by acting according to the policy: $s_0 \sim \rho_0, a_t \sim \pi(Â·|s_t), s_{t+1} \sim P(Â·|s_t, a_t)$. In other words,
ã“ã“ã§ã€ä¸Šå¼ã®æœŸå¾…å€¤ã¯ã€policy(& initial state distribution & state transition function): $s_0 \sim \rho_0, a_t \sim \pi(Â·|s_t), s_{t+1} \sim P(Â·|s_t, a_t)$ ã«å¾“ã£ã¦actionã—ãŸäº‹ã§å¾—ã‚‰ã‚Œã‚‹ **trajectories(è»Œé“ = stateã¨actionã®çµæœ)**$\tau = (s_0, a_0, s_1, \cdots)$ ã‚’å…¥åŠ›ã¨ã—ã¦è¨ˆç®—ã•ã‚Œã‚‹.

ã‚ˆã£ã¦ä¸Šå¼ã®$J(\pi)$ã‚’æ›¸ãæ›ãˆã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚‹.

$$
J(\pi) = E_{s_0 \sim \rho_0, a_t \sin \pi(\cdot|s_t), s_{t+1} \sim P(\cdot|s_t, a_t)}[\sum_{t=0}^{|\tau|} r(s_t, a_t)]
\\
= E_{s_t \sim d_t^{\pi}(\cdot), a_t \sin \pi(\cdot|s_t)}[\sum_{t'= t}^{|\tau|} r(s_{t'}, a_{t'})]
\tag{1}
$$

ã“ã“ã§ã€

- å¼(1)ã®ä¸€æ®µç›®ã«é–¢ã—ã¦.
  - åˆæœŸçŠ¶æ…‹ $s_0$ ã¯ã€ initial state distributionã«åŸºã¥ãç”Ÿæˆã•ã‚Œã‚‹.
  - æ™‚é–“tã«ãŠã‘ã‚‹action $a_t$ ã¯ã€æ™‚é–“tã«ãŠã‘ã‚‹state $s_t$ã§æ¡ä»¶ã¥ã‘ãŸ(=å…¥åŠ›ã¨ã—ãŸ)policyé–¢æ•°ã«åŸºã¥ãç”Ÿæˆã•ã‚Œã‚‹.
  - æ™‚é–“t+1ã«ãŠã‘ã‚‹ state $s_{t+1}$ã¯ã€$a_t$ã¨$s_t$ã§æ¡ä»¶ã¥ã‘ãŸ(=å…¥åŠ›ã¨ã—ãŸ)state transaction probabilityé–¢æ•°$P$ã«åŸºã¥ã„ã¦ç”Ÿæˆã•ã‚Œã‚‹.
- å¼(1)ã®äºŒæ®µç›®ã«é–¢ã—ã¦.
  - $d_t^{\pi}(\cdot)$ã¯ã€policy $\pi$ ã®ä¸‹ã§æ™‚é–“tã«ãŠã‘ã‚‹ (discounted) **state visitation frequency(çŠ¶æ…‹è¨ªå•é »åº¦)**ã‚’æ„å‘³ã™ã‚‹.
    - state visitation frequency(çŠ¶æ…‹è¨ªå•é »åº¦): agentãŒå­¦ç¿’ä¸­ã«ã©ã®stateã«ã©ã®ç¨‹åº¦ã®é »åº¦ã§visitã—ãŸã‹ã‚’ç¤ºã™æŒ‡æ¨™.
    - reward functionã®æ¨å®šã«å½¹ç«‹ã¤ã‚‰ã—ã„.
    -

Here ğ‘‘ ğœ‹ ğ‘¡ (Â·) denotes the (discounted) state visitation frequency at time ğ‘¡ under the policy ğœ‹. Different families of methods are available to solve such an RL problems: Q-learning [38], Policy Gradient [26, 36, 48] and black box optimization [15]. Here we focus on a policy-gradient-based approach, i.e., REINFORCE [48].
ã“ã®ã‚ˆã†ãª RL å•é¡Œã‚’è§£ããŸã‚ã«ã€æ§˜ã€…ãªæ‰‹æ³•ã®family(?)ãŒåˆ©ç”¨å¯èƒ½ã§ã‚ã‚‹.(ãªã‚‹ã»ã©...**æœŸå¾…ç´¯ç©å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ãªpolicyé–¢æ•°ã‚’æ¢ç´¢ã™ã‚‹äº‹**ãŒRLã®åŸºæœ¬çš„ãªã‚¿ã‚¹ã‚¯ãªã®ã‹ãª...!)
Q-learning [38], Policy Gradient [26, 36, 48], Black Box Optimization [15] ãªã©.
ã“ã“ã§ã¯ã€policy-gradientã«åŸºã¥ãã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€ã™ãªã‚ã¡ã€REINFORCE [48]ã«æ³¨ç›®ã™ã‚‹.

We assume a function form of the policy ğœ‹ğœƒ , parametrised by ğœƒ âˆˆ R ğ‘‘ .
policy $\pi_{\theta}$ ã¯ $\theta \in \mathbb{R}^{d}$ ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚ŒãŸé–¢æ•°å½¢å¼ã‚’ä»®å®šã™ã‚‹. (dã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°. $\theta$ ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ™ã‚¯ãƒˆãƒ«.)
The gradient of the expected cumulative reward with respect to the policy parameters can be derived analytically thanks to the â€œlog-trickâ€, yielding the following REINFORCE gradient
**policyé–¢æ•°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æœŸå¾…ç´¯ç©å ±é…¬(expected cumulative reward)ã®å‹¾é…**ã¯ï¼Œ"log-trick(??)"ã«ã‚ˆã£ã¦è§£æçš„(ãªã‚‹ã»ã©!ã˜ã‚ƒã‚æ™®é€šã«è§£ã„ãŸã‚‰å¾—ã‚‰ã‚Œã‚‹?)ã«å°ã‹ã‚Œï¼Œä»¥ä¸‹ã®REINFORCE gradientã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹.

$$
\nabla_{\theta} J(\pi_{\theta}) =
E_{s_t \sim d_t^{\pi}(\cdot), a_t \sin \pi(\cdot|s_t)}
[(\sum_{t'= t}^{|\tau|} r(s_{t'}, a_{t'})) \nabla_{\theta} \log{\pi_{\theta}(a_t|s_t)}]
\\
= \sum_{s_t \sim d_t^{\pi}(\cdot), a_t \sin \pi(\cdot|s_t)}
R_{t}(s_t, a_t) \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)
\tag{2}
$$

Here $R_{t}(s_t, a_t)= \sum_{t'=t}^{|\tau|} r(s_{t'}, a_{t'})$ is the discounted future reward for action at time ğ‘¡. The discounting factor ğ›¾ is applied to reduce variance in the gradient estimate. In on-line RL, where the policy gradient is computed on trajectories generated by the policy under consideration, the monte carlo estimate of the policy gradient is unbiased.
ã“ã“ã§ã€ $R_{t}(s_t, a_t)= \sum_{t'=t}^{|\tau|} r(s_{t'}, a_{t'})$ ã¯æ™‚åˆ»tã«ãŠã‘ã‚‹actionã«å¯¾ã™ã‚‹å‰²å¼•å¾Œã®å°†æ¥å ±é…¬(æ™‚åˆ»tä»¥é™ã«ç²å¾—ã™ã‚‹å ±é…¬ã®åˆè¨ˆã®äºˆæ¸¬å€¤??discountã£ã¦ä½•?)ã§ã‚ã‚‹.
å‰²å¼•ä¿‚æ•°$\gamma$ã¯ã€å‹¾é…æ¨å®šå€¤ã®åˆ†æ•£ã‚’æ¸›ã‚‰ã™ãŸã‚ã«é©ç”¨ã•ã‚Œã‚‹.(ã»ã†...)
ã‚ªãƒ³ãƒ©ã‚¤ãƒ³RLã§ã¯ã€policy-gradientã¯æ¤œè¨ä¸­ã®policyã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸè»Œé“ã§è¨ˆç®—ã•ã‚Œã‚‹ãŸã‚ã€policy-gradientã®ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ¨å®šå€¤ã¯ä¸åã§ã‚ã‚‹.(ä¸åæ¨å®šé‡ã«ãªã‚‹ç†ç”±ã¯ã‚ˆãã‚ã‹ã‚‰ãªã„...)

# 4. Off-Policy Correction(è£œæ­£)

Unlike classical reinforcement learning, our learner does not have real-time interactive control of the recommender due to learning and infrastructure constraints.
å¤å…¸çš„ãªå¼·åŒ–å­¦ç¿’ã¨ã¯ç•°ãªã‚Šã€æˆ‘ã€…ã®å­¦ç¿’å™¨ã¯ã€å­¦ç¿’ã¨ã‚¤ãƒ³ãƒ•ãƒ©ã®åˆ¶ç´„ã‹ã‚‰ã€ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã«åˆ¶å¾¡ã™ã‚‹ã“ã¨ãŒã§ããªã„.
In other words, we cannot perform online updates to the policy and generate trajectories according to the updated policy immediately.
ã¤ã¾ã‚Šã€å­¦ç¿’è€…ã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§policyã‚’æ›´æ–°ã—ã€æ›´æ–°ã•ã‚ŒãŸpolicyã«å¾“ã£ãŸè»Œé“ã‚’å³åº§ã«ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ããªã„.
Instead we receive logged feedback of actions chosen by a historical policy (or a mixture of policies), which could have a different distribution over the action space than the policy we are updating.
ãã®ä»£ã‚ã‚Šã«ã€æˆ‘ã€…ã¯ã€æ›´æ–°ä¸­ã®policyã¨ã¯ç•°ãªã‚‹action spaceä¸Šã®åˆ†å¸ƒã‚’æŒã¡ã†ã‚‹éå»ã®policy(=ç¾åœ¨é‹ç”¨ä¸­ã®æ¨è–¦ãƒ¢ãƒ‡ãƒ«?)(ã¾ãŸã¯policyã®æ··åˆç‰©)ã«ã‚ˆã‚Šé¸æŠã•ã‚ŒãŸactionã®ãƒ­ã‚°feedbackã‚’å—ã‘å–ã‚‹.

We focus on addressing the data biases that arise when applying policy gradient methods under this setting.
æˆ‘ã€…ã¯ï¼Œã“ã®è¨­å®šã®ã‚‚ã¨ã§policy-gradientæ³•ã‚’é©ç”¨ã™ã‚‹éš›ã«ç”Ÿã˜ã‚‹ãƒ‡ãƒ¼ã‚¿ã®åã‚Šã‚’è§£æ±ºã™ã‚‹ã“ã¨ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹.
In particular, the fact that we collect data with a periodicity of several hours and compute many policy parameter updates before deploying a new version of the policy in production implies that the set of trajectories we employ to estimate the policy gradient is generated by a different policy.
ç‰¹ã«ï¼Œæ•°æ™‚é–“ã®å‘¨æœŸã§ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ï¼Œ**æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®policyã‚’æœ¬ç•ªã«å°å…¥ã™ã‚‹å‰ã«å¤šãã®policyé–¢æ•°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°ã‚’è¨ˆç®—ã™ã‚‹**ã¨ã„ã†äº‹å®Ÿã¯ï¼Œ**æˆ‘ã€…ãŒpolicy-gradientã‚’æ¨å®šã™ã‚‹ãŸã‚ã«ç”¨ã„ã‚‹è»Œé“ã®ã‚»ãƒƒãƒˆ(=dataset)ãŒã€ç•°ãªã‚‹policyã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹**ï¼
Moreover, we learn from batched feedback collected by other recommenders as well, which follow drastically different policies.
ã•ã‚‰ã«ï¼Œä»–ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ãŒåé›†ã—ãŸfeedbackã‚‚ä¸€æ‹¬ã—ã¦å­¦ç¿’ã™ã‚‹ãŒï¼Œãã‚Œã‚‰ã¯å…¨ãç•°ãªã‚‹policyã«å¾“ã£ã¦ã„ã‚‹.
A naive policy gradient estimator is no longer unbiased as the gradient in Equation (2) requires sampling trajectories from the updated policy ğœ‹ğœƒ while the trajectories we collected were drawn from a combination of historical policies ğ›½.
å¼(2)ã®gradientã¯æ›´æ–°ã•ã‚ŒãŸpolicy $\pi_{\theta}$ ã‹ã‚‰è»Œé“ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ä¸€æ–¹ï¼Œæˆ‘ã€…ãŒåé›†ã—ãŸè»Œé“ã¯éå»ã®policy $\beta$ ã®çµ„ã¿åˆã‚ã›ã‹ã‚‰å¼•ãå‡ºã•ã‚Œã¦ã„ã‚‹ãŸã‚ï¼Œå˜ç´”ãªpolicy-gradient ã®æ¨å®šå™¨ã¯ã‚‚ã¯ã‚„ä¸åã§ã¯ãªã„.
(ãªã‚‹ã»ã©...!å¼·åŒ–å­¦ç¿’ã—ãŸã„policyãã®ã‚‚ã®ã‚’æœ¬ç•ªé‹ç”¨ã—ã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹ã§ã¯ã€å—ã‘å–ã£ãŸfeedback=è»Œé“ã‚’ç”¨ã„ã¦ã»ã¼æ­£ã—ã„å‹¾é…(ä¸åæ¨å®šé‡=å¹³å‡çš„ã«çœŸã®å‹¾é…ã¨ç­‰ã—ã„ã¿ãŸã„ãªã‚¤ãƒ¡ãƒ¼ã‚¸)ã‚’è¨ˆç®—å¯èƒ½ã§ã€çµæœã¨ã—ã¦æœŸå¾…ç´¯ç©å ±é…¬ã‚’æœ€å¤§ã«ã™ã‚‹ã‚ˆã†ãªpolicyã«æ›´æ–°=å¼·åŒ–ã—ã¦ã„ã‘ã‚‹ã¯ãš.)
(ã—ã‹ã—ã€å¼·åŒ–å­¦ç¿’ã—ãŸã„policyã¨ã¯åˆ¥ã®policyã‚’æœ¬ç•ªé‹ç”¨ã—ã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹ã§ã¯ã€å—ã‘å–ã£ãŸfeedback=è»Œé“ã‚’å¼(2)ã«å½“ã¦ã¯ã‚ã¦è¨ˆç®—ã—ã¦ã‚‚è¦‹å½“é•ã„ãªå‹¾é…ã«ãªã£ã¦ã—ã¾ã†. çµæœã¨ã—ã¦å¤‰ãªæ–¹å‘ã«policyã‚’æ›´æ–°ã—ã¦ã„ã£ã¦ã—ã¾ã†...!)

We address the distribution mismatch with importance weighting [31, 33, 34].
åˆ†å¸ƒã®ä¸ä¸€è‡´ã«ã¯ã€**é‡è¦åº¦é‡ã¿ä»˜ã‘**[31, 33, 34]ã‚’ç”¨ã„ã¦å¯¾å‡¦ã™ã‚‹.
Consider a trajectory $\tau = (s0, a0, s1, \cdots)$ sampled according to a behavior policy $\beta$, the off-policy-corrected gradient estimator is then:
behavior policy $\beta$ ã«å¾“ã£ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸè»Œé“ $\tau = (s0, a0, s1, \cdots)$ ã‚’è€ƒæ…®ã™ã‚‹ã¨ã€off-policy-corrected gradient estimator(off-policyè£œæ­£å‹¾é…æ¨å®šå™¨)ã¯ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚‹:

$$
\nabla_{\theta} J(\pi_{\theta})
= \sum_{s_t \sim d_t^{\beta}(\cdot), a_t \sin \beta(\cdot|s_t)}
w(s_t, a_t) R_{t}(s_t, a_t) \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)
\tag{3}
$$

where below is the importance weight.
ã“ã“ã§ã€ä»¥ä¸‹ã¯é‡è¦åº¦é‡ã¿ã§ã‚ã‚‹.

$$
w(s_t, a_t)
= \frac{d_t^{\pi}(s_t)}{d_t^{\beta}(s_t)}
\times \frac{\pi_{\theta}(a_t|s_t)}{\beta(a_t|s_t)}
\times \Pi_{t'=t+1}^{|\tau|} \frac{\pi_{\theta}(a_{t'}|s_{t'})}{\beta(a_{t'}|s_{t'})}
\tag{3.5}
$$

This correction produces an unbiased estimator whenever the trajectories are collected with actions sampled according to ğ›½.
ã“ã®è£œæ­£ã«ã‚ˆã‚Šã€åˆ¥ã®policy$\beta$ã«å¾“ã£ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸactionã§trajectories(è»Œé“)ãŒåé›†ã•ã‚ŒãŸå ´åˆã¯ã€å¸¸ã«ä¸åã®æ¨å®šå€¤ãŒå¾—ã‚‰ã‚Œã‚‹(ã»ã†,
è¨€ã†ã­...!).
However, the variance of the estimator can be huge when the difference in ğœ‹ğœƒ and the behavior policy ğ›½ results in very low or high values of the importance weights.
ã—ã‹ã—ã€æ›´æ–°å¯¾è±¡ã®policy $\pi$ ã¨ åˆ¥ã®policy $\beta$ ã®é•ã„ã«ã‚ˆã‚Šé‡è¦åº¦é‡ã¿ãŒéå¸¸ã«å°ã•ã„ã‹å¤§ãã„å ´åˆã«ã¯ã€**æ¨å®šå€¤ã®åˆ†æ•£ãŒå¤§ãããªã‚‹**å¯èƒ½æ€§ãŒã‚ã‚‹.(ãã‚Œã§ã‚‚ä¸åæ€§ã¯å¤±ã‚ã‚Œãªã„ã®ã‹...)

To reduce the variance of each gradient term, we take the firstorder approximation and ignore the state visitation differences under the two policies as the importance weights of future trajectories, which yields a slightly biased estimator of the policy gradient with lower variance:
å„å‹¾é…é …ã®åˆ†æ•£ã‚’å°ã•ãã™ã‚‹ãŸã‚ã«ï¼Œ1æ¬¡è¿‘ä¼¼(?)ã‚’è¡Œã„ï¼Œå°†æ¥ã®è»Œé“ã®é‡è¦åº¦é‡ã¿ã¨ã—ã¦2ã¤ã®æ”¿ç­–ä¸‹ã®çŠ¶æ…‹è¨ªå•å·®åˆ†ã‚’ç„¡è¦–ã™ã‚‹ã“ã¨ã§ï¼Œåˆ†æ•£ã‚’å°ã•ãã—ãŸpolicy-gradientã®ã‚„ã‚„åã£ãŸ(=ä¸åæ€§ã¯å¤±ã‚ã‚ŒãŸ)æ¨å®šå€¤ã‚’å¾—ã‚‹ã“ã¨ãŒã§ããŸï¼

$$
\nabla_{\theta} J(\pi_{\theta})
\approx \sum_{s_t \sim d_t^{\beta}(\cdot), a_t \sin \beta(\cdot|s_t)}
\frac{\pi_{\theta}(a_t|s_t)}{\beta(a_t|s_t)} % é‡ã¿ãŒraughã«ãªã£ãŸ?
R_{t}(s_t, a_t) \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)
\tag{4}
$$

Achiam et al. [1] prove that the impact of this first-order approximation on the total reward of the learned policy is bounded in magnitude by ğ‘‚ ğ¸ğ‘ âˆ¼ğ‘‘ ğ›½ [ğ·ğ‘‡ğ‘‰ (ğœ‹
ğ›½) [ğ‘ ]]) where ğ·ğ‘‡ğ‘‰ is the total variation between ğœ‹ (Â·
Achiamã‚‰. [1]ã¯ã€ã“ã®ä¸€æ¬¡è¿‘ä¼¼ã®å½±éŸ¿ã‚’è¨¼æ˜ã™ã‚‹. å­¦ç¿’ã—ãŸpolicyã®ç·å ±é…¬ã«é–¢ã™ã‚‹æ¬¡æ•°è¿‘ä¼¼ã¯ã€ğ‘‚ğ¸ğ‘  ğ‘‘ [ğ·ğ‘‡ (ğ‘ )] ã«ã‚ˆã£ã¦å¤§ãã•ãŒæ‹˜æŸã•ã‚Œã‚‹. ã“ã“ã§ã€ğ·ğ‘‡ğ‘‰ã¯ğ‘  (-|ğ‘ )ã¨ğ›½ğ‘‘ã®ä¸‹ã§ã®å‰²å¼•æœªæ¥çŠ¶æ…‹åˆ†å¸ƒã®åˆè¨ˆå¤‰å‹•ã§ã‚ã‚‹. ã“ã®æ¨å®šå™¨ã¯ã€æ­£ç¢ºãªoff-policyè£œæ­£ã®åˆ†æ•£ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã—ã¤ã¤ã€éè£œæ­£ã®policy-gradientã®å¤§ããªãƒã‚¤ã‚¢ã‚¹ã‚’è£œæ­£ã—ã€on-policyå­¦ç¿’ã«ã‚ˆã‚Šé©ã—ãŸã‚‚ã®ã§ã‚ã‚‹.(ã„ã„ã¨ã“å–ã‚Šçš„ãªestimatorãªã®ã‹...)

## 4.1. Parametrising the policy $\pi_{\theta}$

We model our belief on the user state at each time ğ‘¡, which capture both evolving user interests using a ğ‘›-dimensional vector, that is, sğ‘¡ âˆˆ R ğ‘› .
å„æ™‚é–“ t ã«ãŠã‘ã‚‹ãƒ¦ãƒ¼ã‚¶ã®stateã«é–¢ã™ã‚‹policyã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã€ næ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã€ã™ãªã‚ã¡ $s_t \in \mathbb{R}^n$ã‚’ç”¨ã„ã¦é€²åŒ–ã™ã‚‹ä¸¡æ–¹ã®ãƒ¦ãƒ¼ã‚¶ã®èˆˆå‘³ã‚’æ•æ‰ã™ã‚‹.
The action taken at each time ğ‘¡ along the trajectory is embedded using an ğ‘š-dimensional vector $u_{a_t} \in \mathbb{R}^n$.
è»Œè·¡(trajectory)ã«æ²¿ã£ãŸå„æ™‚åˆ»$t$ã§å–ã‚‰ã‚ŒãŸactionã¯, $u_{a_t} \in \mathbb{R}^n$ ã‚’ç”¨ã„ã¦åŸ‹ã‚è¾¼ã¾ã‚Œã¦ã„ã‚‹.
We model the state transition P : SÃ—AÃ—S with a recurrent neural network [6, 49]
**state transition(çŠ¶æ…‹é·ç§») P : SÃ—AÃ—S ã‚’ recurrent neural network ã§ãƒ¢ãƒ‡ãƒ«åŒ–**ã™ã‚‹[6, 49]:

$$
s_{t+1} = f(s_t, u_{a_t})
$$

We experimented with a variety of popular RNN cells such as Long Short-Term Memory (LSTM) [18] and Gated Recurrent Units (GRU) [10], and ended up using a simplified cell called Chaos Free RNN (CFN) [24] due to its stability and computational efficiency.
æˆ‘ã€…ã¯ã€LSTMï¼ˆLong Short-Term Memoryï¼‰ [18] ã‚„GRUï¼ˆGated Recurrent Unitsï¼‰ [10] ãªã©ã®æœ‰åãªRNNã‚»ãƒ«ã‚’ã„ã‚ã„ã‚ã¨å®Ÿé¨“ã—ãŸçµæœã€å®‰å®šæ€§ã¨è¨ˆç®—åŠ¹ç‡ã®ç‚¹ã‹ã‚‰ã€Chaos Free RNNï¼ˆCFNï¼‰ [24] ã¨ã„ã†ç°¡æ˜“ã‚»ãƒ«ã‚’ä½¿ã†ã“ã¨ã«ã—ãŸ.
The state is updated recursively as
stateã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«**å†å¸°çš„(recursively)**ã«æ›´æ–°ã•ã‚Œã‚‹.

$$
s_{t+1} =  f(s_t, u_{a_t}) = z_t \odot \tanh(s_t) + i_t \odot \tanh(W_{a}u_{a_t})
\\
z_t = \sigma(U_{z}s_{t} + W_{z}u_{a_t} + b_{z})
\\
i_t = \sigma(U_{i}s_{t} + W_{i}u_{a_t} + b_{i})
\\
\tag{5}
$$

where $z_t, i_t \in \mathbb{R}^n$ are the update and input gate respectively.
ã“ã“ã§ã€ $z_t, i_t \in \mathbb{R}^n$ ã¯ãã‚Œãã‚Œ update gate ã¨ input gate ã§ã‚ã‚‹.

Conditioning on a user state s, the policy ğœ‹ğœƒ (ğ‘|s) is then modeled with a simple softmax,
ãƒ¦ãƒ¼ã‚¶ã®state $s$ ã‚’æ¡ä»¶ã¨ã—ã¦ã€policy $\pi_{\theta}(a|s)$ã¯ã€**å˜ç´”ãª softmax** ã§ãƒ¢ãƒ‡ãƒ«åŒ–ã•ã‚Œã‚‹.

$$
\pi_{\theta}(a|s) = \frac{\exp(s^T v_{a} / T)}{\sum_{a'\in A} \exp(s^T v_{a'}/T)}
\tag{6}
$$

where vğ‘ âˆˆ R ğ‘› is another embedding for each action ğ‘ in the action space A and ğ‘‡ is a temperature that is normally set to 1.
ã“ã“ã§ã€ $v_a \in \mathbb{R}^{n}$ ã¯ action space $A$ ã«ãŠã‘ã‚‹å„action $a$ ã®åˆ¥ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«(actionã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒäºŒç¨®é¡ã‚ã‚‹?)ã§ã€ $T$ ã¯é€šå¸¸ 1 ã«è¨­å®šã•ã‚Œã‚‹ temperature ã§ã‚ã‚‹.
Using a higher value in ğ‘‡ produces a smoother policy over the action space.
$T$ ã®å€¤ã‚’å¤§ããã™ã‚‹ã“ã¨ã§ã€action space ä¸Šã§ã‚ˆã‚Šã‚¹ãƒ ãƒ¼ã‚ºãªpolicyãŒå®Ÿç¾ã•ã‚Œã‚‹.
The normalization term in the softmax requires going over all the possible actions, which is in the order of millions in our setting.
softmax ã®æ­£è¦åŒ–é …ã¯ã™ã¹ã¦ã®å¯èƒ½ãª action ã‚’èª¿ã¹ã‚‹å¿…è¦ãŒã‚ã‚Šã€æˆ‘ã€…ã®è¨­å®šã§ã¯æ•°ç™¾ä¸‡ã®ã‚ªãƒ¼ãƒ€ãƒ¼ã¨ãªã‚‹.
To speed up the computation, we perform sampled softmax [4] during training.
ã“ã®è¨ˆç®—ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã«ã€æˆ‘ã€…ã¯å­¦ç¿’æ™‚ã« sampled softmax[4]ã‚’å®Ÿè¡Œã™ã‚‹.
At serving time, we used an efficient nearest neighbor search algorithm to retrieve top actions and approximate the softmax probability using these actions only, as detailed in section 5.
ã“ã®ã¨ãã€åŠ¹ç‡çš„ãªæœ€è¿‘å‚æ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç”¨ã„ã¦**ä¸Šä½ã®actionã‚’å–å¾—ã—ã€ã“ã‚Œã‚‰ã®actionã®ã¿ã‚’ç”¨ã„ã¦softmaxç¢ºç‡ã‚’è¿‘ä¼¼ã™ã‚‹**ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³5ã§è©³è¿°ï¼‰.

In summary, the parameter ğœƒ of the policy ğœ‹ğœƒ contains the two action embeddings U âˆˆ R ğ‘šÃ— |A | and V âˆˆ R ğ‘›Ã— |A | as well as the weight matrices Uğ‘§, Uğ‘– âˆˆ R ğ‘›Ã—ğ‘› , Wğ‘¢, Wğ‘– , Wğ‘ âˆˆ R ğ‘›Ã—ğ‘š and biases bğ‘¢, bğ‘– âˆˆ R ğ‘› in the RNN cell.
ã¾ã¨ã‚ã‚‹ã¨ã€policy $\pi_{\theta}$ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã¯ã€äºŒç¨®é¡ã® action åŸ‹ã‚è¾¼ã¿ $U \in \mathbb{R}^{m \times |A|}$ ã¨ $V \in \mathbb{R}^{n \times |A|}$ ã‚’ å«ã‚“ã§ã„ã‚‹. ã¾ãŸã€é‡ã¿è¡Œåˆ— $U_z, U_i \in \mathbb{R}^{n \times n}$ ã¨ $W_u, W_i, W_a in \mathbb{R}^{n \times m}$ åŠã³ãƒã‚¤ã‚¢ã‚¹é … $b_{u}, b_{i} \in \mathbb{R}^{n}$ ã‚’å«ã‚“ã§ã„ã‚‹.
Figure 1 shows a diagram describing the neural architecture of the main policy ğœ‹ğœƒ.
å›³1ã¯ã€main(?) policy $\pi_{\theta}$ ã®neural architecture ã‚’èª¬æ˜ã™ã‚‹å›³ã§ã‚ã‚‹.
Given an observed trajectory ğœ = (ğ‘ 0, ğ‘0, ğ‘ 1, Â· Â· Â· ) sampled from a behavior policy ğ›½, the new policy first generates a model of the user state sğ‘¡+1 by starting with an initial state s0 âˆ¼ ğœŒ0 1 and iterating through the recurrent cell as in Equation (5).
policy $\beta$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸè¦³æ¸¬ã•ã‚ŒãŸtrajectory(è»Œé“) $\tau = (s_0, a_0, s_1, \cdots)$ ãŒä¸ãˆã‚‰ã‚Œã‚‹ã¨ã€æ–°ã—ã„policyã¯ã¾ãšitinial state $s_0 \sim \rho_{0}$ ã§é–‹å§‹ã—ã€å¼(5) ã®ã‚ˆã†ã« recurrent ã‚»ãƒ«ã‚’åå¾©ã—ã¦ user state $s_{t+1}$ ã®ãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã™ã‚‹.
Given the user state sğ‘¡+1 the policy head casts a distribution on the action space through a softmax as in Equation (6).
With ğœ‹ğœƒ (ğ‘ğ‘¡+1 |sğ‘¡+1), we can then produce a policy gradient as in Equation (4) to update the policy.
user state $s_{ğ‘¡+1}$ãŒä¸ãˆã‚‰ã‚Œã‚‹ã¨ï¼Œpolicy head(policyã®å…ˆç«¯?)ã¯å¼(6)ã®ã‚ˆã†ã«softmaxã‚’ç”¨ã„ã¦ action state ã«åˆ†å¸ƒ(=ç¢ºç‡è³ªé‡åˆ†å¸ƒ)ã‚’æŠ•å½±ã™ã‚‹ï¼
$\pi_{\theta}(a_{t+1}|s_{t+1})$ ãŒä¸ãˆã‚‰ã‚Œã‚‹äº‹ã§ã€å¼(4) ã®ã‚ˆã†ã« policy-gradient ã‚’ç”Ÿæˆã—ã€policy ã‚’æ›´æ–°ã™ã‚‹ã“ã¨ãŒã§ãã‚‹(i.e. æœŸå¾…ç´¯ç©å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ãªpolicyã«è¿‘ã¥ã‘ã‚‹äº‹ãŒã§ãã‚‹...!).

## 4.2. Estimating the behavior policy è¡Œå‹•ãƒãƒªã‚·ãƒ¼ã®æ¨å®š

One difficulty in coming up with the off-policy corrected estimator in Equation (4) is to get the behavior policy ğ›½.
å¼(4)ã®off-policyè£œæ­£æ¨å®šé‡ã‚’è€ƒãˆã‚‹ä¸Šã§é›£ã—ã„ã®ã¯ã€behavior policy $\beta$ ã‚’å¾—ã‚‹ã“ã¨ã§ã‚ã‚‹.
Ideally, for each logged feedback of a chosen action we received, we would like to also log the probability of the behavior policy choosing that action.
ç†æƒ³çš„ã«ã¯ã€**å—ã‘å–ã£ãŸ chosen action ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¨˜éŒ²ã™ã‚‹ã”ã¨ã«ã€ãã®actionã‚’é¸æŠã™ã‚‹behavior policy ã®ç¢ºç‡ã‚‚è¨˜éŒ²ã—ãŸã„ã¨ã“ã‚**ã§ã‚ã‚‹.
Directly logging the behavior policy is however not feasible in our case as (1) there are multiple agents in our system, many of which we do not have control over, and (2) some agents have a deterministic policy, and setting ğ›½ to 0 or 1 is not the most effective way to utilize these logged feedback.
ã—ã‹ã—ã€behavior policy(ã®å‡ºåŠ›ã™ã‚‹ç¢ºç‡ã®å€¤?) ã‚’ç›´æ¥ãƒ­ã‚°ã«è¨˜éŒ²ã™ã‚‹ã“ã¨ã¯ã€(1)æˆ‘ã€…ã®ã‚·ã‚¹ãƒ†ãƒ ã«ã¯è¤‡æ•°ã®agentãŒå­˜åœ¨ã—ã€ãã®å¤šãã¯æˆ‘ã€…ãŒåˆ¶å¾¡ã§ããªã„ã€(2)ã„ãã¤ã‹ã®agentã¯æ±ºå®šè«–çš„æ–¹é‡(deterministic policy)ã‚’æŒã£ã¦ãŠã‚Šã€ $\beta$ ã‚’0ã¾ãŸã¯1ã«è¨­å®šã™ã‚‹ã“ã¨ã¯ã€ã“ã‚Œã‚‰ã®è¨˜éŒ²ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æ´»ç”¨ã™ã‚‹æœ€ã‚‚æœ‰åŠ¹ãªæ–¹æ³•ã§ã¯ãªã„ãŸã‚ã€ã“ã®ã‚±ãƒ¼ã‚¹ã§ã¯å®Ÿç¾å¯èƒ½ã§ã‚ã‚‹ã¨ã¯ã„ãˆãªã„.

Instead we take the approach first introduced in [39], and estimate the behavior policy ğ›½, which in our case is a mixture of the policies of the multiple agents in the system, using the logged actions.
ãã®ä»£ã‚ã‚Šã«ã€æˆ‘ã€…ã¯[39]ã§æœ€åˆã«ç´¹ä»‹ã•ã‚ŒãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å–ã‚Šã€**ã‚·ã‚¹ãƒ†ãƒ å†…ã®è¤‡æ•°ã®agentã® policy ã®æ··åˆã§ã‚ã‚‹behavior policy $\beta$** ã‚’ã€è¨˜éŒ²ã•ã‚ŒãŸactionã‚’ä½¿ç”¨ã—ã¦æ¨å®šã™ã‚‹.
Given a set of logged feedback D = {(sğ‘– , ğ‘ğ‘–),ğ‘– = 1, Â· Â· Â· , ğ‘}, Strehl et al.[39] estimates Ë†ğ›½ (ğ‘) independent of user state by aggregate action frequency throughout the corpus.
è¨˜éŒ²ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ $D = {(s_i, a_i), i = 1, \cdots N}$ ã®ã‚»ãƒƒãƒˆãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€Strehlã‚‰[39]ã¯ã‚³ãƒ¼ãƒ‘ã‚¹å…¨ä½“ã®actioné »åº¦ã‚’é›†ç´„ã—ã¦ user state ã«ä¾å­˜ã—ãªã„ $\hat{\beta}_{\theta}$ ã‚’æ¨å®šã™ã‚‹.
In contrast, we adopt a context-dependent neural estimator.
ã“ã‚Œã«å¯¾ã—ã€æˆ‘ã€…ã¯contextã«ä¾å­˜(?)ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ¨å®šã‚’æ¡ç”¨ã™ã‚‹.
For each state-action pair (ğ‘ , ğ‘) collected, we estimate the probability Ë†ğ›½ğœƒ â€² (ğ‘ğ‘ ) that the mixture of behavior policies choosing that action using another softmax, parametrised by ğœƒ â€² .
åé›†ã—ãŸå„state-actionãƒšã‚¢ $(s, a)$ ã«ã¤ã„ã¦ã€aã§ãƒ‘ãƒ©ãƒ¡ã‚¿ãƒ©ã‚¤ã‚ºã•ã‚ŒãŸåˆ¥ã®softmaxã‚’ç”¨ã„ã¦ã€æ··åˆbehavior policy ãŒãã® action ã‚’é¸æŠã™ã‚‹ç¢ºç‡ $\hat{\beta_{\theta'}}(a|s)$ ã‚’æ¨å®šã™ã‚‹.
As shown in Figure 1, we re-use the user state ğ‘  generated from the RNN model from the main policy, and model the mixed behavior policy with another softmax layer.
å›³1ã«ç¤ºã™ã‚ˆã†ã«ã€main policy ã®RNNãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸ user state $s$ ã‚’å†åˆ©ç”¨ã—ã€åˆ¥ã®ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹å±¤ã§æ··åˆbehavior policy ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹.
To prevent the behavior head from intefering with the user state of the main policy, we block its gradient from flowing back into the RNN.
behavior head(=actionã®æœ€å¾Œå°¾?)ãŒ main policy ã®user state ã«å¹²æ¸‰ã™ã‚‹ã®ã‚’é˜²ããŸã‚ã€ãã®å‹¾é…ãŒRNNã«é€†æµã™ã‚‹ã®ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ã¦ã„ã‚‹.
We also experimented with separating the ğœ‹ğœƒ and ğ›½ğœƒ â€² estimators, which incurs computational overhead for computing another state representation but does not results in any metric improvement in offline and live experiments.
ã¾ãŸã€$\pi_{\theta}$ ã¨ $\beta_{\theta'}$ ã®æ¨å®šå™¨ã‚’åˆ†é›¢ã™ã‚‹å®Ÿé¨“ã‚‚è¡Œã„ã¾ã—ãŸãŒã€ã“ã‚Œã¯åˆ¥ã®stateè¡¨ç¾ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã®è¨ˆç®—ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒç™ºç”Ÿã—ã¾ã™ãŒã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãŠã‚ˆã³ãƒ©ã‚¤ãƒ–å®Ÿé¨“ã§ã¯ãƒ¡ãƒˆãƒªãƒƒã‚¯ã®å‘ä¸Šã«ã¯ã¤ãªãŒã‚‰ãªã‹ã£ãŸ.

Despite a substantial sharing of parameters between the two policy heads ğœ‹ğœƒ and ğ›½ğœƒ â€², there are two noticeable difference between them:
2ã¤ã®ãƒãƒªã‚·ãƒ¼ãƒ˜ãƒƒãƒ‰ $\pi_{\theta}$ ã¨ $\beta_{\theta'}$ ã®é–“ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‹ãªã‚Šå…±æœ‰ã•ã‚Œã¦ã„ã‚‹ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€ä¸¡è€…ã®é–“ã«ã¯2ã¤ã®é¡•è‘—ãªé•ã„ãŒã‚ã‚‹.
(1) While the main policy ğœ‹ğœƒ is effectively trained using a weighted softmax to take into account of long term reward, the behavior policy head ğ›½ğœƒ â€² is trained using only the state-action pairs;
(1) main policy(=æ›´æ–°ã—ãŸã„policy??) $\pi_{\theta}$ ãŒé•·æœŸçš„ãªå ±é…¬ã‚’è€ƒæ…®ã—ãŸé‡ã¿ä»˜ãã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚’åŠ¹æœçš„ã«ç”¨ã„ã¦å­¦ç¿’ã•ã‚Œã‚‹ã®ã«å¯¾ã—ã€behavior policy(=ä»Šé‹ç”¨ã•ã‚Œã¦ã‚‹policy?) $\beta_{\theta'}$ ã¯state-actionãƒšã‚¢ã®ã¿ã‚’ç”¨ã„ã¦å­¦ç¿’ã•ã‚Œã‚‹.
(2) While the main policy head ğœ‹ğœƒ is trained using only items on the trajectory with non-zero reward 3 , the behavior policy ğ›½ğœƒ â€² is trained using all of the items on the trajectory to avoid introducing bias in the ğ›½ estimate.
(2) main policy head $\pi_{\theta}$ ãŒè»Œé“ä¸Šã®éã‚¼ãƒ­å ±é…¬ã®itemã®ã¿ã‚’ç”¨ã„ã¦å­¦ç¿’ã™ã‚‹ã®ã«å¯¾ã—ã€behavior policy $\beta_{\theta'}$ ã¯è»Œé“ä¸Šã®å…¨ã¦ã®itemã‚’ç”¨ã„ã¦å­¦ç¿’ã—ã€$\beta$ ã®æ¨å®šå€¤ã«åã‚ŠãŒç”Ÿã˜ãªã„ã‚ˆã†ã«ã™ã‚‹.

In [39], it is argued that that a behavior policy that is deterministically choosing an action ğ‘ given state ğ‘  at time ğ‘¡1 and action ğ‘ at time ğ‘¡2 can be treated as randomizing between action ğ‘ and ğ‘ over the timespan of the logging.
[39]ã§ã¯ã€æ™‚é–“ $t_1$ ã«ãŠã‘ã‚‹state $s_1$ ã«ãŠã‘ã‚‹action $a$ ã¨ã€æ™‚é–“ $t_2$ ã«ãŠã‘ã‚‹è¡Œå‹• $b$ ã‚’æ±ºå®šè«–çš„ã«é¸æŠã™ã‚‹è¡Œå‹•æ–¹é‡ã¯ã€ãƒ­ã‚°ã®æ™‚é–“å¹…ã«ãŠã„ã¦action $a$ ã¨action $b$ ã®é–“ã§ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã™ã‚‹ã¨æ‰±ã†ã“ã¨ãŒã§ãã‚‹ã¨è«–ã˜ã¦ã„ã‚‹.(??)
Here we could argue the same point, which explains why the behavior policy could be other than 0 or 1 given a deterministic policy.
ã“ã“ã§ã€æ±ºå®šè«–çš„ãªæ–¹é‡ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã«ã€behavior policy ãŒ0ã¾ãŸã¯1ä»¥å¤–ã«ãªã‚Šã†ã‚‹ç†ç”±ã‚’èª¬æ˜ã™ã‚‹ã€åŒã˜ç‚¹ã‚’è«–ã˜ã‚‹ã“ã¨ãŒã§ãã‚‹.
In addition, since we have multiple policies acting simultaneously, if one policy is determinstically choosing action ğ‘ given user state ğ‘ , and another one is determinstically choosing action ğ‘, then estimating Ë†ğ›½ğœƒ â€² in such a way would approximate the expected frequency of action ğ‘ being chosen under the mixture of these behavior policies given user state ğ‘ .
ã¾ãŸã€è¤‡æ•°ã® policy ãŒåŒæ™‚ã«ä½œç”¨ã—ã¦ã„ã‚‹ã®ã§ã€ã‚ã‚‹policyãŒuser state $s$ ã‚’ä¸ãˆã‚‰ã‚ŒãŸã¨ãã«æ±ºå®šè«–çš„ã«action $a$ ã‚’é¸æŠã—ã€åˆ¥ã®ãƒãƒªã‚·ãƒ¼ãŒæ±ºå®šè«–çš„ã«è¡Œå‹• $b$ ã‚’é¸æŠã—ã¦ã„ã‚‹ã¨ã™ã‚‹ã¨ã€ãã®ã‚ˆã†ã« $\hat{\beta_{\theta'}}$ ã‚’æ¨å®šã™ã‚‹ã¨ã€user state $s$ ã‚’ä¸ãˆã‚‰ã‚ŒãŸã“ã‚Œã‚‰ã®behavior policy ã®æ··åˆä¸‹ã§è¡Œå‹• $a$ ãŒé¸ã°ã‚Œã‚‹æœŸå¾…é »åº¦ã«è¿‘ä¼¼ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã ã‚ã†.

## 4.3. Top-ğ¾ Off-Policy Correction Top-áµƒ Off-Policy Correction (ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£)

Another challenge in our setting is that our system recommends a page of ğ‘˜ items to users at a time. As users are going to browse through (the full or partial set of) our recommendations and potentially interact with more than one item, we need to pick a set of relevant items instead of a single one. In other words, we seek a policy Î ğœƒ (ğ´|ğ‘ ), here each action ğ´ is to select a set of ğ‘˜ items, to maximize the expected cumulative reward,
ç§ãŸã¡ã®è¨­å®šã«ãŠã‘ã‚‹ã‚‚ã†ä¸€ã¤ã®èª²é¡Œã¯ã€ç§ãŸã¡ã®ã‚·ã‚¹ãƒ†ãƒ ãŒä¸€åº¦ã«kå€‹ã®ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒšãƒ¼ã‚¸ã‚’ãƒ¦ãƒ¼ã‚¶ã«æ¨è–¦ã™ã‚‹ã“ã¨ã§ã‚ã‚‹.
ãƒ¦ãƒ¼ã‚¶ã¯æ¨è–¦ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã®å…¨éƒ¨ã¾ãŸã¯ä¸€éƒ¨ã‚’é–²è¦§ã—ã€è¤‡æ•°ã®ã‚¢ã‚¤ãƒ†ãƒ ã«è§¦ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€å˜ä¸€ã®ã‚¢ã‚¤ãƒ†ãƒ ã§ã¯ãªãã€é–¢é€£ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚»ãƒƒãƒˆã‚’é¸æŠã™ã‚‹å¿…è¦ãŒã‚ã‚‹.
è¨€ã„æ›ãˆã‚Œã°ã€æˆ‘ã€…ã¯policy $\Pi_{\theta}(A|s)$ ã‚’æ±‚ã‚ã€ã“ã“ã§å„action $A$ ã¯ã€"æœŸå¾…ç´¯ç©å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ãª**kå€‹ã®ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚»ãƒƒãƒˆ**ã‚’é¸æŠã™ã‚‹"äº‹ã‚’æ„å‘³ã™ã‚‹.

$$
\max_{\theta} J(\Pi_{\theta}) E_{s_t \sim d_t^{\Pi}(\cdot), A_t \sim \Pi_{\theta}(\cdot|s_t)}[R_t(s_t, A_t)]
$$

Here ğ‘…ğ‘¡ (ğ‘ ğ‘¡ , ğ´ğ‘¡) denotes the cumulative return of the set ğ´ğ‘¡ at state ğ‘ ğ‘¡ .
ã“ã“ã§ã€$R_t(s_t, A_t)$ ã¯ã€state $s_t$ ã«ãŠã‘ã‚‹é›†åˆ $A_t$ ã®ç´¯ç©å ±é…¬ã‚’ç¤ºã™.
Unfortunately, the action space grows exponentially under this set recommendation formulation [44, 50], which is prohibitively large given the number of items we choose from are in the orders of millions.
æ®‹å¿µãªãŒã‚‰ã€**ã“ã®é›†åˆæ¨è–¦ã®å®šå¼åŒ–ã§ã¯action space ãŒæŒ‡æ•°é–¢æ•°çš„ã«å¢—å¤§**ã— [44, 50]ã€é¸æŠã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®æ•°ãŒæ•°ç™¾ä¸‡ã®ã‚ªãƒ¼ãƒ€ãƒ¼ã§ã‚ã‚‹ã“ã¨ã‚’è€ƒãˆã‚‹ã¨ã€æ³•å¤–ã«å¤§ãã„.

To make the problem tractable, we assume that a user will interact with at most one item from the returned set ğ´.
å•é¡Œã‚’æ‰±ã„ã‚„ã™ãã™ã‚‹ãŸã‚ã«ã€æˆ‘ã€…ã¯ã€ãƒ¦ãƒ¼ã‚¶ãŒè¿”ã•ã‚ŒãŸé›†åˆ $A$ ã‹ã‚‰æœ€å¤§1ã¤ã®ã‚¢ã‚¤ãƒ†ãƒ ã¨ interaction ã™ã‚‹ã“ã¨ã‚’ä»®å®šã™ã‚‹.
In other words, there will be at most one item with non-zero cumulative reward among ğ´.
è¨€ã„æ›ãˆã‚Œã°ã€"**$A$ ã®ä¸­ã§ã‚¼ãƒ­ã§ãªã„ç´¯ç©å ±é…¬ã‚’æŒã¤ã‚¢ã‚¤ãƒ†ãƒ ã¯ã€ã›ã„ãœã„1ã¤ã§ã‚ã‚ã†**"ã¨ã„ã†ä»®å®šã‚’ãŠã.
We further assume that the expected return of an item is independent of other items chosen in the set ğ´ 4 .
ã•ã‚‰ã«ï¼Œã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®æœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³ã¯ï¼Œé›†åˆ$A$ã®ä¸­ã§é¸ã°ã‚ŒãŸä»–ã®ã‚¢ã‚¤ãƒ†ãƒ ã¨ã¯ç‹¬ç«‹ã§ã‚ã‚‹ã¨ä»®å®šã™ã‚‹ï¼
With these two assumptions, we can reduce the set problem to
ã“ã‚Œã‚‰äºŒã¤ã®ä»®å®šã«ã‚ˆã‚Šã€é›†åˆå•é¡Œã¯æ¬¡ã®ã‚ˆã†ã«ç¸®å°ã§ãã‚‹.

$$
J(\Pi_{\theta}) E_{s_t \sim d_t^{\Pi}(\cdot), a_t \in A_t \sim \Pi_{\theta}(\cdot|s_t)}[R_t(s_t, a_t)]
$$

Here ğ‘…ğ‘¡ (ğ‘ ğ‘¡ , ğ‘ğ‘¡) is the cumulative return of the item ğ‘ğ‘¡ the user interacted with, and ğ‘ğ‘¡ âˆˆ ğ´ğ‘¡ âˆ¼ Î ğœƒ (Â·|ğ‘ ğ‘¡) indicates that ğ‘ğ‘¡ was chosen by the set policy. Furthermore, we constrain ourselves to generate the set action ğ´ by independently sampling each item ğ‘ according to the softmax policy ğœ‹ğœƒ described in Equation (6) and then de-duplicate. As a result, the probability of an item ğ‘ appearing in the final non-repetitive set ğ´ is simply ğ›¼ğœƒ (ğ‘|ğ‘ ) = 1 âˆ’ (1 âˆ’ ğœ‹ğœƒ (ğ‘|ğ‘ ))ğ¾, where ğ¾ is the number of times we sample.
ã“ã“ã§ã€ $R_t(s_t, a_t)$ ã¯ãƒ¦ãƒ¼ã‚¶ãŒinteractionã—ãŸã‚¢ã‚¤ãƒ†ãƒ $at$ã®ç´¯ç©ãƒªã‚¿ãƒ¼ãƒ³ã€$a_t \in A_t \sim \Pi_{\theta}(\cdot|s_t)$ ã¯set policy ã«ã‚ˆã£ã¦ $a_t$ ãŒé¸æŠã•ã‚ŒãŸã“ã¨ã‚’è¡¨ã™.
ã•ã‚‰ã«ã€å¼ï¼ˆ6ï¼‰ã§è¨˜è¿°ã—ãŸã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ãƒãƒªã‚·ãƒ¼ $\pi_{\theta}$ ã«å¾“ã£ã¦å„ã‚¢ã‚¤ãƒ†ãƒ  $a$ ã‚’ç‹¬ç«‹ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ã‚»ãƒƒãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³$A$ã‚’ç”Ÿæˆã—ã€é‡è¤‡ã‚’è§£é™¤ã™ã‚‹ã¨ã„ã†åˆ¶ç´„ã‚’è¨­ã‘ã¦ã„ã‚‹.
ãã®çµæœã€ã‚¢ã‚¤ãƒ†ãƒ  $a$ ãŒæœ€çµ‚çš„ãªéåå¾©é›†åˆ$A$ã«ç¾ã‚Œã‚‹ç¢ºç‡ã¯ã€å˜ç´”ã« $\alpha_{\theta}(a|s) = 1 - (1 - \pi_{\theta}(a|s))^K$ ã€ãŸã ã—$K$ã¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®å›æ•°ã¨ã™ã‚‹.

We can then adapt the REINFORCE algorithm to the set recommendation setting by simply modifying the gradient update in Equation (2) to
ãã“ã§ã€REINFORCEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é›†åˆæ¨è–¦ã®è¨­å®šã«é©å¿œã•ã›ã‚‹ã«ã¯ã€å¼ï¼ˆ2ï¼‰ã®å‹¾é…æ›´æ–°ã‚’æ¬¡ã®ã‚ˆã†ã«å¤‰æ›´ã™ã‚‹ã ã‘ã§ã‚ˆã„.

$$
\sum_{s_t \sim d_t^{\pi}(\cdot), a_t \sim \alpha_{\theta}(\cdot|s_t)} R_t(s_t, a_t) \nabla_{\theta} \log \alpha_{\theta}(a_t|s_t)
$$

Accordingly, we can update the off-policy corrected gradient in Equation (4) by replacing ğœ‹ğœƒ with ğ›¼ğœƒ , resulting in the top-ğ¾ off-policy correction factor:
ã—ãŸãŒã£ã¦ã€å¼(4)ã® $\pi_{\theta}$ ã‚’ $\alpha_{\theta}$ ã«ç½®ãæ›ãˆã¦off-policyè£œæ­£å‹¾é…ã‚’æ›´æ–°ã™ã‚Œã°ã€**top-K off-policyè£œæ­£ä¿‚æ•°**ãŒå¾—ã‚‰ã‚Œã‚‹.

$$
\sum_{s_t \sim d_{t}^{\pi}(\cdot), a_t \sim \beta(\cdot|s_t)}
[\frac{\alpha_{\theta}(a_t|s_t)}{\beta_{\theta}(a_t|s_t)} 
R_{t}(s_t, a_t) 
\nabla_{\theta} \log \alpha_{\theta}(a_t|s_t)]
\\
= \sum_{s_t \sim d_{t}^{\pi}(\cdot), a_t \sim \beta(\cdot|s_t)}
[\frac{\pi_{\theta}(a_t|s_t)}{\beta_{\theta}(a_t|s_t)} 
\frac{\partial \alpha_{\theta}(a_t|s_t)}{\partial \pi(a_t|s_t)} 
R_{t}(s_t, a_t) 
\nabla_{\theta} \log \pi_{\theta}(a_t|s_t)]
\tag{7}
$$

Comparing Equation (7) with Equation (4), the top-ğ¾ policy adds an additional multiplier of
å¼ï¼ˆ7ï¼‰ã¨å¼ï¼ˆ4ï¼‰ã‚’æ¯”è¼ƒã™ã‚‹ã¨ã€top-K policy ã¯ã€å…ƒã® off-policyè£œæ­£ä¿‚æ•° $\frac{\pi_{\theta}(a_t|s_t)}{\beta_{\theta}(a_t|s_t)}$ ã«æ¬¡ã®ä¹—æ•°(multiplier)ã‚’è¿½åŠ ã—ã¦ã„ã‚‹.

$$
\lambda_{K}(s_t, a_t) = \frac{\partial \alpha_{\theta}(a_t|s_t)}{\partial \pi(a_t|s_t)} 
= K(1 - \pi_{\theta}(a_t|s_t))^{K-1}
\tag{8}
$$

Now let us take a closer look at this additional multiplier:
ã§ã¯ã€ã“ã®è¿½åŠ å€ç‡ã«ã¤ã„ã¦è©³ã—ãè¦‹ã¦ã„ã“ã†.

- As ğœ‹ğœƒ (ğ‘|ğ‘ ) â†’ 0, ğœ†ğ¾ (ğ‘ , ğ‘) â†’ ğ¾. The top-ğ¾ off-policy correction increases the policy update by a factor of ğ¾ comparing to the standard off-policy correction;
- $\pi_{\theta}(a_t|s_t) -> 0$ å³ã¡ $\lambda_{K}(s_t, a_t) -> K$ ã®å ´åˆã€top-K off-policyè£œæ­£ã¯æ¨™æº–ã®off-policyè£œæ­£ã¨æ¯”è¼ƒã—ã¦ã€policyã®æ›´æ–°ã‚’Kå€ã«å¢—åŠ ã•ã›ã‚‹.

- As ğœ‹ğœƒ (ğ‘|ğ‘ ) â†’ 1, ğœ†ğ¾ (ğ‘ , ğ‘) â†’ 0. This multiplier zeros out the policy update.
- $\pi_{\theta}(a_t|s_t) -> 1$ å³ã¡ $\lambda_{K}(s_t, a_t) -> 0$ ã®å ´åˆã€ã“ã®ä¹—æ•°ã¯policyã®æ›´æ–°ã‚’ã‚¼ãƒ­ã«ã™ã‚‹.

- As ğ¾ increases, this multiplier reduces the gradient to zero faster as ğœ‹ğœƒ (ğ‘|ğ‘ ) reaches a reasonable range. reaches a reasonable range.
- KãŒå¤§ãã„å ´åˆã€ã“ã®ä¹—æ•°ã¯ã€$\pi_{\theta}(a_t|s_t)$ ãŒåˆç†çš„ãªç¯„å›²ã«é”ã™ã‚‹ã¨ã€ã‚ˆã‚Šé€Ÿãå‹¾é…(policy-gradient)ã‚’ã‚¼ãƒ­ã«ã™ã‚‹ã“ã¨ãŒã§ãã‚‹.(ã‚¼ãƒ­ã«ãªã£ãŸã‚‰æ›´æ–°ãŒåœæ­¢ã™ã‚‹.ã“ã‚Œã£ã¦è‰¯ã„ã“ã¨ãªã‚“ã ã£ã‘?)

In summary, when the desirable item has a small mass in the softmax policy ğœ‹ğœƒ (Â·|ğ‘ ), the top-ğ¾ correction more aggressively pushes up its likelihood than the standard correction. 
è¦ç´„ã™ã‚‹ã¨ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ policy (é–¢æ•°) $\pi_{\theta}(a_t|s_t)$ ã«ãŠã„ã¦æœ›ã¾ã—ã„ã‚¢ã‚¤ãƒ†ãƒ (desirable item??)ã®è³ªé‡(=ç¢ºç‡è³ªé‡?)ãŒå°ã•ã„å ´åˆã€**top-Kè£œæ­£ä¿‚æ•°ã¯æ¨™æº–ã®è£œæ­£ä¿‚æ•°ã‚ˆã‚Šã‚‚ç©æ¥µçš„ã«ãã®å°¤åº¦ã‚’æŠ¼ã—ä¸Šã’ã‚‹**. 
Once the softmax policy ğœ‹ğœƒ (Â·|ğ‘ ) casts a reasonable mass on the desirable item (to ensure it will be likely to appear in the top-ğ¾), the correction then zeros out the gradient and no longer tries to push up its likelihood. 
ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ãƒãƒªã‚·ãƒ¼ $\pi_{\theta}(a_t|s_t)$ ãŒæœ›ã¾ã—ã„ã‚¢ã‚¤ãƒ†ãƒ (desirable item??)ã«é©åº¦ãªè³ªé‡(ç¢ºç‡è³ªé‡)ã‚’ä¸ãˆã‚‹ã¨ï¼ˆtop-K ã«ç™»å ´ã™ã‚‹å¯èƒ½æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ï¼‰ã€è£œæ­£ä¿‚æ•°ã¯å‹¾é…ã‚’ã‚¼ãƒ­ã«ã—ã¦å°¤åº¦ã‚’æŠ¼ã—ä¸Šã’ã‚ˆã†ã¨ã¯ã—ãªããªã‚‹. 
This in return allows other items of interest to take up some mass in the softmax policy. 
ã“ã‚Œã«ã‚ˆã‚Šã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ãƒãƒªã‚·ãƒ¼ã«ãŠã„ã¦ã€ä»–ã®èˆˆå‘³ã‚ã‚‹ã‚ã„ã¦ã‚€ãŒã‚ã‚‹ç¨‹åº¦ã®è³ªé‡ã‚’å ã‚ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚‹.
As we are going to demonstrate in the simulation as well as live experiment, while the standard off-policy correction converges to a policy that is optimal when choosing a single item, the top-ğ¾ correction leads to better top-ğ¾ recommendations.
ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨å®Ÿæ©Ÿã§å®Ÿè¨¼ã™ã‚‹ã‚ˆã†ã«ã€æ¨™æº–çš„ãªã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£ã¯1ã¤ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’é¸æŠã™ã‚‹éš›ã«æœ€é©ãªpolicyã«åæŸã™ã‚‹ãŒã€top-Kè£œæ­£ã¯top-Kæ¨è–¦ ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã«ã¤ãªãŒã‚‹.

## 4.4. Variance Reduction Techniques

As detailed at the beginning of this section, we take a first-order approximation to reduce variance in the gradient estimate. 
æœ¬ç¯€ã®å†’é ­ã§è©³è¿°ã—ãŸã‚ˆã†ã«ã€å‹¾é…æ¨å®šå€¤ã®åˆ†æ•£ã‚’æ¸›ã‚‰ã™ãŸã‚ã«ä¸€æ¬¡è¿‘ä¼¼ã‚’è¡Œã†.
Nonetheless, the gradient can still suffer from large variance due to large importance weight of ğœ”(ğ‘ , ğ‘) = ğœ‹ (ğ‘ |ğ‘ ) ğ›½ (ğ‘ |ğ‘ ) as shown in Equation (4), Similarly for top-ğ¾ off-policy correction. 
ãã‚Œã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€å‹¾é…ã¯ã€top-K off-policyè£œæ­£ã¨åŒæ§˜ã«ã€å¼(4)ã«ç¤ºã™ã‚ˆã†ã«ã€ $w(s,a) = \frac{\pi(a|s)}{\beta(a|s)}$ ã®**å¤§ããªé‡è¦åº¦é‡ã¿ã«ã‚ˆã£ã¦å¤§ããªåˆ†æ•£ã«è‹¦ã—ã‚€ã“ã¨ãŒã‚ã‚‹**.
Large importance weight could result from (1) large deviation of the new policy ğœ‹ (Â·|ğ‘ ) from the behavior policy, in particular, the new policy explores regions that are less explored by the behavior policy. That is, ğœ‹ (ğ‘|ğ‘ ) â‰« ğ›½ (ğ‘|ğ‘ ) and (2) large variance in the ğ›½ estimate.
å¤§ããªé‡è¦åº¦é‡ã¿ã¯ã€ä»¥ä¸‹ã®ï¼’ã¤ã®è¦å› ã‹ã‚‰ç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹.ï¼ˆ1ï¼‰new policy $\pi(\cdot|s)$ ã® behavior policy(ç¾åœ¨ã®policy) ã‹ã‚‰ã®å¤§ããªä¹–é›¢ã€ç‰¹ã«ã€new policy ãŒ behavior policy ã«ã‚ˆã£ã¦ã‚ã¾ã‚Šæ¢ç´¢ã•ã‚Œãªã„é ˜åŸŸã‚’æ¢ç´¢ã™ã‚‹ã“ã¨ã«èµ·å› ã™ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹. ã¤ã¾ã‚Šã€$\pi(a|s) >> \beta(a|s)$ ã€(2) $\beta$ æ¨å®šå€¤ã®åˆ†æ•£ãŒå¤§ãã„.
 
We tested several techniques proposed in counterfactual learning and RL literature to control variance in the gradient estimate.
æˆ‘ã€…ã¯ã€**å‹¾é…(policy-gradient)æ¨å®šã®åˆ†æ•£ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚**ã«ã€åå®Ÿä»®æƒ³å­¦ç¿’ã‚„RLã®æ–‡çŒ®ã§ææ¡ˆã•ã‚Œã¦ã„ã‚‹ã„ãã¤ã‹ã®æ‰‹æ³•ã‚’æ¤œè¨¼ã—ãŸ.
Most of these techniques reduce variance at the cost of introducing some bias in the gradient estimate.
ã“ã‚Œã‚‰ã®æ‰‹æ³•ã®ã»ã¨ã‚“ã©ã¯ã€å‹¾é…æ¨å®šå€¤ã«ä½•ã‚‰ã‹ã®ãƒã‚¤ã‚¢ã‚¹ã‚’ã‚‚ãŸã‚‰ã™ä»£å„Ÿã¨ã—ã¦ã€åˆ†æ•£ã‚’æ¸›å°‘ã•ã›ã‚‹.(=ä¸åæ¨å®šé‡ã§ã¯ãªããªã‚‹ãŒã€åˆ†æ•£ãŒæ¸›å°‘ã™ã‚‹ã‚ˆã†ãªæ‰‹æ³•?)

### 4.4.1. Weight Capping.

The first approach we take is to simply cap the weight [8] as
æœ€åˆã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã—ã¦ã€weighet ã‚’å˜ç´”ã« cap(=å¤§ãã•ã«ä¸Šé™ã‚’è¨­ã‘ã‚‹?)ã™ã‚‹[8].

$$
\bar{w}_{c}(s,a) = \min(\frac{\pi(a|s)}{\beta(a|s)}, c)
\tag{9}
$$

Smaller value of $c$ reduces variance in the gradient estimate, but introduces larger bias.
$c$ ã®å€¤ã‚’å°ã•ãã™ã‚‹ã¨ã€å‹¾é…æ¨å®šã®åˆ†æ•£ã¯å°ã•ããªã‚‹ãŒã€ãƒã‚¤ã‚¢ã‚¹ãŒå¤§ãããªã‚‹.

### 4.4.2. Normalized Importance Sampling (NIS). æ­£è¦åŒ–é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°(NIS)

Second technique we employed is to introduce a ratio control variate, where we use classical weight normalization [32] defined by:
ç¬¬äºŒã®æ‰‹æ³•ã¯ã€**ratio control variate(æ¯”ç‡åˆ¶å¾¡å¤‰æ•°)**ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã‚ã‚‹. ã“ã“ã§ã€classical weight normalization(å¤å…¸çš„ãªé‡ã¿ã®**æ­£è¦åŒ–**)[32]ã‚’ç”¨ã„ã¦ã€æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã™ã‚‹.

$$
\bar{w}_{n}(s,a) = \frac{w(s,a)}{\sum_{(s', a') \sim \beta} w(s', a')}
$$

As Eğ›½ [ğœ”(ğ‘ , ğ‘)] = 1, the normalizing constant is equal to ğ‘›, the batch size, in expectation.
$E_{\beta}[w(s,a)] = 1$ ã®å ´åˆã€æ­£è¦åŒ–å®šæ•° $\frac{1}{\sum_{(s', a') \sim \beta} w(s', a')}$ ã¯æœŸå¾…å€¤çš„ã«ãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã‚ã‚‹$n$ã¨ç­‰ã—ããªã‚‹.
As ğ‘› increases, the effect of NIS is equivalent to tuning down the learning rate.
n ãŒå¢—åŠ ã™ã‚‹ã¨ã€NIS ã®åŠ¹æœã¯å­¦ç¿’ç‡ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã¨ç­‰ä¾¡ã«ãªã‚‹.

### 4.4.3. Trusted Region Policy Optimization (TRPO). TRPOï¼ˆTrusted Region Policy Optimizationï¼‰ã€‚

TRPO [36] prevents the new policy ğœ‹ from deviating from the behavior policy by adding a regularization that penalizes the KL divergence of these two policies.
TRPO [36]ã¯ï¼Œnew policy $\pi$ ãŒbehavior policy $\beta$ ã‹ã‚‰é€¸è„±ã—ãªã„ã‚ˆã†ã«ï¼Œã“ã‚Œã‚‰äºŒã¤ã® policy ã® KL Divergence ã«ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’ä¸ãˆã‚‹æ­£å‰‡åŒ–ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦è¡Œã†.
It achieves similar effect as the weight capping.
ã“ã‚Œã¯ï¼Œweight capping ã¨åŒæ§˜ã®åŠ¹æœã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹.

# 5. Exploration æ¢æ±‚

As should be clear by this point, the distribution of training data is important for learning a good policy.
ã“ã“ã¾ã§ã§æ˜ã‚‰ã‹ãªã‚ˆã†ã«ã€è‰¯ã„æ–¹é‡ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒãŒé‡è¦ã§ã‚ã‚‹ã€‚
Exploration policies to inquire about actions rarely taken by the existing system have been extensively studied.
æ—¢å­˜ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã»ã¨ã‚“ã©è¡Œã‚ã‚Œãªã„è¡Œå‹•ã‚’å•ã„åˆã‚ã›ã‚‹æ¢ç´¢æ–¹é‡ã¯åºƒãç ”ç©¶ã•ã‚Œã¦ã„ã‚‹ã€‚
In practice, brute-force exploration, such as ğœ–-greedy, is not viable in a production system like YouTube where this could, and mostly likely would, result in inappropriate recommendations and a bad user experience.
ã—ã‹ã—ï¼ŒYouTube ã®ã‚ˆã†ãªã‚·ã‚¹ãƒ†ãƒ ã«ãŠã„ã¦ï¼Œğœ–è²ªæ¬²ã«æ¢ç´¢ã‚’è¡Œã†ã“ã¨ã¯ï¼Œä¸é©åˆ‡ãªæ¨è–¦ã‚„ãƒ¦ãƒ¼ã‚¶ä½“é¨“ã®ä½ä¸‹ã‚’æ‹›ãå¯èƒ½æ€§ãŒã‚ã‚Šï¼Œç¾å®Ÿçš„ã§ãªã„ã¨è€ƒãˆã‚‰ã‚Œã‚‹ï¼
For example, Schnabel et al. [35] studied the cost of exploration.
ä¾‹ãˆã°ã€Schnabel ã‚‰ [35] ã¯æ¢ç´¢ã®ã‚³ã‚¹ãƒˆã«ã¤ã„ã¦ç ”ç©¶ã—ã¦ã„ã¾ã™ã€‚

Instead we employ Boltzmann exploration [12] to get the benefit of exploratory data without negatively impacting user experience.
ãã®ä»£ã‚ã‚Šã«ã€ãƒœãƒ«ãƒ„ãƒãƒ³æ¢ç´¢ [12] ã‚’æ¡ç”¨ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹ã«æ‚ªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ãªãã€æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿ã®åˆ©ç‚¹ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
We consider using a stochastic policy where recommendations are sampled from ğœ‹ğœƒ rather than taking the ğ¾ items with the highest probability.
æˆ‘ã€…ã¯ã€æœ€ã‚‚ç¢ºç‡ã®é«˜ã„ áµƒé …ç›®ã‚’é¸ã¶ã®ã§ã¯ãªãã€ğœ‹ã‹ã‚‰æ¨è–¦ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ç¢ºç‡çš„ãªãƒãƒªã‚·ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã™ã‚‹ã€‚
This has the challenge of being computationally inefficient because we need to calculate the full softmax, which is prohibitively expensive considering our action space.
ã“ã‚Œã¯ã€å®Œå…¨ãªã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€è¨ˆç®—åŠ¹ç‡ãŒæ‚ªã„ã¨ã„ã†èª²é¡ŒãŒã‚ã‚Šã€è¡Œå‹•ç©ºé–“ã‚’è€ƒæ…®ã™ã‚‹ã¨æ³•å¤–ãªã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹ã€‚
Rather, we make use of efficient approximate nearest neighbor-based systems to look up the top ğ‘€ items in the softmax [14].
ãã®ä»£ã‚ã‚Šã«ã€åŠ¹ç‡çš„ãªè¿‘ä¼¼æœ€è¿‘å‚ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆ©ç”¨ã—ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®ä¸Šä½ Ç”é …ç›®ã‚’æ¤œç´¢ã™ã‚‹[14]ã€‚
We then feed the logits of these ğ‘€ items into a smaller softmax to normalize the probabilities and sample from this distribution.
æ¬¡ã«ã€ã“ã‚Œã‚‰ã®Ç”é …ç›®ã®å¯¾æ•°ã‚’ã‚ˆã‚Šå°ã•ãªã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã«é€ã‚Šè¾¼ã¿ã€ç¢ºç‡ã‚’æ­£è¦åŒ–ã—ã€ã“ã®åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã€‚
By setting ğ‘€ â‰« ğ¾ we can still retrieve most of the probability mass, limit the risk of bad recommendations, and enable computationally efficient sampling.
ğ‘€ â‰« áµƒã¨ã™ã‚‹ã“ã¨ã§ã€ç¢ºç‡ã®å¡Šã®ã»ã¨ã‚“ã©ã‚’å–ã‚Šå‡ºã™ã“ã¨ãŒã§ãã€æ‚ªã„æ¨è–¦ã®ãƒªã‚¹ã‚¯ã‚’åˆ¶é™ã—ã€è¨ˆç®—åŠ¹ç‡ã®è‰¯ã„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚
In practice, we further balance exploration and exploitation by returning the top ğ¾ â€² most probable items and sample ğ¾ âˆ’ ğ¾ â€² items from the remaining ğ‘€ âˆ’ ğ¾ â€² items.
å®Ÿéš›ã«ã¯ã€æœ€ã‚‚ç¢ºç‡ã®é«˜ã„ä¸Šä½ áµƒé …ç›®ã‚’è¿”ã—ã€æ®‹ã‚Šã® áµƒé …ç›®ã‹ã‚‰ áµƒé …ç›®ã‚’æŠ½å‡ºã™ã‚‹ã“ã¨ã§ã€æ¢ç´¢ã¨æŠ½å‡ºã®ãƒãƒ©ãƒ³ã‚¹ã‚’ã•ã‚‰ã«å–ã‚‹ã€‚

# 6. Experimental Results å®Ÿé¨“çµæœ

We showcase the effectiveness of these approaches for addressing data biases in a series of simulated experiments and live experiments in an industrial-scale recommender system.
æˆ‘ã€…ã¯ã€ä¸€é€£ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“ã¨ç”£æ¥­è¦æ¨¡ã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã§ã®ãƒ©ã‚¤ãƒ–å®Ÿé¨“ã§ã€ãƒ‡ãƒ¼ã‚¿ã®åã‚Šã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã®ã“ã‚Œã‚‰ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æœ‰åŠ¹æ€§ã‚’ç´¹ä»‹ã™ã‚‹ã€‚

## 6.1. Simulation ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

We start with designing simulation experiments to shed light on the off-policy correction ideas under more controlled settings.
æˆ‘ã€…ã¯ã€ã‚ˆã‚Šåˆ¶å¾¡ã•ã‚ŒãŸç’°å¢ƒä¸‹ã§ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ãŸã‚ã«ã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“ã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã‚‹ã€‚
To simplify our simulation, we assume the problem is stateless, in other words, the reward ğ‘… is independent of user states, and the action does not alter the user states either.
ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç°¡å˜ã«ã™ã‚‹ãŸã‚ã«ã€æˆ‘ã€…ã¯å•é¡ŒãŒã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹ã§ã‚ã‚‹ã¨ä»®å®šã™ã‚‹ã€‚è¨€ã„æ›ãˆã‚Œã°ã€å ±é…¬Ç”ã¯ãƒ¦ãƒ¼ã‚¶ã®çŠ¶æ…‹ã‹ã‚‰ç‹¬ç«‹ã—ã¦ãŠã‚Šã€è¡Œå‹•ã¯ãƒ¦ãƒ¼ã‚¶ã®çŠ¶æ…‹ã‚‚å¤‰ãˆãªã„ã€‚
As a result, each action on a trajectory can be independently chosen.
ãã®çµæœã€è»Œé“ä¸Šã®å„è¡Œå‹•ã¯ç‹¬ç«‹ã—ã¦é¸æŠã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

### 6.1.1. Off-policy correction. ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ä¿®æ­£

In the first simulation, we assume there are 10 items, that is A = {ğ‘ğ‘– ,ğ‘– = 1, Â· Â· Â· , 10}.
æœ€åˆã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ ãŒ10å€‹ã‚ã‚‹ã¨ä»®å®šã—ã€ã™ãªã‚ã¡A = {áµ„ğ‘– ,ğ‘– = 1, - - , 10}ã¨ã™ã‚‹ã€‚
The reward of each one is equal to its index, that is, ğ‘Ÿ(ğ‘ğ‘–) = ğ‘–.
ãã‚Œãã‚Œã®å ±é…¬ã¯ãã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ç­‰ã—ãã€ã¤ã¾ã‚Šáµ„ğ‘–(â†ªLl_1D45E) = ğ‘–ã§ã‚ã‚‹ã€‚
When we are choosing a single item, the optimal policy under this setting is to always choose the 10ğ‘¡â„ item as it gives the most reward, that is,
ä¸€ã¤ã®é …ç›®ã‚’é¸ã¶ã¨ãã€ã“ã®è¨­å®šã®ä¸‹ã§ã®æœ€é©ãªæ”¿ç­–ã¯ã€æœ€ã‚‚å¤šãã®å ±é…¬ã‚’ä¸ãˆã‚‹ã®ã§ã€å¸¸ã«10ğ‘¡â†ªLl_210E ã®é …ç›®ã‚’é¸ã¶ã“ã¨ã€ã§ã‚ã‚‹ã€‚

$$
\tag{}
$$

We parameterize ğœ‹ğœƒ using a stateless softmax
Å°áœƒã¯ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚’ç”¨ã„ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã™ã‚‹ã€‚

$$
\tag{}
$$

Given observations sampled from the behavior policy ğ›½, naively applying policy gradient without taking into account of data bias as in Equation (2) would converge to a policy
è¡Œå‹•æ”¿ç­–Ç½ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸè¦³æ¸¬å€¤ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãï¼Œå¼(2)ã®ã‚ˆã†ã«ãƒ‡ãƒ¼ã‚¿ã®åã‚Šã‚’è€ƒæ…®ã›ãšã«ç´ æœ´ã«æ”¿ç­–å‹¾é…ã‚’é©ç”¨ã™ã‚‹ã¨ï¼Œæ”¿ç­–

$$
\tag{}
$$

This has an obvious downside: the more the behavior policy chooses a sub-optimal item, the more the new policy will be biased toward choosing the same item.
ã“ã‚Œã«ã¯æ˜ã‚‰ã‹ãªæ¬ ç‚¹ãŒã‚ã‚‹ã€‚è¡Œå‹•æ”¿ç­–ãŒæœ€é©ã§ãªã„é …ç›®ã‚’é¸ã¹ã°é¸ã¶ã»ã©ã€æ–°ã—ã„æ”¿ç­–ã¯åŒã˜é …ç›®ã‚’é¸ã¶æ–¹å‘ã«åã£ã¦ã—ã¾ã†ã€‚

Figure 2 compares the policies ğœ‹ğœƒ , learned without and with off-policy correction using SGD [7], when the behavior policy ğ›½ is skewed to favor items with least reward.
å›³2ã¯ï¼ŒSGD[7]ã‚’ç”¨ã„ã¦ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£ã‚’è¡Œã†ã“ã¨ãªãå­¦ç¿’ã—ãŸãƒãƒªã‚·ãƒ¼Å°ã¨ï¼Œå ±é…¬ãŒæœ€ã‚‚å°‘ãªã„é …ç›®ã‚’å„ªå…ˆã™ã‚‹ã‚ˆã†ã«åã£ãŸè¡Œå‹•ãƒãƒªã‚·ãƒ¼â†ªL_1D6â†©ã‚’æ¯”è¼ƒã—ãŸã‚‚ã®ã§ã‚ã‚‹ï¼
As shown in Figure 2 (left), naively applying the policy gradient without accounting for the data biases leads to a sub-optimal policy.
å›³2ï¼ˆå·¦ï¼‰ã«ç¤ºã™ã‚ˆã†ã«ï¼Œãƒ‡ãƒ¼ã‚¿ã®åã‚Šã‚’è€ƒæ…®ã›ãšã«ç´ æœ´ã«æ”¿ç­–å‹¾é…ã‚’é©ç”¨ã™ã‚‹ã¨ï¼Œæœ€é©ã¨ã¯è¨€ãˆãªã„æ”¿ç­–ã«ãªã‚‹ï¼
In the worst case, if the behavior policy always chooses the action with the lowest reward, we will end up with a policy that is arbitrarily poor and mimicking the behavior policy (i.e., converge to selecting the least rewarded item).
æœ€æ‚ªã®å ´åˆã€è¡Œå‹•æ”¿ç­–ãŒå¸¸ã«å ±é…¬ã®æœ€ã‚‚å°‘ãªã„è¡Œå‹•ã‚’é¸æŠã™ã‚‹å ´åˆã€ä»»æ„ã«è²§å¼±ãªè¡Œå‹•æ”¿ç­–ã‚’æ¨¡å€£ã—ãŸæ”¿ç­–ã«ãªã£ã¦ã—ã¾ã†ï¼ˆã¤ã¾ã‚Šã€å ±é…¬ã®æœ€ã‚‚å°‘ãªã„é …ç›®ã‚’é¸æŠã™ã‚‹ã‚ˆã†ã«åæŸã—ã¦ã—ã¾ã†ï¼‰ã€‚
On the other hand, applying the off-policy correction allows us to converge to the optimal policy ğœ‹ âˆ— regardless of how the data is collected, as shown in Figure 2 (right).
ä¸€æ–¹ã€ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£ã‚’é©ç”¨ã™ã‚‹ã¨ã€å›³2ï¼ˆå³ï¼‰ã®ã‚ˆã†ã«ã€ãƒ‡ãƒ¼ã‚¿ã®åé›†æ–¹æ³•ã«é–¢ã‚ã‚‰ãšã€æœ€é©ãªæ”¿ç­–ğœ‹âˆ—ã«åæŸã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

### 6.1.2. Top-ğ¾ off-policy correction. top-áµƒ off-policy correction.

To understand the difference between the standard off-policy correction and the top-ğ¾ off-policy correction proposed, we designed another simulation in which we can recommend multiple items.
æ¨™æº–çš„ãªã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£ã¨ææ¡ˆã•ã‚ŒãŸ top-\_1D43 ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£ã®é•ã„ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€è¤‡æ•°ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ¨è–¦ã§ãã‚‹åˆ¥ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¨­è¨ˆã—ãŸã€‚
Again we assume there are 10 items, with ğ‘Ÿ(ğ‘1) = 10, ğ‘Ÿ(ğ‘2) = 9, and the remaining items are of much lower reward ğ‘Ÿ(ğ‘ğ‘–) = 1, âˆ€ğ‘– = 3, Â· Â· Â· , 10. Here we focus on recommending two items, that is, ğ¾ = 2.
ã“ã“ã§ã‚‚10å€‹ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒã‚ã‚Šã€Ç±(áµ„) = 10ã€Ç² = 9ã€æ®‹ã‚Šã®ã‚¢ã‚¤ãƒ†ãƒ ã¯å ±é…¬ãŒã‹ãªã‚Šä½ã„á‘Ÿ(ğ‘) = 1ã€âˆ€ğ‘– = 3ã€-ã€10ã¨ã™ã‚‹ã€‚ã“ã“ã§ã¯2å€‹ã®ã‚¢ã‚¤ãƒ†ãƒ ã€ã¤ã¾ã‚Šáµƒ = 2ã®æ¨è–¦ã«ç„¦ç‚¹ã‚’åˆã‚ã›ã‚‹ã€‚
The behavior policy ğ›½ follows a uniform distribution, i.e., choosing each item with equal chance.
è¡Œå‹•æ–¹é‡Ç–ã¯ä¸€æ§˜åˆ†å¸ƒã«å¾“ã†ã€ã¤ã¾ã‚Šã€å„é …ç›®ã‚’ç­‰ã—ã„ç¢ºç‡ã§é¸æŠã™ã‚‹ã€‚

Given an observation (ğ‘ğ‘– , ğ‘Ÿğ‘–) sampled from ğ›½, the standard offpolicy correction has a SGD updates of the following form,
ğ‘ğ‘–ğ‘–ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸè¦³æ¸¬å€¤ï¼ˆáµ…ï¼‰ãŒä¸ãˆã‚‰ã‚Œã‚‹ã¨ã€æ¨™æº–ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£ã¯ä»¥ä¸‹ã®å½¢å¼ã®SGDæ›´æ–°ã‚’æŒã¤ã€‚

$$
\tag{}
$$

where ğœ‚ is the learning rate.
ã“ã“ã§ã€ğœ‚ã¯å­¦ç¿’ç‡ã§ã‚ã‚‹ã€‚
SGD keeps increasing the likelihood of the item ğ‘ğ‘– proportional to the expected reward under ğœ‹ğœƒ until ğœ‹ğœƒ (ğ‘ğ‘–) = 1, under which the gradient goes to 0. The top-ğ¾ off-policy correction, on the other hand, has an update of the following form,
SGDã¯áœ‹ (áœƒ) = 1ã¾ã§ã€áœƒã®ä¸‹ã§æœŸå¾…å ±é…¬ã«æ¯”ä¾‹ã—ã¦é …ç›®á‘ã®å°¤åº¦ã‚’ä¸Šã’ç¶šã‘ã€ãã®ä¸‹ã§å‹¾é…ã¯0ã«ãªã‚‹ã€‚ ä¸€æ–¹ã€top-áµƒ off-policy correctionã¯ä»¥ä¸‹ã®å½¢ã®æ›´æ–°ãŒã‚ã‚‹ã€‚

$$
\tag{}
$$

where ğœ†ğ¾ (ğ‘ğ‘–) is the multiplier as defined in section 4.3.
ã“ã“ã§ã€ğœ† (áµƒ)ã¯ 4.3 ç¯€ã§å®šç¾©ã•ã‚ŒãŸä¹—æ•°ã§ã‚ã‚‹ã€‚
When ğœ‹ğœƒ (ğ‘ğ‘–) is small, ğœ†ğ¾ (ğ‘ğ‘–) â‰ˆ ğ¾, and SGD increases the likelihood of the item ğ‘ğ‘– more aggressively.
áœ‹ãŒå°ã•ã„ã¨ãã¯ã€áœ† (áœ) â‰ˆ â†ªLu_1D43E ã¨ãªã‚Šã€SGD ã¯ã‚ˆã‚Šç©æ¥µçš„ã«é …ç›® áµ„ã®å¯èƒ½æ€§ã‚’å¢—åŠ ã•ã›ã‚‹ã€‚
As ğœ‹ğœƒ (ğ‘ğ‘–) reaches to a large enough value, ğœ†ğ¾ (ğ‘ğ‘–) goes to 0. As a result, SGD will no longer force to increase the likelihood of this item even when ğœ‹ğœƒ (ğ‘ğ‘–) is still less than 1. This in return allows the second-best item to take up some mass in the learned policy.
áœ‹ (áœƒ) ãŒååˆ†ã«å¤§ããªå€¤ã«ãªã‚‹ã¨ã€áœ† (áµ„ğ‘–) ã¯ 0 ã«ãªã‚Šã€ãã®çµæœã€áœ‹ (áµ„ğ‘–) ãŒã¾ã  1 ä»¥ä¸‹ã§ã‚‚ SGD ã¯ã“ã®é …ç›®ã®å°¤åº¦ã‚’ç„¡ç†ã«ä¸Šã’ãªããªã‚‹ã€‚ ãã®ä»£ã‚ã‚Šã«å­¦ç¿’ã—ãŸæ–¹é‡ã«ãŠã„ã¦ã€äºŒç•ªæ‰‹ã®é …ç›®ãŒã‚ã‚‹ç¨‹åº¦ã®é‡ã‚’å ã‚ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ãŒã§ããŸã€‚

Figure 3 shows the policies ğœ‹ğœƒ learned with the standard (left) and top-ğ¾ off-policy correction (right).
å›³3ã¯ã€æ¨™æº–çš„ãªã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£(å·¦)ã¨top-1_703è£œæ­£(å³)ã§å­¦ç¿’ã•ã‚ŒãŸæ”¿ç­–Å°ã‚’ç¤ºã™ã€‚
We can see that with the standard off-policy correction, although the learned policy is calibrated [23] in the sense that it still maintains the ordering of items w.r.t. their expected reward, it converges to a policy that cast almost its entire mass on the top-1 item, that is ğœ‹ (ğ‘1) â‰ˆ 1.0.
æ¨™æº–çš„ãªã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£ã§ã¯ï¼Œå­¦ç¿’ã•ã‚ŒãŸãƒãƒªã‚·ãƒ¼ã¯æœŸå¾…å ±é…¬ã«å¯¾ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®é †åºã‚’ç¶­æŒã™ã‚‹ã¨ã„ã†æ„å‘³ã§è¼ƒæ­£[23]ã•ã‚Œã¦ã„ã‚‹ãŒï¼Œãƒˆãƒƒãƒ—1ã®ã‚¢ã‚¤ãƒ†ãƒ ã«ã»ã¼å…¨é‡ã‚’æŠ•ã˜ã‚‹ãƒãƒªã‚·ãƒ¼ï¼Œã™ãªã‚ã¡ï¼Œáµ„ (â†ªLl_1D1E) â‰ˆ 1.0 ã«åæŸã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚‹ï¼
As a result, the learned policy loses track of the difference between a slightly sub-optimal item (ğ‘2 in this example) and the rest.
ãã®çµæœï¼Œå­¦ç¿’ã•ã‚ŒãŸæ”¿ç­–ã¯ï¼Œã‚ãšã‹ã«æœ€é©ã§ãªã„é …ç›® (ã“ã®ä¾‹ã§ã¯ áµ„) ã¨æ®‹ã‚Šã®é …ç›®ã¨ã®é–“ã®å·®ã‚’è¦‹å¤±ã†ï¼
The top-ğ¾ correction, on the other hand, converges to a policy that has a significant mass on the second optimal item, while maintaining the order of optimality between items.
ä¸€æ–¹ã€top-áµƒã®è£œæ­£ã¯ã€é …ç›®é–“ã®æœ€é©æ€§ã®é †åºã‚’ç¶­æŒã—ãŸã¾ã¾ã€2ç•ªç›®ã«æœ€é©ãªé …ç›®ã«å¤§ããªè³ªé‡ã‚’æŒã¤æ”¿ç­–ã«åæŸã•ã›ã‚‹ã€‚
As a result, we are able to recommend to users two high-reward items and aggregate more reward overall.
ãã®çµæœã€2ã¤ã®é«˜å ±é…¬ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ¦ãƒ¼ã‚¶ã«æ¨å¥¨ã—ã€å…¨ä½“ã¨ã—ã¦ã‚ˆã‚Šå¤šãã®å ±é…¬ã‚’é›†ç´„ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

## 6.2. Live Experiments Live Experiments

While simulated experiments are valuable to understand new methods, the goal of any recommender systems is ultimately to improve real user experience.
æ–°ã—ã„æ‰‹æ³•ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“ã¯è²´é‡ã§ã™ãŒã€ã‚ã‚‰ã‚†ã‚‹ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®ç›®æ¨™ã¯ã€æœ€çµ‚çš„ã«ã¯å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã§ã™ã€‚
We therefore conduct a series of A
ãã®ãŸã‚ã€ç§ãŸã¡ã¯ã€å®Ÿéš›ã«åˆ©ç”¨ã•ã‚Œã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®ä¸€é€£ã®A

We evaluate these methods on a production RNN candidate generation model in use at YouTube, similar to the setup described in [6, 11].
æˆ‘ã€…ã¯ã€[6, 11]ã§èª¬æ˜ã—ãŸã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨åŒæ§˜ã«ã€YouTubeã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹æœ¬ç•ªã®RNNå€™è£œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã“ã‚Œã‚‰ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
The model is one of many candidate generators that produce recommendations, which are scored and ranked by a separate ranking model before being shown to users on the YouTube Homepage or the side panel on the video watch page.
ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€YouTubeãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã‚„å‹•ç”»è¦–è´ãƒšãƒ¼ã‚¸ã®ã‚µã‚¤ãƒ‰ãƒ‘ãƒãƒ«ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è¡¨ç¤ºã•ã‚Œã‚‹å‰ã«ã€åˆ¥ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã¨ãƒ©ãƒ³ã‚¯ä»˜ã‘ãŒè¡Œã‚ã‚Œã‚‹ã€æ¨è–¦æ–‡ã‚’ç”Ÿæˆã™ã‚‹å¤šãã®å€™è£œç”Ÿæˆè€…ã®ã†ã¡ã®1ã¤ã§ã™ã€‚
As described above, the model is trained following the REINFORCE algorithm.
ä¸Šè¿°ã—ãŸã‚ˆã†ã«ã€ãƒ¢ãƒ‡ãƒ«ã¯ REINFORCE ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«å¾“ã£ã¦å­¦ç¿’ã•ã‚Œã¾ã™ã€‚
The immediate reward ğ‘Ÿ is designed to reflect different user activities; videos that are recommended but not clicked receive zero reward.
å³æ™‚å ±é…¬ á½… ã¯ã•ã¾ã–ã¾ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ´»å‹•ã‚’åæ˜ ã™ã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ãŠã‚Šã€æ¨å¥¨ã•ã‚Œã¦ã‚‚ã‚¯ãƒªãƒƒã‚¯ã•ã‚Œãªã„ãƒ“ãƒ‡ã‚ªã¯ã‚¼ãƒ­å ±é…¬ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚
The long term reward ğ‘… is aggregated over a time horizon of 4â€“10 hours.
é•·æœŸå ±é…¬ Ñ… ã¯4-10æ™‚é–“ã®æ™‚é–“è»¸ã§é›†è¨ˆã•ã‚Œã‚‹ã€‚
In each experiment both the control and the test model use the same reward function.
å„å®Ÿé¨“ã«ãŠã„ã¦ã€ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ¢ãƒ‡ãƒ«ã¨ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã¯åŒã˜å ±é…¬é–¢æ•°ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
Experiments are run for multiple days, during which the model is trained continuously with new events being used as training data with a lag under 24 hours.
å®Ÿé¨“ã¯è¤‡æ•°æ—¥ã«ã‚ãŸã£ã¦è¡Œã‚ã‚Œã€ãã®é–“ã€æ–°ã—ã„ã‚¤ãƒ™ãƒ³ãƒˆãŒ24æ™‚é–“ä»¥å†…ã®é…ã‚Œã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã¯ç¶™ç¶šçš„ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã‚‹ã€‚
While we look at various online metrics with the recommender system during live experiments, we are going to focus our discussion on the amount of time user spent watching videos, referred to as ViewTime.
ãƒ©ã‚¤ãƒ–å®Ÿé¨“ã§ã¯ã€ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®æ§˜ã€…ãªã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¦‹ã¾ã™ãŒã€ã“ã“ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒ“ãƒ‡ã‚ªã‚’è¦‹ã‚‹ã®ã«è²»ã‚„ã—ãŸæ™‚é–“ï¼ˆViewTimeã¨å‘¼ã°ã‚Œã‚‹ï¼‰ã«ç„¦ç‚¹ã‚’ã‚ã¦ã¦è­°è«–ã—ãŸã„ã¨æ€ã„ã¾ã™ã€‚

The experiments presented here describe multiple sequential improvements to the production system.
ã“ã“ã§ç´¹ä»‹ã™ã‚‹å®Ÿé¨“ã¯ã€ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ã‚·ã‚¹ãƒ†ãƒ ã‚’è¤‡æ•°å›ã«åˆ†ã‘ã¦æ”¹è‰¯ã—ãŸã‚‚ã®ã§ã‚ã‚‹ã€‚
Unfortunately, in such a setting, the latest recommender system provides the training data for the next experiment, and as a result, once the production system incorporates a new approach, subsequent experiments cannot be compared to the earlier system.
æ®‹å¿µãªãŒã‚‰ã€ã“ã®ã‚ˆã†ãªè¨­å®šã§ã¯ã€æœ€æ–°ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ãŒæ¬¡ã®å®Ÿé¨“ã®ãŸã‚ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æä¾›ã™ã‚‹ãŸã‚ã€ã„ã£ãŸã‚“æœ¬ç•ªã‚·ã‚¹ãƒ†ãƒ ãŒæ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å–ã‚Šè¾¼ã‚€ã¨ã€ãã‚Œä»¥é™ã®å®Ÿé¨“ã¯ä»¥å‰ã®ã‚·ã‚¹ãƒ†ãƒ ã¨æ¯”è¼ƒã™ã‚‹ã“ã¨ãŒã§ããªã„ã€‚
Therefore, each of the following experiments should be taken as the analysis for each component individually, and we state in each section what was the previous recommender system from which the new approach receives data.
ã—ãŸãŒã£ã¦ã€ä»¥ä¸‹ã®å„å®Ÿé¨“ã¯ã€å„æ§‹æˆè¦ç´ ã®å€‹åˆ¥ã®åˆ†æã¨ã—ã¦æ‰ãˆã‚‹ã¹ãã§ã‚ã‚Šã€æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚‹å‰ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ãŒä½•ã§ã‚ã£ãŸã‹ã‚’å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«è¨˜è¼‰ã™ã‚‹ã€‚

### 6.2.1. Exploration. ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ã‚¸ãƒ§ãƒ³

We begin with understanding the value of exploratory data in improving model quality.
ã¾ãšï¼Œãƒ¢ãƒ‡ãƒ«ã®å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®æ¢ç´¢çš„ãªãƒ‡ãƒ¼ã‚¿ ã®ä¾¡å€¤ã‚’ç†è§£ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã‚‹ï¼
In particular, we would like to measure if serving a stochastic policy, under which we sample from the softmax model as described in Section 5, results in better recommendations than serving a deterministic policy where the model always recommends the ğ¾ items with the highest probability according to the softmax.
ç‰¹ã«ï¼Œã‚»ã‚¯ã‚·ãƒ§ãƒ³ 5 ã§è¿°ã¹ãŸã‚ˆã†ã«ï¼Œã‚½ãƒ•ãƒˆãƒãƒƒ ã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ç¢ºç‡çš„ãªæ”¿ç­–ãŒï¼Œã‚½ãƒ•ãƒˆãƒãƒƒ ã‚¯ã‚¹ã«å¾“ã£ã¦å¸¸ã«æœ€ã‚‚é«˜ã„ç¢ºç‡ã§ áµƒ ã®é …ç›®ã‚’æ¨è–¦ã™ã‚‹æ±ºå®šè«–çš„ãªæ”¿ç­–ã‚ˆã‚Šè‰¯ã„æ¨è–¦ ã‚’ã‚‚ãŸã‚‰ã™ã‹ã©ã†ã‹ã‚’æ¸¬å®šã—ãŸã„ï¼

We conducted a first set of experiments to understand the impact of serving a stochastic policy vs. a deterministic one while keeping the training process unchanged.
ã“ã®å®Ÿé¨“ã§ã¯ï¼Œç¢ºç‡çš„ãªãƒãƒªã‚·ãƒ¼ã¨æ±ºå®šè«–çš„ãªãƒãƒªã‚·ãƒ¼ã¨ã‚’æ¯”è¼ƒã— ã¦ï¼Œå­¦ç¿’éç¨‹ã‚’å¤‰æ›´ã—ãªã„å ´åˆã®å½±éŸ¿ã‚’èª¿ã¹ã‚‹ãŸã‚ã«ï¼Œæœ€åˆã®å®Ÿé¨“ ã‚’ãŠã“ãªã£ãŸï¼
In the experiment, the control population is served with a deterministic policy, while a small slice of test traffic is served with the stochastic policy as described in Section 5.
ã“ã®å®Ÿé¨“ã§ã¯ï¼Œåˆ¶å¾¡é›†å›£ã«ã¯æ±ºå®šè«–çš„ãªãƒãƒªã‚·ãƒ¼ã‚’é©ç”¨ã—ï¼Œãƒ†ã‚¹ãƒˆãƒ»ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã®å°ç‰‡ã«ã¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 5 ã§è¿°ã¹ãŸã‚ˆã†ãªç¢ºç‡è«–çš„ãªãƒãƒªã‚·ãƒ¼ã‚’é©ç”¨ã—ãŸï¼
Both policies are based on the same softmax model trained as in Equation (??).
ä¸¡ãƒãƒªã‚·ãƒ¼ã¨ã‚‚å¼ (?) ã®ã‚ˆã†ã«å­¦ç¿’ã•ã‚ŒãŸåŒã˜ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ã‚‚ã¨ã¥ãã‚‚ã®ã§ã‚ã‚‹ï¼
To control the amount of randomness in the stochastic policy at serving, we varied the temperature used in Equation (6).
ã¾ãŸï¼Œã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒƒã‚¯ãƒãƒªã‚·ãƒ¼ã®ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¹é‡ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã«ï¼Œå¼ (6) ã§ä½¿ç”¨ã™ã‚‹æ¸©åº¦ã‚’å¤‰åŒ–ã•ã›ãŸï¼
A lower ğ‘‡ reduces the stochastic policy to a deterministic one, while a higher ğ‘‡ leads to a random policy that recommends any item with equal chance.
â†ªLu_1D447 ãŒä½ã„ã¨ç¢ºç‡çš„æ”¿ç­–ãŒæ±ºå®šè«–çš„æ”¿ç­–ã«ãªã‚Šï¼Œâ†ªLu_1D447 ãŒé«˜ã„ã¨ä»»æ„ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ç­‰ã—ã„ç¢ºç‡ã§æ¨å¥¨ã™ã‚‹ãƒ©ãƒ³ãƒ€ãƒ æ”¿ç­–ã«ãªã‚‹ï¼
With ğ‘‡ set to 1, we observed no statistically significant change in ViewTime during the experiment, which suggests the amount of randomness introduced from sampling does not hurt the user experience directly.
â†ªLu_1D447 ã‚’ 1 ã«è¨­å®šã—ãŸå ´åˆï¼Œå®Ÿé¨“ä¸­ã« ViewTime ã«çµ±è¨ˆçš„ã«æœ‰æ„ãªå¤‰åŒ–ã¯è¦³å¯Ÿã•ã‚Œãšï¼Œ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã£ã¦ç”Ÿã˜ã‚‹ãƒ©ãƒ³ãƒ€ãƒ æ€§ã®é‡ãŒç›´æ¥ãƒ¦ãƒ¼ã‚¶çµŒé¨“ã‚’æãªã‚ãªã„ã“ã¨ã‚’ ç¤ºã—ã¦ã„ã‚‹ï¼

However, this experimental setup does not account for the benefit of having exploratory data available during training.
ã—ã‹ã—ã€ã“ã®å®Ÿé¨“è¨­å®šã§ã¯ã€å­¦ç¿’ä¸­ã«æ¢ç´¢çš„ãªãƒ‡ãƒ¼ã‚¿ã‚’åˆ©ç”¨ã§ãã‚‹ã“ã¨ã®åˆ©ç‚¹ãŒè€ƒæ…®ã•ã‚Œã¦ã„ãªã„ã€‚
One of the main biases in learning from logged data is that the model does not observe feedback of actions not chosen by the previous recommendation policy, and exploratory data alleviates this problem.
ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å­¦ç¿’ã«ãŠã‘ã‚‹ä¸»ãªãƒã‚¤ã‚¢ã‚¹ã®1ã¤ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒå‰å›ã®æ¨è–¦æ–¹é‡ã§é¸æŠã•ã‚Œãªã‹ã£ãŸè¡Œå‹•ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¦³æ¸¬ã—ãªã„ã“ã¨ã§ã‚ã‚Šã€æ¢ç´¢ãƒ‡ãƒ¼ã‚¿ã¯ã“ã®å•é¡Œã‚’è»½æ¸›ã™ã‚‹ã€‚
We conducted a followup experiment where we introduce the exploratory data into training.
ãã“ã§ã€æ¢ç´¢ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã«å°å…¥ã™ã‚‹è¿½è©¦ã‚’è¡Œã£ãŸã€‚
To do that, we split users on the platform into three buckets: 90%, 5%, 5%.
ãã®ãŸã‚ã«ã€ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ä¸Šã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’90ï¼…ã€5ï¼…ã€5ï¼…ã®3ã¤ã®ãƒã‚±ãƒƒãƒˆã«åˆ†å‰²ã—ã¾ã—ãŸã€‚
The first two buckets are served with a deterministic policy based on a deterministic model and the last bucket of users is served with a stochastic policy based on a model trained with exploratory data.
æœ€åˆã®2ã¤ã®ãƒã‚±ãƒƒãƒˆã«ã¯æ±ºå®šè«–çš„ãªãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ãæ±ºå®šè«–çš„ãªãƒãƒªã‚·ãƒ¼ã§ã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã—ã€æœ€å¾Œã®ãƒã‚±ãƒƒãƒˆã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯æ¢ç´¢çš„ãªãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ãç¢ºç‡çš„ãªãƒãƒªã‚·ãƒ¼ã§ã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚
The deterministic model is trained using only data acquired in the first two buckets, while the stochastic model is trained on data from the first and third buckets.
æ±ºå®šè«–çš„ãƒ¢ãƒ‡ãƒ«ã¯ã€æœ€åˆã®2ã¤ã®ãƒã‚±ãƒƒãƒˆã§å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã¦è¨“ç·´ã•ã‚Œã€ç¢ºç‡è«–çš„ãƒ¢ãƒ‡ãƒ«ã¯ã€æœ€åˆã¨3ç•ªç›®ã®ãƒã‚±ãƒƒãƒˆã®ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã•ã‚Œã‚‹ã€‚
As a result, these two models receive the same amount of training data, but the stochastic model is more likely to observe the outcomes of some rarer state, action pairs because of exploration.
ãã®çµæœã€ã“ã‚Œã‚‰2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã¯åŒã˜é‡ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚‹ãŒã€ã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã¯æ¢ç´¢ã®ãŸã‚ã€ã„ãã¤ã‹ã®ç¨€ãªçŠ¶æ…‹ã¨è¡Œå‹•ã®ãƒšã‚¢ã®çµæœã‚’è¦³å¯Ÿã™ã‚‹å¯èƒ½æ€§ãŒé«˜ããªã‚‹ã€‚

Following this experimental procedure, we observe a statistically significant increase in ViewTime by 0.07% in the test population.
ã“ã®å®Ÿé¨“æ‰‹é †ã«å¾“ã†ã¨ã€ãƒ†ã‚¹ãƒˆæ¯é›†å›£ã«ãŠã„ã¦ã€ViewTime ãŒ 0.07% ã¨çµ±è¨ˆçš„ã«æœ‰æ„ã«å¢—åŠ ã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã¾ ã—ãŸã€‚
While the improvement is not large, it comes from a relatively small amount of exploration data (only 5% of users experience the stochastic policy).
ã“ã®æ”¹å–„ã¯å¤§ããã¯ã‚ã‚Šã¾ã›ã‚“ãŒï¼Œæ¯”è¼ƒçš„å°‘ãªã„æ¢ç´¢ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸã‚‚ã®ã§ã™ (ç¢ºç‡çš„ãƒãƒªã‚·ãƒ¼ã‚’çµŒé¨“ã—ãŸãƒ¦ãƒ¼ã‚¶ã¯ã‚ãšã‹ 5%)ï¼
We expect higher gain now that the stochastic policy has been fully launched.
ã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒƒã‚¯ãƒ»ãƒãƒªã‚·ãƒ¼ãŒå®Œå…¨ã«é–‹å§‹ã•ã‚ŒãŸä»Šã€ã‚ˆã‚Šé«˜ã„åˆ©å¾—ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚

### 6.2.2. Off-Policy Correction. ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ä¿®æ­£

Following the use of a stochastic policy, we tested incorporating off-policy correction during training.
ã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒƒã‚¯ãƒãƒªã‚·ãƒ¼ã«ç¶šã„ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã—ãŸã€‚
Here, we follow a more traditional A
ã“ã“ã§ã¯ã€ã‚ˆã‚Šä¼çµ±çš„ãªA

During experiments, we observed the learned policy (test) starts to deviate from the behavior policy (control) that is used to acquire the traffic.
ã¾ãŸï¼Œå®Ÿé¨“ä¸­ã«ï¼Œå­¦ç¿’ã•ã‚ŒãŸãƒãƒªã‚·ãƒ¼(test)ãŒãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’ç²å¾—ã™ã‚‹ãŸã‚ã®è¡Œå‹•ãƒãƒªã‚·ãƒ¼(control)ã‹ã‚‰ä¹–é›¢ã—å§‹ã‚ã‚‹ã“ã¨ãŒè¦³å¯Ÿã•ã‚ŒãŸï¼
Figure 4 plots a CDF of videos selected by our nominator in control and experiment according to the rank of videos in control population (rank 1 is the most nominated video by the control model, and the rightmost is least nominated).
å›³4ã¯ã€åˆ¶å¾¡æ¯é›†å›£ã«ãŠã‘ã‚‹å‹•ç”»ã®ãƒ©ãƒ³ã‚¯ã«å¿œã˜ã¦ã€åˆ¶å¾¡ã¨å®Ÿé¨“ã§ãƒãƒŸãƒãƒ¼ã‚¿ãŒé¸æŠã—ãŸå‹•ç”»ã®CDFã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ãŸã‚‚ã®ã§ã™ï¼ˆãƒ©ãƒ³ã‚¯1ã¯åˆ¶å¾¡ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦æœ€ã‚‚ãƒãƒŸãƒãƒ¼ãƒˆãŒå¤šã„å‹•ç”»ã€å³ç«¯ã¯æœ€ã‚‚ãƒãƒŸãƒãƒ¼ãƒˆãŒå°‘ãªã„å‹•ç”»ã§ã™ï¼‰ã€‚
We see that instead of mimicking the model (shown in blue) used for data collection, the test model (shown in green) favors videos that are less explored by the control model.
ãƒ‡ãƒ¼ã‚¿åé›†ã«ç”¨ã„ãŸãƒ¢ãƒ‡ãƒ«ï¼ˆé’è‰²ã§è¡¨ç¤ºï¼‰ã‚’æ¨¡å€£ã™ã‚‹ã®ã§ã¯ãªãã€ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆç·‘è‰²ã§è¡¨ç¤ºï¼‰ã¯ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã‚ã¾ã‚Šæ¢ç´¢ã•ã‚Œã¦ã„ãªã„å‹•ç”»ã‚’å„ªå…ˆã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
We observed that the proportion of nominations coming from videos outside of the top ranks is increased by nearly a factor of three in experiment.
å®Ÿé¨“ã§ã¯ã€ä¸Šä½ãƒ©ãƒ³ã‚¯ä»¥å¤–ã®å‹•ç”»ã‹ã‚‰ã®ãƒãƒŸãƒãƒ¼ãƒˆã®å‰²åˆãŒã€ã»ã¼1/3ã«å¢—åŠ ã™ã‚‹ã“ã¨ãŒè¦³å¯Ÿã•ã‚Œã¾ã—ãŸã€‚
This aligns with what we observed in the simulation shown in Figure 2.
ã“ã‚Œã¯ã€å›³2ã«ç¤ºã—ãŸã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§è¦³å¯Ÿã•ã‚ŒãŸã‚‚ã®ã¨ä¸€è‡´ã—ã¦ã„ã¾ã™ã€‚
While ignoring the bias in the data collection process creates a â€œrich get richerâ€œ phenomenon, whereby a video is nominated in the learned policy simply because it was heavily nominated in the behavior policy, incorporating the off-policy correction reduces this effect.
ãƒ‡ãƒ¼ã‚¿åé›†éç¨‹ã§ã®åã‚Šã‚’ç„¡è¦–ã™ã‚‹ã¨ï¼Œè¡Œå‹•ãƒãƒªã‚·ãƒ¼ã§å¤šããƒãƒŸãƒãƒ¼ãƒˆã•ã‚ŒãŸã‹ã‚‰ã¨ã„ã£ã¦å­¦ç¿’ãƒãƒªã‚·ãƒ¼ã§ãƒãƒŸãƒãƒ¼ãƒˆã•ã‚Œã‚‹ã¨ã„ã†ã€Œé‡‘æŒã¡ãŒã‚ˆã‚Šé‡‘æŒã¡ã«ãªã‚‹ã€ç¾è±¡ãŒç”Ÿã˜ã‚‹ãŒï¼Œã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã“ã®åŠ¹æœã‚’ä½æ¸›ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼

Interestingly, in live experiment, we did not observe a statistically significant change in ViewTime between control and test population.
èˆˆå‘³æ·±ã„ã“ã¨ã«ã€ãƒ©ã‚¤ãƒ–å®Ÿé¨“ã§ã¯ã€ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«é›†å›£ã¨ãƒ†ã‚¹ãƒˆé›†å›£ã®é–“ã§ã€ViewTimeã®çµ±è¨ˆçš„ã«æœ‰æ„ãªå¤‰åŒ–ã¯è¦³å¯Ÿã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚
However, we saw an increase in the number of videos viewed by 0.53%, which was statistically significant, suggesting that users are indeed getting more enjoyment.
ã—ã‹ã—ã€å‹•ç”»ã®è¦–è´å›æ•°ãŒ0.53%å¢—åŠ ã—ã€çµ±è¨ˆçš„ã«æœ‰æ„ã§ã‚ã£ãŸã“ã¨ã‹ã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚ˆã‚Šæ¥½ã—ã‚ã¦ã„ã‚‹ã“ã¨ãŒä¼ºãˆã¾ã™ã€‚

### 6.2.3. Top-ğ¾ Off-Policy. Top-áµƒ Off-Policy.

We now focus on understanding if the top-ğ¾ off-policy learning improves the user experience over the standard off-policy approach.
æˆ‘ã€…ã¯ã€ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼å­¦ç¿’ãŒæ¨™æº–çš„ãªã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚ˆã‚Šã‚‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ã‹ã©ã†ã‹ã‚’ç†è§£ã™ã‚‹ã“ã¨ã«é‡ç‚¹ã‚’ç½®ãã€‚
In this case, we launched an equivalently structured model now trained with the top-ğ¾ off-policy corrected gradient update given in Equation (7) and compared its performance to the previous off-policy model, described in Section 6.2.2.
ã“ã®å ´åˆï¼Œå¼(7)ã§ä¸ãˆã‚‰ã‚Œã‚‹top-áµƒ off-policyè£œæ­£å‹¾é…æ›´æ–°ã§å­¦ç¿’ã—ãŸç­‰ä¾¡æ§‹é€ ãƒ¢ãƒ‡ãƒ«ã‚’èµ·å‹•ã—ï¼Œã‚»ã‚¯ã‚·ãƒ§ãƒ³6.2.2ã§è¿°ã¹ãŸä»¥å‰ã®off-policyãƒ¢ãƒ‡ãƒ«ã¨æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¾ã—ãŸï¼
In this experiment, we use ğ¾ = 16 and capping ğ‘ = ğ‘’ 3 in Equation (9); we will explore these hyperparameters in more detail below.
ã“ã®å®Ÿé¨“ã§ã¯ï¼Œå¼ (9) ã§ áµƒ = 16 ã¨ã‚­ãƒ£ãƒƒãƒ”ãƒ³ã‚° ğ‘’ = ğ‘“ ã‚’ä½¿ç”¨ã—ã¾ã—ãŸï¼

As described in Section 4.3 and demonstrated in the simulation in Section 6.1.2, while the standard off-policy correction we tested before leads to a policy that is overly-focused on getting the top-1 item correct, the top-ğ¾ off-policy correction converges to a smoother policy under which there is a non-zero mass on the other items of interest to users as well.
ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 4.3 ã§èª¬æ˜ã—ï¼Œã‚»ã‚¯ã‚·ãƒ§ãƒ³ 6.1.2 ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ç¤ºã—ãŸã‚ˆã†ã«ï¼Œä»¥å‰ãƒ†ã‚¹ãƒˆã—ãŸæ¨™æº–çš„ãªã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£ã¯ãƒˆãƒƒãƒ— 1 ã®é …ç›®ã‚’æ­£ã™ã“ã¨ã«éåº¦ã«é›†ä¸­ã—ãŸãƒãƒªã‚·ãƒ¼ã‚’å°ããŒï¼Œãƒˆãƒƒãƒ— áµƒã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£ã¯ï¼Œãƒ¦ãƒ¼ã‚¶ãŒèˆˆå‘³ã‚’æŒã¤ä»–ã®é …ç›®ã«ã‚‚ã‚¼ãƒ­ã§ã¯ãªã„è³ªé‡ãŒã‚ã‚‹ï¼Œã‚ˆã‚Šæ»‘ã‚‰ã‹ãªãƒãƒªã‚·ãƒ¼ã«åæŸã•ã›ã‚‹ï¼
This in turn leads to better top-ğ¾ recommendation.
ã“ã‚Œã«ã‚ˆã‚Šã€ã‚ˆã‚Šè‰¯ã„ãƒˆãƒƒãƒ— áµƒã®æ¨è–¦ã«ã¤ãªãŒã‚‹ã€‚
Given that we can recommend multiple items, the top-ğ¾ off-policy correction leads us to present a better fullpage experience to users than the standard off-policy correction.
è¤‡æ•°ã®é …ç›®ã‚’æ¨è–¦ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã“ã¨ã‚’è€ƒãˆã‚‹ã¨ã€ãƒˆãƒƒãƒ—\_1ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£ã¯æ¨™æº–çš„ãªã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£ã‚ˆã‚Šã‚‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è‰¯ã„ãƒ•ãƒ«ãƒšãƒ¼ã‚¸ä½“é¨“ã‚’æä¾›ã™ã‚‹ã“ã¨ã«ã¤ãªãŒã‚‹ã€‚
In particular, we find that the amount of ViewTime increased by 0.85% in the test traffic, with the number of videos viewed slightly decreasing by 0.16%.
ç‰¹ã«ã€ãƒ†ã‚¹ãƒˆãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã§ã¯ã€ViewTime ã®é‡ãŒ 0.85% å¢—åŠ ã—ã€å‹•ç”»ã®è¦–è´æ•°ã¯ 0.16% ã¨ã‚ãšã‹ã«æ¸›å°‘ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

### 6.2.4. Understanding Hyperparameters. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç†è§£ã™ã‚‹ã€‚

Last, we perform a direct comparison of how different hyperparameter choices affect the top-ğ¾ off-policy correction, and in turn the user experience on the platform.
æœ€å¾Œã«ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é¸æŠãŒtop-áµƒ off-policyè£œæ­£ã€ã²ã„ã¦ã¯ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ã‚’ç›´æ¥æ¯”è¼ƒã—ã¾ã—ãŸã€‚
We perform these tests after the top-ğ¾ off-policy correction became the production model.
ã“ã‚Œã‚‰ã®ãƒ†ã‚¹ãƒˆã¯ã€top-áµƒ off-policyè£œæ­£ãŒãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã«ãªã£ãŸå¾Œã«å®Ÿæ–½ã—ã¾ã—ãŸã€‚

#### 6.2.4.1. Number of actions. ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æ•°

We first explore the choice of ğ¾ in the top-ğ¾ off-policy correction.
ã¾ãšã€top-áµƒ off-policyè£œæ­£ã«ãŠã‘ã‚‹áµƒã®é¸æŠã«ã¤ã„ã¦æ¤œè¨ã™ã‚‹ã€‚
We train three structurally identical models, using ğ¾ âˆˆ {1, 2, 16, 32}; The control (production) model is the top-ğ¾ off-policy model with ğ¾ = 16.
áµƒâˆˆ {1, 2, 16, 32} ã‚’ç”¨ã„ã¦ã€3ã¤ã®æ§‹é€ çš„ã«åŒä¸€ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹ã€‚å¯¾ç…§ï¼ˆç”Ÿç”£ï¼‰ãƒ¢ãƒ‡ãƒ«ã¯ã€â†ªLu_1D43E = 16 ã® top-áµƒ off-policy ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã€‚
We plot the results during a 5-day experiment in Figure 5.
5 æ—¥é–“ã®å®Ÿé¨“ã®çµæœã‚’å›³ 5 ã«ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ã€‚
As explained in Section 4.3, with ğ¾ = 1, the top-ğ¾ off-policy correction reduces to the standard off-policy correction.
4.3 ç¯€ã§èª¬æ˜ã—ãŸã‚ˆã†ã«ã€â†ªLu_1D43E = 1 ã®å ´åˆã€top-ğ¾ off-policy ã®è£œæ­£ã¯æ¨™æº–ã® off-policy ã®è£œæ­£ã«æ¸›å°‘ã™ã‚‹ã€‚
A drop of 0.66% ViewTime was observed for ğ¾ = 1 compared with the baseline with ğ¾ = 16.
áµƒ = 1 ã§ã¯ã€áµƒ = 16 ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒã—ã¦ 0.66% ã® ViewTime ã®æ¸›å°‘ãŒè¦³å¯Ÿã•ã‚Œã¾ã—ãŸã€‚
This further confirms the gain we observed shifting from the standard off-policy correction to the top-ğ¾ off-policy correction.
ã“ã‚Œã¯ã€æ¨™æº–ã®ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£ã‹ã‚‰ top-áµƒã‚ªãƒ•ãƒãƒªã‚·ãƒ¼è£œæ­£ã«ç§»è¡Œã—ã¦è¦³æ¸¬ã•ã‚ŒãŸåˆ©å¾—ã‚’ã•ã‚‰ã«ç¢ºèªã™ã‚‹ã‚‚ã®ã§ã™ã€‚
Setting ğ¾ = 2 still performs worse than the production model, but the gap is reduced to 0.35%.
áµƒ = 2ã«è¨­å®šã—ã¦ã‚‚ã€è£½å“ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚æ€§èƒ½ã¯åŠ£ã‚‹ãŒã€ãã®å·®ã¯0.35%ã«ç¸®ã¾ã£ãŸã€‚
ğ¾ = 32 achieves similar performance as the baseline.
áµƒ = 32ã¯ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨åŒæ§˜ã®æ€§èƒ½ã‚’é”æˆã—ãŸã€‚
We conducted follow up experiment which showed mildly positive gain in ViewTime (+0.15% statistically significant) when ğ¾ = 8.
ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—å®Ÿé¨“ã‚’è¡Œã£ãŸã¨ã“ã‚ã€áµƒ = 8 ã®ã¨ãã«ã€ViewTime ãŒã‚ãšã‹ã«å¢—åŠ ã—ã¾ã—ãŸ (+0.15% çµ±è¨ˆçš„ã«æœ‰æ„)ã€‚

#### 6.2.4.2. Capping. ã‚­ãƒ£ãƒƒãƒ”ãƒ³ã‚°

Here we consider the effect of the variance reduction techniques on the final quality of the learned recommender.
ã“ã“ã§ã¯ã€å­¦ç¿’æ¸ˆã¿ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã®æœ€çµ‚çš„ãªå“è³ªã«å¯¾ã™ã‚‹åˆ†æ•£å‰Šæ¸›æŠ€è¡“ã®åŠ¹æœã«ã¤ã„ã¦è€ƒå¯Ÿã™ã‚‹ã€‚
Among the techniques discussed in Section 4.4, weight capping brings the biggest gain online in initial experiments.
ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.4ã§è­°è«–ã—ãŸæŠ€è¡“ã®ä¸­ã§ã€é‡ã¿ã‚­ãƒ£ãƒƒãƒ”ãƒ³ã‚°ã¯åˆæœŸã®å®Ÿé¨“ã«ãŠã„ã¦ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§æœ€å¤§ã®åˆ©å¾—ã‚’ã‚‚ãŸã‚‰ã™ã€‚
We did not observe further metric improvements from normalized importance sampling, or TRPO [36].
ã¾ãŸã€æ­£è¦åŒ–é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚„TRPO[36]ã«ã‚ˆã‚‹æ›´ãªã‚‹ãƒ¡ãƒˆãƒªãƒƒã‚¯ã®å‘ä¸Šã¯è¦³å¯Ÿã•ã‚Œãªã‹ã£ãŸã€‚
We conducted a regression test to study the impact of weight capping.
æˆ‘ã€…ã¯ã€ã‚¦ã‚§ã‚¤ãƒˆã‚­ãƒ£ãƒƒãƒ”ãƒ³ã‚°ã®å½±éŸ¿ã‚’èª¿æŸ»ã™ã‚‹ãŸã‚ã«å›å¸°ãƒ†ã‚¹ãƒˆã‚’å®Ÿæ–½ã—ãŸã€‚
We compare a model trained using cap ğ‘ = ğ‘’ 3 (as in production model) in Equation (9) with one trained using ğ‘ = ğ‘’ 5 .
ã“ã‚Œã¯ï¼Œå¼(9)ã®cap ğ‘ = ğ‘’ï¼ˆç”Ÿç”£ãƒ¢ãƒ‡ãƒ«ã®ã‚ˆã†ã«ï¼‰ã‚’ç”¨ã„ã¦å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã¨ï¼Œ ğ‘ = ğ‘’ã‚’ç”¨ã„ã¦å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã™ã‚‹ã‚‚ã®ã§ã‚ã‚‹ï¼
As we lift the restriction on the importance weight, the learned policy ğœ‹ğœƒ could potentially overfit to a few logged actions that accidentally receives high reward.
é‡è¦åº¦é‡ã¿ã®åˆ¶é™ã‚’è§£é™¤ã™ã‚‹ã¨ï¼Œå­¦ç¿’ã•ã‚ŒãŸæ”¿ç­–áœƒã¯å¶ç„¶ã«é«˜ã„å ±é…¬ã‚’å—ã‘å–ã‚‹å°‘æ•°ã®è¨˜éŒ²ã•ã‚ŒãŸè¡Œå‹•ã«å¯¾ã—ã¦éå‰°ã«é©åˆã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ï¼
Swaminathan and Joachims [43] described a similar effect of propensity overfitting.
Swaminathanã¨Joachims [43]ã¯ã€å‚¾å‘ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã®åŒæ§˜ã®åŠ¹æœã«ã¤ã„ã¦è¿°ã¹ã¦ã„ã‚‹ã€‚
During live experiment, we observe a significant drop of 0.52% in ViewTime when the cap on importance weight was lifted.
ãƒ©ã‚¤ãƒ–å®Ÿé¨“ã§ã¯ã€é‡è¦åº¦ã‚¦ã‚§ã‚¤ãƒˆã®ä¸Šé™ãŒè§£é™¤ã•ã‚ŒãŸã¨ãã«ã€ViewTimeãŒ0.52%ã¨ã„ã†å¤§å¹…ãªä½ä¸‹ã‚’è¦³æ¸¬ã—ã¦ã„ã¾ã™ã€‚

# 7. Conclusion çµè«–

In this paper we have laid out a practical implementation of a policy gradient-based top-ğ¾ recommender system in use at YouTube.
ã“ã®è«–æ–‡ã§ã¯ã€YouTubeã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹æ”¿ç­–å‹¾é…ã«åŸºã¥ããƒˆãƒƒãƒ—1æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿç”¨çš„ãªå®Ÿè£…ã‚’ç¤ºã—ã¾ã—ãŸã€‚
We scale up REINFORCE to an action space in the orders of millions and have it stably running in a live production system.
ã¾ãŸã€REINFORCEã‚’æ•°ç™¾ä¸‡å˜ä½ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¹ãƒšãƒ¼ã‚¹ã«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã—ã€æœ¬ç•ªã‚·ã‚¹ãƒ†ãƒ ã§å®‰å®šçš„ã«å‹•ä½œã•ã›ã‚‹ã“ã¨ãŒã§ããŸã€‚
To realize the full benefits of such an approach, we have demonstrated how to address biases in logged data through incorporating a learned logging policy and a novel top-ğ¾ off-policy correction.
ã“ã®ã‚ˆã†ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®åˆ©ç‚¹ã‚’æœ€å¤§é™ã«å®Ÿç¾ã™ã‚‹ãŸã‚ã«ã€å­¦ç¿’ã•ã‚ŒãŸãƒ­ã‚®ãƒ³ã‚°ãƒãƒªã‚·ãƒ¼ã¨æ–°ã—ã„ top-áµƒ off-policy correction ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã«ã‚ˆã£ã¦ã€ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚¤ã‚¢ã‚¹ã«å¯¾å‡¦ã™ã‚‹æ–¹æ³•ã‚’å®Ÿè¨¼ã—ã¾ã—ãŸã€‚
We conducted extensive analysis and live experiments to measure empirically the importance of accounting for and addressing these underlying biases.
ç§ãŸã¡ã¯ã€ã“ã‚Œã‚‰ã®æ ¹åº•ã«ã‚ã‚‹ãƒã‚¤ã‚¢ã‚¹ã‚’è€ƒæ…®ã—ã€å¯¾å‡¦ã™ã‚‹ã“ã¨ã®é‡è¦æ€§ã‚’å®Ÿè¨¼çš„ã«æ¸¬å®šã™ã‚‹ãŸã‚ã«ã€åºƒç¯„ãªåˆ†æã¨ãƒ©ã‚¤ãƒ–å®Ÿé¨“ã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚
We believe these are important steps in making reinforcement learning practically impactful for recommendation and will provide a solid foundation for researchers and practitioners to explore new directions of applying RL to recommender systems.
æˆ‘ã€…ã¯ã€å¼·åŒ–å­¦ç¿’ãŒæ¨è–¦ã«å®Ÿç”¨çš„ãªã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚’ä¸ãˆã‚‹ãŸã‚ã®é‡è¦ãªã‚¹ãƒ†ãƒƒãƒ—ã§ã‚ã‚Šã€ç ”ç©¶è€…ã‚„å®Ÿå‹™å®¶ãŒæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«å¼·åŒ–å­¦ç¿’ã‚’é©ç”¨ã™ã‚‹æ–°ã—ã„æ–¹å‘æ€§ã‚’æ¢ã‚‹ãŸã‚ã®å¼·å›ºãªåŸºç¤ã‚’æä¾›ã™ã‚‹ã¨è€ƒãˆã¦ã„ã‚‹ã€‚
