# 1. Uplift-based Evaluation and Optimization of Recommenders

published date: 16 September 2019,
authors: Masahiro Sato, Janmajay Singh, Sho Takemori, Takashi Sonoda, Qian Zhang, and Tomoko Ohkuma
url(paper): https://dl.acm.org/doi/pdf/10.1145/3298689.3347018
(勉強会発表者: morinota)

---

## 1.1. どんなもの?

- 一般的な推薦システムの評価では、推薦された商品を購入することが"成功"であるとされる.
- しかし、**レコメンドされなくても購入されている可能性**があり、真に評価すべきは、"レコメンドによるユーザーアクションの増加 = **Uplift**"である.
- しかし，ある時点の(user, item)ペアに対して，推薦がある場合 & ない場合の両方の状況を観測することは不可能であり、Upliftに基づく評価と最適化は困難.
- 本論文では、推薦システムにおける Uplift の新しい評価指標と最適化手法を提案する.
- 評価指標:
  - **因果推論の枠組み**を適用し，推薦システムを評価するための平均的なUpliftを推定する.
  - 本論文では，推薦システムにおける購買履歴と推薦履歴の両方を用いて，推薦がある場合とない場合のシミュレーションを行う．
  - これにより，新たに生成された推薦リストのUpliftを正当に評価することができる．
- 最適化手法:
  - 最適化のためには、Uplift-based アプローチに特化したポジティブサンプルとネガティブサンプルを定義する必要がある.
  - そのために、**購買履歴と推薦履歴から4つの分類**を導き出す.
  - そして，これら4つのクラス間の相対的な優先順位を高揚度の観点から導き出し，それを用いてUplift最適化のためのpoint-wise サンプリングと pair-wise サンプリングの手法を構築する．
  - また，3つの公共データセットを用いた実験により，我々の最適化手法がUpliftの改善に有効であることを実証する．

## 1.2. 先行研究と比べて何がすごい？

本論文の貢献は以下のようにまとめられる.

- 推薦の Uplift に基づく評価指標を提案する（セクション2）．
- また， Uplift に基づく推薦のための point-wise および pair-wise 最適化手法を提案する（セクション3）．
- ベースラインとの比較により，最適化手法の有効性を示す（5.2節）．
- また，最適化結果（5.3節）と推薦結果（5.4節）の特徴を明らかにする．

## 1.3. 技術や手法の肝は？

### 1.3.1. Uplift に基づく 評価指標:

レコメンダーは通常、推薦精度の観点から評価される.
レコメンダーは、推薦した商品がより多く購入されれば、他のレコメンダーより優れているとみなされる.
この評価手法をaccuracy-based evaluationと呼ぶ.
レコメンダーの精度の指標としてよく使われる"Precision"は、"the number of purchases / the number of recommendations"で定義される.
しかし、推薦がなくても、ユーザーがその商品を認知し、嗜好していれば、購入される可能性がある.
なので本論文では、リコメンダーは "Uplift" という観点で評価されることを目指す.

#### 1.3.1.1. Accuracy と Uplift の不一致 の話

"推薦がある場合"と"ない場合"の2つのケースを説明するために，因果推論[15, 28, 37]の **potential outcome** の概念を採用する．
$Y^T=1$, $Y^C=1$ は，それぞれ推薦されたケース と されないケース にアイテムが購入されることを示す.
**あるユーザに対するアイテムの Uplift $\tau$**は、$Y^T - Y^C$ である.
2つのシナリオ(推薦がある場合 & 推薦がない場合)におけるユーザの2つの行動(購入 & not購入)を考慮すると、ユーザにとって以下の4つのアイテムクラスが存在することになる.

- True Uplift (TU): $Y^T= 1$ 且つ $Y^C = 0$ -> $\tau = 1$.
  - 勧められれば購入されるが、勧められなければ購入されない.
- False Uplift (FU): $Y^T= Y^C = 1$, -> $\tau= 0$.
  - 推奨されるかどうかに関わらず購入される.
- True Drop (TD). $Y^T= 0$ 且つ $Y^C = 1$, -> $\tau = −1$.
  - 推奨されない場合は購入されるが、推奨される場合は購入されない.
- False Drop (FD). $Y^T = Y^C = 0$, -> $\tau= 0$.
  - 推奨されるかどうかに関わらず購入されない.

![](https://camo.qiitausercontent.com/a7306517e1ca5d6c2f6e8415c24384be4c5f488b/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f66613233356434662d313265642d353431652d626138642d3238626238333031613736392e706e67)

図1に示すように、レコメンドの Uplift と accuracy の違いを直感的に理解できるように、10個の推薦アイテムからなる4つのリストについて考えてみる.
ここで、オフラインデータセットとして、**現在導入しているレコメンダーの購入ログと推薦ログ(userに推薦したか否か)の両方を持つもの**を想定する.
TUは推薦された場合のみ購入され、TDは推薦されない場合のみ購入されることに注意.
その他のFUやFDの購入はレコメンデーションに依存しない.

![](https://camo.qiitausercontent.com/5c5403f92a913430a081e00b27e19ecee496c203/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f32653536646534612d343438612d363161372d373266642d6465663262613162393464652e706e67)

過去に10アイテム全てが推薦された場合に得られたであろう Uplift の合計は表1のようになる.
また、リスト上の全アイテムを推薦した場合と、過去ログで推薦されたアイテムのみ(??)を推薦した場合の2つの設定での Precision を記載する.
前者はレコメンダーの評価でよく用いられる設定である[9]．
後者はレコメンダーのオンライン性能を推定(??)するための先行研究[7, 25]で採用されている設定である.
これらのサンプルでは，**Precision と Uplift が逆の傾向を示している**．
つまり，**この Precision から，より高い Uplift を達成するための最適なモデルを選択することはできない**．
レコメンデーションのないアイテムを除外しても、この問題は解決されない.
また、推薦アイテム（図1の実線枠のアイテム）のみを用いて計算した後者の Precision は、すべてのリストで同じ値を示しており、やはり最適なモデルを選択することはできない.

このように、レコメンダーによるUplift を評価するためには、accuracy-based evaluationは適さない.
そこで、uplift-based evaluation のために設計された評価指標を採用する必要がある.
しかし，ある時間におけるユーザとアイテムのペアは$Y^T$か$Y^C$の**どちらかしか観測できない**ため，直接的にアップリフト$\tau = Y^T - Y^C$の総量を算出することはできない．
そこで、因果推論の枠組みを用いて、**average treatment effect** を推定する.

#### 1.3.1.2. 因果推論Framework

このサブセクションでは、因果推論フレームワーク [15, 28, 37] を紹介し、次のサブセクションでレコメンダーのUplift-based evaluationに適用する.
treatment efect $\tau$は、「treatmentの有無によるpotential outcomeの差」として定義される： $tau \equiv Y^T - Y^C$.
ただし$Y^T$か$Y^C$のどちらかのみ観測されるため、treatment efect $\tau$は直接測定できないことに注意.
しかし、**average treatment efect(ATE)**を推定する事ができる:

$$
ATE = E[\tau] = E[Y^T] - E[Y^C]
$$

ここで、$Z \in {0, 1}$はtreatmentのbinary indicator とし、$Z = 1$は治療を受けたことを、$Z = 0$は治療を受けなかったことをそれぞれ示すとする.
被験者に関連する共変量(covariates)は$X$で表し、例えば、treatment割り当て前の被験者の人口統計学的記録と過去の記録である.
$n$をindexとした$N$人の被験者を考える.
ここで、$S^T$と$S^C$を、**それぞれtreatmentを受ける被験者と受けない被験者の集合**を表すのに用いる.
直観的には、ATEは**2つの集合の平均結果の差**として推定できる.

$$
\hat{\tau} = \frac{1}{|S_T|} \sum_{n \in S_T} Y_n^T - \frac{1}{|S_C|} \sum_{n \in S_C} Y_n^C
\tag{1}
$$

**treatment が potential outcome とは無関係に被験者にランダムに割り当てられる場合**、すなわち$(Y^T ,Y^C)⊥Z$の場合、$N \rightarrow \infty$において $\hat{\tau}$ はほぼ確実にATEに収束する([31, Theorem 9.2]の証明参照).

**独立条件(independence condition)**$(Y^T,Y^C)⊥Z$は強い仮定なので、**代わりに共変量Xに$(Y^T,Y^C)$と$Z$の交絡因子がすべて含まれる** **条件付き独立(conditional independence)** $(Y^T,Y^C)⊥Z｜X$を考える [28]．
この条件付き独立性のもとで、以下の**inverse propensity scores(IPS)推定量**は、ATE の不偏推定量であることが知られている:

$$
\hat{\tau}_{IPS} = \frac{1}{N} \sum_{n \in S_T} \frac{Y_n^T}{e(X_n)} - \frac{1}{N} \sum_{n \in S_C} \frac{Y_n^C}{1 - e(X_n)}
\tag{2}
$$

ここで、$e(X_n) = p(Z_n = 1|X_n)$は**共変量Xを条件としたtreatment割り当て確率**であり、**傾向スコア(propensity score)**と呼ばれる[36].
しかし、IPSは推定値の分散が大きくなりがち.
これは、傾向スコアが小さいと、ある被験者の結果に対するウェイトが大きくなるためである.
これを改善するために、**自己正規化逆強度スコアリング（self-normalized inverse propensity scoring：SNIPS）**が提案されている[45].
これは **inverse propensity score の和で推定値を調整**するものである.

$$
\hat{\tau}_{SNIPS} = \frac{
    \sum_{n\in S_T} \frac{Y_n^T}{e(X_n)}
}{
    \sum_{n\in S_T} \frac{1}{e(X_n)}
}
- \frac{
    \sum_{n\in S_C} \frac{Y_n^C}{e(X_n)}
}{
    \sum_{n\in S_C} \frac{1}{e(X_n)}
}
\tag{3}
$$

独立条件$(Y^T,Y^C)⊥Z|X$の下で、$\hat{\tau}_{SNIPS}$ の推定量は$N \tau}_{infty$ のとき、ほぼ確実にATEに収束する.

#### 1.3.1.3. Uplift Estimates for Recommenders

本節では，前節で述べた因果推論frameworkをもとに，レコメンデーションによる Uplift の評価プロトコルを設計する．
**目標は新しい推薦モデル M の Uplift 性能を評価すること**である.
**現在導入されているモデル D の下での購入と推薦ログからなるオフラインデータセットがある**とする.
レコメンダーのUplift評価では、**treatment Zは推薦モデルDによる推薦**であり、$Y^T_{ui}=1$は**ユーザーuが商品iを推薦されたときに購入すること**を意味する.
Rは、Mがアイテムを推薦した場合に$R=1$となるような2値変数とする.
$E[\tau] = E[Y^T-Y^C|R=1]$を評価したい.
ここで、$p = E[Y^T|R=1]$、$p = E[Y^C|R=1]$を、Dによる実際の推薦がある場合とない場合の、Mが選んだアイテムの購入確率と定義する.
そして、 Uplift は、推薦による購買確率の上昇分、$p^T-p^C$と解釈できる.

評価したい新しい推薦モデルMによって生成されたユーザuに対する推薦リストを$L^M_u$とする.
$L^M_u$ に含まれるアイテムは、$L^D_u$ に**含まれるものと含まれないものがある**とする.
$L^{M \cap D}_u$は$L^M_u$ と$L^D_u$ の両方に含まれるアイテム、$L^{M \setminus D}_u$ は$L^M_u$ に含まれるが$L^D_u$ には含まれないアイテムであるとする.
したがって、式(1)は次のようになる.

$$
\hat{\tau}_{L_u^M} = \frac{1}{|L^{M \cap D}_u|}
\sum_{i \in L^{M \cap D}_u} Y_{ui}^T
- \frac{1}{|L^{M \setminus D}_u|} \sum_{L^{M \setminus D}_u} Y_{ui}^C
\tag{4}
$$

左の項と右の項はそれぞれ、$L_u^M$のアイテムを推薦した場合と推薦しなかった場合の購入確率である.

この指標を用いて、図1の推薦リストを評価した.
その結果を表1の最下段に示す.
この指標は総上昇量とよく一致しており，提案した指標は Uplift という観点でレコメンダーの評価を行うのに適していることがわかる．

また、式(3)からSNIPSによる Uplift の推定値を導き出すことができる.

$$
(\hat{\tau}_{L_u^M})_{SNIPS} = \frac{
    \sum_{i \in L_u^{M \cap D}} \frac{Y_{ui}^T}{e(X_{ui})}
}{
    \sum_{i \in L_u^{M \cap D}} \frac{1}{e(X_{ui})}
}
- \frac{
    \sum_{i \in L^{M \setminus D}_u} \frac{Y_{ui}^C}{1 - e(X_{ui})}
}{
    \sum_{i \in L^{M \setminus D}_u} \frac{1}{1 - e(X_{ui})}
}
\tag{5}
$$

推薦者の場合，Xui は過去の購入・推薦の記録，ユーザの属性，アイテムの内容などである．

モデル M の評価指標として，両推定量に対する全ユーザ U の平均をとる．

$$
\bar{\tau} = \frac{1}{|U|} \sum_{u \in U} \hat{\tau}_{L_u^M}
\\
\bar{\tau}_{SNIPS} = \frac{1}{|U|} \sum_{u \in U} (\hat{\tau}_{L_u^M})_{SNIPS}
\tag{6}
$$

本研究では，これらの指標を Uplift 性能の実測評価に用いる.
$\bar{\tau}$をUplift@N, $\bar{\tau}_{SNIPS}$を$Uplift_{SNIPS}@N$と呼ぶ. ($N = |L_u^M|$は推薦アイテムの個数)
本節で説明したプロトコルを用いて、**現在導入している推薦モデルDでの購入履歴や推薦履歴から、新しいモデルMの Uplift 性能を評価する**.

広告クリックの場合など、推薦がない場合の購入確率が無視できる場合は、式（4）、（5）の右項が消失する。この場合，式は従来の反実仮想的なofine評価[7, 25]に類似したものとなる.
我々の評価は、**推薦されなくても購入される可能性を考慮した拡張版**である.

また、式(5)は、傾向の推定に用いる共変量Xが$(Y^T,Y^C)$とZの間の**依存関係を解消するのに十分な情報を含んでいる(=全ての交絡因子が含まれている)**という仮定に依存する.
この仮定を保証することは難しいが、**実際にはUplift@NとUpliftSNIPS@Nの両方でモデル比較の結果が一致すれば**、評価に確信が持てるだろう.

### 1.3.2. Uplift に基づく 最適化手法:

前セクションで定義したTU、FU、TD、TDの4つのアイテムのうち、推薦時に Uplift をもたらすのはTUのみである.
しかし、この4つの分類を識別するためには、YTとYCの両方を観測する必要があり、本来は実現不可能.
このことは、モデルを学習させるための観測可能なグランドトゥルースがないことを意味する.
本節では、この問題を解決するために、Uplift 最適化手法を提案する.

#### 1.3.2.1. Classifcation of the Observations

前セクションではptential outcome の組み合わせに基づき，アイテムを4つの**隠れクラス(hidden classses)**に分類した.
ここでは，購入ログと推薦ログから，アイテムを**observable classesに分類**し，hiddenクラスと整合させる.
観測データセットでは、あるユーザと時間のインスタンスに対して、アイテムは推薦(R)か非推薦(NR)か、購入(P)か非購入(NP)かのどちらかである.
これにより、以下の4種のobservable classesが得られる:

![](https://camo.qiitausercontent.com/12f53f15777682946def39e4e73a579b47a6b685/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f32646666636563352d666565352d353837362d633238632d3064633466316535623134392e706e67)

- あるアイテムが勧められ、購入された(R-P): 観測されたアイテムのhidden classはTU or FU.
- 推薦されたが、購入されない(R-NP): 観測されたアイテムのhidden class はFD or TD.
- 推薦されなかったが、購入された(NR-P): 観察されたアイテムのhidden classは FU or TD.
- 推薦も購入もされなかった(NR-NP): 観察されたアイテムのhidden classは TU or FD.

$C_{class}$を、あるユーザu∈Uの$class \in {R-P, R-NP, NR-P, NR-NP}$ に含まれるアイテムの集合と定義する.
また、$I^+_u$と$I^-_u$を、そのユーザのpositiveとnegativeのアイテムの集合として定義する.
従来の精度ベースの最適化[14, 32, 35]では、以下のようにアイテム集合を定義していた. (implicit feedbackを想定してる...!)

- $I^+_u \sim C_{R-P} \cup C_{NR-P}$ (purchased item)
- $I^-_U \sim C_{R-NP} \cup C_{NR-NP}$ (non-purchased item)

本論文ではこのサンプリング方法がUplift的には最適でないことを主張し、**positive & negativeサンプルを再定義**する...!!
TUアイテムは Uplift をもたらすので、**TUアイテムを含むobservableクラスをポジティブと見なしたい**...!
したがって、（$C_{R-P} \cup C_{NR-NP}$）はpositiveアイテムのサンプリングとして妥当な選択となるはずである.
同じ理由で、$C_{R-NP}$ と $C_{NR-P}$ には TU アイテムが含まれないので、$I^-_u \sim C_{R-NP} \cup C_{NR-P}$となる.

しかし，提案したpositiveサンプルを利用するには，いくつかの問題がある.
ほとんどの購買ログは非常に疎であり(=NPが多い)、ほとんどのレコメンダーは推薦を少数に限定している(NR が大きい).
つまり、**$C_{NR-NP}$ のデータ数(論文ではcardinality=unique値の数みたいな?と表現してる)は他のobservable クラスよりはるかに大きく**、商品数の総和に近い.
消費者の購買力は限られているため、TUアイテムの数は全アイテム数よりもはるかに少ないと想定される.
従って、$C_{NR-NP}$のアイテムがTUに属する確率は低くなるはず...!(i.e. $C_{NR-NP}$の多くはFDって意味...??)(=以下の数式...!!)

$$
P(i \in TU | i \in C_{NR-NP}) = \frac{|TU \cap C_{NR-NP}|}{|C_{NR-NP}|}
\\
\approx \frac{|TU \cap C_{NR-NP}|}{|I|} < \frac{|TU|}{|I|} << 1
\tag{7}
$$

逆に、レコメンダーが一般的に売上を大きく向上させる(productによるのでは...???)ことを考えると[1]、$C_{R-P}$のアイテムが TU に属する可能性は相対的に(まあ確かにC\_{NR-NP}と比べると...!)低くないと推測される. したがって...

$$
P(i \in TU | i \in C_{R-P}) > P(i \in TU | i \in C_{NR-NP})
$$

以上のことから、$C_{NR-NP}$を完全にpositiveとみなすことはできない.
そこで、集合$C_{NR-NP}$からのアイテムがpositiveとしてサンプリングされる確率である(ハイパー?)パラメータ$\alpha$を提案する. これについては、次のサブセクションでさらに説明する.

#### 1.3.2.2. 提案されたサンプリング手法

## 1.4. どうやって有効だと検証した?

## 1.5. 議論はある？

## 1.6. 次に読むべき論文は？

## 1.7. お気持ち実装
