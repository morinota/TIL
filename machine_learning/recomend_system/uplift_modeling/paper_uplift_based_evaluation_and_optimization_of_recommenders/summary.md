# 1. Uplift-based Evaluation and Optimization of Recommenders

published date: 16 September 2019,
authors: Masahiro Sato, Janmajay Singh, Sho Takemori, Takashi Sonoda, Qian Zhang, and Tomoko Ohkuma
url(paper): https://dl.acm.org/doi/pdf/10.1145/3298689.3347018
(勉強会発表者: morinota)

---

## 1.1. どんなもの?

- 一般的な推薦システムの評価では、推薦された商品を購入することが"成功"であるとされる.
- しかし、**レコメンドされなくても購入されている可能性**があり、真に評価すべきは、"レコメンドによるユーザアクションの増加 = **Uplift**"である.
- しかし，ある時点の(user, item)ペアに対して，推薦がある場合 & ない場合の両方の状況を観測することは不可能であり、Upliftに基づく評価と最適化は困難.
- 本論文では、推薦システムにおける Uplift の新しい評価指標と最適化手法を提案する.
- 評価指標:
  - **因果推論の枠組み**を適用し，推薦システムを評価するための平均的なUpliftを推定する.
  - 本論文では，推薦システムにおける購買履歴と推薦履歴の両方を用いて，推薦がある場合とない場合のシミュレーションを行う．
  - これにより，新たに生成された推薦リストのUpliftを正当に評価することができる．
- 最適化手法:
  - 最適化のためには、Uplift-based アプローチに特化したポジティブサンプルとネガティブサンプルを定義する必要がある.
  - そのために、**購買履歴と推薦履歴から4つの分類**を導き出す.
  - そして，これら4つのクラス間の相対的な優先順位を高揚度の観点から導き出し，それを用いてUplift最適化のためのpoint-wise サンプリングと pair-wise サンプリングの手法を構築する．
  - また，3つの公共データセットを用いた実験により，提案された最適化手法がUpliftの改善に有効であることを実証している．

## 1.2. 先行研究と比べて何がすごい？

- Accuracy-basedの手法と比べて、Uplift性能の高い推薦モデルを学習可能.
- 既存のUpliftを考慮した推薦システムに比べて、Uplift性能に対して直接的にモデルを最適化している.(既存研究では一旦Accuracy-basedで学習させたのち、reranking的な事をしてるっぽい)

## 1.3. 技術や手法の肝は？

### 1.3.1. Uplift に基づく 評価指標:

レコメンダーは通常、推薦精度の観点から評価される.
レコメンダーは、推薦した商品がより多く購入されれば、他のレコメンダーより優れているとみなされる.
この評価手法をaccuracy-based evaluationと呼ぶ.
レコメンダーの精度の指標としてよく使われる"Precision"は、"the number of purchases / the number of recommendations"で定義される.
しかし、推薦がなくても、ユーザーがその商品を認知し、嗜好していれば、購入される可能性がある.
なので本論文では、リコメンダーは "Uplift" という観点で評価されることを目指す.

#### 1.3.1.1. Accuracy と Uplift の不一致 の話

"推薦がある場合"と"ない場合"の2つのケースを説明するために，因果推論[15, 28, 37]の **potential outcome** の概念を採用する．
$Y^T=1$, $Y^C=1$ は，それぞれ推薦されたケース と されないケース にアイテムが購入されることを示す.
**あるユーザに対するアイテムの Uplift $\tau$**は、$Y^T - Y^C$ である.
2つのシナリオ(推薦がある場合 & 推薦がない場合)におけるユーザの2つの行動(購入 & not購入)を考慮すると、ユーザにとって以下の4つのアイテムクラスが存在することになる.

- True Uplift (TU): $Y^T= 1$ 且つ $Y^C = 0$ -> $\tau = 1$.
  - 勧められれば購入されるが、勧められなければ購入されない.
- False Uplift (FU): $Y^T= Y^C = 1$, -> $\tau= 0$.
  - 推奨されるかどうかに関わらず購入される.
- True Drop (TD). $Y^T= 0$ 且つ $Y^C = 1$, -> $\tau = −1$.
  - 推奨されない場合は購入されるが、推奨される場合は購入されない.
- False Drop (FD). $Y^T = Y^C = 0$, -> $\tau= 0$.
  - 推奨されるかどうかに関わらず購入されない.

![](https://camo.qiitausercontent.com/a7306517e1ca5d6c2f6e8415c24384be4c5f488b/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f66613233356434662d313265642d353431652d626138642d3238626238333031613736392e706e67)

図1に示すように、レコメンドの Uplift と accuracy の違いを直感的に理解できるように、10個の推薦アイテムからなる4つのリストについて考えてみる.
ここで、オフラインデータセットとして、**現在導入しているレコメンダーの購入ログと推薦ログ(userに推薦したか否か)の両方を持つもの**を想定する.
TUは推薦された場合のみ購入され、TDは推薦されない場合のみ購入されることに注意.
その他のFUやFDの購入はレコメンデーションに依存しない.

![](https://camo.qiitausercontent.com/5c5403f92a913430a081e00b27e19ecee496c203/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f32653536646534612d343438612d363161372d373266642d6465663262613162393464652e706e67)

過去に10アイテム全てが推薦された場合に得られたであろう Uplift の合計は表1のようになる.
また、リスト上の全アイテムを推薦した場合と、過去ログで推薦されたアイテムのみ(??)を推薦した場合の2つの設定での Precision を記載する.
前者はレコメンダーの評価でよく用いられる設定である[9]．
後者はレコメンダーのオンライン性能を推定(??)するための先行研究[7, 25]で採用されている設定である.
これらのサンプルでは，**Precision と Uplift が逆の傾向を示している**．
つまり，**この Precision から，より高い Uplift を達成するための最適なモデルを選択することはできない**．
レコメンデーションのないアイテムを除外しても、この問題は解決されない.
また、推薦アイテム（図1の実線枠のアイテム）のみを用いて計算した後者の Precision は、すべてのリストで同じ値を示しており、やはり最適なモデルを選択することはできない.

このように、レコメンダーによるUplift を評価するためには、accuracy-based evaluationは適さない.
そこで、uplift-based evaluation のために設計された評価指標を採用する必要がある.
しかし，ある時間におけるユーザとアイテムのペアは$Y^T$か$Y^C$の**どちらかしか観測できない**ため，直接的にアップリフト$\tau = Y^T - Y^C$の総量を算出することはできない．
そこで、因果推論の枠組みを用いて、**average treatment effect** を推定する.

#### 1.3.1.2. 因果推論Framework

このサブセクションでは、因果推論フレームワーク [15, 28, 37] を紹介し、次のサブセクションでレコメンダーのUplift-based evaluationに適用する.
treatment efect $\tau$は、「treatmentの有無によるpotential outcomeの差」として定義される： $tau \equiv Y^T - Y^C$.
ただし$Y^T$か$Y^C$のどちらかのみ観測されるため、treatment efect $\tau$は直接測定できないことに注意.
しかし、**average treatment efect(ATE)**を推定する事ができる:

$$
ATE = E[\tau] = E[Y^T] - E[Y^C]
$$

ここで、$Z \in {0, 1}$はtreatmentのbinary indicator とし、$Z = 1$は治療を受けたことを、$Z = 0$は治療を受けなかったことをそれぞれ示すとする.
被験者に関連する共変量(covariates)は$X$で表し、例えば、treatment割り当て前の被験者の人口統計学的記録と過去の記録である.
$n$をindexとした$N$人の被験者を考える.
ここで、$S^T$と$S^C$を、**それぞれtreatmentを受ける被験者と受けない被験者の集合**を表すのに用いる.
直観的には、ATEは**2つの集合の平均結果の差**として推定できる.

$$
\hat{\tau} = \frac{1}{|S_T|} \sum_{n \in S_T} Y_n^T - \frac{1}{|S_C|} \sum_{n \in S_C} Y_n^C
\tag{1}
$$

**treatment が potential outcome とは無関係に被験者にランダムに割り当てられる場合**、すなわち$(Y^T ,Y^C)⊥Z$の場合、$N \rightarrow \infty$において $\hat{\tau}$ はほぼ確実にATEに収束する([31, Theorem 9.2]の証明参照).

**独立条件(independence condition)**$(Y^T,Y^C)⊥Z$は強い仮定なので、**代わりに共変量Xに$(Y^T,Y^C)$と$Z$の交絡因子がすべて含まれる** **条件付き独立(conditional independence)** $(Y^T,Y^C)⊥Z｜X$を考える [28]．
この条件付き独立性のもとで、以下の**inverse propensity scores(IPS)推定量**は、ATE の不偏推定量であることが知られている:

$$
\hat{\tau}_{IPS} = \frac{1}{N} \sum_{n \in S_T} \frac{Y_n^T}{e(X_n)} - \frac{1}{N} \sum_{n \in S_C} \frac{Y_n^C}{1 - e(X_n)}
\tag{2}
$$

ここで、$e(X_n) = p(Z_n = 1|X_n)$は**共変量Xを条件としたtreatment割り当て確率**であり、**傾向スコア(propensity score)**と呼ばれる[36].
しかし、IPSは推定値の分散が大きくなりがち.
これは、傾向スコアが小さいと、ある被験者の結果に対するウェイトが大きくなるためである.
これを改善するために、**自己正規化逆強度スコアリング（self-normalized inverse propensity scoring：SNIPS）**が提案されている[45].
これは **inverse propensity score の和で推定値を調整**するものである.

$$
\hat{\tau}_{SNIPS} = \frac{
    \sum_{n\in S_T} \frac{Y_n^T}{e(X_n)}
}{
    \sum_{n\in S_T} \frac{1}{e(X_n)}
}
- \frac{
    \sum_{n\in S_C} \frac{Y_n^C}{e(X_n)}
}{
    \sum_{n\in S_C} \frac{1}{e(X_n)}
}
\tag{3}
$$

独立条件$(Y^T,Y^C)⊥Z|X$の下で、$\hat{\tau}_{SNIPS}$ の推定量は$N \tau}_{infty$ のとき、ほぼ確実にATEに収束する.

#### 1.3.1.3. Uplift Estimates for Recommenders

本節では，前節で述べた因果推論frameworkをもとに，レコメンデーションによる Uplift の評価プロトコルを設計する．
**目標は新しい推薦モデル M の Uplift 性能を評価すること**である.
**現在導入されているモデル D の下での購入と推薦ログからなるオフラインデータセットがある**とする.
レコメンダーのUplift評価では、**treatment Zは推薦モデルDによる推薦**であり、$Y^T_{ui}=1$は**ユーザーuが商品iを推薦されたときに購入すること**を意味する.
Rは、Mがアイテムを推薦した場合に$R=1$となるような2値変数とする.
$E[\tau] = E[Y^T-Y^C|R=1]$を評価したい.
ここで、$p = E[Y^T|R=1]$、$p = E[Y^C|R=1]$を、Dによる実際の推薦がある場合とない場合の、Mが選んだアイテムの購入確率と定義する.
そして、 Uplift は、推薦による購買確率の上昇分、$p^T-p^C$と解釈できる.

評価したい新しい推薦モデルMによって生成されたユーザuに対する推薦リストを$L^M_u$とする.
$L^M_u$ に含まれるアイテムは、$L^D_u$ に**含まれるものと含まれないものがある**とする.
$L^{M \cap D}_u$は$L^M_u$ と$L^D_u$ の両方に含まれるアイテム、$L^{M \setminus D}_u$ は$L^M_u$ に含まれるが$L^D_u$ には含まれないアイテムであるとする.
したがって、式(1)は次のようになる.

$$
\hat{\tau}_{L_u^M} = \frac{1}{|L^{M \cap D}_u|}
\sum_{i \in L^{M \cap D}_u} Y_{ui}^T
- \frac{1}{|L^{M \setminus D}_u|} \sum_{L^{M \setminus D}_u} Y_{ui}^C
\tag{4}
$$

左の項と右の項はそれぞれ、$L_u^M$のアイテムを推薦した場合と推薦しなかった場合の購入確率である.

この指標を用いて、図1の推薦リストを評価した.
その結果を表1の最下段に示す.
この指標は総上昇量とよく一致しており，提案した指標は Uplift という観点でレコメンダーの評価を行うのに適していることがわかる．

また、式(3)からSNIPSによる Uplift の推定値を導き出すことができる.

$$
(\hat{\tau}_{L_u^M})_{SNIPS} = \frac{
    \sum_{i \in L_u^{M \cap D}} \frac{Y_{ui}^T}{e(X_{ui})}
}{
    \sum_{i \in L_u^{M \cap D}} \frac{1}{e(X_{ui})}
}
- \frac{
    \sum_{i \in L^{M \setminus D}_u} \frac{Y_{ui}^C}{1 - e(X_{ui})}
}{
    \sum_{i \in L^{M \setminus D}_u} \frac{1}{1 - e(X_{ui})}
}
\tag{5}
$$

推薦者の場合，Xui は過去の購入・推薦の記録，ユーザの属性，アイテムの内容などである．

モデル M の評価指標として，両推定量に対する全ユーザ U の平均をとる．

$$
\bar{\tau} = \frac{1}{|U|} \sum_{u \in U} \hat{\tau}_{L_u^M}
\\
\bar{\tau}_{SNIPS} = \frac{1}{|U|} \sum_{u \in U} (\hat{\tau}_{L_u^M})_{SNIPS}
\tag{6}
$$

本研究では，これらの指標を Uplift 性能の実測評価に用いる.
$\bar{\tau}$をUplift@N, $\bar{\tau}_{SNIPS}$を$Uplift_{SNIPS}@N$と呼ぶ. ($N = |L_u^M|$は推薦アイテムの個数)
本節で説明したプロトコルを用いて、**現在導入している推薦モデルDでの購入履歴や推薦履歴から、新しいモデルMの Uplift 性能を評価する**.

広告クリックの場合など、推薦がない場合の購入確率が無視できる場合は、式（4）、（5）の右項が消失する。この場合，式は従来の反実仮想的なofine評価[7, 25]に類似したものとなる.
我々の評価は、**推薦されなくても購入される可能性を考慮した拡張版**である.

また、式(5)は、傾向の推定に用いる共変量Xが$(Y^T,Y^C)$とZの間の**依存関係を解消するのに十分な情報を含んでいる(=全ての交絡因子が含まれている)**という仮定に依存する.
この仮定を保証することは難しいが、**実際にはUplift@NとUpliftSNIPS@Nの両方でモデル比較の結果が一致すれば**、評価に確信が持てるだろう.

### 1.3.2. Uplift に基づく 最適化手法:

前セクションで定義したTU、FU、TD、TDの4つのアイテムのうち、推薦時に Uplift をもたらすのはTUのみである.
しかし、この4つの分類を識別するためには、YTとYCの両方を観測する必要があり、本来は実現不可能.
このことは、モデルを学習させるための観測可能なグランドトゥルースがないことを意味する.
本節では、この問題を解決するために、Uplift 最適化手法を提案する.

#### 1.3.2.1. Classifcation of the Observations

前セクションではptential outcome の組み合わせに基づき，アイテムを4つの**隠れクラス(hidden classses)**に分類した.
ここでは，購入ログと推薦ログから，アイテムを**observable classesに分類**し，hiddenクラスと整合させる.
観測データセットでは、あるユーザと時間のインスタンスに対して、アイテムは推薦(R)か非推薦(NR)か、購入(P)か非購入(NP)かのどちらかである.
これにより、以下の4種のobservable classesが得られる:

![](https://camo.qiitausercontent.com/12f53f15777682946def39e4e73a579b47a6b685/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f32646666636563352d666565352d353837362d633238632d3064633466316535623134392e706e67)

- あるアイテムが勧められ、購入された(R-P): 観測されたアイテムのhidden classはTU or FU.
- 推薦されたが、購入されない(R-NP): 観測されたアイテムのhidden class はFD or TD.
- 推薦されなかったが、購入された(NR-P): 観察されたアイテムのhidden classは FU or TD.
- 推薦も購入もされなかった(NR-NP): 観察されたアイテムのhidden classは TU or FD.

$C_{class}$を、あるユーザu∈Uの$class \in {R-P, R-NP, NR-P, NR-NP}$ に含まれるアイテムの集合と定義する.
また、$I^+_u$と$I^-_u$を、そのユーザのpositiveとnegativeのアイテムの集合として定義する.
従来の精度ベースの最適化[14, 32, 35]では、以下のようにアイテム集合を定義していた. (implicit feedbackを想定してる...!)

- $I^+_u \sim C_{R-P} \cup C_{NR-P}$ (purchased item)
- $I^-_U \sim C_{R-NP} \cup C_{NR-NP}$ (non-purchased item)

本論文ではこのサンプリング方法がUplift的には最適でないことを主張し、**positive & negativeサンプルを再定義**する...!!
TUアイテムは Uplift をもたらすので、**TUアイテムを含むobservableクラスをポジティブと見なしたい**...!
したがって、（$C_{R-P} \cup C_{NR-NP}$）はpositiveアイテムのサンプリングとして妥当な選択となるはずである.
同じ理由で、$C_{R-NP}$ と $C_{NR-P}$ には TU アイテムが含まれないので、$I^-_u \sim C_{R-NP} \cup C_{NR-P}$となる.

しかし，提案したpositiveサンプルを利用するには，いくつかの問題がある.
ほとんどの購買ログは非常に疎であり(=NPが多い)、ほとんどのレコメンダーは推薦を少数に限定している(NR が大きい).
つまり、**$C_{NR-NP}$ のデータ数(論文ではcardinality=unique値の数みたいな?と表現してる)は他のobservable クラスよりはるかに大きく**、商品数の総和に近い.
消費者の購買力は限られているため、TUアイテムの数は全アイテム数よりもはるかに少ないと想定される.
従って、$C_{NR-NP}$のアイテムがTUに属する確率は低くなるはず...!(i.e. $C_{NR-NP}$の多くはFDって意味...??)(=以下の数式...!!)

$$
P(i \in TU | i \in C_{NR-NP}) = \frac{|TU \cap C_{NR-NP}|}{|C_{NR-NP}|}
\\
\approx \frac{|TU \cap C_{NR-NP}|}{|I|} < \frac{|TU|}{|I|} << 1
\tag{7}
$$

逆に、レコメンダーが一般的に売上を大きく向上させる(productによるのでは...???)ことを考えると[1]、$C_{R-P}$のアイテムが TU に属する可能性は相対的に(まあ確かにC\_{NR-NP}と比べると...!)低くないと推測される. したがって...

$$
P(i \in TU | i \in C_{R-P}) > P(i \in TU | i \in C_{NR-NP})
$$

以上のことから、$C_{NR-NP}$を完全にpositiveとみなすことはできない.
そこで、集合$C_{NR-NP}$からのアイテムがpositiveとしてサンプリングされる確率である(ハイパー?)パラメータ$\alpha$を提案する. これについては、次のサブセクションでさらに説明する.

#### 1.3.2.2. 提案されたサンプリング手法

レコメンダーモデルの最適化手法は，一般的にポイントワイズ法[11, 14, 32]とペアワイズ法[35, 43]の2つに分類される．このサブセクションでは，アップリフトのためのポイントワイズ（$ULO_{point}$）とペアワイズ（$ULO_{pair}$）の最適化手法を提案する．

前サブセクションより、$C_{R-P}$のアイテムは他のクラスのアイテムよりも相対的に優れている(TUアイテムを最も含んでいそう...!!)ので，正のラベルを割り当てる．
逆に，$C_{NR-P}$と$C_{R-NP}$のアイテムは相対的に悪いので(i.e. TUアイテムを確実に含んでいない...!!)，負のラベルが割り当てられる.
$C_{NR-NP}$のアイテムは確率αで正、確率1 - αで負となる.

更に各クラス毎に観測データ数が異なる為、stratified sampling(層別サンプリング)を行う.

このとき、**購入済みアイテムからのサンプリングの比率**を表すパラメータ$\gamma_{P}$を導入する.
このような未購入アイテムのダウンサンプリングは、implicit feedbackデータでよく用いられる手法であり[11]、未購入アイテムのダウンウェイト(=implicit ALSも、未購入アイテムのconfidenceを最も低くしてるし...!!)に相当する[14, 32].
同様に、は推薦アイテムからのサンプリングの比率を表すパラメータ$\gamma_{R}$を導入する

- ex)
  - $C_{R-P}$からサンプリングするアイテムの比率: $\gamma_{R} \cdot \gamma_{P}$
  - $C_{NR-NP}$からサンプリングするアイテムの比率: $(1 -\gamma_{R}) \cdot (1- \gamma_{P})$

pair-wise最適化では、正と負のサンプルを同時に選択する.
$C_{R-P} \cup C_{NR-NP}$からは確率αで, C*{R-P}からは確率1-αで，正サンプルを選択する.
負のサンプルは$C*{NR-P}$及び$C\_{R-NP}$から選択する.

各アルゴリズムの詳細は、アルゴリズム1、2にて説明する.

$r_{ui}$ は u-iペアのラベル、L は損失関数、η は学習率、λ は正則化係数である。学習には確率的勾配降下を用いる.
各点またはペアに関連するパラメータ$\Theta$は反復毎に更新される.
point^wise最適化の損失関数にはロジスティック損失を用いる.

$$
L_{point}^{ll} = - (r_{ui} \log(\sigma(\hat{x}_{ui})) + (1 - r_{ui}) \log(1 - \sigma(\hat{x}_{ui}))).
\tag{9}
$$

推薦モデルの予測値 $\hat{x}_{ui}$ はシグモイド関数を用いてラベル予測に変換される.
また、pair-wise最適化にはBayesian Personalized Ranking (BPR) Loss [35]を用いる.

$$
L_{pair}^{bpr} = - \log(\sigma(\hat{x}_{ui} - \hat{x}_{uj}))
\tag{10}
$$

ここで、iは正サンプル、jは負サンプルである.

いずれの学習においても、パラメータ$\Theta$のoverfitting を防ぐため、L2正則化項$Omega = ||Theta||^2_2$ を加える.
$\hat{x}_{ui}$にはMatrix Factorization (MF) [22]を使用する.
アルゴリズム1 & 2で学習させる事で MF を改良したものを **UpLift-optimized Regularized MF (ULRMF)** および **UpLift-optimized BPR(ULBPR)**と呼ぶ.

## 1.4. どうやって有効だと検証した?

以下の3つのResearch Questionを解決する為に実験を行っている.

- RQ1: Uplift-based の 推薦モデル は他の既存手法と比較してどのようなパフォーマンスを示すか？
- RQ2: Uplift-based の最適化は、どのような性質を持つか？(Accuracy-basedと比較して.)
- RQ3: 従来(Accuracy-based)の 推薦手法 と Uplift-based の 推薦手法 では、推薦されるアイテムはどのように異なるのか？

三種類のオープンソースのデータセット(Dunnhumby, Tafeng, Xing)を用いている.

- Dunnhumby: 小売店での購入履歴とpromotion履歴.
- Tafeng: 中国の食料品店からの価格情報を含む購入ログ. 商品の割引率が0.1以上あれば、その商品を"推薦されたアイテム"とみなす.
- Xing: オンライン求職サイトにおけるユーザのinteractionログ.
  - クリック、ブックマーク、応募というpositiveな user interaction を "購入されたアイテム"とみなす.
  - impressionログ = ユーザに表示されたアイテムを"推薦されたアイテム"とみなす.
  - (↑のようなinteractionログの分け方であれば、NPでも使えるかも...??)

### 1.4.1. EvaluationのProtocols

提案する Uplift@N と UpliftSNIPS@N を用いて、N =10、30、10010でそれぞれの手法のUplift 性能を評価した. また、参考として Precision@30 (Accuracy-basedの評価指標)も測定した.

- validation phase: 1から(t_d - 2 t_e)までの期間でモデルを学習し、(t_d - 2 t_e + 1)から(t_d - t_e)までの期間でモデルを評価する.
- test phase: (t_e +1)から(t_d − t_e)までの期間でモデルを学習し、(t_d − t_e + 1)から t_d までの期間でモデルを評価する.

### 1.4.2. compareするmethod

比較対象は以下の手法(元となる推薦モデル=$\hat{y}_{ui}$を予測するモデルは、全てMatrix Factorization).

- Accuracy-basedの推薦手法:
  - RMF [14, 32] 12: 正則化 MF を精度よく学習させたもの．
  - BPR [35]: 精度に基づく BPR ロスを用いて学習させた MF．
- 既存研究のUplift-Basedの手法:
  - RecResp [40]: ユーザとアイテムに依存した推薦のためのバイアス項を持つMF．
  - CausE [3]: レコメンデーションがある場合とない場合の2つのMFを合同で学習させる．
  - CausE-Prod [3]: CausE の変形版で，2つのMFに共通のユーザ係数を持つ．
- 本論文で提案された、Uplift-Basedの手法:
  - ULRMF: 提案するULOpointで学習させたMF．
  - ULBPR: 提案するULOpairで学習させたMF．

RMF と BPR は従来のAccuracy-basedの最適化, すなわち $C_{R-P} \cup C_{NR-P}$ を positive サンプルとして学習させるもの.
RecRespとCausEは，Uplift を対象とした推薦手法. これらの手法は，[40]にあるように，推薦がある場合とない場合の購買確率の差分を用いて Uplift を予測し，top-N 推薦に用いる.
これらの手法は，一旦，Accuracy-Basedの手法でモデルを学習させ、それを用いて間接的にUpliftの度合いを推薦アイテムリストを調整する(?)．
本論文の提案手法である ULRMF と ULBPR のみ、直接的にUpliftに基づいてモデルが最適化されている.

## 1.5. 議論はある？

### 1.5.1. 結果1: Performance Comparison (RQ1)

![](https://camo.qiitausercontent.com/8317c78cce1b17b33da5043cef72b2de983bb6f6/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f61336664663566652d353863392d663561662d386632382d3939643361656430663534392e706e67)

table 4 は、各手法のUplift性能の評価指標の結果である. その結果、以下のような知見が得られた.

- 提案手法(ULRMFとULBPR)はほとんどの場合，Uplift@NとUpliftSNIPS@Nで最良のスコアを達成する.
- 精度ベースの手法(RMFとBPR)はPrecisionで最も良い結果を出すが，ほとんどの場合，他の手法よりもupliftのmetricsで悪い結果を出す.
- Upliftを対象とした手法(RecResp，CausE, 提案手法)は，RMFやBPRを上回る傾向にある. このことは，我々のUplift metrics(Uplift@NとUpliftSNIPS@N)が期待通りのuplift性能の向上を計測できることを意味している.

### 1.5.2. 結果2: Uplift-based Optimization Properties (RQ2)

![](https://camo.qiitausercontent.com/1d14777cd31f9bcded4ad9ee70146f007bb592e3/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f63646339313139382d316634382d373064322d343964322d6665303439636133316638312e706e67)

図2-aは、学習曲線(横軸=iteration回数, 縦軸=Uplift性能の評価指標)の推移.

- Uplift@10はiteration回数が増えるにつれて増加する. ULBPR の学習曲線は ULRMF よりも安定する傾向がある.
- ULRMF と ULBPR は RMF と BPR よりも収束が速く、計算時間の点で 提案手法のScalability があることがわかる.

図2-bは、学習データに使用するユニークアイテム数 - Uplift性能評価指標の推移.

- Uplift性能において、いずれの条件においてもULBPRはBPRを上回った.

図3は, 提案手法のハイパーパラメータa(NR-NPをpositiveラベルとみなす確率) と Uplift性能評価指標の推移.

- 最適なαは1.0以下(あれ?当たり前じゃない...?)であり、3.1節で述べたNR-NPを正と負の中間として扱うという主張が支持されている.

![](https://camo.qiitausercontent.com/34b0a304a47e5f32aaaad470e0d694a4d41ad7a8/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f32353639373964662d646633392d633330352d656332662d3136373633386434376134392e706e67)

提案手法はR-Pをpositive, NR-PPをnegativeとして扱う. 一方でaccuracy-basedの手法ではどちらもpositiveとして扱う. この違いを見るために，観測可能な4つのクラスにおける推薦アイテムの分布を確認した(表5).

- ULRMFとULBPRは、推薦がなくても購入されるNR-Pクラスの推薦を減少させることに成功している.
- また、R-NP比も減少することで、結果が出ないレコメンドを回避することができた.
- さらに，R-P比とR-NP比の和(推薦ログに含まれるアイテムの割合に等しい)は，ULRMFとULBPRではRMFとBPRと比べて高くならない.
  - このことから、提案された最適化手法は、新モデルMの最適化の方向として、現在導入されているモデルDの推薦方針を模倣するような結果になっていない事がわかる.(推薦されたアイテムが更に推薦されやすくなる、ようなフィードバックループを回避するような方向に学習できているって事...??)

### 1.5.3. 結果3: Trends of the Recommended Items (RQ3)

**accuracy-basedの最適化とuplift-basedの最適化の推薦結果の違い**を直感的に理解するために、DunnhumbyデータセットにおいてRMFとULRMFがよく推薦するアイテム群が表6に示されている.
(parentheses braket内の数字は、purchaseログにおける人気ランク.)

![](https://camo.qiitausercontent.com/5c23db80bae182840d0d0a1ac77ca739cb5d30c3/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313639373237392f66663633396263332d303862652d653564322d363261332d3063666165663466373161632e706e67)

RMFは人気のあるアイテムを推奨する傾向があるのに対し、ULRMFは人気のあるアイテムを重視せずに推薦している. ULRMFでよく推薦されるアイテムには、パスタソースや温めるだけの料理など、**衝動買い(=オススメした事によりユーザが買ってしまうようなアクション...!)**を誘発するようなアイテムが含まれている.

## 1.6. 次に読むべき論文は？

## 1.7. お気持ち実装
