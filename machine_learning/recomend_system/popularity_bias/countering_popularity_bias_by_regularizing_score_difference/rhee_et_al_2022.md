## 0.1. link

https://dl.acm.org/doi/abs/10.1145/3523227.3546757
https://dl.acm.org/doi/pdf/10.1145/3523227.3546757?casa_token=3kkYhpFdUDkAAAAA:zipZZnov8a0YaVfYEm7PKzszWsmMqV31imC5L6xAW_jwgKe74UF4d-xifHIa3b5IUr43CBTSuoN2KIXj

## 0.2. title

Countering Popularity Bias by Regularizing Score Differences

## 0.3. abstruct

Recommendation system often suffers from popularity bias. Often the training data inherently exhibits long-tail distribution in item popularity (data bias). Moreover, the recommendation systems could give unfairly higher recommendation scores to popular items even among items a user equally liked, resulting in over-recommendation of popular items (model bias). In this study we propose a novel method to reduce the model bias while maintaining accuracy by directly regularizing the recommendation scores to be equal across items a user preferred. Akin to contrastive learning, we extend the widely used pairwise loss (BPR loss) which maximizes the score differences between preferred and unpreferred items, with a regularization term that minimizes the score differences within preferred and unpreferred items, respectively, thereby achieving both high debias and high accuracy performance with no additional training. To test the effectiveness of the proposed method, we design an experiment using a synthetic dataset which induces model bias with baseline training; we showed applying the proposed method resulted in drastic reduction of model bias while maintaining accuracy. Comprehensive comparison with earlier debias methods showed the proposed method had advantages in terms of computational validity and efficiency. Further empirical experiments utilizing four benchmark datasets and four recommendation models indicated the proposed method showed general improvements over performances of earlier debias methods. We hope that our method could help users enjoy diverse recommendations promoting serendipitous findings. Code available at https://github.com/stillpsy/popbias.

![](https://developers.cyberagent.co.jp/blog/wp-content/uploads/2022/10/popularity_bias_fig1-768x296.jpg)

# 1. Introduction

Recommendation systems are used in many domains such as ecommerce, movie, and music [4, 5, 19]. Often recommendation systems learn the user preference from the implicit feedback information such as clicks, purchase, and item consumption [17, 26, 42]. Meanwhile, the systems are prone to popularity bias, which can come in many forms [2, 8, 9, 29]. From the data side, the user-item feedback data shows long tail distribution in item frequency with most interaction focused on small number of popular items (data bias) [2, 25]. From the model side, the recommendation systems, trained on such data, often give higher recommendation scores to more popular items even among items equally liked by a user, resulting in overrecommending popular items (model bias) [3, 34, 42]. Furthermore, such biased recommendations to users could form a feedback loop which may result in filter bubble or echo chamber [6, 12, 22, 23].

These varied forms of popularity bias often require individual approaches and solutions. The data bias could be solved in the data collection stage by controlling exposure bias [8, 9]. On the other hand, the model bias is a result of incorrect model training of the collected data [34, 42]. Hence the model engineer needs to correctly train the model to give fair recommendations across all the items the user liked, instead of giving prioritized recommendations of popular items. In this work, we focus on the solution of the model bias. Our effort can be combined with additional solutions for data bias and feedback loop to systematically eliminate popularity bias.

A variety of methods have been proposed to tackle the popularity bias. These methods include inverse propensity weighting (IPW) [16, 27, 29], causal intervention [34, 36, 38, 40], and reranking [2, 37]. IPW aims to produce unbiased model prediction by weighting the imbalanced data with propensity weights [16, 27, 29]. Causal intervention methods attempt to model and remove the causal effect that item popularity has on the recommendation score [34, 36, 38]. Reranking methods apply post-hoc ranking adjustment to the recommendation result [2, 37].

However, these methods have limitations in solving model bias since they often suffer the accuracy-debias tradeoff. These methods adjust the biased recommendation scores (or ranks) in proportion to item popularity; scores of popular items are discounted and unpopular items are boosted [1, 2, 8, 27, 34, 36]. Such mechanism may sacrifice the scores of positive items to uplift the scores of negative items [1, 2, 8, 34, 36], potentially harming accuracy. Often such methods require extensive hyperparameter tuning to find an appropriate balance of accuracy and debias performance [34, 36, 42]. A debias method that does not involve collective score/rank adjustment can be a good alternative to avoid potential loss in accuracy, and computational cost.

To this end, we propose a novel debias method to reduce model bias. We refer to earlier studies of contrastive learning [11, 21], and propose to add a regularization term to the loss function to aid the recommendation system to predict equal recommendation scores across positive items for each user. Specifically, we extend the Bayesian Personalized Ranking Loss [26] which maximizes the score differences between positive and negative items, with a regularization term which minimizes the score differences within positive and negative items, respectively. As a result, the recommendation system model will predict equal scores across positive items, solving the model bias. Simultaneously, the model will contrast item scores for positive and negative items, maintaining high accuracy. Few prior research took the similar approach of regulating the scores only for positive items [7, 42]. These studies introduced a regularization term penalizing the size of the Pearson correlation between item popularity and scores of positive items. Our method shares similar motivation while improving performance and cost efficiency.

Our research proceeds as follows. We propose to extend the BPR loss with a new regularization term to achieve both high accuracy and high debias performance. To systematically test the effectiveness of our method, we design a synthetic experiment. We design a data with explicit popularity bias which induces model bias when training a recommendation system using the baseline BPR loss. We then apply our regularization term and analyze its performance. As a result, our method outperformed earlier debias methods in terms of accuracy and debias performance. Additional comparison shows our method has advantages over the earlier methods in terms of computational validity and efficiency.

We further conduct extensive empirical experiments utilizing 4 benchmark datasets [10, 13, 31–33], and 4 recommendation models: MF [26], NeuCF [15], NGCF [35], and LightGCN [14]. The proposed method showed high accuracy and debias performance across MF, NGCF, LightGCN models in 3 of the 4 datasets, whereas the earlier methods failed to show consistent performance. Hence, the proposed method showed general improvement over the earlier methods.

The contributions of our work are summarized as follows:

- We propose a novel method to reduce model bias, by extending the existing BPR loss with a new regularization term which regulates the score differences within positive and negative items, respectively.
- The proposed method shows high debias performance, minimal sacrifice in accuracy, with no additional training.
- Quantitative and qualitative evaluation using a synthetic data is conducted to show advantages over earlier debias methods.
- Empircal experiments using 4 benchmark datasets and 4 recommendation system models show the proposed method generally improved over earlier debias methods.

# 2. Related Work

## 2.1. Popularity Bias

Various studies have been conducted on popularity bias. First, the popularity bias can be understood in terms of data (data bias) [2, 8, 25, 36, 38]. The user-item interaction data usually shows a long tail distribution in item popularity [2, 25]. This may be due to selective item exposure [8, 9], as well as unequal preference of the users [38, 39].

On the other hand, the popularity bias also can be understood in terms of the recommendation system model (model bias). Some studies addressed how the recommendation system trained on imbalanced data can amplify the bias in the data by over-recommending popular items than the data warrants [3, 7, 34, 38, 42]. In particular, [42] defined such model bias as the degree the model gives higher scores to more popular positive items among positive items equally liked by a user. Such model bias results in the recommendation system not being able to produce personalized recommendation, which could harm user experience. In addition, some studies explore how over-recommendation can cause a feedback loop resulting in filter bubble or echo chamber [12, 22, 23].

The varied forms of popularity bias require individual approaches and solutions. Since the data bias is caused by external selection bias [8, 9], a solution should be applied by controlling external factors in the data collection stage; simple debiasing of the collected data may distort the user preference shown in the data [38, 39]. In contrast, the model bias is a computational problem where the model fails to learn the user preference from the collected data. Hence proper model training is required to remedy such bias, and allow the model to recommend items fairly across items of different popularity. In addition, some researchers investigated solving the feedback loops or the filter bubble, which takes place in the online setting where the live system gives continual recommendation [6, 22].

Among various types of popularity biases, our work mainly focuses on the model bias.

## 2.2. Countering Popularity Bias

Various methods were proposed to address popularity bias, such as the inverse propensity weighting (IPW) [16, 27], causal intervention [34, 36, 38, 40], and reranking [2, 37]. IPW weights each training instance with the inverse of the item popularity to produce unbiased model prediction [16, 27, 29]. Causal intervention often assumes a causal graph which models the effect item popularity has on the recommendation score, and computes counterfactual recommendation scores by removing such effect [34, 36, 38]. Some works utilizes unbiased data to better learn the item embeddings [40]. Reranking methods conduct ranking adjustment of item recommendation list [2, 37].

The above debias methods shares the logic of lowering the recommendation score of popular items and lifting those of unpopular items. For instance the IPW method lowers the training weights of popular items while lifting those of unpopular items [27, 29]. The causal intervention methods often removes the causal effect of item popularity by discounting the recommendation scores proportionally to the item popularity [34, 36, 38]. The reranking method explicitly boosts the rank of the tail items while sacrificing the rank of popular items [2, 37]. Although these measures could alleviate the model bias by balancing the recommendation scores, it holds the risk of overly penalizing the scores of positive items while compensating those of negative items. Hence accuracy may be sacrificed as part of the debias process.

A debias method that regulates the recommendation score imbalance only for positive items may lead to successful debias with minimal sacrifice in accuracy. Few studies took such approach [7, 42]. The studies similarly proposed a method to regulate the Pearson correlation of item popularity and item score for positive items such that the recommendation scores can be independent of item popularity. However, the approach has two limitations: regularizing the correlation coefficient may not necessarily lead to independence, and computation is costly.

Our work overcomes limitations of the earlier methods and proposes a cost-efficient debias method to reduce model bias while maintaining accuracy by regularizing recommendation scores.

## 2.3. Contrastive Learning

Metric learning is a branch of machine learning which utilizes the distance or similarity of the training data to enhance learning [21]. A related approach is contrastive learning which compares and contrasts training instances for efficient representation learning [18]. Contrastive loss [11], triplet loss [28], infoNCE loss [24] are representative loss functions in contrastive learning which follow the basic principles - 1) learning similar representations for instances of the same category, and 2) learning contrasting representations for instances of different categories.

A similar contrastive approach is used in the training of recommendation system models. The pairwise ranking loss [26] such as the Bayesian Personalized Ranking (BPR) loss is often used to train the relative ranking of positive and negative items by contrasting the recommendation scores. However, unlike the contrastive learning losses, the BPR loss does not include a term to minimize the score differences within positive and negative items, respectively. Extending the BPR loss to include a term to minimize the score differences within positive(negative) items can help in reducing the model bias while maintaining accuracy.

Although a few works studied contrastive learning in the context of recommendation systems [20, 41], these studies focused on the sampling scheme of negative items, and did not discuss how minimizing the positive and negative scores, respectively, can reduce the model bias. One study outside the context of contrastive learning took a similar approach of penalizing the score differences of the average recommendation scores across different item groups to promote fairness [43]. However, this study did not suggest a method to reduce the score differences at the individual item level. To the best of our knowledge, our work is the first to introduce a method to reduce model bias while maintaining accuracy with a regularization term that minimizes the score differences within positive and negative items, respectively.

# 3. Preliminaries

## 3.1. Implicit Recommendation System and Bayesian Personalized Ranking Loss

Implicit recommendation systems are trained with implicit user feedback [14, 17, 26]. Implicit data consists of tuple (u,i) meaning that a user $u \in U$ consumed an item $i \in I$. We denote the set of items user u consumed as $Po_{s_u}$, since the items implicitly show the positive preference of u. In contrast, we denote the items that u did not consume as $Ne_{g_u}$. The recommendation system learns the preference of the users to produce a recommendation score $\hat{y_{ui}}$ based on the predicted preference useru has on itemi. Subsequently, the item with the highest predicted score is recommended to the user. Implicit recommendation systems are usually trained using the pairwise ranking loss such as the Bayesian Personalized Ranking (BPR) loss [14, 15, 26]. The BPR loss is given as Equation (1), where $\sigma()$ refers to the sigmoid function. The L2 regularization term is omitted for brevity.

$$
Loss_{BPR} = - \sum_{u \in U} \sum_{p \in Po_{s_u}, n \in Ne_{g_u}} \log \sigma(\hat{y}_{u,p} - \hat{y}_{u,n})
\tag{1}
$$

During training, positive and negative items of each user are paired and the recommendation system learns to maximize the score differences between the paired items. The accuracy of the recommendation system is evaluated based on how accurately the model scores positive items higher than negative items. Common accuracy metrics such as Hit, NDCG [17] is computed based on the ranking of the positive test item relative to the negative test items.

## 3.2. Popularity Bias in Model Prediction (Model Bias)

The data used to train recommendation systems usually exhibits long-tail distribution in item popularity [2, 25]. Often, the recommendation systems trained with such data produce higher recommendation scores to more popular items even among items equally liked by a user [3, 34, 42]. The tendency that recommendation systems show popularity bias in model prediction is what we refer to as model bias.

A few prior research suggested metrics to measure the model bias [7, 42]. One study [42] suggested a metric computing the popularityrank correlation for items (PRI). PRI computes the Spearman rank correlation coefficient (SRC) of item popularity and the average ranking position, conditioned on the positive items. The PRI is given as in Equation (2):

$$
\text{PRI} = - \text{SRC} (\text{popularity} (I), \text{ave_rank} (I))
\tag{2}
$$

where the `ave_rank` of each item i is computed as follows: for each item i, we locate each user u who have i in Posu and compute the rank position quantile of i within $Po_{s_u}$. Then the rank position quantile is averaged across all user u who have i in $Po_{s_u}$. Note that an item with smaller average rank quantile close to 0 means the item on average scores higher among the $Po_{s_u}$ of each user, whereas a greater average rank quantile close to 1 means that the item generally scores lower than other positive items. Hence, a PRI value close to 1 implies the model gives higher scores to more popular items, and a PRI value close to 0 implies the model shows less model bias, since the popularity of positive items and the recommendation score ranking shows no correlation.

We further propose a metric computing the average popularity quantile of the top scoring positive items of each user (PopQ@1) as in Equation (3):

$$
PopQ@1 = \frac{1}{|U|}\sum_{u\in U} \text{PopQuantile}_u(\argmax_{x\in Po_{s_u}}(\hat{y}_{ui}))
\tag{3}
$$

where $\text{PopQuantile}_u$ returns the popularity quantile of the item conditioned on $Po_{s_u}$ . The item of $Po_{s_u}$ which has the highest global popularity has a $\text{PopQuantile}_u$ of 0, and the item with the lowest global popularity has a $\text{PopQuantile}_u$ of 1. Hence, a` PopQ@1` value close to 0 implies high model bias, since the top scoring positive items of each user is usually the positive item with the highest global popularity. A `PopQ@1` value close to 0.5 implies no model bias, since the popularity quantile of the top scoring positive items is likely to be spread out across 0 and 1. Whereas the PRI computes the overall correlation between the item popularity and the average rank for all positive items, `PopQ@1` focuses on the popularity quantile only for the top scoring positive items which has a high chance of being recommended. The two metrics allow detailed evaluation of the model bias.

A recommendation system should be trained to achieve both high accuracy and low model bias. The accuracy and debias performance of a recommendation system is often orthogonal. Accuracy metrics measure how well the system distinguishes between positive and negative preference, while debias metrics assess how item popularity is (un)correlated with recommendation score conditioned on positive items. Note that it is important for the recommendation system to maintain accuracy while reducing model bias. A system with low model bias but also low accuracy can not give personalized recommendations. For instance, a model giving random recommendation would show no model bias but does not consider the user preference at all.

## 3.3. Visual Illustration on Synthetic Data

Throughout the study, we design a synthetic data with explicit data bias to illustrate model bias and subsequent debias performance of various methods. The synthetic data is a 200 x 200 user-item interaction matrix R. The matrix is filled with binary interaction information such that the item popularity linearly decreases as the item index increases, as seen in Equation (4). Although such data deviates from the sparse and noisy real-world data, it provides a systematic way to observe how a recommendation system exhibits model bias.

$$
R[u,i] = 1 ( \text{if} i + j <= 200) \\
R[u, i] = 0 ( o.w. )
\tag{4}
$$

Using such data, we train a matrix factorization (MF) model, which is a basic collaborative filtering recommendation system [30]. We use BPR loss as the loss function [26]. Figure 1a shows the synthetic data in matrix form: the white area corresponds to positive items, and the black negative items. Figure 1b shows the recommendation score of the trained MF model, showing salient model bias. The popular items (smaller item index) shows higher score even when conditioned on the positive items. For instance, for user index 100, the model predicts higher score to popular items of index 0 ~ 20, even though the user equally consumed items from index 0 ~ 100.

The accuracy and debias performance of the model was quantitatively evaluated. To measure accuracy, the average frequency of the positive item being scored higher than the negative item was computed over all positive-negative item pairs. We report the error rate ((1-accuracy)\*100) of 0.01%. Hence, the model trained with the BPR loss shows high accuracy. The debias performance was evaluated using the PRI and `PopQ@1` metrics. Figure 1c shows the average item rank quantile of the items, where the x axis shows the item index of the synthetic data (items with smaller x index is more popular). Specifically, we see the most popular item on average is ranked at the top 0.0% among the positive items; and as the item popularity decreases, the item no longer has the highest rank. This indicates high model bias, and the PRI is also computed at 0.99. Figure 1d shows the histogram of the popularity quantile of top scoring positive items of each user. We see the popularity quantile is focused around 0, meaning the top scoring positive items mostly consists of the most popular positive items of each user. Taking the mean of the 200 quantiles, the `PopQ@1` is computed at 0.02. Both PRI and `PopQ@1` metrics indicate the model trained with the BPR loss showing high model bias.

# 4. Proposed Method

## 4.1. Regularization Term ot Minimize Score Difference

To reduce model bias, we propose a method to extend the BPR loss with an additional regularization term which minimizes the score differences between positive and negative items, respectively. Thus while the BPR loss contrasts the recommendation scores between positive and negative items, the regularization term will additionally force the scores to be equal within positive(negative) items. Thus the model can achieve both high accuracy and debias performance. We propose a total loss function of the following form as in Equation (5):

$$
\text{Total Loss} = \text{BPR Loss} + \text{Reg Term} \tag{5}
$$

We propose two variations of regularization term:

- Pos2Neg2 Term : 2 positive and 2 negative items are sampled per user at a time, and the score difference of the positive(negative) items are minimized, respectively.

$$
\text{Reg Term} = - \sum_{u \in U} \sum_{p_1, p_2 \in Po_{s_u}, n_1, n_2 \in Ne_{g_u}}
\log (1 - \tanh(|\hat{y}_{u, p_1} - \hat{y}_{u, p_2}|))
+ \log (1 - \tanh(|\hat{y}_{u, n_1} - \hat{y}_{u, n_2}|))
\tag{6}
$$

- Zerosum Term : 1 positive and 1 negative item is sampled per user at a time, and the sum of the recommendation scores of positive and negative items is regularized to be close to 0. Through training, the regularization will propagate across random positive-negative item pairs of the user, forcing the positive and negative items to have symmetric recommendation scores. Eventually, the scores of the positive items of the user will converge to a single value, while the scores of the negative items converge to a symmetric value.

$$
\text{Reg Term} = - \sum_{u \in U} \sum_{p \in Po_{s_u}, n \in Ne_{g_u}}
\log (1 - \tanh(|\hat{y}_{u, p} + \hat{y}_{u, n}|))
\tag{7}
$$

Both of the regularization terms aim to reduce model bias by leading the model to give equal scores to positive(negative) items. Such approach has two advantages. First, it is robust to the accuracydebias tradeoff. In contrast to debias methods collectively adjusting the scores for all items, the proposed method does not sacrifice the scores of positive items to boost those of the negative items. The second advantage is the simplicity. The proposed method is applicable to any model which uses the BPR loss, with no additional training.

Finally, experiments showed only regularizing the scores of positive items lead to deteriorated accuracy, hence regularization is applied for both positive and negative items.

## 4.2. Illustration of the Proposed Method on the Synthetic Data

We test the debias performance of the proposed methods using the synthetic data. We add the regularization term to the BPR loss and train the matrix factorization model to see if the model bias exhibited in the baseline training is alleviated. Note, the baseline refers to the method of using BPR loss for training. Figure 2 shows the model prediction when using the baseline BPR loss as well as the proposed methods. Whereas the baseline BPR loss resulted in high model bias in Figure 2a, the proposed methods show equal scores across positive items for each user (Figure 2b, 2c), while accurately contrasting the scores between positive and negative items. Such qualitative analysis shows high debias and accuracy performance of the proposed methods.

Next we quantitatively analyze the accuracy and debias performance. Table 1 shows the error rate of the proposed methods compared to the baseline. Both Pos2Neg2 and Zerosum method show low error rate, and the Zerosum method even shows higher accuracy than the baseline. For the debias performance, the upper graph of Figure 2d, 2e, 2f show the average rank quantile of the positive items. For the proposed methods, we see the average rank is around 0.25 ~ 0.75 for most of the items including the popular items (small item index), meaning the ranking is no longer affected by popularity. However, the few tail items with least popularity (item index 190 ~ 200) is still ranked at the bottom on average, indicating the debias was not effective for tail items. Similarly, Table 1 shows the PRI of the proposed methods are reduced from 0.99 to 0.42 and 0.50, respectively. The lower graph of Figure 2d, 2e, 2f show the histogram of popularity quantiles of the top scoring positive items of each user. For both Pos2Neg2 and Zerosum methods, the popularity quantiles are spread across 0 ~ 1. We see in Table 1 that the `PopQ@1` is computed at 0.62 and 0.61, respectively, which is closer to the ideal 0.5. All of these results show that both of the proposed regularization terms have the effect of reducing model bias while maintaining high accuracy.

Finally, we compare the proposed methods. The Zerosum method reports higher accuracy than the Pos2Neg2 method. We suggest two explanations. One reason may be due to the wider range of item pairing of the Zerosum method. Figure 2b shows the Pos2Neg2 method failing to distinguish positive and negative preference for users with low item consumption (user index 180 ~ 200), whereas in Figure 2c the Zerosum method showed accurate predictions for the same users. For users with small positive item consumptions, the Pos2Neg2 method may suffer from limited choices in positive item pairing, whereas the Zerosum method selects a pair for the positive items from the negative items. Consequently, the Pos2Neg2 method may not be able to learn the preference of some users due to limited range of training samples. The second reason may be due to the symmetric scoring effect of the Zerosum method. Figure 2b shows the Pos2Neg2 method gives unequal scores for negative items for different users, whereas Figure 2c shows the Zerosum method gives equal scores to negative items across different users, which also forms a symmetry with the positive items. The different score distribution of the Pos2Neg2 method may lead to unequal training for item pairs of different users, whereas the training is uniform for those of the Zerosum method. In sum, we conclude that the Zerosum is more than a simplified version of the Pos2Neg2 method but has additional effect of promoting accuracy. Hence, we present the Zerosum term as our main proposed method and contribution.

# 5. Advangtages of the Proposed Method

We compare the performance of the proposed method with earlier debias methods: inverse propensity weighting (IPW) [16, 27], causal intervention [36, 38, 40], reranking [2, 37], and Pearson regularization method [42]. The IPW method applies weights to the training instance to reduce the data bias, where the weight is inverse to the item popularity [16, 27]. Causal intervention models the causal effect the item popularity has on the recommendation score and removes such effect [36, 38]. We select the state of the art PD (Popularity-bias Deconfounding) method for comparison [38]. The reranking method applies post-hoc rank adjustment of the recommendation list. However, in the context of model bias this corresponds to the oracle method, hence comparison is not appropriate. Finally [42] suggested to reduce the model bias by adding a regularization term to the loss function to force the Pearson correlation of item popularity and recommendation score to be close to 0, conditioned on the positive items. We first overview the debias performance of IPW, PD, and Pearson methods, then conduct in-depth comparison with the proposed method.

## 5.1. Performances of Earlier Methods

Each debias methods (IPW, PD, Pearson) was applied to the training of MF model on the synthetic dataset. Figure 3 shows the model score results. Figure 3a shows the IPW method was not able to clearly distinguish the contrasting preference between positive and negative items. Moreover, the method was not able to reduce model bias as seen in the results of Figure 3d. Figure 3b shows the PD method performed satisfactory debias performance, while also accurately distinguishing positive and negative preference. However the debias is not even throughout the items. This is also reflected in Figure 3e, where the upper graph of Figure 3e shows the average item rank quantile exhibiting a curved shape, with the most popular item still being ranked at the top. The lower graph shows about half of the popularity quantiles focused on value close to 0. Figure 3c shows the Pearson method resulted in low scores of popular items for some users (e.g. user index 0 ~ 20) while it is high for other users (e.g. user index 100 ~ 200). This indicates the method penalized the scores of popular items for some users to force the Pearson correlation coefficient close to 0. However, such superficial balancing of scores is undesirable because it introduces additional model bias of different directions. For instance, now the popular items are under-recommended to users of index 0 ~ 20. The upper graph of Figure 3f shows that the average rank quantile of the popular items (item index 0 ~ 20) is close to 0.5, but the model bias is not reduced for the rest of the items. The lower graph of Figure 3f shows fair spread of popularity quantiles.

The accuracy and debias performance metrics of the earlier methods is reported in Table 2. All of the earlier methods showed worse accuracy performance compared to the Zerosum method. The methods also showed worse debias performance with IPW showing no debias effect; PD reporting PRI value of –0.52, indicating the model favoring unpopular items; and the Pearson method reporting a high PRI value of 0.80. Both PD and Pearson reports a `PopQ@1` value around 0.3, which is farther from 0.5 than the Zerosum method.

## 5.2. Comparing the Zerosum Method with the PD Method

The PD method has two limitations in terms of computational validity and efficiency compared to the Zerosum method. Methods such as PD adjusts the biased recommendation scores by lowering the scores of popular items while boosting the scores of unpopular items. This score adjusting scheme can lower the scores of positive items while boosting those of negative items, potentially leading to accuracy-debias tradeoff.

Such debias method has further limitation in terms of computational validity because the score adjusting scheme is often heuristically designed instead of being based on the computational mechanism of the recommendation model [36, 38, 42]. For instance the PD method uses the formula $\text{model score} = ELU (debiased score)\times(item pop)^a$ to derive the debiased score [38]. However, this formula may not generalize to other recommendation models with different computational process. In contrast, the logic of the Zerosum method is generalizable to other models. To elaborate, we test how the Zerosum and PD method perform when switching the recommendation system model from matrix factorization to a different model such as NeuCF [15]. The score result of the baseline BPR loss, Zerosum, and PD method, when training the NeuCF model is compared. Figure 4a shows the baseline prediction of the NeuCF model differs from when using matrix factorization. Figure 4c shows PD method performed poorly despite extensive hyperparameter tuning, whereas Figure 4b shows the Zerosum method continued to excel. This is also seen in Table 3. In terms of computational efficiency, methods such as PD often require costly hyperparameter tuning to adjust the accuracy and debias performance. In contrast, the Zerosum method trains the model to simultaneously achieve high accuracy and debias performance without additional hyperparameter tuning.

## 5.3. Comparing the Zerosum Method with the Pearson Method

Some works suggested penalizing the size of the correlation value between item popularity and item scores conditioned on the positive items, in addition to the original loss function [7, 42]. This strategy aims to balance only the scores of positive items without boosting the negative items. Therefore, such method can be more robust to accuracy-debias tradeoff. Indeed, the Zerosum method shares similar motivation.

However such Pearson regularization method also has two limitations. In terms of computational validity, we see from Figure 3c that the Pearson method selectively lowered the scores of popular items to bring the correlation coefficient close to 0; without achieving the intended independence between item popularity and score. In contrast, the Zerosum method employs a simpler logic which is effective in directly predicting debiased scores. In terms of computational efficiency, the Pearson method requires the costly computation of all positive items scores for one step of training. In contrast, the Zerosum method allows efficient mini-batch training which integrates naturally with the BPR loss.

# 6. Empirical Experiments

We conduct experiments to validate the effectiveness of the proposed method.

## 6.1. Experimental Settings

6.1.1 Data. We use Movielens [13], Gowalla [10], Goodreads [32, 33], and Ciao [31] datasets. The datasets were preprocessed to filter out users and items with too few interactions. The details of the preprocessed data is in Table 4.

6.1.2 Recommendation Systems Models. We used BPR-MF [26], NeuCF [15], NGCF [35], and LightGCN [14]; ranging from classical matrix factorization to the state of the art graph neural networks models.

6.1.3 Comparison Methods. The following methods are compared. Baseline is training the model with the BPR loss [26]. IPW weights each training instance with the inverse of item popularity [27]. PD is a causal intervention method which divides a factor proportional to the item popularity from the predicted score. The hyperparameter $\gamma$ was tuned in the range of $\gamma \in [0.05, 0.25]$ [38]. MACR is a causal intervention method that subtracts a factor proportional to the item popularity from the predicted score. The hyperparameter c was tuned in the range of $c \in [0, 30]$ [36]. Pearson method regulates the square of the Pearson correlation between the item popularity and item recommendation scores of positive items. The weight of the regularization term was tuned in the range of ${1e1, 1e2, 1e3, 1e4}$ [42]. Post-Process reduces the model bias by adjusting the model scores according to item popularity. The hyperparameter α, β was tuned in the range of $\alpha \in [0.1, 1.5]$, $\beta \in [0.0, 1.0]$ [42]. Zerosum is the proposed method which extends the BPR loss with a term which regulates the sum of paired positive and negative item scores of each user to be close to 0. The weight of the term is fixed at 0.1 for all experiments.

6.1.4 Training. Positive data was split into 60%, 20%, 20% for train, test, validation. For each user, 100 test negative items were selected before training. The same learning rate (0.001), batch size (2048), and embedding size (64 dim for MF, NeuCF model, and 10 dim with 3 GCN layers for NGCF, LightGCN) was used for each combination of (data, model, debias method).

6.1.5 Evaluation. To evaluate the accuracy, each positive test item was paired with the 100 test negative items, andHit@10, NDCG@10 was measured. Higher value means higher accuracy. To evaluate the debias performance, we used the PopQ@1 metric. We computed the value for users having at least 5 positive test items, since too few positive test items can misleadingly result in extreme popularity quantile. A PopQ@1 score close to 0.5 is desirable. For methods requiring hyperparameter search, we report the best result in terms of accuracy loss, since good performance in debias is meaningful when the model first reports high accuracy

## 6.2. Results & Discussion

Table 5 shows the experiments results. The bold font emphasizes the results showing an accuracy (Hit@10) loss within 2% compared to the baseline method, while also showing an improvement of at least 0.07 for the debias performance (PopQ@1).

The Zerosum method showed high accuracy and debias performance in the Movielens, Gowalla, and Goodreads datasets when using the MF, NGCF, and LightGCN models. However, Zerosum method generally did not show high performance when using the NeuCF model or the Ciao dataset. Other methods such as PD and Pearson sometimes showed good performances but this was not consistent; the PD method did not show good performance in the Movielens and Gowalla datasets when using the NGCF and LightGCN method; the Pearson method showed high accuracy loss when using the NGCF or LightGCN method when training the Movielens dataset. Based on the consistent performance of the Zerosum method in addition to the computational advantages discussed earlier, we conclude the Zerosum method to be more effective than earlier methods. Finally, IPW, MACR, and Post-Process often showed excessive accuracy loss and did not show competent performance.

Finally we discuss why Zerosum method did not show debias effect with the Ciao dataset. We observed the debias performance of the Zerosum method depends on baseline model accuracy. Figure 5 shows the NeuCF scores of positive (blue), negative (orange) items of one user. Smaller x index indicates higher popularity. The Zerosum method predicted scores focused on symmetric values of +2 and -2 as intended. However, since the model accuracy is low when training with the baseline BPR loss, the model trained with the Zerosum method (which extends the BPR loss) also inaccurately scored -2 for some positive items. Hence, although the Zerosum method balances the scores for items predicted to be positive, such balancing may be misdirected since the predicted items can differ from actual positive items. Hence the Zerosum method may have limited debias effect if the baseline model accuracy is low. However, this also hints the effectiveness of the Zerosum method can increase proportionally with baseline model accuracy.

# 7. Conclusion

In this study we tackle the popularity bias, particularly model bias, where the recommendation systems give higher score to popular items among items the user equally liked. We propose a novel approach to extend the BPR loss with a regularization term which tries to minimize the score differences within positive and negative items, respectively, thus reducing model bias while maintaining high accuracy. We conduct an experiment to test our method: we embedded explicit popularity bias in a synthetic dataset, which results in model bias in naive training setting. We apply our method to counter such model bias. The results showed our method outperformed earlier debias methods in terms of accuracy and debias performance. We further conducted empirical experiments using four benchmark datasets and four recommendation models. The proposed method showed consistent debias performance with minimal accuracy loss in 3 of the 4 datasets, in which earlier debias methods lacked consistency. Our work provides a new way of reducing model bias which can be applied both in academic and industrial settings. For future work, we plan to further investigate improved regularization methods. We hope that our method can promote diverse recommendations.

# References

- [1] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Controlling popularity bias in learning-to-rank recommendation. In Proceedings of the eleventh ACM conference on recommender systems. 42–46.
- [2] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2019. Managing popularity bias in recommender systems with personalized re-ranking. In The thirty-second international flairs conference.
- [3] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher. 2020. The connection between popularity bias, calibration, and fairness in recommendation. In Fourteenth ACM conference on recommender systems. 726–731.
- [4] Gediminas Adomavicius, Alexander Tuzhilin, Shlomo Berkovsky, Ernesto W De Luca, and Alan Said. 2010. Context-awareness in recommender systems: research workshop and movie recommendation challenge. In Proceedings of the fourth ACM conference on Recommender systems. 385–386.
- [5] Ashton Anderson, Lucas Maystre, Ian Anderson, Rishabh Mehrotra, and Mounia Lalmas. 2020. Algorithmic effects on the diversity of consumption on spotify. In Proceedings of The Web Conference 2020. 2155–2165.
- [6] Guy Aridor, Duarte Goncalves, and Shan Sikdar. 2020. Deconstructing the filter bubble: User decision-making and recommender systems. In Fourteenth ACM Conference on Recommender Systems. 82–91.
- [7] Ludovico Boratto, Gianni Fenu, and Mirko Marras. 2021. Connecting user and item perspectives in popularity debiasing for collaborative recommendation. Information Processing & Management 58, 1 (2021), 102387.
- [8] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. 2020. Bias and debias in recommender system: A survey and future directions. arXiv preprint arXiv:2010.03240 (2020).
- [9] Jiawei Chen, Xiang Wang, Fuli Feng, and Xiangnan He. 2021. Bias Issues and Solutions in Recommender System: Tutorial on the RecSys 2021. In Fifteenth ACM Conference on Recommender Systems. 825–827.
- [10] Eunjoon Cho, Seth A Myers, and Jure Leskovec. 2011. Friendship and mobility: user movement in location-based social networks. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. 1082–1090.
- [11] Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005. Learning a similarity metric discriminatively, with application to face verification. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05), Vol. 1. IEEE, 539–546.
- [12] Yingqiang Ge, Shuya Zhao, Honglu Zhou, Changhua Pei, Fei Sun, Wenwu Ou, and Yongfeng Zhang. 2020. Understanding echo chambers in e-commerce recommender systems. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval. 2261–2270.
- [13] F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis) 5, 4 (2015), 1–19.
- [14] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 639–648.
- [15] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web. 173–182.
- [16] Thorsten Joachims, Adith Swaminathan, and Tobias Schnabel. 2017. Unbiased learning-to-rank with biased feedback. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining. 781–789.
- [17] Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. 426–434.
- [18] Phuc H Le-Khac, Graham Healy, and Alan F Smeaton. 2020. Contrastive representation learning: A framework and review. IEEE Access 8 (2020), 193907–193934.
- [19] Mingyang Li, Hongchen Wu, and Huaxiang Zhang. 2019. Matrix factorization for personalized recommendation with implicit feedback and temporal information in social ecommerce networks. IEEE Access 7 (2019), 141268–141276.
- [20] Zhuang Liu, Yunpu Ma, Yuanxin Ouyang, and Zhang Xiong. 2021. Contrastive learning for recommender system. arXiv preprint arXiv:2101.01317 (2021).
- [21] Jiwen Lu, Junlin Hu, and Jie Zhou. 2017. Deep metric learning for visual understanding: An overview of recent advances. IEEE Signal Processing Magazine 34, 6 (2017), 76–84.
- [22] Masoud Mansoury, Himan Abdollahpouri, Mykola Pechenizkiy, Bamshad Mobasher, and Robin Burke. 2020. Feedback loop and bias amplification in recommender systems. In Proceedings of the 29th ACM international conference on information & knowledge management. 2145–2148.
- [23] Tien T Nguyen, Pik-Mai Hui, F Maxwell Harper, Loren Terveen, and Joseph A Konstan. 2014. Exploring the filter bubble: the effect of using recommender systems on content diversity. In Proceedings of the 23rd international conference on World wide web. 677–686.
- [24] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
- [25] Yoon-Joo Park and Alexander Tuzhilin. 2008. The long tail of recommender systems and how to leverage it. In Proceedings of the 2008 ACM conference on Recommender systems. 11–18.
- [26] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618 (2012).
- [27] Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and Thorsten Joachims. 2016. Recommendations as treatments: Debiasing learning and evaluation. In international conference on machine learning. PMLR, 1670– 1679.
- [28] Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition. 815–823.
- [29] Harald Steck. 2011. Item popularity and recommendation accuracy. In Proceedings of the fifth ACM conference on Recommender systems. 125–132.
- [30] Gábor Takács, István Pilászy, Bottyán Németh, and Domonkos Tikk. 2008. Matrix factorization and neighbor based algorithms for the netflix prize problem. In Proceedings of the 2008 ACM conference on Recommender systems. 267–274.
- [31] Jiliang Tang, Huiji Gao, and Huan Liu. 2012. mTrust: Discerning multi-faceted trust in a connected world. In Proceedings of the fifth ACM international conference on Web search and data mining. 93–102.
- [32] Mengting Wan and Julian J. McAuley. 2018. Item recommendation on monotonic behavior chains. In Proceedings of the 12th ACM Conference on Recommender Systems, RecSys 2018, Vancouver, BC, Canada, October 2-7, 2018, Sole Pera, Michael D. Ekstrand, Xavier Amatriain, and John O’Donovan (Eds.). ACM, 86–94. https://doi.org/10.1145/3240323.3240369
- [33] Mengting Wan, Rishabh Misra, Ndapa Nakashole, and Julian J. McAuley. 2019. Fine-Grained Spoiler Detection from Large-Scale Review Corpora. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, Anna Korhonen, David R. Traum, and Lluís Màrquez (Eds.). Association for Computational Linguistics, 2605–2610. https://doi.org/10.18653/v1/p19-1248
- [34] Wenjie Wang, Fuli Feng, Xiangnan He, Xiang Wang, and Tat-Seng Chua. 2021. Deconfounded recommendation for alleviating bias amplification. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 1717–1725.
- [35] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural graph collaborative filtering. In Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval. 165–174.
- [36] Tianxin Wei, Fuli Feng, Jiawei Chen, Ziwei Wu, Jinfeng Yi, and Xiangnan He. 2021. Model-agnostic counterfactual reasoning for eliminating popularity bias in recommender system. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 1791–1800.
- [37] Mi Zhang and Neil Hurley. 2008. Avoiding monotony: improving the diversity of recommendation lists. In Proceedings of the 2008 ACM conference on Recommender systems. 123–130.
- [38] Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui Ling, and Yongdong Zhang. 2021. Causal intervention for leveraging popularity bias in recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 11–20.
- [39] Zihao Zhao, Jiawei Chen, Sheng Zhou, Xiangnan He, Xuezhi Cao, Fuzheng Zhang, and Wei Wu. 2021. Popularity Bias Is Not Always Evil: Disentangling Benign and Harmful Bias for Recommendation. arXiv preprint arXiv:2109.07946 (2021).
- [40] Yu Zheng, Chen Gao, Xiang Li, Xiangnan He, Yong Li, and Depeng Jin. 2021. Disentangling user interest and conformity for recommendation with causal embedding. In Proceedings of the Web Conference 2021. 2980–2991.
- [41] Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, and Hongxia Yang. 2021. Contrastive learning for debiased candidate generation in large-scale recommender systems. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 3985–3995.
- [42] Ziwei Zhu, Yun He, Xing Zhao, Yin Zhang, Jianling Wang, and James Caverlee. 2021. Popularity-opportunity bias in collaborative filtering. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining. 85–93.
- [43] Ziwei Zhu, Jianling Wang, and James Caverlee. 2020. Measuring and mitigating item under-recommendation bias in personalized ranking systems. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval. 449–458.
