## link ãƒªãƒ³ã‚¯

https://arxiv.org/pdf/2203.13366.pdf
https://arxiv.org/pdf/2203.13366.pdf

## title ã‚¿ã‚¤ãƒˆãƒ«

Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)
è¨€èªå‡¦ç†ã¨ã—ã¦ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆRLPï¼‰ï¼š çµ±ä¸€ã•ã‚ŒãŸäº‹å‰è¨“ç·´ã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨äºˆæ¸¬ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ï¼ˆP5ï¼‰

## abstract æŠ„éŒ²

For a long time, different recommendation tasks typically require designing task-specific architectures and training objectives.
é•·ã„é–“ã€**æ§˜ã€…ãªæ¨è–¦ã‚¿ã‚¹ã‚¯ã¯é€šå¸¸ã€ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç›®æ¨™ã‚’è¨­è¨ˆã™ã‚‹å¿…è¦ãŒã‚ã£ãŸ**ã€‚
As a result, it is hard to transfer the learned knowledge and representations from one task to another, thus restricting the generalization ability of existing recommendation approaches, e.g., a sequential recommendation model can hardly be applied or transferred to a review generation method.
ãã®çµæœã€å­¦ç¿’ã—ãŸçŸ¥è­˜ã‚„è¡¨ç¾ã‚’ã‚ã‚‹ã‚¿ã‚¹ã‚¯ã‹ã‚‰åˆ¥ã®ã‚¿ã‚¹ã‚¯ã«ç§»è¡Œã™ã‚‹ã“ã¨ãŒé›£ã—ãã€æ—¢å­˜ã®æ¨è–¦ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ±åŒ–èƒ½åŠ›ãŒåˆ¶é™ã•ã‚Œã‚‹ã€‚ä¾‹ãˆã°ã€é€æ¬¡æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”Ÿæˆæ‰‹æ³•ã«é©ç”¨ã—ãŸã‚Šç§»è¡Œã—ãŸã‚Šã™ã‚‹ã“ã¨ã¯é›£ã—ã„ã€‚
To deal with such issues, considering that language can describe almost anything and language grounding is a powerful medium to represent various problems or tasks, we present a flexible and unified text-to-text paradigm called â€œPretrain, Personalized Prompt, and Predict Paradigmâ€ (P5) for recommendation, which unifies various recommendation tasks in a shared framework.
ã“ã®ã‚ˆã†ãªå•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€è¨€èªãŒã»ã¨ã‚“ã©ã®ã‚‚ã®ã‚’è¨˜è¿°ã™ã‚‹ã“ã¨ãŒã§ãã€è¨€èªæ¥åœ°ãŒæ§˜ã€…ãªå•é¡Œã‚„ã‚¿ã‚¹ã‚¯ã‚’è¡¨ç¾ã™ã‚‹ãŸã‚ã®å¼·åŠ›ãªåª’ä½“ã§ã‚ã‚‹ã“ã¨ã‚’è€ƒæ…®ã—ã€æˆ‘ã€…ã¯ã€**æ§˜ã€…ãªæ¨è–¦ã‚¿ã‚¹ã‚¯ã‚’å…±æœ‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§çµ±ä¸€ã™ã‚‹**ã€æŸ”è»Ÿã§çµ±ä¸€ã•ã‚ŒãŸæ¨è–¦ã®ãŸã‚ã® **"Pretrain, Personalized Prompt, and Predict Paradigm"ï¼ˆP5ï¼‰**ã¨å‘¼ã°ã‚Œã‚‹**text-to-textãƒ‘ãƒ©ãƒ€ã‚¤ãƒ **ã‚’ææ¡ˆã™ã‚‹ã€‚(paradigmã¯ã–ã£ãã‚Šæ–¹æ³•è«–ã€ã¿ãŸã„ãªã‚¤ãƒ¡ãƒ¼ã‚¸?)
In P5, all data such as user-item interactions, user descriptions, item metadata, and user reviews are converted to a common format â€” natural language sequences.
P5ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã€ãƒ¦ãƒ¼ã‚¶ã®èª¬æ˜ã€ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã€ãƒ¦ãƒ¼ã‚¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ãªã©ã®**ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒã€å…±é€šã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã‚ã‚‹è‡ªç„¶è¨€èªã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¤‰æ›**ã•ã‚Œã‚‹ã€‚
Then, P5 learns to generate personalized recommendations by predicting the next tokens in the sequences.
The rich information from natural language assists P5 to capture deeper semantics for personalization and recommendation.
è‡ªç„¶è¨€èªã‹ã‚‰ã®è±Šå¯Œãªæƒ…å ±ã¯ã€P5ãŒãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã¨æ¨è–¦ã®ãŸã‚ã®ã‚ˆã‚Šæ·±ã„æ„å‘³ã‚’æ‰ãˆã‚‹ã®ã‚’æ”¯æ´ã™ã‚‹ã€‚
Specifically, P5 learns different tasks with the same language modeling objective during pretraining.
å…·ä½“çš„ã«ã¯ã€P5ã¯äº‹å‰å­¦ç¿’ä¸­ã«åŒã˜è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ç›®çš„ã§ç•°ãªã‚‹ã‚¿ã‚¹ã‚¯ã‚’å­¦ç¿’ã™ã‚‹ã€‚
Thus, it serves as the foundation model for various downstream recommendation tasks, allows easy integration with other modalities, and enables instruction-based recommendation based on prompts.
ãã®ãŸã‚ã€**P5ã¯ã€ã•ã¾ã–ã¾ãªä¸‹æµã®æ¨è–¦ã‚¿ã‚¹ã‚¯ã®åŸºç¤ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æ©Ÿèƒ½ã—**ã€ä»–ã®modalitiesã¨ã®ç°¡å˜ãªçµ±åˆã‚’å¯èƒ½ã«ã—ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ã„ãŸæŒ‡ç¤ºã«åŸºã¥ãæ¨è–¦ã‚’å®Ÿç¾ã™ã‚‹ã€‚(ã“ã“ã§modalityã¯ã€ä»–ã®domainã¨ã‹ã®æ„å‘³ã ã‚ã†ã‹...??:thinking:)
P5 advances recommender systems from shallow model to deep model to large model, and will revolutionize the technical form of recommender systems towards universal recommendation engine.
P5ã¯ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚’æµ…ã„ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ·±ã„ãƒ¢ãƒ‡ãƒ«ã€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã«é€²åŒ–ã•ã›ã€æ™®éçš„ãªæ¨è–¦ã‚¨ãƒ³ã‚¸ãƒ³ã«å‘ã‘ã¦æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®æŠ€è¡“å½¢æ…‹ã‚’é©å‘½çš„ã«å¤‰ãˆã‚‹ã ã‚ã†ã€‚
With adaptive personalized prompt for different users, P5 is able to make predictions in a zero-shot or few-shot manner and largely reduces the necessity for extensive fine-tuning.
ã•ã¾ã–ã¾ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é©å¿œçš„ã«ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ˆã‚Šã€P5ã¯ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã¾ãŸã¯æ•°ã‚·ãƒ§ãƒƒãƒˆã§äºˆæ¸¬ã‚’è¡Œã†ã“ã¨ãŒã§ãã€å¤§è¦æ¨¡ãªå¾®èª¿æ•´ã®å¿…è¦æ€§ã‚’å¤§å¹…ã«æ¸›ã‚‰ã™ã“ã¨ãŒã§ãã‚‹ã€‚
On several recommendation benchmarks, we conduct experiments to show the effectiveness of P5.
ã„ãã¤ã‹ã®æ¨è–¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ãŠã„ã¦ã€P5ã®æœ‰åŠ¹æ€§ã‚’ç¤ºã™å®Ÿé¨“ã‚’è¡Œã£ãŸã€‚
To help advance future research on Recommendation as Language Processing (RLP), Personalized Foundation Models (PFM), and Universal Recommendation Engine (URE), we release the source code, dataset, prompts, and pretrained P5 model at https://github.com/jeykigung/P5.
Recommendation as Language Processing(RLP, è¨€èªå‡¦ç†ã¨ã—ã¦ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³)ã€Personalized Foundation Models(PFM, ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸåŸºç¤ãƒ¢ãƒ‡ãƒ«)ã€Universal Recommendation Engine(URE, æ™®éçš„ãªæ¨è–¦ã‚¨ãƒ³ã‚¸ãƒ³)ã«é–¢ã™ã‚‹å°†æ¥ã®ç ”ç©¶ã‚’é€²ã‚ã‚‹ãŸã‚ã«ã€ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€äº‹å‰å­¦ç¿’æ¸ˆã¿P5ãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã—ã¦ã„ã‚‹ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

# Introduction ã¯ã˜ã‚ã«

For the past decades, recommender systems have witnessed significant advancements and played an essential role in peopleâ€™s daily life, helping their micro decisions and fulfilling their demands with outstanding accuracy.
éå»æ•°åå¹´ã«ã‚ãŸã‚Šã€ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã¯è‘—ã—ã„é€²æ­©ã‚’é‚ã’ã€äººã€…ã®æ—¥å¸¸ç”Ÿæ´»ã«ãŠã„ã¦é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¦ããŸã€‚å„ªã‚ŒãŸç²¾åº¦ã§äººã€…ã®ãƒã‚¤ã‚¯ãƒ­æ„æ€æ±ºå®šã‚’æ”¯æ´ã—ã€è¦æ±‚ã‚’æº€ãŸã—ã¦ããŸã€‚
In retrospect, we can summarize the development trend of modern recommender systems â€“ towards a more comprehensive system that accommodates diverse features and a wide spectrum of application scenarios.
æŒ¯ã‚Šè¿”ã£ã¦ã¿ã‚‹ã¨ã€ç¾ä»£ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®ç™ºå±•å‚¾å‘ã‚’è¦ç´„ã™ã‚‹ã¨ã€**å¤šæ§˜ãªç‰¹å¾´ã¨å¹…åºƒã„ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒŠãƒªã‚ªã«å¯¾å¿œã™ã‚‹åŒ…æ‹¬çš„ãªã‚·ã‚¹ãƒ†ãƒ ã«å‘ã‹ã£ã¦ã„ã‚‹ã€‚**(è‰²ã‚“ãªusecaseã«å¯¾å¿œã™ã‚‹generalãªã‚·ã‚¹ãƒ†ãƒ ã£ã¦ã“ã¨??)

On one hand, feature engineering and learning in recommender systems has evolved greatly from simple to complex.
ä¸€æ–¹ã§ã¯ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹feature engineeringã¨å­¦ç¿’ã¯ã€å˜ç´”ã‹ã‚‰è¤‡é›‘ã¸ã¨å¤§ããé€²åŒ–ã—ã¦ããŸã€‚
In early ages, recommender systems typically adopt logistic regression or collaborative filtering [25, 35, 50, 52] which utilize user-item interaction records to model usersâ€™ behavioral patterns.
åˆæœŸã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€**ãƒ¦ãƒ¼ã‚¶ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®ç›¸äº’ä½œç”¨ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’åˆ©ç”¨ã—ã¦ãƒ¦ãƒ¼ã‚¶ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹**ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚„å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°[25, 35, 50, 52]ãŒä¸€èˆ¬çš„ã§ã‚ã£ãŸã€‚(ã†ã‚“ã†ã‚“...!)
Later on, the contextual features such as user profile and item metadata are further integrated into the system through more sophisticated models such as factorization machines [48] and GBDT [20].
ãã®å¾Œã€å› æ•°åˆ†è§£ãƒã‚·ãƒ³[48]ã‚„GBDT[20]ã®ã‚ˆã†ãªã€**ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’é€šã˜ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚„ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã‚ˆã†ãªcontextual featuresãŒã‚·ã‚¹ãƒ†ãƒ ã«ã•ã‚‰ã«çµ±åˆã•ã‚ŒãŸ**ã€‚(ã†ã‚“ã†ã‚“...!)
Recently, deep neural network models [3, 5, 19, 74] facilitate crossing and combination among even more diverse and sophisticated features.
æœ€è¿‘ã§ã¯ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«[3, 5, 19, 74]ãŒã€**ã•ã‚‰ã«å¤šæ§˜ã§æ´—ç·´ã•ã‚ŒãŸç‰¹å¾´é‡é–“ã®crossingã‚„combinationã‚’å®¹æ˜“ã«ã—ã¦ã„ã‚‹**ã€‚(ã†ã‚“ã†ã‚“...!) (sequential recommenderçš„ãªã€ç›¸äº’ä½œç”¨ãƒ¬ã‚³ãƒ¼ãƒ‰ã®æ‰ãˆæ–¹ã®æ´—ç·´ã®è©±ã‚‚ã‚ã‚‹ã‚ˆã­...:thinking:)
As a result, these models gain better representation ability compared with traditional feature engineering based approaches.
ãã®çµæœã€ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€å¾“æ¥ã®feature engineeringã«åŸºã¥ãã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨æ¯”è¼ƒã—ã¦ã€ã‚ˆã‚Šå„ªã‚ŒãŸè¡¨ç¾èƒ½åŠ›ã‚’ç²å¾—ã—ã¦ã„ã‚‹ã€‚

On the other hand, more recommendation tasks have emerged.
ãã®ä¸€æ–¹ã§ã€ã‚ˆã‚Šå¤šãã®æ¨è–¦ã‚¿ã‚¹ã‚¯ãŒå‡ºç¾ã—ã¦ã„ã‚‹ã€‚
Except for classical rating prediction and direct user-item matchingbased recommendation tasks, recent works are broadening the spectrum to new tasks and scenarios such as sequential recommendation [21, 60, 63, 80], conversational recommendation [8, 61, 76], explainable recommendation [17, 31, 62, 70, 75, 77] and so on.
å¤å…¸çš„ãªè©•ä¾¡äºˆæ¸¬ã‚„ç›´æ¥çš„ãªãƒ¦ãƒ¼ã‚¶ã‚¢ã‚¤ãƒ†ãƒ ãƒãƒƒãƒãƒ³ã‚°ã«åŸºã¥ãæ¨è–¦ã‚¿ã‚¹ã‚¯ã«åŠ ãˆã¦ã€æœ€è¿‘ã®ç ”ç©¶ã¯ã€**sequential recommendation(é€æ¬¡æ¨è–¦)[21, 60, 63, 80]ã€conversational recommendation(ä¼šè©±å‹æ¨è–¦)[8, 61, 76]ã€explainable recommendation(èª¬æ˜å¯èƒ½ãªæ¨è–¦)[17, 31, 62, 70, 75, 77]** ãªã©ã®æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚„ã‚·ãƒŠãƒªã‚ªã«ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’åºƒã’ã¦ã„ã‚‹ã€‚
While the approaches to the aforementioned recommendation tasks are often proposed separately, there is an evident trend of utilizing multiple recommendation tasks to jointly learn the transferable representations [31, 56, 57, 72].
å‰è¿°ã®æ¨è–¦ã‚¿ã‚¹ã‚¯ã¸ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€é€šå¸¸åˆ¥ã€…ã«ææ¡ˆã•ã‚Œã‚‹ãŒã€**è»¢é€å¯èƒ½ãªè¡¨ç¾ã‚’å…±åŒã§å­¦ç¿’ã™ã‚‹ãŸã‚ã«è¤‡æ•°ã®æ¨è–¦ã‚¿ã‚¹ã‚¯ã‚’åˆ©ç”¨ã™ã‚‹å‚¾å‘ãŒæ˜ã‚‰ã‹ã«ãªã£ã¦ã„ã‚‹**ã€‚(ãªã‚‹ã»ã©? ã“ã‚ŒãŒæœ€åˆã«ä¸»å¼µã—ã¦ãŸã€Œå¤šæ§˜ãªç‰¹å¾´ã¨å¹…åºƒã„ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒŠãƒªã‚ªã«å¯¾å¿œã™ã‚‹åŒ…æ‹¬çš„ãªã‚·ã‚¹ãƒ†ãƒ ã«å‘ã‹ã£ã¦ã„ã‚‹ã€ã¨ã„ã†è©±ã‹...!)
Although existing recommender systems achieved great success, there is still a considerable gap between current solutions and the foreseeable intersection of the aforementioned trends â€“ a comprehensive recommender system that can accommodate diverse features and different types of tasks.
æ—¢å­˜ã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¯å¤§ããªæˆåŠŸã‚’åã‚ã¦ã„ã‚‹ãŒã€ç¾åœ¨ã®ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã¨å‰è¿°ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã®äºˆæƒ³ã•ã‚Œã‚‹äº¤å·®ç‚¹ã¨ã®é–“ã«ã¯ã€å¤šæ§˜ãªç‰¹å¾´ã¨ã•ã¾ã–ã¾ãªã‚¿ã‚¤ãƒ—ã®ã‚¿ã‚¹ã‚¯ã«å¯¾å¿œã§ãã‚‹åŒ…æ‹¬çš„ãªæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¨ã®é–“ã«ã¯ã€ã‹ãªã‚Šã®ã‚®ãƒ£ãƒƒãƒ—ãŒã‚ã‚‹ã€‚
Since recommendation tasks usually share a common userâ€“item pool and have overlapping contextual features, we believe it is promising to merge even more recommendation tasks into a unified framework so that they can implicitly transfer knowledge to benefit each other and enable generalization to other unseen tasks.
æ¨è–¦ã‚¿ã‚¹ã‚¯ã¯é€šå¸¸ã€å…±é€šã®ãƒ¦ãƒ¼ã‚¶-ã‚¢ã‚¤ãƒ†ãƒ ãƒ—ãƒ¼ãƒ«ã‚’å…±æœ‰ã—ã€é‡è¤‡ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã‚’æŒã£ã¦ã„ã‚‹ãŸã‚ã€**ã•ã‚‰ã«å¤šãã®æ¨è–¦ã‚¿ã‚¹ã‚¯ã‚’çµ±ä¸€ã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«çµ±åˆã—ã€ãã‚Œã‚‰ãŒäº’ã„ã«åˆ©ç›Šã‚’ã‚‚ãŸã‚‰ã™ãŸã‚ã«çŸ¥è­˜ã‚’æš—é»™çš„ã«è»¢é€ã—ã€ä»–ã®æœªè¦‹ã®ã‚¿ã‚¹ã‚¯ã«æ±åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã‚‹**ã¨è€ƒãˆã¦ã„ã‚‹ã€‚(generalizeã§ãã‚‹ã¯ãšã€ã¨ã„ã†ä¸»å¼µãªã‚“ã ...!:thinking:)

Inspired by the recent progress in multitask prompt-based training [1, 51, 67], in this work, we propose a unified â€œPretrain, Personalized Prompt & Predict Paradigmâ€ (denoted as P5).
ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°[1, 51, 67]ã®æœ€è¿‘ã®é€²æ­©ã«è§¦ç™ºã•ã‚Œã€æœ¬ç ”ç©¶ã§ã¯ã€**çµ±ä¸€ã•ã‚ŒãŸã€ŒPretrain, Personalized Prompt & Predict Paradigmã€ï¼ˆP5ï¼‰**ã‚’ææ¡ˆã™ã‚‹ã€‚
We show that P5 is possible to learn multiple recommendation related tasks together through a unified sequence-to-sequence framework by formulating these problems as prompt-based natural language tasks, where userâ€“item information and corresponding features are integrated with personalized prompt templates as model inputs.
P5ã¯ã€ãƒ¦ãƒ¼ã‚¶-ã‚¢ã‚¤ãƒ†ãƒ æƒ…å ±ã¨å¯¾å¿œã™ã‚‹ç‰¹å¾´é‡ãŒã€ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ã¨ã—ã¦ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨çµ±åˆã•ã‚ŒãŸã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ã®è‡ªç„¶è¨€èªã‚¿ã‚¹ã‚¯ã¨ã—ã¦ã“ã‚Œã‚‰ã®å•é¡Œã‚’å®šå¼åŒ–ã™ã‚‹ã“ã¨ã§ã€çµ±ä¸€ã•ã‚ŒãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹-ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’é€šã˜ã¦è¤‡æ•°ã®æ¨è–¦é–¢é€£ã‚¿ã‚¹ã‚¯ã‚’ä¸€ç·’ã«å­¦ç¿’ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚
P5 sheds light on a promising technical route for unified and instruction-based recommendation.
P5ã¯ã€çµ±ä¸€ã•ã‚ŒãŸæŒ‡å°ã«åŸºã¥ãæ¨è–¦ã®ãŸã‚ã®æœ‰æœ›ãªæŠ€è¡“çš„ãƒ«ãƒ¼ãƒˆã«å…‰ã‚’å½“ã¦ã‚‹ã€‚
It has three main advantages: 1) P5 deeply immerses recommendation models into a full language environment, where all recommendation tasks are reformulated to NLP tasks with the help of personalized prompts.
P5ã«ã¯3ã¤ã®åˆ©ç‚¹ãŒã‚ã‚‹ï¼š 1)P5ã¯æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã‚’å®Œå…¨ãªè¨€èªç’°å¢ƒã«æ·±ãæµ¸ã—ã€ã™ã¹ã¦ã®æ¨è–¦ã‚¿ã‚¹ã‚¯ã¯ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®åŠ©ã‘ã‚’å€Ÿã‚Šã¦NLPã‚¿ã‚¹ã‚¯ã«å†å®šå¼åŒ–ã•ã‚Œã‚‹ã€‚
Since language grounding is sufficiently flexible and powerful to express various kinds of features in text templates, so there is no need to design feature-specific encoders.
è¨€èªæ¥åœ°ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§æ§˜ã€…ãªç¨®é¡ã®ç‰¹å¾´ã‚’è¡¨ç¾ã™ã‚‹ã®ã«ååˆ†æŸ”è»Ÿã§å¼·åŠ›ã§ã‚ã‚‹ãŸã‚ã€ç‰¹å¾´ã«ç‰¹åŒ–ã—ãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’è¨­è¨ˆã™ã‚‹å¿…è¦ã¯ãªã„ã€‚
As a result, P5 can exploit the abundant semantics and knowledge inside the training corpora; 2) P5 integrates multiple recommendation tasks into a shared textto-text encoder-decoder architecture and trains them with the same language modeling loss rather than designing task-specific architectures and objective functions.
ãã®çµæœã€P5ã¯å­¦ç¿’ã‚³ãƒ¼ãƒ‘ã‚¹å†…ã®è±Šå¯Œãªã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹ã¨çŸ¥è­˜ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚2) P5ã¯ã€ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚„ç›®çš„é–¢æ•°ã‚’è¨­è¨ˆã™ã‚‹ã®ã§ã¯ãªãã€è¤‡æ•°ã®æ¨è–¦ã‚¿ã‚¹ã‚¯ã‚’å…±æœ‰ã®ãƒ†ã‚­ã‚¹ãƒˆ-ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€-ãƒ‡ã‚³ãƒ¼ãƒ€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«çµ±åˆã—ã€åŒã˜è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãƒ­ã‚¹ã§å­¦ç¿’ã™ã‚‹ã€‚
In other words, P5 treats all personalized tasks as a conditional text generation problem; 3) Trained with instruction-based prompts, P5 attains sufficient zero-shot performance when generalizing to novel personalized prompts or unseen items in other domains.
è¨€ã„æ›ãˆã‚Œã°ã€P5ã¯ã™ã¹ã¦ã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’æ¡ä»¶ä»˜ããƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå•é¡Œã¨ã—ã¦æ‰±ã†ã€‚3) æŒ‡ç¤ºãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§è¨“ç·´ã•ã‚ŒãŸP5ã¯ã€æ–°è¦ã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„ä»–ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã®æœªè¦‹ã®ã‚¢ã‚¤ãƒ†ãƒ ã«æ±åŒ–ã™ã‚‹éš›ã«ã€ååˆ†ãªã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ€§èƒ½ã‚’é”æˆã™ã‚‹ã€‚
In our experiments, we study how P5 performs compared with task-specific approaches on all five task families as well as evaluating P5â€™s zero-shot generalization ability.
å®Ÿé¨“ã§ã¯ã€P5ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ±åŒ–èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ã ã‘ã§ãªãã€5ã¤ã®ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã™ã¹ã¦ã«ã¤ã„ã¦ã€ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨æ¯”è¼ƒã—ã¦P5ãŒã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã‹ã‚’ç ”ç©¶ã—ãŸã€‚
We also conduct several ablation studies to justify the design details of P5 framework.
ã¾ãŸã€P5ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®è¨­è¨ˆã®è©³ç´°ã‚’æ­£å½“åŒ–ã™ã‚‹ãŸã‚ã«ã€ã„ãã¤ã‹ã®ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ã‚‚è¡Œã£ã¦ã„ã‚‹ã€‚
Overall, our main contributions can be outlined as follows: â€¢ To the best of our knowledge, this is the first work to propose a unified â€œPretrain, Personalized Prompt & Predict Paradigmâ€ which integrates various recommendation related tasks into a shared conditional language generation framework.
å…¨ä½“ã¨ã—ã¦ã€æˆ‘ã€…ã®ä¸»ãªè²¢çŒ®ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«æ¦‚èª¬ã§ãã‚‹ï¼š - æˆ‘ã€…ã®çŸ¥ã‚‹é™ã‚Šã€ã“ã‚Œã¯æ§˜ã€…ãªæ¨è–¦é–¢é€£ã‚¿ã‚¹ã‚¯ã‚’å…±æœ‰æ¡ä»¶è¨€èªç”Ÿæˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«çµ±åˆã™ã‚‹ã€çµ±ä¸€ã•ã‚ŒãŸã€ŒPretrain, Personalized Prompt & Predict Paradigmã€ã‚’ææ¡ˆã™ã‚‹æœ€åˆã®ç ”ç©¶ã§ã‚ã‚‹ã€‚
â€¢ We create a collection of personalized prompts that cover five different recommendation task families.

- ç§ãŸã¡ã¯ã€5ã¤ã®ç•°ãªã‚‹æ¨è–¦ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’ã‚«ãƒãƒ¼ã™ã‚‹ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¾ã™ã€‚
  â€¢ According to the experimental results, P5 achieves promising performances on the five task families when taking seen prompt templates as model inputs.
- å®Ÿé¨“çµæœã«ã‚ˆã‚‹ã¨ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ã¨ã—ãŸå ´åˆã€P5ã¯5ã¤ã®ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã§æœ‰æœ›ãªæ€§èƒ½ã‚’é”æˆã—ãŸã€‚
  â€¢ P5 shows sufficient zero-shot generalization ability for novel personalized prompts and new items in unseen domains.
- P5ã¯ã€æ–°è¦ã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„ã€æœªçŸ¥ã®é ˜åŸŸã«ãŠã‘ã‚‹æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã—ã¦ã€ååˆ†ãªã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ±åŒ–èƒ½åŠ›ã‚’ç¤ºã™ã€‚

# Related Work é–¢é€£ä½œå“

Unified Frameworks.
çµ±ä¸€ã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‚
Many prior works have pursued to solve various tasks in a unified model.
å¤šãã®å…ˆè¡Œç ”ç©¶ãŒã€çµ±ä¸€ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã™ã‚‹ã“ã¨ã‚’è¿½æ±‚ã—ã¦ããŸã€‚
As early pioneers, T5 [47] and GPT3 [2] unifies NLP downstream tasks through text-to-text encoderâ€“ decoder framework and autoregressive language modeling, respectively.
åˆæœŸã®ãƒ‘ã‚¤ã‚ªãƒ‹ã‚¢ã¨ã—ã¦ã€T5 [47]ã¨GPT3 [2]ã¯ã€ãã‚Œãã‚Œãƒ†ã‚­ã‚¹ãƒˆ-ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼-ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨è‡ªå·±å›å¸°è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ã‚ˆã£ã¦ã€NLPã®ä¸‹æµã‚¿ã‚¹ã‚¯ã‚’çµ±åˆã—ã¦ã„ã‚‹ã€‚
They both allow effective knowledge sharing among different tasks based on a common pretrained language model.
ã©ã¡ã‚‰ã‚‚ã€å…±é€šã®äº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ã„ã¦ã€ç•°ãªã‚‹ã‚¿ã‚¹ã‚¯é–“ã§åŠ¹æœçš„ã«çŸ¥è­˜ã‚’å…±æœ‰ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
Following this trend, recent advances started to focus on unifying large-scale language tasks [1, 51, 67] or cross-modality applications [6, 66, 71] through a shared sequence-to-sequence framework, where different types of tasks and modalities are all expressed in the format of natural language.
ã“ã®å‚¾å‘ã‚’å—ã‘ã€æœ€è¿‘ã®é€²æ­©ã¯ã€ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®ã‚¿ã‚¹ã‚¯ã‚„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãŒã™ã¹ã¦è‡ªç„¶è¨€èªã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¡¨ç¾ã•ã‚Œã‚‹ã€å…±æœ‰ã•ã‚ŒãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹é–“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’é€šã˜ã¦ã€å¤§è¦æ¨¡ãªè¨€èªã‚¿ã‚¹ã‚¯[1, 51, 67]ã‚„ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³[6, 66, 71]ã‚’çµ±ä¸€ã™ã‚‹ã“ã¨ã«ç„¦ç‚¹ã‚’å½“ã¦å§‹ã‚ãŸã€‚
However, aforementioned methods never consider personalization in their sequence-to-sequence models.
ã—ã‹ã—ã€å‰è¿°ã®æ–¹æ³•ã§ã¯ã€é…åˆ—é–“ãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ãŒè€ƒæ…®ã•ã‚Œã‚‹ã“ã¨ã¯ãªã„ã€‚
Recently, a line of work [56, 57, 72] attempt to learn universal user representations which are easily transferrable to downstream tasks.
æœ€è¿‘ã€ä¸‹æµã®ã‚¿ã‚¹ã‚¯ã«å®¹æ˜“ã«ç§»è¡Œã§ãã‚‹æ™®éçš„ãªãƒ¦ãƒ¼ã‚¶ãƒ¼è¡¨ç¾ã‚’å­¦ç¿’ã—ã‚ˆã†ã¨ã™ã‚‹ä¸€é€£ã®ç ”ç©¶ [56, 57, 72] ãŒè¡Œã‚ã‚Œã¦ã„ã‚‹ã€‚
One limitation of these methods is that they still require additional finetuning on downstream datasets.
ã“ã‚Œã‚‰ã®æ–¹æ³•ã®é™ç•Œã®ã²ã¨ã¤ã¯ã€ä¸‹æµã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã•ã‚‰ã«å¾®èª¿æ•´ãŒå¿…è¦ãªã“ã¨ã§ã‚ã‚‹ã€‚
In contrast, our P5 first takes personalization into an encoder-decoder Transformer model that can generalize to a wide spectrum of recommendation related application scenarios â€“ tasks that naturally require personalization.
å¯¾ç…§çš„ã«ã€æˆ‘ã€…ã®P5ã¯ã€ã¾ãšãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã«å–ã‚Šè¾¼ã¿ã€æ¨è–¦ã«é–¢é€£ã™ã‚‹å¹…åºƒã„ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒŠãƒªã‚ªï¼ˆå½“ç„¶ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¿…è¦ã¨ã™ã‚‹ã‚¿ã‚¹ã‚¯ï¼‰ã«ä¸€èˆ¬åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
Moreover, with the help of prompt-based pretraining, P5 acquires zero-shot generalization ability when transferring to unseen prompts and items.
ã•ã‚‰ã«ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ãäº‹å‰è¨“ç·´ã«ã‚ˆã‚Šã€P5ã¯æœªè¦‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„ã‚¢ã‚¤ãƒ†ãƒ ã«ç§»è¡Œã™ã‚‹éš›ã«ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ±åŒ–èƒ½åŠ›ã‚’ç²å¾—ã™ã‚‹ã€‚

Prompt Learning.
å­¦ç¿’ã‚’ä¿ƒã™ã€‚
The success of GPT series especially GPT-3 [2] marked the beginning of promptâ€™s popularization on NLP tasks.
GPTã‚·ãƒªãƒ¼ã‚ºã€ç‰¹ã«GPT-3 [2]ã®æˆåŠŸã¯ã€NLPã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ™®åŠã®å§‹ã¾ã‚Šã¨ãªã£ãŸã€‚
Trained with huge language data from the Web, GPT-3 exhibited the capability of solving NLP tasks when provided a number of inputoutput examples as exemplar prompts.
ã‚¦ã‚§ãƒ–ä¸Šã®è†¨å¤§ãªè¨€èªãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦å­¦ç¿’ã—ãŸGPT-3ã¯ã€å¤šãã®å…¥å‡ºåŠ›ä¾‹ã‚’æ¨¡ç¯„çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã—ã¦æä¾›ã™ã‚‹ã“ã¨ã§ã€è‡ªç„¶è¨€èªå‡¦ç†ã‚¿ã‚¹ã‚¯ã‚’è§£ãèƒ½åŠ›ã‚’ç¤ºã—ãŸã€‚
Besides exemplar prompts, many prompt design methods have proliferated following the â€œpretrain, prompt, and predictâ€ paradigm [37].
æ¨¡ç¯„çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»¥å¤–ã«ã‚‚ã€ã€Œäº‹å‰è¨“ç·´ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€äºˆæ¸¬ã€ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ  [37]ã«å¾“ã£ã¦ã€å¤šãã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆæ‰‹æ³•ãŒæ™®åŠã—ã¦ã„ã‚‹ã€‚
One type of the methods [16, 23, 36, 40, 58] explored prompt search for proper discrete prompts.
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¢ç´¢æ‰‹æ³•ã®ä¸€ç¨® [16, 23, 36, 40, 58] ã¯ã€é©åˆ‡ãªé›¢æ•£ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¢ç´¢ã™ã‚‹ã‚‚ã®ã§ã‚ã‚‹ã€‚
Meanwhile, another line of work [18, 28, 33, 38, 45, 81] exploited continuous vector embeddings as prompts.
ä¸€æ–¹ã€åˆ¥ã®ç ”ç©¶ [18, 28, 33, 38, 45, 81]ã§ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã—ã¦é€£ç¶šãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹ã€‚
Compared with the aforementioned prompt types, instruction-based prompts contain detailed task descriptions and adhere more to the natural language format.
å‰è¿°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¿ã‚¤ãƒ—ã¨æ¯”è¼ƒã—ã¦ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ã€è©³ç´°ãªã‚¿ã‚¹ã‚¯ã®èª¬æ˜ã‚’å«ã¿ã€ã‚ˆã‚Šè‡ªç„¶è¨€èªå½¢å¼ã«å¿ å®Ÿã§ã‚ã‚‹ã€‚
Since instruction-based prompts are flexible and close to how humans communicate with each other, several pioneer works [11, 68] claim that learning from crowd-sourced NLP datasets is a promising route for general purpose NLP systems.
æŒ‡ç¤ºãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯æŸ”è»Ÿæ€§ãŒã‚ã‚Šã€äººé–“åŒå£«ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ–¹æ³•ã«è¿‘ã„ãŸã‚ã€ã„ãã¤ã‹ã®å…ˆé§†çš„ãªç ”ç©¶[11, 68]ã§ã¯ã€ã‚¯ãƒ©ã‚¦ãƒ‰ã‚½ãƒ¼ã‚·ãƒ³ã‚°ã•ã‚ŒãŸNLPãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ã®å­¦ç¿’ã¯ã€æ±ç”¨NLPã‚·ã‚¹ãƒ†ãƒ ã®æœ‰æœ›ãªãƒ«ãƒ¼ãƒˆã§ã‚ã‚‹ã¨ä¸»å¼µã—ã¦ã„ã‚‹ã€‚
Recent works such as FLAN [67] and T0 [51] finetuned pretrained language models on large-scale NLP datasets verbalized via humanreadable prompts.
FLAN [67]ã‚„T0 [51]ã®ã‚ˆã†ãªæœ€è¿‘ã®ç ”ç©¶ã¯ã€äººé–“ãŒèª­ã‚€ã“ã¨ã®ã§ãã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä»‹ã—ã¦è¨€èªåŒ–ã•ã‚ŒãŸå¤§è¦æ¨¡ãªNLPãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸Šã§äº‹å‰ã«è¨“ç·´ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¦ã„ã‚‹ã€‚
As a result, such multitask prompt-based tuning brings powerful models that exhibit strong zero-shot ability on unseen tasks.
ãã®çµæœã€ã“ã®ã‚ˆã†ãªãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ããƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€æœªè¦‹ã®ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦å¼·åŠ›ãªã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆèƒ½åŠ›ã‚’ç™ºæ®ã™ã‚‹å¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«ã‚’ã‚‚ãŸã‚‰ã™ã€‚
Inspired by the success of these approaches, we create a collection of personalized prompts and then train a sequenceto-sequence model on a variety of recommendation related tasks verbalized according to the constructed personalized prompts.
ã“ã‚Œã‚‰ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æˆåŠŸã«è§¦ç™ºã•ã‚Œã€æˆ‘ã€…ã¯ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã€æ§‹ç¯‰ã•ã‚ŒãŸãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¾“ã£ã¦è¨€èªåŒ–ã•ã‚ŒãŸæ¨è–¦ã«é–¢é€£ã™ã‚‹æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹-ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹ã€‚

NLP for Recommendation.
æ¨è–¦ã®ãŸã‚ã®NLPã€‚
Recommendation has been interacting with NLP techniques for a long time.
ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¯é•·ã„é–“ã€NLPãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã¨ç›¸äº’ä½œç”¨ã—ã¦ããŸã€‚
The main work mostly address four lines of research: 1) explainable recommendation [4, 10, 30â€“32, 75, 77] where NLP models help generating text explanations for a given recommendation; 2) sequential recommendation as language modeling [9, 60, 80] which considers user interaction histories as word token sequences; 3) text feature extraction [69, 74, 79] which aims to extract informative text encodings that can improve the performance of recommendation; and 4) conversational recommendation [8, 12â€“14, 22, 61, 76] that reasons the intent of users and gives recommendation in an interactive dialog format.
ä¸»ãªç ”ç©¶ãƒ†ãƒ¼ãƒã¯ä»¥ä¸‹ã®4ã¤ã§ã‚ã‚‹ï¼š 1) èª¬æ˜å¯èƒ½ãªæ¨è–¦ [4, 10, 30-32, 75, 77]: NLP ãƒ¢ãƒ‡ãƒ«ãŒã€ä¸ãˆã‚‰ã‚ŒãŸæ¨è–¦ã«å¯¾ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆèª¬æ˜ã‚’ç”Ÿæˆã™ã‚‹ã®ã«å½¹ç«‹ã¤ã€‚2) è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã¨ã—ã¦ã®é€æ¬¡æ¨è–¦ [9, 60, 80]: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®å¯¾è©±å±¥æ­´ã‚’å˜èªãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã¨ã—ã¦è€ƒæ…®ã™ã‚‹ã€‚
In our work, we explicitly covers the tasks of sequential recommendation and explanation generation, and additionally offers insights on how to formulate a unified NLP framework for other recommendation problems including rating prediction, top-k recommendation, and review summarization.
ç§ãŸã¡ã®ç ”ç©¶ã§ã¯ã€é€æ¬¡æ¨è–¦ã¨èª¬æ˜ç”Ÿæˆã®ã‚¿ã‚¹ã‚¯ã‚’æ˜ç¤ºçš„ã«ã‚«ãƒãƒ¼ã—ã€ã•ã‚‰ã«ã€è©•ä¾¡äºˆæ¸¬ã€ãƒˆãƒƒãƒ—kæ¨è–¦ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦ç´„ã‚’å«ã‚€ä»–ã®æ¨è–¦å•é¡Œã®ãŸã‚ã®çµ±ä¸€çš„ãªNLPãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ç­–å®šã™ã‚‹æ–¹æ³•ã«ã¤ã„ã¦ã®æ´å¯Ÿã‚’æä¾›ã™ã‚‹ã€‚
Furthermore, pretrained with instructionbased prompts that share similarity with conversational recommendation, our P5 benefits from the natural language environment and improves the performance on a series of recommendation tasks.
ã•ã‚‰ã«ã€ä¼šè©±ã«ã‚ˆã‚‹æ¨è–¦ã¨é¡ä¼¼æ€§ã‚’æŒã¤æŒ‡ç¤ºãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§äº‹å‰ã«è¨“ç·´ã™ã‚‹ã“ã¨ã§ã€æˆ‘ã€…ã®P5ã¯è‡ªç„¶è¨€èªç’°å¢ƒã®æ©æµã‚’å—ã‘ã€ä¸€é€£ã®æ¨è–¦ã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚
Zero-shot and Cold Start Recommendation.
ã‚¼ãƒ­ãƒ»ã‚·ãƒ§ãƒƒãƒˆã¨ã‚³ãƒ¼ãƒ«ãƒ‰ãƒ»ã‚¹ã‚¿ãƒ¼ãƒˆã®æ¨å¥¨ã€‚
Recommender systemsâ€™ performances heavily rely on the available training data, but there are always zero-shot cases where the history records are limited.
ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½ã¯ã€åˆ©ç”¨å¯èƒ½ãªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¤§ããä¾å­˜ã™ã‚‹ãŒã€å±¥æ­´è¨˜éŒ²ãŒé™ã‚‰ã‚Œã¦ã„ã‚‹ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã®ã‚±ãƒ¼ã‚¹ã¯å¸¸ã«å­˜åœ¨ã™ã‚‹ã€‚
The evidences of performing well on such startup cases signal a good generalization ability of recommendation models.
ã“ã®ã‚ˆã†ãªã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã®ã‚±ãƒ¼ã‚¹ã§è‰¯å¥½ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ãŸã“ã¨ã¯ã€æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–èƒ½åŠ›ã®é«˜ã•ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
One widely studied problem under this setting is the cold-start recommendation where users [26] or items [53] are new to the system with no previous interaction records.
ã“ã®è¨­å®šã®ä¸‹ã§åºƒãç ”ç©¶ã•ã‚Œã¦ã„ã‚‹å•é¡Œã®1ã¤ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼[26]ã‚„ã‚¢ã‚¤ãƒ†ãƒ [53]ãŒã€éå»ã«ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã®è¨˜éŒ²ãŒãªãã€ã‚·ã‚¹ãƒ†ãƒ ã«ã¨ã£ã¦æ–°ã—ã„ã‚‚ã®ã§ã‚ã‚‹å ´åˆã®ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆæ¨è–¦ã§ã‚ã‚‹ã€‚
Solutions to this problem either learn to model content features [15, 29, 44, 55] so that inference can be made without interaction records or learn to transfer representations from auxiliary domains [42, 56, 59, 72, 82].
ã“ã®å•é¡Œã«å¯¾ã™ã‚‹è§£æ±ºç­–ã¯ã€ç›¸äº’ä½œç”¨ã®è¨˜éŒ²ãŒãªãã¦ã‚‚æ¨è«–ãŒã§ãã‚‹ã‚ˆã†ã«ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç‰¹å¾´ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ã“ã¨ã‚’å­¦ç¿’ã™ã‚‹ã‹[15, 29, 44, 55]ã€è£œåŠ©çš„ãªãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã‚‰è¡¨ç¾ã‚’è»¢é€ã™ã‚‹ã“ã¨ã‚’å­¦ç¿’ã™ã‚‹[42, 56, 59, 72, 82]ã€‚
Another line of work for zero-shot or few-shot recommendation discusses the quick adaptation to the new domain instead of providing recommendation for cold-start cases only.
ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã¾ãŸã¯æ•°ã‚·ãƒ§ãƒƒãƒˆã®æ¨è–¦ã®ãŸã‚ã®åˆ¥ã®ä½œæ¥­ãƒ©ã‚¤ãƒ³ã¯ã€ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆã®ã‚±ãƒ¼ã‚¹ã®ã¿ã«æ¨è–¦ã‚’æä¾›ã™ã‚‹ã®ã§ã¯ãªãã€æ–°ã—ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ã¸ã®è¿…é€Ÿãªé©å¿œã«ã¤ã„ã¦è­°è«–ã—ã¦ã„ã‚‹ã€‚
Solutions typically follow the meta learning [27, 64] or causal learning [34] frameworks that make the model robust to domain adaptations.
è§£æ±ºç­–ã¯é€šå¸¸ã€ãƒ¡ã‚¿å­¦ç¿’[27, 64]ã¾ãŸã¯å› æœå­¦ç¿’[34]ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«å¾“ã†ã€‚
In our work, we ask P5 model pretrained on an auxiliary domain to solve tasks on target domains, where the users are known to P5 but the items have never been seen by the model before.
æˆ‘ã€…ã®ç ”ç©¶ã§ã¯ã€è£œåŠ©ãƒ‰ãƒ¡ã‚¤ãƒ³ã§äº‹å‰å­¦ç¿’ã•ã‚ŒãŸP5ãƒ¢ãƒ‡ãƒ«ã«ã€P5ã«ã¨ã£ã¦ãƒ¦ãƒ¼ã‚¶ã¯æ—¢çŸ¥ã§ã‚ã‚‹ãŒã‚¢ã‚¤ãƒ†ãƒ ã¯è¦‹ãŸã“ã¨ãŒãªã„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã™ã‚‹ã‚ˆã†ä¾é ¼ã™ã‚‹ã€‚

# Personalized Prompt Collection ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ»ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³

To facilitate the multitask prompt-based pretraining for recommendation, we create a collection of personalized prompt templates.
æ¨è–¦ã®ãŸã‚ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ã®äº‹å‰å­¦ç¿’ã‚’å®¹æ˜“ã«ã™ã‚‹ãŸã‚ã«ã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ»ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹ã€‚
The collection covers five different task families â€“ rating, sequential recommendation, explanation, review, and direct recommendation.
ã“ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã¯ã€æ ¼ä»˜ã‘ã€é€æ¬¡æ¨è–¦ã€èª¬æ˜ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€ç›´æ¥æ¨è–¦ã¨ã„ã†5ã¤ã®ç•°ãªã‚‹ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹ã€‚
Each of these task families contains multiple personalized prompts to help P5 discover various aspects about users and items.
ã“ã‚Œã‚‰ã®ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã¯ãã‚Œãã‚Œã€P5ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚„ã‚¢ã‚¤ãƒ†ãƒ ã«é–¢ã™ã‚‹ã•ã¾ã–ã¾ãªå´é¢ã‚’ç™ºè¦‹ã§ãã‚‹ã‚ˆã†ã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸè¤‡æ•°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã‚“ã§ã„ã‚‹ã€‚
As mentioned in [51], a prompt is considered as consisting of an input template and a target template, along with a collection of associated metadata.
51]ã§è¿°ã¹ãŸã‚ˆã†ã«ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯å…¥åŠ›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã€ ãŠã‚ˆã³é–¢é€£ã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€‚
In this work, we further define a personalized prompt as a prompt that includes personalized fields for different users and items.
ã“ã®ç ”ç©¶ã§ã¯ã€ã•ã‚‰ã«ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã€ã•ã¾ã–ã¾ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚„ã‚¢ã‚¤ãƒ†ãƒ ã«ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å®šç¾©ã™ã‚‹ã€‚
For example, a userâ€™s preference can be indicated through either an ID number or a description of the user such as name, gender, age, etc.
ä¾‹ãˆã°ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å—œå¥½ã¯ã€IDç•ªå·ã‹ã€åå‰ã€æ€§åˆ¥ã€å¹´é½¢ãªã©ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èª¬æ˜ã®ã©ã¡ã‚‰ã‹ã«ã‚ˆã£ã¦ç¤ºã™ã“ã¨ãŒã§ãã‚‹ã€‚
Moreover, the expected model output of a given personalized prompt should also vary according to its item field.
ã•ã‚‰ã«ã€ä¸ãˆã‚‰ã‚ŒãŸãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœŸå¾…ã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‚‚ã€ãã®é …ç›®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã‚ˆã£ã¦ç•°ãªã‚‹ã¯ãšã§ã‚ã‚‹ã€‚
This implies the change of userâ€™s preferences towards different items.
ã“ã‚Œã¯ã€ç•°ãªã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å—œå¥½ã®å¤‰åŒ–ã‚’æ„å‘³ã™ã‚‹ã€‚
Such item fields can be represented by either item ID numbers or item metadata that contains detailed descriptions.
ã“ã®ã‚ˆã†ãªé …ç›®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ã€é …ç›®IDç•ªå·ã¾ãŸã¯è©³ç´°ãªèª¬æ˜ã‚’å«ã‚€é …ç›®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã„ãšã‚Œã‹ã§è¡¨ã™ã“ã¨ãŒã§ãã‚‹ã€‚
We designed basic P5 personalized prompt collection for each task family.
ç§ãŸã¡ã¯ã€å„ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ãŸã‚ã«ã€åŸºæœ¬çš„ãªP5ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé›†ã‚’ãƒ‡ã‚¶ã‚¤ãƒ³ã—ã¾ã—ãŸã€‚
For rating prediction task family, we divide the prompts into three categories: 1) Given the information about a user and an item, directly predict the rating score ranging from 1 to 5; 2) Predict whether a user will rate an item a given score.
è©•ä¾¡äºˆæ¸¬ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã§ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’3ã¤ã®ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã™ã‚‹ï¼š 1) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚¤ãƒ†ãƒ ã«é–¢ã™ã‚‹æƒ…å ±ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€1ã‹ã‚‰5ã¾ã§ã®è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’ç›´æ¥äºˆæ¸¬ã™ã‚‹ã€‚
The expected output is yes or no; 3) Predict if a user likes or dislikes an item.
æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ã¯ã€Œã¯ã„ã€ã‹ã€Œã„ã„ãˆã€ã§ã‚ã‚‹ã€‚3) ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå•†å“ã‚’å¥½ãã‹å«Œã„ã‹ã‚’äºˆæ¸¬ã™ã‚‹ã€‚
Here we consider a star rating equal to or greater than 4 to be a like preference of the user, whereas lower scores indicate a dislike preference.
ã“ã“ã§ã¯ã€â˜…ã®è©•ä¾¡ãŒ4ä»¥ä¸Šã§ã‚ã‚Œã°ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ãŒã€Œå¥½ãã€ã§ã‚ã‚‹ã¨ã¿ãªã—ã€ãã‚Œä»¥ä¸‹ã§ã‚ã‚Œã°ã€Œå«Œã„ã€ã§ã‚ã‚‹ã¨ã¿ãªã™ã€‚
For sequential recommendation task family, we create three types of prompts: 1) Directly predict the next item based on user interaction history; 2) Given user interaction history, choose the possible next item from a candidate list, where only one item is positive; 3) Based on user interaction history, predict whether a given item will be interacted next by the user.
é€æ¬¡æ¨è–¦ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã§ã¯ã€3ç¨®é¡ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã™ã‚‹ï¼š 1) ãƒ¦ãƒ¼ã‚¶ã¨ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´ã«åŸºã¥ã„ã¦ã€æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ç›´æ¥äºˆæ¸¬ã™ã‚‹ã€‚2) ãƒ¦ãƒ¼ã‚¶ã¨ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´ã«åŸºã¥ã„ã¦ã€å€™è£œãƒªã‚¹ãƒˆã‹ã‚‰æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’é¸æŠã™ã‚‹ã€‚
For explanation task family, we ask P5 model to generate a textual explanation to justify a userâ€™s preference towards a given item.
èª¬æ˜ã‚¿ã‚¹ã‚¯ãƒ»ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã§ã¯ã€P5ãƒ¢ãƒ‡ãƒ«ã«ã€ä¸ãˆã‚‰ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã‚’æ­£å½“åŒ–ã™ã‚‹ãŸã‚ã®ãƒ†ã‚­ã‚¹ãƒˆèª¬æ˜ã‚’ç”Ÿæˆã™ã‚‹ã‚ˆã†ä¾é ¼ã™ã‚‹ã€‚
There are two prompt categories in this task family: 1) Directly generate an explanation sentence with user/item information; 2) Generate explanation based on a feature word as hint [31].
ã“ã®ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã«ã¯2ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚«ãƒ†ã‚´ãƒªãŒã‚ã‚‹ï¼š 1)ãƒ¦ãƒ¼ã‚¶/ã‚¢ã‚¤ãƒ†ãƒ æƒ…å ±ã‚’ç”¨ã„ã¦ç›´æ¥èª¬æ˜æ–‡ã‚’ç”Ÿæˆã™ã‚‹ã€2)ç‰¹å¾´èªã‚’ãƒ’ãƒ³ãƒˆã«èª¬æ˜æ–‡ã‚’ç”Ÿæˆã™ã‚‹[31]ã€‚
For each category, there could be other auxiliary information included such as the review headline and the star rating.
å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«ã¯ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®è¦‹å‡ºã—ã‚„æ˜Ÿã®è©•ä¾¡ãªã©ã€ãã®ä»–ã®è£œåŠ©çš„ãªæƒ…å ±ãŒå«ã¾ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€‚
For review related task family, we create two types of prompts: 1) Summarize review comment to a shorter review title; 2) Predict the corresponding rating score based on the given review comment.
ãƒ¬ãƒ“ãƒ¥ãƒ¼ã«é–¢é€£ã™ã‚‹ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã§ã¯ã€2ç¨®é¡ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã™ã‚‹ï¼š 1)ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆã‚’çŸ­ã„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¿ã‚¤ãƒˆãƒ«ã«è¦ç´„ã™ã‚‹ã€2)ä¸ãˆã‚‰ã‚ŒãŸãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆã«åŸºã¥ã„ã¦å¯¾å¿œã™ã‚‹è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’äºˆæ¸¬ã™ã‚‹ã€‚
For direct recommendation, we also create two types of prompts: 1) Predict whether to recommend an item to a user, the answer should be yes or no; 2) Select the most suitable item from a list of candidate items to recommend to the user.
ç›´æ¥æ¨è–¦ã®ãŸã‚ã«ã€2ç¨®é¡ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚‚ä½œæˆã™ã‚‹ï¼š 1) ã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æ¨è–¦ã™ã‚‹ã‹ã©ã†ã‹ã‚’äºˆæ¸¬ã—ã€ãã®ç­”ãˆãŒã€Œã¯ã„ã€ã‹ã€Œã„ã„ãˆã€ã§ã‚ã‚‹ã“ã¨ã€‚2) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æ¨è–¦ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®å€™è£œãƒªã‚¹ãƒˆã‹ã‚‰æœ€é©ãªã‚¢ã‚¤ãƒ†ãƒ ã‚’é¸æŠã™ã‚‹ã“ã¨ã€‚
We provide some example prompts in Figure 2, and the complete collection of personalized prompts are provided in the Appendix.
å›³ 2 ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¾‹ã‚’ã„ãã¤ã‹ç¤ºã—ã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å…¨é›†ã¯ä»˜éŒ²ã§æä¾›ã™ã‚‹ã€‚
With the prompts, we can directly build inputâ€“target pairs from raw data.
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ãˆã°ã€ç”Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å…¥åŠ›ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒšã‚¢ã‚’ç›´æ¥ä½œã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
As illustrated in Figure 2, we can simply substitute the fields in braces with the corresponding information in the raw data and thus create training inputâ€“target pairs or zero-shot testing personalized prompts.
å›³2ã«ç¤ºã™ã‚ˆã†ã«ã€ä¸­æ‹¬å¼§å†…ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ç”Ÿãƒ‡ãƒ¼ã‚¿ã®å¯¾å¿œã™ã‚‹æƒ…å ±ã«ç½®ãæ›ãˆã‚‹ã ã‘ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å…¥åŠ›ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒšã‚¢ã€ã¾ãŸã¯ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
The training data and pre-training tasks will distill the rich semantics from diverse modalities into the user and item tokens for preference understanding and personalization.
å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨äº‹å‰å­¦ç¿’ã‚¿ã‚¹ã‚¯ã¯ã€å¤šæ§˜ãªãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‹ã‚‰è±Šå¯Œãªã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹ã‚’æŠ½å‡ºã—ã€å—œå¥½ã®ç†è§£ã¨ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›ã™ã‚‹ã€‚
Note that we divide the raw data into three partsâ€”rating/review/explanation share the same raw data, while sequential and direct recommendation differ in terms of whether to use interaction history as input information.
ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’3ã¤ã«åˆ†ã‘ã¦ã„ã‚‹ã€‚è©•ä¾¡ï¼ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼èª¬æ˜ã¯åŒã˜ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’å…±æœ‰ã—ã¦ã„ã‚‹ãŒã€é€æ¬¡ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰ã¨ç›´æ¥ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰ã¯ã€äº¤æµå±¥æ­´ã‚’å…¥åŠ›æƒ…å ±ã¨ã—ã¦ä½¿ã†ã‹ã©ã†ã‹ã¨ã„ã†ç‚¹ã§ç•°ãªã‚‹ã€‚
During pretraining, we mix the inputâ€“target pairs from different task families together to serve as the training data.
äº‹å‰è¨“ç·´ã§ã¯ã€ç•°ãªã‚‹ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®å…¥åŠ›ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒšã‚¢ã‚’æ··ãœã¦è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ã™ã‚‹ã€‚
To enhance P5â€™s robustness and zero-shot generalization, for each raw datum, we only sample a portion of rather than all of the personalized prompts in each task family.
P5ã®ãƒ­ãƒã‚¹ãƒˆæ€§ã¨ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ±åŒ–ã‚’é«˜ã‚ã‚‹ãŸã‚ã€å„ç”Ÿãƒ‡ãƒ¼ã‚¿ãƒ ã«ã¤ã„ã¦ã€å„ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã™ã¹ã¦ã§ã¯ãªãã€ä¸€éƒ¨ã®ã¿ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã€‚
In sequential and direct recommendation task families, we also randomly select a group of negative items for those prompts that require a candidate list.
é€æ¬¡æ¨è–¦ã¨ç›´æ¥æ¨è–¦ã®ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã§ã¯ã€å€™è£œãƒªã‚¹ãƒˆãŒå¿…è¦ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã¦ã€ãƒã‚¬ãƒ†ã‚£ãƒ–é …ç›®ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚‚ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠã™ã‚‹ã€‚

# The P5 Paradigm and Model P5ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¨ãƒ¢ãƒ‡ãƒ«

## The P5 Architecture P5ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

The collection of personalized prompts introduced in the previous section makes it convenient to create a large amount of available pretraining data that covers a wide range of recommendation related tasks.
å‰ç¯€ã§ç´¹ä»‹ã—ãŸãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã¯ã€æ¨è–¦ã«é–¢é€£ã™ã‚‹å¹…åºƒã„ã‚¿ã‚¹ã‚¯ã‚’ã‚«ãƒãƒ¼ã™ã‚‹å¤§é‡ã®åˆ©ç”¨å¯èƒ½ãªäº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹ã®ã«ä¾¿åˆ©ã§ã‚ã‚‹ã€‚
Thanks to the prompt templates, all pretraining data shares a unified format of inputâ€“target token sequences, which breaks the boundaries among different tasks.
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ»ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ãŠã‹ã’ã§ã€ã™ã¹ã¦ã®ãƒ—ãƒ¬ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»ãƒ‡ãƒ¼ã‚¿ã¯å…¥åŠ›-ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ»ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®çµ±ä¸€ã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å…±æœ‰ã—ã€ç•°ãªã‚‹ã‚¿ã‚¹ã‚¯é–“ã®å¢ƒç•Œã‚’ãªãã™ã“ã¨ãŒã§ãã‚‹ã€‚
We claim that pretraining multiple recommendation tasks under a unified framework of conditional generation can facilitate all involving tasks together.
æˆ‘ã€…ã¯ã€æ¡ä»¶ç”Ÿæˆã¨ã„ã†çµ±ä¸€çš„ãªæ çµ„ã¿ã®ã‚‚ã¨ã§è¤‡æ•°ã®æ¨è–¦ã‚¿ã‚¹ã‚¯ã‚’äº‹å‰å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€ã™ã¹ã¦ã®æ¨è–¦ã‚¿ã‚¹ã‚¯ã‚’ã¾ã¨ã‚ã¦ä¿ƒé€²ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã¨ä¸»å¼µã™ã‚‹ã€‚
By immersing P5 in the full language environment throughout the pretraining stage, we also expect its zero-shot generalization capability of understanding unseen personalized prompts with detailed item descriptions.
ãƒ—ãƒ¬ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®æ®µéšã‹ã‚‰P5ã‚’å®Œå…¨ãªè¨€èªç’°å¢ƒã«æµ¸ã™ã“ã¨ã§ã€è©³ç´°ãªé …ç›®èª¬æ˜ã®ã‚ã‚‹æœªè¦‹ã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç†è§£ã™ã‚‹ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ±åŒ–èƒ½åŠ›ã‚‚æœŸå¾…ã§ãã‚‹ã€‚
That is the reason why P5 is called a unified â€œPretrain, Personalized Prompt, and Predict Paradigmâ€.
ã“ã‚ŒãŒã€P5ãŒ "Pretrain, Personalized Prompt, and Predict Paradigm "ã¨å‘¼ã°ã‚Œã‚‹æ‰€ä»¥ã§ã‚ã‚‹ã€‚
In terms of the model architecture, our P5 is established upon a basic encoderâ€“decoder framework.
ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«é–¢ã—ã¦ã¯ã€æˆ‘ã€…ã®P5ã¯åŸºæœ¬çš„ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ä¸Šã«ç¢ºç«‹ã•ã‚Œã¦ã„ã‚‹ã€‚
We employ Transformer [65] blocks to build both the encoder and decoder.
ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¨ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®ä¸¡æ–¹ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã«ã€Transformer [65]ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã€‚
Suppose the embeddings of an input token sequence is x = [ğ‘¥1, Â· Â· Â· , ğ‘¥ğ‘›].
å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®åŸ‹ã‚è¾¼ã¿ã‚’x = [ğ‘¥1, - - , ğ‘¥ğ‘›]ã¨ã™ã‚‹ã€‚
As depicted in Figure 3, before feeding the embedding sequence into the bidirectional text encoder E (Â·), we add positional encodings P to the raw embeddings to capture their position information in the sequence.
å›³3ã«ç¤ºã™ã‚ˆã†ã«ã€åŸ‹ã‚è¾¼ã¿ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åŒæ–¹å‘ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€E (-)ã«å…¥åŠ›ã™ã‚‹å‰ã«ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ä½ç½®æƒ…å ±ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹ãŸã‚ã«ã€ç”Ÿã®åŸ‹ã‚è¾¼ã¿ã«ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°Pã‚’è¿½åŠ ã™ã‚‹ã€‚
Furthermore, to make P5 aware of the personalized information contained in the input sequence, we also apply whole-word embeddings W to indicate whether consecutive sub-word tokens are from the same original word.
ã•ã‚‰ã«ã€å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å«ã¾ã‚Œã‚‹ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸæƒ…å ±ã‚’P5ã«èªè­˜ã•ã›ã‚‹ãŸã‚ã€é€£ç¶šã™ã‚‹ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒˆãƒ¼ã‚¯ãƒ³ãŒåŒã˜å…ƒã®å˜èªã‹ã‚‰ã®ã‚‚ã®ã§ã‚ã‚‹ã‹ã©ã†ã‹ã‚’ç¤ºã™ãƒ›ãƒ¼ãƒ«ãƒ¯ãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿Wã‚‚é©ç”¨ã™ã‚‹ã€‚
For instance, if we directly represent the item with ID number 7391 as â€œitem*7391â€, then the word will be split into 4 separate tokens (i.e., â€œitemâ€, â€œ*â€, â€œ73â€, â€œ91â€) by SentencePiece tokenizer [54].
ä¾‹ãˆã°ã€IDç•ªå·7391ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ "item*7391 "ã¨ç›´æ¥è¡¨ç¾ã™ã‚‹å ´åˆã€ã“ã®å˜èªã¯SentencePieceãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼[54]ã«ã‚ˆã£ã¦4ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆã™ãªã‚ã¡ã€"item"ã€"*"ã€"73"ã€"91"ï¼‰ã«åˆ†å‰²ã•ã‚Œã‚‹ã€‚
With the assistance of the shared whole-word embedding â€œâŸ¨w10âŸ©â€ (e.g., in Figure 3), P5 can better recognize the important field with personalized information.
å…±æœ‰ã•ã‚ŒãŸå…¨å˜èªåŸ‹ã‚è¾¼ã¿"âŸ¨w10âŸ©"ï¼ˆä¾‹ãˆã°å›³3ï¼‰ã®æ”¯æ´ã«ã‚ˆã‚Šã€P27ã¯ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸæƒ…å ±ã‚’æŒã¤é‡è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚ˆã‚Šã‚ˆãèªè­˜ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
Another alternative is to represent each user/item by an independent extra token (e.g., â€œâŸ¨item_7391âŸ©â€).
åˆ¥ã®æ–¹æ³•ã¨ã—ã¦ã€å„ãƒ¦ãƒ¼ã‚¶ãƒ¼/ã‚¢ã‚¤ãƒ†ãƒ ã‚’ç‹¬ç«‹ã—ãŸè¿½åŠ ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆä¾‹ãˆã°"âŸ¨item_7391âŸ©"ï¼‰ã§è¡¨ã™ã“ã¨ã‚‚ã§ãã‚‹ã€‚
However, this may incur huge amounts of additional tokens when there is a large pool of users and items.
ã—ã‹ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒ—ãƒ¼ãƒ«ãŒå¤§ãã„å ´åˆã€ãƒˆãƒ¼ã‚¯ãƒ³ã®è¿½åŠ ã‚³ã‚¹ãƒˆãŒè†¨å¤§ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
Hence, in this paper, we adopt multiple sub-word units to represent a user or item.
ãã“ã§æœ¬ç¨¿ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚„ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¡¨ç¾ã™ã‚‹ãŸã‚ã«è¤‡æ•°ã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰å˜ä½ã‚’æ¡ç”¨ã™ã‚‹ã€‚
Afterwards, the text encoder takes the sum of the aforementioned three embeddings e = [ğ‘’1, Â· Â· Â· , ğ‘’ğ‘›] and outputs their contextualized representations t = [ğ‘¡1, Â· Â· Â· , ğ‘¡ğ‘›] = E (e).
ãã®å¾Œã€ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯ã€å‰è¿°ã®3ã¤ã®åŸ‹ã‚è¾¼ã¿e = [â†ªLl_1D452, - - , ğ‘’ğ‘›]ã®åˆè¨ˆã‚’å–ã‚Šã€ãã‚Œã‚‰ã®æ–‡è„ˆåŒ–è¡¨ç¾t = [â†ªLl_1D461, - - , ğ‘¡ğ‘›] = E (e)ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚
The decoder D (Â·) then attends to both the previously generated tokens y<ğ‘— and the encoder output t and predicts the probability distribution of future tokens: ğ‘ƒğœƒ yğ‘— | y<ğ‘— , x  = D (y<ğ‘— , t).
ãƒ‡ã‚³ãƒ¼ãƒ€ D (-) ã¯ã€ä»¥å‰ã«ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ y<\_1D457 ã¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ› t ã®ä¸¡æ–¹ã«æ³¨ç›®ã—ã€å°†æ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºç‡åˆ†å¸ƒã‚’äºˆæ¸¬ã™ã‚‹ï¼š ğ‘ƒ | yğ‘— , x = D (y<ğ‘— , t)ã€‚
During the pretraining stage, P5 learns the model parameters ğœƒ by minimizing the negative log-likelihood of label tokens y conditioned on input text x in an
äº‹å‰å­¦ç¿’æ®µéšã§ã¯ã€P5ã¯å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆxã‚’æ¡ä»¶ã¨ã™ã‚‹ãƒ©ãƒ™ãƒ«ãƒˆãƒ¼ã‚¯ãƒ³yã®è² ã®å¯¾æ•°å°¤åº¦ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Å°ã‚’å­¦ç¿’ã™ã‚‹ã€‚

end-to-end manne:
ã‚¨ãƒ³ãƒ‰ãƒ»ãƒ„ãƒ¼ãƒ»ã‚¨ãƒ³ãƒ‰ã®ãƒãƒ³ãƒ

$$
\tag{1}
$$

This same objective function is shared by all recommendation tasks under P5.
ã“ã®åŒã˜ç›®çš„é–¢æ•°ã¯ã€P5ã®ã™ã¹ã¦ã®æ¨è–¦ã‚¿ã‚¹ã‚¯ã§å…±æœ‰ã•ã‚Œã‚‹ã€‚
As a result, we unify recommendation tasks with one model, one loss, and one data format
ãã®çµæœã€æ¨è–¦ã‚¿ã‚¹ã‚¯ã¯1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã€1ã¤ã®æå¤±ã€1ã¤ã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã§çµ±ä¸€ã•ã‚Œã‚‹ã€‚

## Recommendation with Pretrained P5 äº‹å‰è¨“ç·´ã•ã‚ŒãŸP5ã«ã‚ˆã‚‹æ¨è–¦

After pretraining, P5 can directly perform different tasks with either seen or unseen personalized prompts.
äº‹å‰å­¦ç¿’å¾Œã€P5ã¯ã€ç›®ã«è¦‹ãˆã‚‹ã€ã¾ãŸã¯ç›®ã«è¦‹ãˆãªã„ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€ã•ã¾ã–ã¾ãªã‚¿ã‚¹ã‚¯ã‚’ç›´æ¥å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
For rating, explanation, and review tasks, we simply use greedy decoding to generate answers.
è©•ä¾¡ã€èª¬æ˜ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã‚¿ã‚¹ã‚¯ã§ã¯ã€å˜ã«è²ªæ¬²ãªè§£èª­ã‚’ä½¿ã£ã¦ç­”ãˆã‚’ç”Ÿæˆã™ã‚‹ã€‚
In contrast, sequential and direct recommendation tasks usual require an item list as target output.
å¯¾ç…§çš„ã«ã€é€æ¬¡æ¨è–¦ã‚¿ã‚¹ã‚¯ã‚„ç›´æ¥æ¨è–¦ã‚¿ã‚¹ã‚¯ã¯ã€é€šå¸¸ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå‡ºåŠ›ã¨ã—ã¦ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆã‚’å¿…è¦ã¨ã™ã‚‹ã€‚
In view of this, for sequential recommendation, we apply beam search to generate a list of potential next items and evaluate it under the all-item setting.
ãã“ã§ã€é€æ¬¡æ¨è–¦ã§ã¯ã€ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã‚’é©ç”¨ã—ã¦æ¬¡ã®å€™è£œãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã—ã€å…¨é …ç›®è¨­å®šã®ä¸‹ã§è©•ä¾¡ã™ã‚‹ã€‚
For direct recommendation, we predict the recommended items from a candidate set S = {ğ‘†1, Â· Â· Â· , ğ‘†ğ‘š}, where only one of the ğ‘š candidates is positive.
ç›´æ¥æ¨è–¦ã§ã¯ã€å€™è£œé›†åˆS = {â†ªLu_1D446, - - , â†ªLu_1D445A} ã‹ã‚‰æ¨è–¦é …ç›®ã‚’äºˆæ¸¬ã™ã‚‹ã€‚
Here, we also use beam search to decode a list of potential target items with the highest scores and then conduct evaluations.
ã“ã“ã§ã‚‚ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã‚’ä½¿ã£ã¦ã€ã‚¹ã‚³ã‚¢ã®é«˜ã„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€™è£œã®ãƒªã‚¹ãƒˆã‚’è§£èª­ã—ã€è©•ä¾¡ã‚’è¡Œã†ã€‚
Both of the above decoding processes can be written as:
ä¸Šè¨˜ã®ãƒ‡ã‚³ãƒ¼ãƒ‰å‡¦ç†ã¯ã€ã©ã¡ã‚‰ã‚‚æ¬¡ã®ã‚ˆã†ã«æ›¸ãã“ã¨ãŒã§ãã‚‹ï¼š

$$
\tag{2}
$$

where ğµ denotes the beam size and C is the output item list.
ã“ã“ã§ã€â†ªL_1D435 ã¯ãƒ“ãƒ¼ãƒ ã‚µã‚¤ã‚ºã‚’è¡¨ã—ã€C ã¯å‡ºåŠ›é …ç›®ãƒªã‚¹ãƒˆã‚’è¡¨ã™ã€‚

# Experiments å®Ÿé¨“

In this section, we evaluate the performance of the proposed P5 approach on real-world data and compare it with various representative methods targeting at different task families.
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ææ¡ˆã™ã‚‹P5ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ€§èƒ½ã‚’å®Ÿä¸–ç•Œã®ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ã—ã€ç•°ãªã‚‹ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’å¯¾è±¡ã¨ã—ãŸæ§˜ã€…ãªä»£è¡¨çš„æ‰‹æ³•ã¨æ¯”è¼ƒã™ã‚‹ã€‚
Through the performance comparison and ablation studies, we aim to answer the following research questions regarding our unified â€œPretrain, Personalized Prompt, and Predict Pargadigmâ€ (P5): â€¢ RQ1: How does our unified P5 framework perform compared with task-specific methods on all five task families? â€¢ RQ2: Does P5 have enough zero-shot generalization ability when transferring to unseen personalized prompts for either existing or new items? â€¢ RQ3: How do scaling factors such as model size, number of task families, and number of prompts affect the performance of P5? â€¢ RQ4: Which is a better way to implement personalization in P5: adopting an independent extra token for each user or item (e.g., â€œâŸ¨user*23âŸ©â€) or the default setting, i.e., tokenizing each user or item into multiple sub-word units (e.g., â€œuserâ€, â€œ*â€, â€œ23â€)? â€¢ RQ5: How long does it take for P5 to conduct pretraining? Is it efficient to make inference with the pretrained P5 model? We provide statistics on training and inference time in the Appendix.
æ€§èƒ½æ¯”è¼ƒã¨ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ã‚’é€šã˜ã¦ã€æˆ‘ã€…ã®çµ±ä¸€çš„ãªã€ŒPretrain, Personalized Prompt, and Predict Pargadigmã€(P5)ã«é–¢ã™ã‚‹ä»¥ä¸‹ã®ç ”ç©¶è³ªå•ã«ç­”ãˆã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã™ã‚‹ï¼š - RQ1ï¼š RQ1ï¼šæˆ‘ã€…ã®çµ±ä¸€ã•ã‚ŒãŸP5ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€5ã¤ã®ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼å…¨ã¦ã«ãŠã„ã¦ã€ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®æ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦ã©ã®ã‚ˆã†ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™ã‹ï¼Ÿ- RQ2ï¼š P5ã¯ã€æ—¢å­˜ã¾ãŸã¯æ–°è¦ã‚¢ã‚¤ãƒ†ãƒ ã®æœªçŸ¥ã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ç§»è¡Œã™ã‚‹éš›ã«ã€ååˆ†ãªã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ±åŒ–èƒ½åŠ›ã‚’æŒã¤ã‹ï¼Ÿ- RQ3ï¼š ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã€ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼æ•°ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ•°ãªã©ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°è¦å› ã¯P5ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ï¼Ÿ- RQ4ï¼š P5ã«ãŠã‘ã‚‹ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè£…æ–¹æ³•ã¨ã—ã¦ã€ã©ã¡ã‚‰ãŒå„ªã‚Œã¦ã„ã‚‹ã‹ï¼š ä¾‹ï¼š"âŸ¨user*23âŸ©"ï¼‰ã¨ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è¨­å®šï¼ˆä¾‹ï¼š"user", "*", "23"ï¼‰ã§ã¯ã€å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚„ã‚¢ã‚¤ãƒ†ãƒ ã¯è¤‡æ•°ã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰å˜ä½ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚Œã‚‹ã€‚- RQ5ï¼š P5ãŒäº‹å‰å­¦ç¿’ã‚’è¡Œã†ã®ã«ã‹ã‹ã‚‹æ™‚é–“ã¯ï¼Ÿäº‹å‰å­¦ç¿’ã•ã‚ŒãŸP5ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸæ¨è«–ã¯åŠ¹ç‡çš„ã‹ï¼Ÿå­¦ç¿’ã¨æ¨è«–ã«ã‹ã‹ã‚‹æ™‚é–“ã®çµ±è¨ˆã‚’ä»˜éŒ²ã§æä¾›ã—ã¾ã™ã€‚

## Experimental Setup å®Ÿé¨“ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

Datasets.
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
We conduct extensive experiments over four real-world datasets.
æˆ‘ã€…ã¯ã€4ã¤ã®å®Ÿä¸–ç•Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦åºƒç¯„ãªå®Ÿé¨“ã‚’è¡Œã£ãŸã€‚
The Amazon1 datasets are collected from Amazon.com platform with user ratings and reviews on 29 categories of products.
Amazon1ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Amazon.comãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‹ã‚‰åé›†ã•ã‚ŒãŸã‚‚ã®ã§ã€29ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®å•†å“ã«é–¢ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è©•ä¾¡ã¨ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã€‚
In this paper, we adopt three of them to evaluate our method, namely Sports & Outdoors, Beauty, as well as Toys & Games.
æœ¬ç¨¿ã§ã¯ã€ã‚¹ãƒãƒ¼ãƒ„ï¼†ã‚¢ã‚¦ãƒˆãƒ‰ã‚¢ã€ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼ã€ãƒˆã‚¤ï¼†ã‚²ãƒ¼ãƒ ã®3ã¤ã‚’æ¡ç”¨ã—ã€æœ¬æ‰‹æ³•ã‚’è©•ä¾¡ã™ã‚‹ã€‚
Besides, Yelp2 dataset contains a large number of user ratings and reviews for business recommendation.
ã•ã‚‰ã«ã€Yelp2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ã€ãƒ“ã‚¸ãƒã‚¹æ¨è–¦ã®ãŸã‚ã®å¤šæ•°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è©•ä¾¡ã¨ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã€‚
We follow [80] and use transaction records between January 1, 2019 to December 31, 2019.
80]ã«å¾“ã„ã€2019å¹´1æœˆ1æ—¥ã‹ã‚‰2019å¹´12æœˆ31æ—¥ã¾ã§ã®å–å¼•è¨˜éŒ²ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
Due to space limit and that the results on Yelp show similar trends with other datasets, we put the experimental results on Yelp dataset in the Appendix.
ç´™é¢ã®éƒ½åˆä¸Šã€ã¾ãŸYelpã®çµæœã¯ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨åŒæ§˜ã®å‚¾å‘ã‚’ç¤ºã—ã¦ã„ã‚‹ãŸã‚ã€Yelpãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å®Ÿé¨“çµæœã¯ä»˜éŒ²ã¨ã—ã¦æ²è¼‰ã™ã‚‹ã€‚
The detailed statistics of these datasets are presented in Table 1.
ã“ã‚Œã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è©³ç´°ãªçµ±è¨ˆã¯è¡¨1ã«ç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚

Task splits.
ã‚¿ã‚¹ã‚¯ã®åˆ†å‰²ã€‚
For rating, explanation, and review task families, we randomly split each dataset into training (80%), validation (10%) and testing (10%) sets, and ensure that there is at least one instance included in the training set for each user and item.
è©•ä¾¡ã€èª¬æ˜ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã«ã¤ã„ã¦ã¯ã€å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆï¼ˆ80ï¼…ï¼‰ã€æ¤œè¨¼ã‚»ãƒƒãƒˆï¼ˆ10ï¼…ï¼‰ã€ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆï¼ˆ10ï¼…ï¼‰ã«ãƒ©ãƒ³ãƒ€ãƒ ã«åˆ†å‰²ã—ã€å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚¤ãƒ†ãƒ ã«ã¤ã„ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã«å°‘ãªãã¨ã‚‚1ã¤ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒå«ã¾ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
To obtain the ground-truth explanations, following the natural language explanation works [30, 31], we first extract item feature words from the reviews with the help of the Sentires toolkit3 [77, 78], and then extract the sentences from reviews that comment on one or more item feature words as usersâ€™ explanation about their preference.
ã‚°ãƒ©ãƒ³ãƒ‰ãƒ»ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ã®èª¬æ˜ã‚’å¾—ã‚‹ãŸã‚ã«ã€è‡ªç„¶è¨€èªèª¬æ˜ã®ç ”ç©¶[30, 31]ã«å¾“ã£ã¦ã€ã¾ãšã€Sentires toolkit3 [77, 78]ã®åŠ©ã‘ã‚’å€Ÿã‚Šã¦ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰é …ç›®ç‰¹å¾´èªã‚’æŠ½å‡ºã—ã€æ¬¡ã«ã€1ã¤ä»¥ä¸Šã®é …ç›®ç‰¹å¾´èªã«ã‚³ãƒ¡ãƒ³ãƒˆã™ã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æ–‡ç« ã‚’ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã«é–¢ã™ã‚‹èª¬æ˜ã¨ã—ã¦æŠ½å‡ºã™ã‚‹ã€‚
In terms of sequential recommendation task family, for each user interaction sequence, the last item is used as the test data, the item before the last one is used as the validation data, and the remaining data is used for training.
é€æ¬¡æ¨è–¦ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®å ´åˆã€å„ãƒ¦ãƒ¼ã‚¶ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã—ã¦ã€æœ€å¾Œã®ã‚¢ã‚¤ãƒ†ãƒ ãŒãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã€æœ€å¾Œã®ã‚¢ã‚¤ãƒ†ãƒ ã®å‰ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒæ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã€æ®‹ã‚Šã®ãƒ‡ãƒ¼ã‚¿ãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã•ã‚Œã‚‹ã€‚
To avoid data leakage during pretraining, we follow the training split of sequential recommendation to build the training set for direct recommendation task family.
äº‹å‰å­¦ç¿’ä¸­ã®ãƒ‡ãƒ¼ã‚¿æ¼æ´©ã‚’é¿ã‘ã‚‹ãŸã‚ã€é€æ¬¡æ¨è–¦ã®å­¦ç¿’åˆ†å‰²ã«å¾“ã£ã¦ã€ç›´æ¥æ¨è–¦ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®å­¦ç¿’ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
Implementation Details.
å®Ÿæ–½å†…å®¹
Our P5 model utilizes the pretrained T5 checkpoints [47] as backbone.
æˆ‘ã€…ã®P5ãƒ¢ãƒ‡ãƒ«ã¯ã€äº‹å‰ã«è¨“ç·´ã•ã‚ŒãŸT5ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ[47]ã‚’ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦åˆ©ç”¨ã—ã¦ã„ã‚‹ã€‚
According to the size of T5 backbone, we create two versions of P5, namely P5-small (P5-S) and P5-base (P5-B).
T5ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã®ã‚µã‚¤ã‚ºã«å¿œã˜ã¦ã€P5-smallï¼ˆP5-Sï¼‰ã¨P5-baseï¼ˆP5-Bï¼‰ã®2ç¨®é¡ã®P5ã‚’ä½œæˆã™ã‚‹ã€‚
For P5-small, there are 6 layers for both encoder and decoder, the model dimensionality is 512 with 8-headed attention, and the number of parameters is 60.75 million.
P5-smallã®å ´åˆã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¨ãƒ‡ã‚³ãƒ¼ãƒ€ã¨ã‚‚ã«6å±¤ã§ã€ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒæ•°ã¯512ã€æ³¨ç›®åº¦ã¯8ãƒ˜ãƒƒãƒ‰ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯6075ä¸‡ã§ã‚ã‚‹ã€‚
For P5-base, encoder and decoder both have 12 Transformer blocks.
P5ãƒ™ãƒ¼ã‚¹ã®å ´åˆã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¨ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®ä¸¡æ–¹ã«12å€‹ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ãŒã‚ã‚‹ã€‚
The model has an embedding dimensionality of 768 and a 12-headed attention, and the number of parameters is 223.28 million.
ã“ã®ãƒ¢ãƒ‡ãƒ«ã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã¯768ã€æ³¨ç›®åº¦ã¯12ãƒ˜ãƒƒãƒ‰ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯2å„„2,328ä¸‡ã§ã‚ã‚‹ã€‚
For tokenization, we use the SentencePiece [54] tokenizer with a vocabulary size of 32,128 for parsing sub-word units.
ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã«ã¯ã€32,128ã®èªå½™ã‚µã‚¤ã‚ºã‚’æŒã¤SentencePiece [54]ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã—ã€ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰å˜ä½ã‚’æ§‹æ–‡è§£æã™ã‚‹ã€‚
We pretrain P5 for 10 epochs with AdamW optimization [39] on four NVIDIA RTX A5000 GPUs.
4ã¤ã®NVIDIA RTX A5000 GPUã§ã€AdamWæœ€é©åŒ–[39]ã‚’ç”¨ã„ã¦P5ã‚’10ã‚¨ãƒãƒƒã‚¯äº‹å‰è¨“ç·´ã—ãŸã€‚
The batch size is set to 16 for P5-base and 32 for P5-small.
ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯P5-baseãŒ16ã€P5-smallãŒ32ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã€‚
We choose 1 Ã— 10âˆ’3 as the peak learning rate and set the maximum length of input tokens to 512.
ãƒ”ãƒ¼ã‚¯å­¦ç¿’ç‡ã¨ã—ã¦1Ã—10-3ã‚’é¸ã³ã€å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®æœ€å¤§é•·ã‚’512ã«è¨­å®šã—ãŸã€‚
The warmup strategy is used to adjust the learning rate during training, the warmup stage is set to be the first 5% of all iterations.
ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æˆ¦ç•¥ã¯è¨“ç·´ä¸­ã®å­¦ç¿’ç‡ã‚’èª¿æ•´ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã€ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æ®µéšã¯å…¨åå¾©ã®æœ€åˆã®5ï¼…ã«è¨­å®šã•ã‚Œã‚‹ã€‚
When negative sampling is needed for training, we use 1:1 positive vs.
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦ãªå ´åˆã¯ã€1:1ã®ãƒã‚¸ãƒ†ã‚£ãƒ–å¯¾ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
negative sampling for both P5 and baselines.
P5ã¨ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ä¸¡æ–¹ã§ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚
Our default pretrainâ€“predict combination adopts the last prompt in each task family for zero-shot evaluation while all remaining prompts are utilized for multitask prompted pretraining.
ç§ãŸã¡ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ—ãƒªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨äºˆæ¸¬ã®çµ„ã¿åˆã‚ã›ã¯ã€å„ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®æœ€å¾Œã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆè©•ä¾¡ã«æ¡ç”¨ã—ã€æ®‹ã‚Šã®ã™ã¹ã¦ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ—ãƒªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«åˆ©ç”¨ã•ã‚Œã¾ã™ã€‚
For rating prediction, we use Gaussian sampling to convert the original integer scores to float numbers rounded to 1 decimal place.
è©•ä¾¡äºˆæ¸¬ã«ã¯ã€ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ã€å…ƒã®æ•´æ•°ã‚¹ã‚³ã‚¢ã‚’å°æ•°ç‚¹ä»¥ä¸‹1æ¡ã«ä¸¸ã‚ãŸæµ®å‹•å°æ•°ç‚¹æ•°ã«å¤‰æ›ã™ã‚‹ã€‚
In this way, we can avoid overfitting the limited score types.
ã“ã†ã™ã‚‹ã“ã¨ã§ã€é™ã‚‰ã‚ŒãŸã‚¹ã‚³ã‚¢ã‚¿ã‚¤ãƒ—ã¸ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’é¿ã‘ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
After this change, we increase the number of score classes from 5 to 41.
ã“ã®å¤‰æ›´å¾Œã€å¾—ç‚¹ã‚¯ãƒ©ã‚¹ã®æ•°ã¯5ã‹ã‚‰41ã«å¢—ãˆã‚‹ã€‚
For sequential recommendation, we set the beam size ğµ to 20.
é€æ¬¡æ¨è–¦ã§ã¯ã€ãƒ“ãƒ¼ãƒ ã‚µã‚¤ã‚ºâ†ªL_1D435â†©ã‚’20ã«è¨­å®šã—ãŸã€‚
For direct recommendation, the beam size is also 20 and the candidate pool contains 100 items, which consist of one ground-truth item and 99 sampled negative ones that the user has not interacted with.
ç›´æ¥æ¨è–¦ã®å ´åˆã‚‚ã€ãƒ“ãƒ¼ãƒ ã‚µã‚¤ã‚ºã¯20ã§ã‚ã‚Šã€å€™è£œãƒ—ãƒ¼ãƒ«ã¯100é …ç›®ã‹ã‚‰æ§‹æˆã•ã‚Œã€1ã¤ã®çœŸå®Ÿé …ç›®ã¨99ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸè² ã®é …ç›®ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ã€‚
Metrics.
æŒ‡æ¨™
For rating prediction, we adopt Root Mean Square Error (RMSE) and Mean Absolute Error (MAE).
è¦–è´ç‡äºˆæ¸¬ã«ã¯ã€äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®ï¼ˆRMSEï¼‰ã¨å¹³å‡çµ¶å¯¾èª¤å·®ï¼ˆMAEï¼‰ã‚’æ¡ç”¨ã™ã‚‹ã€‚
For sequential recommendation and direct recommendation tasks, we employ top-ğ‘˜ Hit Ratio (HR@ğ‘˜) and Normalized Discounted Cumulative Gain (NDCG@ğ‘˜) to evaluate the performance and report HR@1, 5, 10 and NGCG@5, 10.
é€æ¬¡æ¨è–¦ã‚¿ã‚¹ã‚¯ã¨ç›´æ¥æ¨è–¦ã‚¿ã‚¹ã‚¯ã«ã¤ã„ã¦ã¯ã€æ€§èƒ½è©•ä¾¡ã¨ã—ã¦ãƒˆãƒƒãƒ—á¹ãƒ’ãƒƒãƒˆç‡(HR@ğ—¥)ã¨æ­£è¦åŒ–å‰²å¼•ç´¯ç©åˆ©å¾—(NDCGá¹)ã‚’æ¡ç”¨ã—ã€HR@1, 5, 10ã¨NGCG@5, 10ã‚’å ±å‘Šã™ã‚‹ã€‚
For explanation generation and review summarization, we evaluate different methods with BLEU-4, as well as ROUGE-1, ROUGE-2, and ROUGE-L.
èª¬æ˜ç”Ÿæˆã¨ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦ç´„ã®ãŸã‚ã«ã€BLEU-4ã€ROUGE-1ã€ROUGE-2ã€ROUGE-Lã§ç•°ãªã‚‹æ–¹æ³•ã‚’è©•ä¾¡ã—ãŸã€‚
RMSE and MAE are â€œthe lower, the betterâ€, while all other metrics are â€œthe higher, the betterâ€.
RMSEã¨MAEã¯ "ä½ã‘ã‚Œã°ä½ã„ã»ã©è‰¯ã„"ã€ãã®ä»–ã®æŒ‡æ¨™ã¯ "é«˜ã‘ã‚Œã°é«˜ã„ã»ã©è‰¯ã„"ã€‚
For all tables in the following, bold numbers refer to the best performance, while underlined numbers indicate the second best performance.
ä»¥ä¸‹ã®ã™ã¹ã¦ã®è¡¨ã«ãŠã„ã¦ã€å¤ªå­—ã®æ•°å­—ã¯ãƒ™ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã€ä¸‹ç·šã®æ•°å­—ã¯ã‚»ã‚«ãƒ³ãƒ‰ãƒ™ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

## Baselines for Multiple Tasks è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

To demonstrate P5â€™s competence on a wide range of recommendation related tasks, we gather a collection of representative approaches for difference task families.
P5ãŒæ¨è–¦ã«é–¢é€£ã™ã‚‹å¹…åºƒã„ã‚¿ã‚¹ã‚¯ã§èƒ½åŠ›ã‚’ç™ºæ®ã§ãã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã™ã‚‹ãŸã‚ã€ç•°ãªã‚‹ã‚¿ã‚¹ã‚¯ç¾¤ã«å¯¾ã™ã‚‹ä»£è¡¨çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’é›†ã‚ãŸã€‚
Rating Prediction and Direct Recommendation.
æ ¼ä»˜ã‘äºˆæ¸¬ã¨ç›´æ¥æ¨è–¦ã€‚
These tasks take the userâ€“item rating/interaction data, but no content or side information is provided.
ã“ã‚Œã‚‰ã®ã‚¿ã‚¹ã‚¯ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚¤ãƒ†ãƒ ã®è©•ä¾¡ï¼ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚‹ãŒã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚„ã‚µã‚¤ãƒ‰æƒ…å ±ã¯æä¾›ã•ã‚Œãªã„ã€‚
We aim to justify whether the models are able to provide accurate rating prediction or recommendation lists that align with the user preferences.
æˆ‘ã€…ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒæ­£ç¢ºãªè©•ä¾¡äºˆæ¸¬ã‚„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å—œå¥½ã«æ²¿ã£ãŸæ¨è–¦ãƒªã‚¹ãƒˆã‚’æä¾›ã§ãã‚‹ã‹ã©ã†ã‹ã‚’æ­£å½“åŒ–ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã‚‹ã€‚
We use MF [25] and MLP [5] under mean square root loss as rating prediction baselines.
è©•ä¾¡äºˆæ¸¬ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ã€MF [25]ã¨MLP [5]ã‚’å¹³å‡å¹³æ–¹æ ¹æå¤±ä¸‹ã§ä½¿ç”¨ã™ã‚‹ã€‚
For direct recommendation, we use BPR-MF [49], BPR-MLP [5], and a state-of-the-art contrastive learning-based collaborative filtering model SimpleX [43] as baselines.
ç›´æ¥æ¨è–¦ã«ã¯ã€BPR-MF[49]ã€BPR-MLP[5]ã€ãã—ã¦æœ€æ–°ã®å¯¾ç…§å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«SimpleX[43]ã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã€‚
Sequential Recommendation.
é †å½“ãªæ¨è–¦ã€‚
We adopt several representative sequential recommendation approaches as our baselines.
æˆ‘ã€…ã¯ã€ã„ãã¤ã‹ã®ä»£è¡¨çš„ãªé€æ¬¡æ¨è–¦ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦æ¡ç”¨ã™ã‚‹ã€‚
Caser [63] treats sequential recommendation as a Markov Chain and employs convolutional neural networks to model user interests.
Caser [63]ã¯ã€é€æ¬¡çš„ãªæ¨è–¦ã‚’ãƒãƒ«ã‚³ãƒ•é€£é–ã¨ã—ã¦æ‰±ã„ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ãŸã‚ã«ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã€‚
HGN [41] adopts a hierarchical gating networks to learn user behaviors from the perspectives of both long and short terms.
HGN[41]ã¯ã€éšå±¤çš„ãªã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ¡ç”¨ã—ã€é•·æœŸã¨çŸ­æœŸã®ä¸¡æ–¹ã®è¦³ç‚¹ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã‚’å­¦ç¿’ã™ã‚‹ã€‚
GRU4Rec [21] is originally proposed for session-based recommendation.
GRU4Rec [21]ã¯ã‚‚ã¨ã‚‚ã¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ã®ãŸã‚ã«ææ¡ˆã•ã‚ŒãŸã€‚
It utilizes GRU [7] to model the user click history sequence.
GRU [7]ã‚’åˆ©ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ãƒªãƒƒã‚¯å±¥æ­´ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ã€‚
BERT4Rec [60] mimics the BERT-style masked language modeling and learns a bidirectional representation for sequential recommendation.
BERT4Rec [60]ã¯ã€BERTã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’æ¨¡å€£ã—ã€é€æ¬¡æ¨è–¦ã®ãŸã‚ã®åŒæ–¹å‘è¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹ã€‚
FDSA [73] focuses on the feature transition patterns by modeling feature sequence with a self-attention module.
FDSA[73]ã¯ã€ç‰¹å¾´åˆ—ã‚’è‡ªå·±æ³¨ç›®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ã“ã¨ã§ã€ç‰¹å¾´é·ç§»ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ç€ç›®ã—ã¦ã„ã‚‹ã€‚
SASRec [24] adopts selfattention mechanism in a sequential recommendation model, which reconciles the properties of Markov Chains and RNN-based approaches.
SASRec [24]ã¯ã€ãƒãƒ«ã‚³ãƒ•é€£é–ã¨RNNãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ç‰¹æ€§ã‚’èª¿å’Œã•ã›ãŸé€æ¬¡æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€è‡ªå·±æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã€‚
S 3 -Rec [80] leverages self-supervised objectives to help sequential recommendation model better discover the correlations among different items and their attributes.
S 3 -Rec [80]ã¯ã€é€æ¬¡æ¨è–¦ãƒ¢ãƒ‡ãƒ«ãŒç•°ãªã‚‹é …ç›®ã¨ãã®å±æ€§é–“ã®ç›¸é–¢é–¢ä¿‚ã‚’ã‚ˆã‚Šè‰¯ãç™ºè¦‹ã§ãã‚‹ã‚ˆã†ã«ã€è‡ªå·±æ•™å¸«ä»˜ãç›®æ¨™ã‚’æ´»ç”¨ã—ã¦ã„ã‚‹ã€‚
We use the implementation of S3 -Rec and its baselines for comparison4 .
æ¯”è¼ƒã«ã¯ã€S3 -Recã®å®Ÿè£…ã¨ãã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹4 ã€‚
Explanation Generation.
èª¬æ˜ä¸–ä»£ã€‚
For performance comparison, we consider several baselines with regard to the task of explanation generation.
æ€§èƒ½æ¯”è¼ƒã®ãŸã‚ã«ã€èª¬æ˜ç”Ÿæˆã®ã‚¿ã‚¹ã‚¯ã«é–¢ã—ã¦ã„ãã¤ã‹ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’æ¤œè¨ã—ãŸã€‚
Attn2Seq [10] learns to encode attributes into vectors, and then invokes an attention mechanism to generate reviews conditioned on the attribute vector.
Attn2Seq [10]ã¯ã€å±æ€§ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã‚’å­¦ç¿’ã—ã€æ¬¡ã«å±æ€§ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ¡ä»¶ã¨ã™ã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’å‘¼ã³å‡ºã™ã€‚
NRT [32] utilizes GRU [7] to generate explanations based on user and item IDs.
NRT[32]ã¯ã€GRU[7]ã‚’åˆ©ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã¨ã‚¢ã‚¤ãƒ†ãƒ IDã«åŸºã¥ã„ã¦èª¬æ˜ã‚’ç”Ÿæˆã™ã‚‹ã€‚
PETER [31] is a simple and effective framework that attempts to utilize user and item IDs to generate explanations.
PETER [31]ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã¨ã‚¢ã‚¤ãƒ†ãƒ IDã‚’åˆ©ç”¨ã—ã¦èª¬æ˜ã‚’ç”Ÿæˆã—ã‚ˆã†ã¨ã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ã§åŠ¹æœçš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚‹ã€‚
It is built upon a modified attention mask of the Transformer architecture.
ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¼ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’æ”¹è‰¯ã—ã¦ä½œã‚‰ã‚Œã¦ã„ã‚‹ã€‚
There is also a variant PETER+, which takes a hint feature word to assist the explanation generation.
ã¾ãŸã€PETER+ã¨ã„ã†ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚‚ã‚ã‚Šã€ã“ã‚Œã¯ãƒ’ãƒ³ãƒˆã¨ãªã‚‹ç‰¹å¾´èªã‚’å—ã‘å–ã‚Šã€èª¬æ˜ã®ç”Ÿæˆã‚’è£œåŠ©ã™ã‚‹ã€‚
Review Related.
ãƒ¬ãƒ“ãƒ¥ãƒ¼é–¢é€£
For review summarization, we adopt pretrained T0 [51] and GPT-2 [46] checkpoints hosted by Hugging Face5 as baselines.
ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦ç´„ã«ã¯ã€Hugging Face5ãŒãƒ›ã‚¹ãƒˆã™ã‚‹äº‹å‰å­¦ç¿’æ¸ˆã¿ã®T0 [51]ã¨GPT-2 [46]ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦æ¡ç”¨ã—ãŸã€‚
For review preference prediction, we only use T0 to make comparisons because GPT-2 cannot perform this task.
ãƒ¬ãƒ“ãƒ¥ãƒ¼å—œå¥½äºˆæ¸¬ã§ã¯ã€GPT-2ã¯ã“ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ããªã„ãŸã‚ã€T0ã®ã¿ã‚’ä½¿ç”¨ã—ã¦æ¯”è¼ƒã‚’è¡Œã†ã€‚

## Performance Comparison on Different Task Families (RQ1) ç•°ãªã‚‹ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒï¼ˆRQ1ï¼‰

In this section, we pretrain P5 with prompts from all five task families to verify its multitask learning ability.
æœ¬ç¯€ã§ã¯ã€P5ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’èƒ½åŠ›ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã€5ã¤ã®ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”¨ã„ã¦P5ã®äº‹å‰å­¦ç¿’ã‚’è¡Œã†ã€‚
According to the default pretrainâ€“predict task combination, we leave Prompt 1-10, Prompt 2-13, Prompt 3-12, Prompt 4-4, and Prompt 5-8 for zeroshot evaluation and pretrain P5 with the remaining personalized prompts.
ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®äº‹å‰è¨“ç·´ã¨äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã®çµ„ã¿åˆã‚ã›ã«å¾“ã„ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1-10ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ2-13ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ3-12ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ4-4ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ5-8ã‚’ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆè©•ä¾¡ç”¨ã«æ®‹ã—ã€æ®‹ã‚Šã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§P5ã‚’äº‹å‰è¨“ç·´ã™ã‚‹ã€‚
The performances of P5 and relevant baselines on the five task families are presented in Table 2 to Table 7.
5ã¤ã®ã‚¿ã‚¹ã‚¯ãƒ»ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã«ãŠã‘ã‚‹P5ã¨é–¢é€£ã™ã‚‹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®æ€§èƒ½ã‚’è¡¨2ã‹ã‚‰è¡¨7ã«ç¤ºã™ã€‚
For each task family, we choose one or more seen prompts as supplement to the aforementioned zero-shot unseen prompts to perform evaluations.5.3.1 Rating Prediction.
å„ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã«ã¤ã„ã¦ã€å‰è¿°ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã®æœªè¦‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è£œè¶³ã¨ã— ã¦ã€1ã¤ã¾ãŸã¯è¤‡æ•°ã®è¦‹ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é¸æŠã—ã¦è©•ä¾¡ã‚’å®Ÿæ–½ã™ã‚‹ã€‚
Prompt 1-6 and Prompt 1-10 are used for evaluating P5â€™s performance on rating prediction.
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1-6ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1-10ã¯ã€ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°äºˆæ¸¬ã«é–¢ã™ã‚‹P5ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ã€‚
The performance comparison is presented in Table 2.
æ€§èƒ½æ¯”è¼ƒã‚’è¡¨2ã«ç¤ºã™ã€‚
We can see that when testing with seen Prompt 1-6, P5-B gets better MAE and slightly higher RMSE on all three datasets compared with MF.
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1ï½6ã§ãƒ†ã‚¹ãƒˆã—ãŸå ´åˆã€P5-Bã¯MFã¨æ¯”è¼ƒã—ã¦ã€3ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã™ã¹ã¦ã§MAEãŒè‰¯ãã€RMSEãŒã‚ãšã‹ã«é«˜ã„ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
When testing with unseen Prompt 1-10, P5-B can achieve similar performance as Prompt 1-6.
æœªè¦‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1ï½10ã§ãƒ†ã‚¹ãƒˆã—ãŸå ´åˆã€P5-Bã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1ï½6ã¨åŒæ§˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã§ãã‚‹ã€‚
Moreover, P5-S usually has better MAE but higher RMSE.
ã•ã‚‰ã«ã€P5-Sã¯é€šå¸¸ã€MAEã¯è‰¯ã„ãŒRMSEã¯é«˜ã„ã€‚
It seems that P5 is overfitting these data since the task complexity of rating prediction is relatively lower than other recommendation tasks.
æ ¼ä»˜ã‘äºˆæ¸¬ã®ã‚¿ã‚¹ã‚¯ã®è¤‡é›‘ã•ã¯ä»–ã®æ¨è–¦ã‚¿ã‚¹ã‚¯ã«æ¯”ã¹ã¦æ¯”è¼ƒçš„ä½ã„ãŸã‚ã€P5ã¯ã“ã‚Œã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã«ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒˆã—ã¦ã„ã‚‹ã‚ˆã†ã ã€‚
Overall, these results show that it is feasible to perform rating prediction on a conditional text generation framework.5.3.2 Sequential Recommendation.
5.3.2é€æ¬¡ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ å…¨ä½“ã¨ã—ã¦ã€ã“ã‚Œã‚‰ã®çµæœã¯ã€æ¡ä»¶ä»˜ããƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ä¸Šã§ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°äºˆæ¸¬ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
As illustrated in Table 3, Prompt 2-3 and Prompt 2-13 are employed for the evaluation of sequential recommendation under all-item setting, i.e., using all items as candidates rather than sampling 100 or 1,000 items for ranking.
è¡¨3ã«ç¤ºã™ã‚ˆã†ã«ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ2-3ãŠã‚ˆã³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ2-13ã¯ã€å…¨é …ç›®è¨­å®šã€ã™ãªã‚ã¡ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ãŸã‚ã«100é …ç›®ã¾ãŸã¯1,000é …ç›®ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã®ã§ã¯ãªãã€ã™ã¹ã¦ã®é …ç›®ã‚’å€™è£œã¨ã—ã¦ä½¿ç”¨ã™ã‚‹å ´åˆã®é€æ¬¡æ¨è–¦ã®è©•ä¾¡ã«æ¡ç”¨ã•ã‚Œã‚‹ã€‚
From the table, we can see that P5-B surpasses all competitive baselines with a relatively large gap on both seen (Prompt 2-3) and unseen (Prompt 2-13) prompts.
è¡¨ã‹ã‚‰ã€P5-Bã¯ã€è¦‹ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ2-3ï¼‰ã¨è¦‹ãªã‹ã£ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ2-13ï¼‰ã®ä¸¡æ–¹ã§æ¯”è¼ƒçš„å¤§ããªå·®ã‚’ã¤ã‘ã€ã™ã¹ã¦ã®ç«¶åˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
On Toys, P5-S can get even better performance than P5-B.
Toysã§ã¯ã€P5-Sã¯P5-Bã‚ˆã‚Šã•ã‚‰ã«è‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
While on Beauty and Sports, P5-B achieves the advantage over P5-S.
ç¾å®¹ã¨ã‚¹ãƒãƒ¼ãƒ„ã§ã¯ã€P5-BãŒP5-Sã‚ˆã‚Šå„ªä½ã«ç«‹ã£ã¦ã„ã‚‹ã€‚
The results show that the P5 architecture is effective in modeling the user interaction history and conducting next item prediction with the help of beam search.5.3.3 Explanation Generation.
5.3.3èª¬æ˜ã®ç”Ÿæˆ P5ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã®å±¥æ­´ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã€ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã®åŠ©ã‘ã‚’å€Ÿã‚Šã¦æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã®äºˆæ¸¬ã‚’è¡Œã†ã®ã«åŠ¹æœçš„ã§ã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚
In Table 4, Prompt 3-9 and Prompt 3-12 are used to evaluate P5â€™s performance on explanation generation under feature-based setup, while Prompt 3-3 is used for direct explanation generation without providing a hint word.
è¡¨4ã§ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ3-9ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ3-12ã¯ã€ç‰¹å¾´ãƒ™ãƒ¼ã‚¹ã®è¨­å®šã«ãŠã‘ã‚‹P5ã®èª¬æ˜ç”Ÿæˆæ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ3-3ã¯ã€ãƒ’ãƒ³ãƒˆå˜èªã‚’æä¾›ã—ãªã„ç›´æ¥èª¬æ˜ç”Ÿæˆã«ä½¿ç”¨ã•ã‚Œã‚‹ã€‚
We can see that for Prompt 3-3, P5 achieves the best performances against all baselines.
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ3-3ã§ã¯ã€P5ãŒã™ã¹ã¦ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«å¯¾ã—ã¦æœ€é«˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
For feature-based prompts (Prompts 3-9 & 3-12), P5 can outperform PETER+ on most cases, especially for Beauty and Toys.5.3.4 Review Related.
æ©Ÿèƒ½ãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ3-9ãŠã‚ˆã³3-12ï¼‰ã§ã¯ã€P5ã¯ã»ã¨ã‚“ã©ã®ã‚±ãƒ¼ã‚¹ã§PETER+ã‚’ä¸Šå›ã‚‹ã“ã¨ãŒã§ãã€ç‰¹ã«ç¾å®¹ã¨ç©å…·ã§ã¯PETER+ã‚’ä¸Šå›ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
We take Prompts 4-2 and 4-4 to compare P5â€™s performance with T0 on review preference prediction, as shown in Table 5.
è¡¨5ã«ç¤ºã™ã‚ˆã†ã«ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ4-2ã¨4-4ã‚’å–ã‚Šä¸Šã’ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼å—œå¥½äºˆæ¸¬ã«ãŠã‘ã‚‹P5ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’T0ã¨æ¯”è¼ƒã™ã‚‹ã€‚
We can see that P5-S achieves better RMSE and MAE on Beauty and Toys, while P5-B shows better performance on Sports.
P5-Sã¯ç¾å®¹ã¨ç©å…·ã§ã‚ˆã‚Šè‰¯ã„RMSEã¨MAEã‚’é”æˆã—ã€P5-Bã¯ã‚¹ãƒãƒ¼ãƒ„ã§ã‚ˆã‚Šè‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
Additionally, we take Prompt 4-1 to evaluate P5â€™s ability on review summarization, as shown in Table 6.
ã•ã‚‰ã«ã€è¡¨6ã«ç¤ºã™ã‚ˆã†ã«ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ4-1ã‚’ç”¨ã„ã¦P5ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦ç´„èƒ½åŠ›ã‚’è©•ä¾¡ã—ãŸã€‚
For this task, P5-S clearly outperforms T0 and GPT-2 on both Beauty and Toys datasets.
ã“ã®ã‚¿ã‚¹ã‚¯ã§ã¯ã€P5-Sã¯Beautyã¨Toysã®ä¸¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§T0ã¨GPT-2ã‚’æ˜ã‚‰ã‹ã«ä¸Šå›ã£ãŸã€‚
It is worth noting that GPT-2 and T0 has 1.5B and 11B parameters, respectively.
æ³¨ç›®ã™ã¹ãã¯ã€GPT-2ã¨T0ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ãŒãã‚Œãã‚Œ1.5Bã¨11Bã§ã‚ã‚‹ã“ã¨ã ã€‚
This shows that P5 can achieve better performances than these competitive baselines with a much smaller model size.5.3.5 Direct Recommendation.
5.3.5ç›´æ¥ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ ã“ã‚Œã¯ã€P5ãŒã€ã¯ã‚‹ã‹ã«å°ã•ãªãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã§ã€ã“ã‚Œã‚‰ã®ç«¶åˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒ ãƒ³ã‚¹ã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
Finally, Prompts 5-1, 5-4, 5-5 and 5-8 are applied to evaluate the direct recommendation task under the 1-out-of-100 evaluation setting.
æœ€å¾Œã«ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ5-1ã€5-4ã€5-5ã€5-8ã‚’é©ç”¨ã—ã¦ã€100ç‚¹æº€ç‚¹ä¸­1ç‚¹ã¨ã„ã†è©•ä¾¡è¨­å®šã§ç›´æ¥æ¨è–¦ã‚¿ã‚¹ã‚¯ã‚’è©•ä¾¡ã™ã‚‹ã€‚
For binary question prompts (5-1 & 5-4), which are discriminative prompts, we use the softmax generation probability of â€œyesâ€ to rank the candidate items.
è­˜åˆ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã‚ã‚‹2å€¤è³ªå•ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆ5-1ãŠã‚ˆã³5-4ï¼‰ã«ã¤ã„ã¦ã¯ã€ã€Œã¯ã„ã€ã®ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ç”Ÿæˆç¢ºç‡ã‚’ä½¿ç”¨ã—ã¦ã€å€™è£œé …ç›®ã®ãƒ©ãƒ³ã‚¯ä»˜ã‘ã‚’è¡Œã†ã€‚
For open question prompts (5-5 & 5-8), which are generative prompts, we use beam-search (Eq.(2)) to generate the top-ğ‘˜ list.
ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã‚ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆ5-5 & 5-8ï¼‰ã«ã¤ã„ã¦ã¯ã€ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒï¼ˆå¼(2)ï¼‰ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒƒãƒ—\_1ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚
The results are presented in Table 7.
çµæœã‚’è¡¨7ã«ç¤ºã™ã€‚
From the table, we can see that P5-B and P5-S have great advantages over BPR-MF and BPR-MLP on all three datasets.
è¡¨ã‹ã‚‰ã€3ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã™ã¹ã¦ã«ãŠã„ã¦ã€P5-Bã¨P5-SãŒBPR-MFã¨BPR-MLPã«å¯¾ã—ã¦å¤§ããªå„ªä½æ€§ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
Comparing with SimpleX, we can see that P5 works especially well on top-1 item ranking, which is more than two times better than SimpleX on HR@1.
SimpleXã¨æ¯”è¼ƒã™ã‚‹ã¨ã€P5ã¯ãƒˆãƒƒãƒ—1ã‚¢ã‚¤ãƒ†ãƒ ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§ç‰¹ã«åŠ¹æœã‚’ç™ºæ®ã—ã€HR@1ã§ã¯SimpleXã®2å€ä»¥ä¸Šå„ªã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
Besides, P5 also achieves the best result on most of the other metrics.
ãã®ä¸Šã€P5ã¯ä»–ã®ã»ã¨ã‚“ã©ã®æŒ‡æ¨™ã§ã‚‚æœ€é«˜ã®çµæœã‚’å‡ºã—ã¦ã„ã‚‹ã€‚
The success of P5 on direct recommendation shows the competence of the sequence-to-sequence generation framework in recommendation domain.
ç›´æ¥æ¨è–¦ã«ãŠã‘ã‚‹P5ã®æˆåŠŸã¯ã€æ¨è–¦é ˜åŸŸã«ãŠã‘ã‚‹é…åˆ—é–“ç”Ÿæˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®èƒ½åŠ›ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

## Zero-shot Generalization to Unseen Prompts and Items in New Domain (RQ2) æ–°ã—ã„é ˜åŸŸã«ãŠã‘ã‚‹æœªè¦‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã‚¢ã‚¤ãƒ†ãƒ ã¸ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ±åŒ–ï¼ˆRQ2ï¼‰

5.4.1 Transfer to Unseen Personalized Prompts.
5.4.1 è¦‹ãˆãªã„ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¸ã®ç§»è¡Œã€‚
In this section, we transfer the pretrained P5 models to the previously heldout prompts during pretraining.
æœ¬ç¯€ã§ã¯ã€äº‹å‰å­¦ç¿’ã§ä¿æŒã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã¦ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ã®P5ãƒ¢ãƒ‡ãƒ«ã‚’è»¢é€ã™ã‚‹ã€‚
These unseen prompts are from the same task families, and the testing items have been seen by P5 during pretraining at least once.
ã“ã‚Œã‚‰ã®æœªé–²è¦§ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯åŒã˜ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ã‚‚ã®ã§ã‚ã‚Šã€ãƒ†ã‚¹ãƒˆé …ç›®ã¯P5ãŒãƒ—ãƒ¬ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«å°‘ãªãã¨ã‚‚ä¸€åº¦ã¯è¦‹ãŸã“ã¨ãŒã‚ã‚‹ã‚‚ã®ã§ã‚ã‚‹ã€‚
The experimental results are also reported in Table 2 to Table 7.
å®Ÿé¨“çµæœã‚’è¡¨2ã‹ã‚‰è¡¨7ã«ç¤ºã™ã€‚
As previously discussed in Section 5.3, P5 achieves surprisingly good performances on various task families when being challenged by unseen prompts.
ã‚»ã‚¯ã‚·ãƒ§ãƒ³5.3ã§å‰è¿°ã—ãŸã‚ˆã†ã«ã€P5ã¯ã€æœªçŸ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æŒ‘æˆ¦ã•ã‚ŒãŸã¨ãã€æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã§é©šãã»ã©å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ãŸã€‚
On some specific datasets, the performances of P5 on unseen prompts even surpass seen prompts, e.g., P5-B gets the best performance under Prompt 2-13 on Sports.
ä¾‹ãˆã°ã€ã‚¹ãƒãƒ¼ãƒ„ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ2-13ã§ã¯ã€P5-BãŒæœ€é«˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ãŸã€‚
These results show that multitask prompted pretraining empowers P5 enough robustness to understand unseen prompts with wording variations.5.4.2 Transfer to Items in New Domain.
5.4.2æ–°é ˜åŸŸã®é …ç›®ã¸ã®ç§»è¡Œã€‚ã“ã‚Œã‚‰ã®çµæœã¯ã€ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆäº‹å‰è¨“ç·´ãŒã€æ–‡è¨€ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹æœªçŸ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç†è§£ã™ã‚‹ã®ã«ååˆ†ãªé ‘å¥æ€§ã‚’P5ã«ä¸ãˆã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
Next, we increase the difficulty level of zero-shot transfer.
æ¬¡ã«ã€ã‚¼ãƒ­ã‚·ãƒ¥ãƒ¼ãƒˆç§»ç±ã®é›£æ˜“åº¦ã‚’ä¸Šã’ã‚‹ã€‚
We collect a group of 741 users that exist in all the three domains with their interaction and review histories in other domains.
3ã¤ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã™ã¹ã¦ã«å­˜åœ¨ã™ã‚‹741äººã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã€ä»–ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®å±¥æ­´ã‚’åé›†ã™ã‚‹ã€‚
The detailed statistics of these domain transfer evaluation sets are illustrated in Table 8.
ã“ã‚Œã‚‰ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ç§»è»¢è©•ä¾¡ã‚»ãƒƒãƒˆã®è©³ç´°ãªçµ±è¨ˆã¯è¡¨8ã«ç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚
We then challenge P5-B pretrained on one domain with unseen prompts from the Task Family Z, whose item fields are filled with the information from a new product domain.
æ¬¡ã«ã€1ã¤ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã§äº‹å‰å­¦ç¿’ã—ãŸP5-Bã«ã€ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼Zã®æœªè¦‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§æŒ‘æˆ¦ã™ã‚‹ã€‚
For example, we ask the P5 model pretrained on the Toys domain about an existing userâ€™s preference towards an item in the Beauty domain.
ä¾‹ãˆã°ã€Toysãƒ‰ãƒ¡ã‚¤ãƒ³ã§äº‹å‰å­¦ç¿’ã•ã‚ŒãŸP5ãƒ¢ãƒ‡ãƒ«ã«ã€Beautyãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã™ã‚‹æ—¢å­˜ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å—œå¥½ã‚’å°‹ã­ã¾ã™ã€‚
The full results on all six directions are reported in Table 9.
å…¨6æ–¹å‘ã«é–¢ã™ã‚‹å…¨çµæœã‚’è¡¨9ã«å ±å‘Šã™ã‚‹ã€‚
From the table, we notice P5 still maintains sufficient performances for rating prediction (Prompts Z-2 & Z-3), like/dislike prediction (Prompts Z-1 & Z4), as well as explanation generation with feature word (Prompt Z-6).
è¡¨ã‹ã‚‰ã€è©•ä¾¡äºˆæ¸¬ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆZ-2ã€Z-3ï¼‰ã€å¥½ãå«Œã„äºˆæ¸¬ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆZ-1ã€Z4ï¼‰ã€ç‰¹å¾´èªã«ã‚ˆã‚‹èª¬æ˜ç”Ÿæˆï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆZ-6ï¼‰ã«ãŠã„ã¦ã€P5ã¯ä¾ç„¶ã¨ã—ã¦ååˆ†ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
In contrast, direct explanation generation without feature word (Prompts Z-5 & Z-7) is very difficult for P5 because it lacks awareness of relevant knowledge in the new domain.
ä¸€æ–¹ã€ç‰¹å¾´èªã‚’ç”¨ã„ãªã„ç›´æ¥çš„ãªèª¬æ˜ç”Ÿæˆï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆZ-5ã¨Z-7ï¼‰ã¯ã€P5ã«ã¨ã£ã¦éå¸¸ã«å›°é›£ã§ã‚ã‚‹ã€‚
In Figure 4, we provide some example explanations generated by P5-B under the setup of zero-shot domain transfer (Prompt Z-6).
å›³4ã§ã¯ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆé ˜åŸŸç§»è­²ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆZ-6ï¼‰ã®è¨­å®šä¸‹ã§P5-BãŒç”Ÿæˆã—ãŸèª¬æ˜ã®ä¾‹ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
We can see that P5 is able to catch different usersâ€™ rating preferences and hint feature words, then integrate them with the knowledge learned from previous domain to generate plausible explanations.
P5ã¯ã€ã•ã¾ã–ã¾ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è©•ä¾¡å—œå¥½ã¨ãƒ’ãƒ³ãƒˆã¨ãªã‚‹ç‰¹å¾´èªã‚’ã‚­ãƒ£ãƒƒãƒã—ã€ãã‚Œã‚‰ã‚’å‰ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã‚‰å­¦ç¿’ã—ãŸçŸ¥è­˜ã¨çµ±åˆã—ã¦ã€ã‚‚ã£ã¨ã‚‚ã‚‰ã—ã„èª¬æ˜ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚

## Ablation on Model Size (RQ3) ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«é–¢ã™ã‚‹ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆRQ3ï¼‰

In this section, we will discuss the influence of model size on the performance of P5 on different recommendation tasks.
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ç•°ãªã‚‹æ¨è–¦ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹P5ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®å½±éŸ¿ã«ã¤ã„ã¦è­°è«–ã™ã‚‹ã€‚
Here, we train two size variants of P5, namely P5-small and P5-base.
ã“ã“ã§ã¯ã€P5ã®2ã¤ã®ã‚µã‚¤ã‚ºãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã€ã™ãªã‚ã¡P5-smallã¨P5-baseã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã€‚
The parameter numbers of these two P5 models are 60.75M and 223.28M, respectively.
ã“ã‚Œã‚‰2ã¤ã®P5ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯ã€ãã‚Œãã‚Œ60.75Mã¨223.28Mã§ã‚ã‚‹ã€‚
From Table 2 to Table 7, we can see that although P5-S is only 1/4 of the size of P5-B, P5-S can beats P5-B on a series of tasks and datasets.
è¡¨2ã‹ã‚‰è¡¨7ã‹ã‚‰ã€P5-Sã¯P5-Bã®1/4ã®ã‚µã‚¤ã‚ºã—ã‹ãªã„ãŒã€P5-Sã¯ä¸€é€£ã®ã‚¿ã‚¹ã‚¯ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§P5-Bã«å‹ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
For example, P5-S achieves better sequential recommendation, review preference prediction, and direct recommendation (Prompts 5-5 & 5-8) performances than P5-B on Toys.
ä¾‹ãˆã°ã€P5-Sã¯P5-Bã«æ¯”ã¹ã€é€æ¬¡ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼å—œå¥½äºˆæ¸¬ã€ç›´æ¥ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ5-5ã¨5-8ï¼‰ã«ãŠã„ã¦å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã¦ã„ã‚‹ã€‚
In contrast, P5-B shows advantages on sequential recommendation and review preference prediction tasks for Sports.
å¯¾ç…§çš„ã«ã€P5-Bã¯ã‚¹ãƒãƒ¼ãƒ„ã®é€æ¬¡æ¨è–¦ã¨ãƒ¬ãƒ“ãƒ¥ãƒ¼å—œå¥½äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã§å„ªä½æ€§ã‚’ç¤ºã—ãŸã€‚
Since Sports contains more users, items and reviews and has a lower sparsity, it requires a model with higher capacity to discover latent correlation among different personalized factors.
ã‚¹ãƒãƒ¼ãƒ„ã«ã¯ã€ã‚ˆã‚Šå¤šãã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€ã‚¢ã‚¤ãƒ†ãƒ ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒå«ã¾ã‚Œã€ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ãŒä½ã„ãŸã‚ã€ã•ã¾ã–ã¾ãªãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸè¦ç´ é–“ã®æ½œåœ¨çš„ãªç›¸é–¢é–¢ä¿‚ã‚’ç™ºè¦‹ã™ã‚‹èƒ½åŠ›ãŒé«˜ã„ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã§ã™ã€‚
The findings indicate that larger P5 models may be needed when the dataset is large, while for smaller datasets, smaller P5 models could be enough.
ã“ã®çµæœã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå¤§ãã„å ´åˆã«ã¯ã€ã‚ˆã‚Šå¤§ããªP5ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã§ã‚ã‚Šã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå°ã•ã„å ´åˆã«ã¯ã€ã‚ˆã‚Šå°ã•ãªP5ãƒ¢ãƒ‡ãƒ«ã§ååˆ†ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
As a result, we should decide an appropriate model size that matches the scale of the training data.
ãã®çµæœã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®è¦æ¨¡ã«åˆã‚ã›ã¦é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’æ±ºã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

## Ablation on Task Scaling (RQ3) èª²é¡Œã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«é–¢ã™ã‚‹ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆRQ3ï¼‰

Moreover, we explore whether multitask prompted pretraining is superior than pretraining on each task family alone.
ã•ã‚‰ã«ã€ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ã«ä¿ƒã—ãŸäº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒã€å„ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼å˜ç‹¬ã§ã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’èª¿æŸ»ã™ã‚‹ã€‚
We pretrain P5-small on Beauty dataset with prompts from every single task family, resulting in five models â€“ P5-S1, P5-S2, P5-S3, P5-S4, and P5-S5.
P5-smallã‚’ã™ã¹ã¦ã®ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã‚€Beautyãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§äº‹å‰å­¦ç¿’ã—ã€P5-S1ã€P5-S2ã€P5-S3ã€P5-S4ã€P5-S5ã®5ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ãŸã€‚
We then compare P5-S on various recommendation tasks with the corresponding single task P5 model.
æ¬¡ã«ã€æ§˜ã€…ãªæ¨è–¦ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹P5-Sã‚’ã€å¯¾å¿œã™ã‚‹å˜ä¸€ã‚¿ã‚¹ã‚¯ã®P5ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã™ã‚‹ã€‚
The performance comparison between P5-S and P5-SN (ğ‘ âˆˆ [1, 2, 3, 4, 5]) is illustrated in Figure 5.
P5-Sã¨P5-SNï¼ˆâ†ªLu_1âˆˆ[1,2,3,4,5]ï¼‰ã®æ€§èƒ½æ¯”è¼ƒã‚’å›³5ã«ç¤ºã™ã€‚
As shown in the figure, P5-S achieves comparable or better performance than P5-SN on rating prediction, sequential recommendation and direct recommendation tasks, while on text generation tasks such as explanation generation (Prompts 3-9 & 3-12) and review summarization (Prompt 4-1), P5-SN is better than P5-S.
å›³ã«ç¤ºã™ã‚ˆã†ã«ã€è©•ä¾¡äºˆæ¸¬ã€é€æ¬¡æ¨è–¦ã€ç›´æ¥æ¨è–¦ã®ã‚¿ã‚¹ã‚¯ã§ã¯ã€P5-Sã¯P5-SNã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’é”æˆã—ã€èª¬æ˜ç”Ÿæˆï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ3-9ã€3-12ï¼‰ã‚„ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦ç´„ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ4-1ï¼‰ã®ã‚ˆã†ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¿ã‚¹ã‚¯ã§ã¯ã€P5-SNã¯P5-Sã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ã€‚
This indicates that multitask modeling (P5-S) seeks a good balance among tasks and improves recommendation performance by leveraging the power of language understanding.
ã“ã‚Œã¯ã€ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ»ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ï¼ˆP5-Sï¼‰ãŒã‚¿ã‚¹ã‚¯é–“ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è¿½æ±‚ã—ã€è¨€èªç†è§£ã®åŠ›ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§æ¨è–¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
Besides, both P5-S and P5-SN perform better than or comparable with state-ofthe-art baselines on all tasks, as shown in Table 2 through Table 7, which demonstrates the power of P5 for recommendation.
ã•ã‚‰ã«ã€è¡¨2ã‹ã‚‰è¡¨7ã«ç¤ºã™ã‚ˆã†ã«ã€P5-Sã¨P5-SNã¯ã€ã™ã¹ã¦ã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€æœ€æ–°ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã‚‹ã‹ã€åŒç­‰ã§ã‚ã‚‹ã€‚

## Ablation on Prompt Scaling (RQ3) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ»ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ãŠã‘ã‚‹ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆRQ3ï¼‰

As mentioned in implementation details, our default pretrainâ€“predict task combination follows the leave-one-out strategy.
å®Ÿè£…ã®è©³ç´°ã§è¿°ã¹ãŸã‚ˆã†ã«ã€æˆ‘ã€…ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®äº‹å‰è¨“ç·´ã¨äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã®çµ„ã¿åˆã‚ã›ã¯ã€ãƒªãƒ¼ãƒ–ãƒ¯ãƒ³ã‚¢ã‚¦ãƒˆæˆ¦ç•¥ã«å¾“ã£ã¦ã„ã‚‹ã€‚
However, do we need so many prompts during pretraining to enable P5â€™s zeroshot generalization ability? In this section, we explore to reduce the number of pretraining prompts and then make comparisons with the P5 model pretrained under default setup.
ã—ã‹ã—ã€P5ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ±åŒ–èƒ½åŠ›ã‚’ç™ºæ®ã•ã›ã‚‹ãŸã‚ã«ã€äº‹å‰å­¦ç¿’æ™‚ã«å¤šãã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒå¿…è¦ãªã®ã ã‚ã†ã‹ï¼Ÿã“ã®ç¯€ã§ã¯ã€äº‹å‰å­¦ç¿’ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ•°ã‚’æ¸›ã‚‰ã™ã“ã¨ã‚’æ¤œè¨ã—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è¨­å®šã§äº‹å‰å­¦ç¿’ã—ãŸP5ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒã‚’è¡Œã†ã€‚
To this end, we choose a collection of pretraining prompts that has the minimum number of prompts to cover all important personalized fields.
ã“ã®ãŸã‚ã€ã™ã¹ã¦ã®é‡è¦ãªãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚«ãƒãƒ¼ã™ã‚‹æœ€å°é™ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ•°ã‚’æŒã¤ãƒ—ãƒ¬ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã—ã¾ã™ã€‚
Specifically, this combination contains the following 18 personalized prompts: {1-5, 1-6, 1-8, 1-9, 2-1, 2-3, 2-8, 2-11, 3-2, 3-3, 3-6, 3-9, 4-1, 4-2, 4-3, 5-2, 5-5, 5-7}.
å…·ä½“çš„ã«ã¯ã€ã“ã®çµ„ã¿åˆã‚ã›ã«ã¯ä»¥ä¸‹ã®18ã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒå«ã¾ã‚Œã‚‹ï¼š {1-5, 1-6, 1-8, 1-9, 2-1, 2-3, 2-8, 2-11, 3-2, 3-3, 3-6, 3-9, 4-1, 4-2, 4-3, 5-2, 5-5, 5-7}.
Similar to the default pretrainâ€“predict combination, the last prompt in each task family is for zero-shot evaluation.
ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®pretrain-predictã®çµ„ã¿åˆã‚ã›ã¨åŒæ§˜ã«ã€å„ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®æœ€å¾Œã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆè©•ä¾¡ã®ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚
We name this prompt scaling variant of P5-small as P5-PS and then pretrain P5-PS on Beauty dataset.
ã“ã®P5-smallã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ»ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ»ãƒãƒªã‚¢ãƒ³ãƒˆã‚’P5-PSã¨åä»˜ã‘ã€Beautyãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§P5-PSã‚’äº‹å‰è¨“ç·´ã™ã‚‹ã€‚
The performance comparison between P5-S and P5-PS is also presented in Figure 5.
P5-Sã¨P5-PSã®æ€§èƒ½æ¯”è¼ƒã‚‚å›³5ã«ç¤ºã™ã€‚
From the figure, we can observe that P5-S beats P5-PS on most tasks except for some generation tasks (i.e., Prompts 3-3, 3-9 & 4-1).
å›³ã‹ã‚‰ã€P5-SãŒP5-PSã«å‹ã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚ãŸã ã—ã€ã„ãã¤ã‹ã®ç”Ÿæˆã‚¿ã‚¹ã‚¯ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ3-3ã€3-9ã€4-1ï¼‰ã¯ä¾‹å¤–ã§ã‚ã‚‹ã€‚
Interestingly, P5-S outperforms P5-PS on Prompt 3-12 â€“ a zero-shot explanation generation task.
èˆˆå‘³æ·±ã„ã“ã¨ã«ã€P5-Sã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ3-12ï¼ˆã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆèª¬æ˜ç”Ÿæˆã‚¿ã‚¹ã‚¯ï¼‰ã«ãŠã„ã¦P5-PSã‚’ä¸Šå›ã£ã¦ã„ã‚‹ã€‚
In fact, P5-S also shows its superiority on other zero-shot tasks such as Prompts 1-10, 2-13, and 5-8.
å®Ÿéš›ã€P5-Sã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1-10ã€2-13ã€5-8ã¨ã„ã£ãŸä»–ã®ã‚¼ãƒ­ãƒ»ã‚·ãƒ§ãƒƒãƒˆãƒ»ã‚¿ã‚¹ã‚¯ã§ã‚‚å„ªä½æ€§ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
Overall, we can find that larger number of high quality personalized prompts can generally help P5 achieve better performances on various recommendation tasks especially zero-shot tasks with unseen prompts.
å…¨ä½“ã¨ã—ã¦ã€è³ªã®é«˜ã„ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ•°ãŒå¤šã‘ã‚Œã°å¤šã„ã»ã©ã€P5ãŒæ§˜ã€…ãªæ¨è–¦ã‚¿ã‚¹ã‚¯ã€ç‰¹ã«æœªè¦‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã‚€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã‚¿ã‚¹ã‚¯ã§ã‚ˆã‚Šè‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã™ã‚‹ã®ã«å½¹ç«‹ã¤ã“ã¨ãŒã‚ã‹ã‚‹ã€‚

## How to Implement Personalization (RQ4) ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã®å°å…¥æ–¹æ³•ï¼ˆRQ4ï¼‰

In this section, we discuss different strategies to implement personalization in P5.
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€P5ã§ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè£…ã™ã‚‹ãŸã‚ã®ã•ã¾ã–ã¾ãªæˆ¦ç•¥ã«ã¤ã„ã¦èª¬æ˜ã™ã‚‹ã€‚
The default practice is using SentencePiece tokenizer to split personalized fields into multiple sub-word units and meanwhile using whole-word embedding to preserve the field information (Figure 3).
ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€SentencePieceãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ã£ã¦ã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¤‡æ•°ã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰å˜ä½ã«åˆ†å‰²ã—ã€ä¸€æ–¹ã€å…¨å˜èªåŸ‹ã‚è¾¼ã¿ã‚’ä½¿ã£ã¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æƒ…å ±ã‚’ä¿æŒã™ã‚‹ï¼ˆå›³3ï¼‰ã€‚
A straightforward alternative is creating an independent extra token for each user and item.
ç°¡å˜ãªä»£æ›¿æ¡ˆã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚¤ãƒ†ãƒ ã”ã¨ã«ç‹¬ç«‹ã—ãŸè¿½åŠ ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½œæˆã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
Here we name this P5-small variant as P5-I and also pretrain it on Beauty dataset.
ã“ã“ã§ã¯ã€ã“ã®P5-small variantã‚’P5-Iã¨åä»˜ã‘ã€Beautyãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§äº‹å‰å­¦ç¿’ã™ã‚‹ã€‚
While the former utilizes collaborative learning to implicitly optimize the latent correlations among different sub-word tokens, the latter learns a unique personalized representation for every extra token.
å‰è€…ãŒç•°ãªã‚‹ã‚µãƒ–å˜èªãƒˆãƒ¼ã‚¯ãƒ³é–“ã®æ½œåœ¨çš„ãªç›¸é–¢é–¢ä¿‚ã‚’æš—é»™ã®ã†ã¡ã«æœ€é©åŒ–ã™ã‚‹ãŸã‚ã«å”èª¿å­¦ç¿’ã‚’åˆ©ç”¨ã™ã‚‹ã®ã«å¯¾ã—ã€å¾Œè€…ã¯ä½™åˆ†ãªãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã«ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸè¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹ã€‚
The performance comparison between P5-S and P5-I is shown in Figure 6.
P5-Sã¨P5-Iã®æ€§èƒ½æ¯”è¼ƒã‚’å›³6ã«ç¤ºã™ã€‚
We can see that P5-I achieves similar performances as P5-S on regression tasks (Prompts 1-6 & 1-10 for rating prediction, Prompts 4-2 & 4-4 for review-based rating regression) and review summarization tasks (Prompt 4-1).
P5-Iã¯ã€å›å¸°ã‚¿ã‚¹ã‚¯ï¼ˆè©•ä¾¡äºˆæ¸¬ã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1-6ã¨1-10ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡å›å¸°ã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ4-2ã¨4-4ï¼‰ãŠã‚ˆã³ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦ç´„ã‚¿ã‚¹ã‚¯ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ4-1ï¼‰ã«ãŠã„ã¦ã€P5-Sã¨åŒæ§˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
Also, P5-I is slightly better than P5-S on explanation generation tasks (Prompts 3-3, 3-9 & 3-12).
ã¾ãŸã€èª¬æ˜ç”Ÿæˆã‚¿ã‚¹ã‚¯ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ3-3ã€3-9ã€3-12ï¼‰ã§ã¯ã€P5-Iã¯P5-Sã‚ˆã‚Šã‚ãšã‹ã«å„ªã‚Œã¦ã„ã‚‹ã€‚
However, P5-I significantly underperforms P5-S by a large margin on both sequential and direct recommendation tasks (all prompts in Figure 6 (c) & (d)).
ã—ã‹ã—ã€P5-Iã¯ã€é€æ¬¡æ¨è–¦ã‚¿ã‚¹ã‚¯ã¨ç›´æ¥æ¨è–¦ã‚¿ã‚¹ã‚¯ã®ä¸¡æ–¹ã§ã€P5-Sã‚’å¤§ããä¸‹å›ã£ã¦ã„ã‚‹ï¼ˆå›³6 (c)ã¨(d)ã®ã™ã¹ã¦ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ã€‚
The reason behind P5-Iâ€™s lower performance lies in that the newly introduced huge number of extra tokens and embeddings cannot be well trained compared with the original sub-word units initialized from T5.
P5-Iã®æ€§èƒ½ãŒä½ã„ç†ç”±ã¯ã€æ–°ãŸã«å°å…¥ã•ã‚ŒãŸè†¨å¤§ãªæ•°ã®ä½™åˆ†ãªãƒˆãƒ¼ã‚¯ãƒ³ã¨åŸ‹ã‚è¾¼ã¿ãŒã€T5ã‹ã‚‰åˆæœŸåŒ–ã•ã‚ŒãŸå…ƒã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰å˜ä½ã«æ¯”ã¹ã¦ã†ã¾ãå­¦ç¿’ã§ããªã„ã“ã¨ã«ã‚ã‚‹ã€‚
This shows that our default setting can achieve better recommendation and overall performances with the help of collaborative learning while keeping a small and constant amount of learnable tokens.
ã“ã‚Œã¯ã€å­¦ç¿’å¯èƒ½ãªãƒˆãƒ¼ã‚¯ãƒ³ã®é‡ã‚’å°‘ãªãä¸€å®šã«ä¿ã¡ãªãŒã‚‰ã€æˆ‘ã€…ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãŒå”èª¿å­¦ç¿’ã®åŠ©ã‘ã‚’å€Ÿã‚Šã¦ã€ã‚ˆã‚Šè‰¯ã„æ¨è–¦ã¨å…¨ä½“çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

# Conclusion and Future Work çµè«–ã¨ä»Šå¾Œã®èª²é¡Œ

In this paper, we present P5 which unifies different recommendation tasks into a shared language modeling and natural language generation framework.
æœ¬ç¨¿ã§ã¯ã€ç•°ãªã‚‹æ¨è–¦ã‚¿ã‚¹ã‚¯ã‚’è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã¨è‡ªç„¶è¨€èªç”Ÿæˆã®å…±æœ‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«çµ±åˆã—ãŸP5ã‚’ç´¹ä»‹ã™ã‚‹ã€‚
By designing a collection of personalized prompts covering five recommendation task families, we transfer all raw data such as the user-item interactions, user descriptions, item metadata, and user reviews to the same format â€“ input-target text pairs.
5ã¤ã®æ¨è–¦ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’ã‚«ãƒãƒ¼ã™ã‚‹ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€ãƒ¦ãƒ¼ã‚¶ã¨ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã€ãƒ¦ãƒ¼ã‚¶ã®èª¬æ˜ã€ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã€ãŠã‚ˆã³ãƒ¦ãƒ¼ã‚¶ãƒ¬ãƒ“ãƒ¥ãƒ¼ãªã©ã®ã™ã¹ã¦ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆå…¥åŠ›ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ï¼‰ã«è»¢é€ã™ã‚‹ã€‚
We then pretrain P5 in a full language environment to help it discover deeper semantics for various recommendation tasks.
æ¬¡ã«ã€P5ãŒæ§˜ã€…ãªæ¨è–¦ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦ã‚ˆã‚Šæ·±ã„ã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹ã‚’ç™ºè¦‹ã§ãã‚‹ã‚ˆã†ã«ã€å®Œå…¨ãªè¨€èªç’°å¢ƒã§P5ã‚’äº‹å‰è¨“ç·´ã™ã‚‹ã€‚
According to our experiments, P5 can beat or achieve similar performance with several representative approaches on all five task families.
æˆ‘ã€…ã®å®Ÿé¨“ã«ã‚ˆã‚‹ã¨ã€P5ã¯5ã¤ã®ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã™ã¹ã¦ã«ãŠã„ã¦ã€ã„ãã¤ã‹ã®ä»£è¡¨çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«å‹ã¤ã‹ã€åŒç­‰ã®æ€§èƒ½ã‚’é”æˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
Moreover, P5 shows the generalization ability on performing zeroshot transfer to new items, new domains, and new personalized prompts.
ã•ã‚‰ã«ã€P5ã¯ã€æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ã€æ–°ã—ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ã€æ–°ã—ã„ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¸ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚¡ãƒ¼ã«é–¢ã™ã‚‹æ±åŒ–èƒ½åŠ›ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
In the future, we will continue exploring to further enlarge the model size of P5 and employ more powerful base models such as GPT-3, OPT, and BLOOM.
å°†æ¥çš„ã«ã¯ã€P5ã®ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«æ‹¡å¤§ã—ã€GPT-3ã€OPTã€BLOOMãªã©ã€ã‚ˆã‚Šå¼·åŠ›ãªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’æ¡ç”¨ã™ã‚‹ãŸã‚ã®æ¢æ±‚ã‚’ç¶šã‘ã¦ã„ãã€‚
Besides, P5 is a very flexible paradigm and it is promising to further extend P5 to diverse modalities and more tasks such as conversational recommendation, comparative recommendation, cross-platform recommendation, or even various search tasks by incorporating user queries into P5.
ã•ã‚‰ã«ã€P5ã¯éå¸¸ã«æŸ”è»Ÿãªãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§ã‚ã‚Šã€P5ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒªã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã«ã‚ˆã£ã¦ã€ä¼šè©±å‹æ¨è–¦ã€æ¯”è¼ƒå‹æ¨è–¦ã€ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ¨è–¦ã€ã‚ã‚‹ã„ã¯æ§˜ã€…ãªæ¤œç´¢ã‚¿ã‚¹ã‚¯ãªã©ã€å¤šæ§˜ãªãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚„ã‚ˆã‚Šå¤šãã®ã‚¿ã‚¹ã‚¯ã«P5ã‚’ã•ã‚‰ã«æ‹¡å¼µã™ã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã‚‹ã€‚
Finally, in this work, we designed explicit prompts since they are intuitive, flexible, and close to the natural way of how humans communicate with each other, which enables instruction-based recommendation, while in the future, we will also investigate prompt search and/or latent prompt techniques to achieve instruction prompts or leverage retrieval-enhanced generation to further boost P5â€™s performance on downstream tasks.
æœ€å¾Œã«ã€æœ¬ç ”ç©¶ã§ã¯ã€æ˜ç¤ºçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ç›´æ„Ÿçš„ã§æŸ”è»Ÿæ€§ãŒã‚ã‚Šã€äººé–“åŒå£«ã®è‡ªç„¶ãªã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«è¿‘ã„ãŸã‚ã€æŒ‡ç¤ºã«åŸºã¥ãæ¨è–¦ãŒå¯èƒ½ã§ã‚ã‚‹ã¨ã—ã¦ã€æ˜ç¤ºçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­è¨ˆã—ãŸãŒã€ä»Šå¾Œã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¤œç´¢ã‚„æ½œåœ¨çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æŠ€è¡“ã‚‚ç ”ç©¶ã—ã€æŒ‡ç¤ºã«åŸºã¥ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å®Ÿç¾ã—ãŸã‚Šã€æ¤œç´¢ã‚’æ´»ç”¨ã—ãŸç”Ÿæˆã‚’è¡Œã†ã“ã¨ã§ã€ä¸‹æµã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹P5ã®æ€§èƒ½ã‚’ã•ã‚‰ã«å‘ä¸Šã•ã›ã‚‹äºˆå®šã§ã‚ã‚‹ã€‚
