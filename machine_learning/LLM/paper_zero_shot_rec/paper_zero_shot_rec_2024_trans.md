refs: https://arxiv.org/abs/2305.08845

# Large Language Models are Zero-Shot Rankers for Recommender Systems å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãƒ©ãƒ³ã‚«ãƒ¼ã§ã‚ã‚‹

Recently, large language models(LLMs) (e.g.,GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks.
æœ€è¿‘ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ï¼ˆä¾‹ï¼šGPT-4ï¼‰ã¯ã€æ¨è–¦ã‚¿ã‚¹ã‚¯ã«ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã™ã‚‹å¯èƒ½æ€§ã‚’å«ã‚€ã€å°è±¡çš„ãªæ±ç”¨ã‚¿ã‚¹ã‚¯è§£æ±ºèƒ½åŠ›ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems.
ã“ã®ç ”ç©¶ã®æµã‚Œã«æ²¿ã£ã¦ã€æœ¬ç ”ç©¶ã¯ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹LLMsã®èƒ½åŠ›ã‚’èª¿æŸ»ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚
We first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by other candidate generation models as candidates.
ã¾ãšã€æ¨è–¦å•é¡Œã‚’æ¡ä»¶ä»˜ããƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã¨ã—ã¦å½¢å¼åŒ–ã—ã€é€æ¬¡çš„ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´ã‚’æ¡ä»¶ã¨ã—ã¦ã€ä»–ã®å€™è£œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å–å¾—ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’å€™è£œã¨ã—ã¦è€ƒæ…®ã—ã¾ã™ã€‚
To solve the ranking task by LLMs, we carefully design the prompting template and conduct extensive experiments on two widely-used datasets.
LLMsã«ã‚ˆã‚‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æ…é‡ã«è¨­è¨ˆã—ã€åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹2ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§åºƒç¯„ãªå®Ÿé¨“ã‚’è¡Œã„ã¾ã™ã€‚
We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts.
ç§ãŸã¡ã¯ã€LLMsãŒæœ‰æœ›ãªã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãƒ©ãƒ³ã‚­ãƒ³ã‚°èƒ½åŠ›ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ãŒã€(1) æ­´å²çš„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã®é †åºã‚’èªè­˜ã™ã‚‹ã®ã«è‹¦åŠ´ã—ã€(2) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®äººæ°—ã‚„ã‚¢ã‚¤ãƒ†ãƒ ã®ä½ç½®ã«ã‚ˆã£ã¦ãƒã‚¤ã‚¢ã‚¹ãŒã‹ã‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
We demonstrate that these issues can be alleviated using specially designed prompting and bootstrapping strategies.
ã“ã‚Œã‚‰ã®å•é¡Œã¯ã€ç‰¹åˆ¥ã«è¨­è¨ˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ”ãƒ³ã‚°æˆ¦ç•¥ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§è»½æ¸›ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
Equipped with these insights, zero-shot LLMs can even challenge conventional recommendation models when ranking candidates are retrieved by multiple candidate generators.
ã“ã‚Œã‚‰ã®æ´å¯Ÿã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆLLMsã¯ã€è¤‡æ•°ã®å€™è£œç”Ÿæˆå™¨ã«ã‚ˆã£ã¦å€™è£œãŒå–å¾—ã•ã‚Œã‚‹éš›ã«ã€å¾“æ¥ã®æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã«æŒ‘æˆ¦ã™ã‚‹ã“ã¨ã•ãˆå¯èƒ½ã§ã™ã€‚
The code and processed datasets are available at https://github.com/RUCAIBox/LLMRank.
ã‚³ãƒ¼ãƒ‰ã¨å‡¦ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€https://github.com/RUCAIBox/LLMRank ã§å…¥æ‰‹å¯èƒ½ã§ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## 1Introduction 1 ã¯ã˜ã‚ã«

In the literature of recommender systems, most existing models are trained with user behavior data from a specific domain or scenario[49,26,28], suffering from two major issues. 
æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®æ–‡çŒ®ã«ãŠã„ã¦ã€æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã®ã»ã¨ã‚“ã©ã¯ç‰¹å®šã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã¾ãŸã¯ã‚·ãƒŠãƒªã‚ªã‹ã‚‰ã®ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã•ã‚Œã¦ãŠã‚Š[49,26,28]ã€2ã¤ã®ä¸»è¦ãªå•é¡Œã«æ‚©ã¾ã•ã‚Œã¦ã„ã¾ã™ã€‚
Firstly, it is difficult to capture user preference by solely modeling historical behaviors, e.g., clicked item sequences[28,33,82], limiting the expressive power to model more complicated but explicit user interests (e.g., intentions expressed in natural language). 
ç¬¬ä¸€ã«ã€éå»ã®è¡Œå‹•ã€ä¾‹ãˆã°ã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹[28,33,82]ã®ã¿ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ã“ã¨ã§ãƒ¦ãƒ¼ã‚¶ã®å¥½ã¿ã‚’æ‰ãˆã‚‹ã“ã¨ãŒé›£ã—ãã€ã‚ˆã‚Šè¤‡é›‘ã§æ˜ç¤ºçš„ãªãƒ¦ãƒ¼ã‚¶ã®èˆˆå‘³ï¼ˆä¾‹ãˆã°è‡ªç„¶è¨€èªã§è¡¨ç¾ã•ã‚ŒãŸæ„å›³ï¼‰ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹è¡¨ç¾åŠ›ãŒåˆ¶é™ã•ã‚Œã¾ã™ã€‚
Secondly, these models are essentially â€œnarrow expertsâ€, lacking more comprehensive knowledge in solving complicated recommendation tasks that rely on background or commonsense knowledge[23]. 
ç¬¬äºŒã«ã€**ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯æœ¬è³ªçš„ã«ã€Œç‹­ã„å°‚é–€å®¶ã€**ã§ã‚ã‚Šã€èƒŒæ™¯çŸ¥è­˜ã‚„å¸¸è­˜çš„çŸ¥è­˜[23]ã«ä¾å­˜ã™ã‚‹è¤‡é›‘ãªæ¨è–¦ã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªçŸ¥è­˜ãŒæ¬ ã‘ã¦ã„ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

To improve recommendation performance and interactivity, there have been increasing efforts that explore the use of pre-trained language models (PLMs) in recommender systems[21,30,62]. 
æ¨è–¦æ€§èƒ½ã¨ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹äº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆPLMsï¼‰ã®ä½¿ç”¨ã‚’æ¢ã‚‹åŠªåŠ›ãŒå¢—åŠ ã—ã¦ã„ã¾ã™[21,30,62]ã€‚
They aim to explicitly capture user preference in natural language[21] or transfer rich world knowledge from text corpora[30,29]. 
ã“ã‚Œã‚‰ã¯ã€è‡ªç„¶è¨€èª[21]ã§ãƒ¦ãƒ¼ã‚¶ã®å¥½ã¿ã‚’æ˜ç¤ºçš„ã«æ‰ãˆãŸã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰è±Šå¯Œãªä¸–ç•ŒçŸ¥è­˜ã‚’è»¢é€ã—ãŸã‚Šã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™[30,29]ã€‚
Despite their effectiveness, thoroughly fine-tuning the recommendation models on task-specific data is still a necessity, making it less capable of solving diverse recommendation tasks[30]. 
ãã®åŠ¹æœã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€ã‚¿ã‚¹ã‚¯ç‰¹æœ‰ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã‚’å¾¹åº•çš„ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã¯ä¾ç„¶ã¨ã—ã¦å¿…è¦ã§ã‚ã‚Šã€ã•ã¾ã–ã¾ãªæ¨è–¦ã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã™ã‚‹èƒ½åŠ›ãŒä½ä¸‹ã—ã¾ã™[30]ã€‚
More recently, large language models (LLMs) have shown great potential to serve as zero-shot task solvers[64,52]. 
æœ€è¿‘ã§ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ãŒã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã‚¿ã‚¹ã‚¯ã‚½ãƒ«ãƒãƒ¼ã¨ã—ã¦ã®å¤§ããªå¯èƒ½æ€§ã‚’ç¤ºã—ã¦ã„ã¾ã™[64,52]ã€‚
Indeed, there are some preliminary attempts that employ LLMs for solving recommendation tasks[20,59,60,13,40,74]. 
å®Ÿéš›ã€æ¨è–¦ã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«LLMsã‚’åˆ©ç”¨ã™ã‚‹ã„ãã¤ã‹ã®åˆæœŸã®è©¦ã¿ãŒã‚ã‚Šã¾ã™[20,59,60,13,40,74]ã€‚
These studies mainly focus on discussing the possibility of building a capable recommender with LLMs. 
ã“ã‚Œã‚‰ã®ç ”ç©¶ã¯ä¸»ã«ã€LLMsã‚’ç”¨ã„ã¦æœ‰èƒ½ãªæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹å¯èƒ½æ€§ã«ã¤ã„ã¦è­°è«–ã™ã‚‹ã“ã¨ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚
While promising, the insufficient understanding of the new characteristics when making recommendations using LLMs could hinder the development of this new paradigm. 
æœŸå¾…ãŒæŒã¦ã‚‹ä¸€æ–¹ã§ã€LLMsã‚’ä½¿ç”¨ã—ã¦æ¨è–¦ã‚’è¡Œã†éš›ã®æ–°ã—ã„ç‰¹æ€§ã«å¯¾ã™ã‚‹ç†è§£ãŒä¸ååˆ†ã§ã‚ã‚‹ã“ã¨ã¯ã€ã“ã®æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã®ç™ºå±•ã‚’å¦¨ã’ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

In this paper, we conduct empirical studies to investigate what determines the capacity of LLMs that serve as recommendation models. 
æœ¬è«–æ–‡ã§ã¯ã€æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹LLMsã®èƒ½åŠ›ã‚’æ±ºå®šã™ã‚‹è¦å› ã‚’èª¿æŸ»ã™ã‚‹ãŸã‚ã«å®Ÿè¨¼ç ”ç©¶ã‚’è¡Œã„ã¾ã™ã€‚
Typically, recommender systems are developed in a pipeline architecture[10], consisting of candidate generation (retrieving relevant items) and ranking (ranking relevant items at a higher position) procedures. 
é€šå¸¸ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£[10]ã§é–‹ç™ºã•ã‚Œã€å€™è£œç”Ÿæˆï¼ˆé–¢é€£ã‚¢ã‚¤ãƒ†ãƒ ã®å–å¾—ï¼‰ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆé–¢é€£ã‚¢ã‚¤ãƒ†ãƒ ã‚’é«˜ã„ä½ç½®ã«ãƒ©ãƒ³ã‚¯ä»˜ã‘ã™ã‚‹ï¼‰æ‰‹é †ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚
This work mainly focuses on the ranking stage of recommender systems, since LLMs are more expensive to run on a large-scale candidate set. 
**ã“ã®ç ”ç©¶ã¯ä¸»ã«æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ®µéšã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚ãªãœãªã‚‰ã€LLMsã¯å¤§è¦æ¨¡ãªå€™è£œã‚»ãƒƒãƒˆã§å®Ÿè¡Œã™ã‚‹ã®ã«ã‚ˆã‚Šã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹ã‹ã‚‰ã§ã™**ã€‚
Further, the ranking performance is sensitive to the retrieved candidate items, which is more suitable to examine the subtle differences in the recommendation abilities of LLMs. 
ã•ã‚‰ã«ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ã¯å–å¾—ã•ã‚ŒãŸå€™è£œã‚¢ã‚¤ãƒ†ãƒ ã«æ•æ„Ÿã§ã‚ã‚Šã€LLMsã®æ¨è–¦èƒ½åŠ›ã®å¾®å¦™ãªé•ã„ã‚’æ¤œè¨¼ã™ã‚‹ã®ã«ã‚ˆã‚Šé©ã—ã¦ã„ã¾ã™ã€‚

To carry out this study, we first formalize the recommendation process of LLMs as a conditional ranking task. 
ã“ã®ç ”ç©¶ã‚’å®Ÿæ–½ã™ã‚‹ãŸã‚ã«ã€ã¾ãšLLMsã®æ¨è–¦ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ¡ä»¶ä»˜ããƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã¨ã—ã¦å½¢å¼åŒ–ã—ã¾ã™ã€‚
Given prompts that include sequential historical interactions as â€œconditionsâ€, LLMs are instructed to rank a set of â€œcandidatesâ€ (e.g., items retrieved by candidate generation models), according to LLMâ€™s intrinsic knowledge. 
**ã€Œæ¡ä»¶ã€ã¨ã—ã¦ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ãªéå»ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒä¸ãˆã‚‰ã‚Œã€LLMsã¯LLMã®å†…åœ¨çš„ãªçŸ¥è­˜ã«åŸºã¥ã„ã¦ã€Œå€™è£œã€ï¼ˆä¾‹ãˆã°ã€å€™è£œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å–å¾—ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ï¼‰ã®ã‚»ãƒƒãƒˆã‚’ãƒ©ãƒ³ã‚¯ä»˜ã‘ã™ã‚‹ã‚ˆã†æŒ‡ç¤ºã•ã‚Œã¾ã™**ã€‚
Then we conduct control experiments to systematically study the empirical performance of LLMs as rankers by designing specific configurations for â€œconditionsâ€ and â€œcandidatesâ€, respectively. 
æ¬¡ã«ã€ã€Œæ¡ä»¶ã€ã¨ã€Œå€™è£œã€ã«å¯¾ã—ã¦ãã‚Œãã‚Œç‰¹å®šã®æ§‹æˆã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€LLMsã‚’ãƒ©ãƒ³ã‚«ãƒ¼ã¨ã—ã¦ã®å®Ÿè¨¼æ€§èƒ½ã‚’ä½“ç³»çš„ã«ç ”ç©¶ã™ã‚‹ãŸã‚ã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«å®Ÿé¨“ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚
Overall, we attempt to answer the following key questions: 
å…¨ä½“ã¨ã—ã¦ã€ä»¥ä¸‹ã®é‡è¦ãªè³ªå•ã«ç­”ãˆã‚‹ã“ã¨ã‚’è©¦ã¿ã¾ã™ï¼š

- What factors affect the zero-shot ranking performance of LLMs? 
   LLMsã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹è¦å› ã¯ä½•ã§ã™ã‹ï¼Ÿ

- What data or knowledge do LLMs rely on for recommendation? 
   LLMsã¯æ¨è–¦ã®ãŸã‚ã«ã©ã®ã‚ˆã†ãªãƒ‡ãƒ¼ã‚¿ã‚„çŸ¥è­˜ã«ä¾å­˜ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ

Our empirical experiments are conducted on two public datasets for recommender systems.
ç§ãŸã¡ã®å®Ÿè¨¼å®Ÿé¨“ã¯ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ãŸã‚ã®2ã¤ã®å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å®Ÿæ–½ã•ã‚Œã¾ã™ã€‚
The results lead to several key findings that potentially shed light on how to develop LLMs as powerful ranking models for recommender systems. 
ãã®çµæœã€LLMsã‚’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®å¼·åŠ›ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦é–‹ç™ºã™ã‚‹æ–¹æ³•ã«å…‰ã‚’å½“ã¦ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ã„ãã¤ã‹ã®é‡è¦ãªç™ºè¦‹ãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚
We summarize the key findings as follows: 
ç§ãŸã¡ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«é‡è¦ãªç™ºè¦‹ã‚’è¦ç´„ã—ã¾ã™ï¼š

- LLMs struggle to perceive the order of the given sequential interaction histories. 
   LLMsã¯ä¸ãˆã‚‰ã‚ŒãŸã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´ã®é †åºã‚’èªè­˜ã™ã‚‹ã®ã«è‹¦åŠ´ã—ã¾ã™ã€‚
   By employing specifically designed promptings, LLMs can be triggered to perceive the order, leading to improved ranking performance. 
   ç‰¹åˆ¥ã«è¨­è¨ˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€LLMsã¯é †åºã‚’èªè­˜ã™ã‚‹ã‚ˆã†ã«ãƒˆãƒªã‚¬ãƒ¼ã•ã‚Œã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ãŒå‘ä¸Šã—ã¾ã™ã€‚

- LLMs suffer from position bias and popularity bias while ranking, which can be alleviated by bootstrapping or specially designed prompting strategies. 
   LLMsã¯ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä¸­ã«ä½ç½®ãƒã‚¤ã‚¢ã‚¹ã¨äººæ°—ãƒã‚¤ã‚¢ã‚¹ã«æ‚©ã¾ã•ã‚Œã¦ãŠã‚Šã€ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ”ãƒ³ã‚°ã‚„ç‰¹åˆ¥ã«è¨­è¨ˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæˆ¦ç•¥ã«ã‚ˆã£ã¦è»½æ¸›ã§ãã¾ã™ã€‚

- LLMs outperform existing zero-shot recommendation methods, showing promising zero-shot ranking abilities, especially on candidates retrieved by multiple candidate generation models with different practical strategies. 
   LLMsã¯æ—¢å­˜ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ¨è–¦æ‰‹æ³•ã‚’ä¸Šå›ã‚Šã€æœ‰æœ›ãªã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãƒ©ãƒ³ã‚­ãƒ³ã‚°èƒ½åŠ›ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ç‰¹ã«ã€ç•°ãªã‚‹å®Ÿç”¨çš„æˆ¦ç•¥ã‚’æŒã¤è¤‡æ•°ã®å€™è£œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å–å¾—ã•ã‚ŒãŸå€™è£œã«å¯¾ã—ã¦é¡•è‘—ã§ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## 2General Framework for LLMs as Rankers LLMã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã®ä¸€èˆ¬çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

To investigate the recommendation abilities of LLMs, we first formalize the recommendation process as a conditional ranking task. 
LLMsã®æ¨è–¦èƒ½åŠ›ã‚’èª¿æŸ»ã™ã‚‹ãŸã‚ã«ã€ã¾ãšæ¨è–¦ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ¡ä»¶ä»˜ããƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã¨ã—ã¦å½¢å¼åŒ–ã—ã¾ã™ã€‚ 
Then, we describe a general framework that adapts LLMs to solve the recommendation task.
æ¬¡ã«ã€LLMsã‚’æ¨è–¦ã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«é©å¿œã•ã›ã‚‹ä¸€èˆ¬çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’èª¬æ˜ã—ã¾ã™ã€‚ 

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### 2.1 Problem Fromulation å•é¡Œã®å®šå¼åŒ–

Given the historical interactions â„‹ = {iâ‚, iâ‚‚, â€¦, iâ‚™} of one user (in chronological order of interaction time) as conditions, the task is to rank the candidate items â„­ = {iâ±¼}_{j=1}^{m} such that the items of interest would be ranked at a higher position. 
1äººã®ãƒ¦ãƒ¼ã‚¶ã®éå»ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ $H = \{i_{1}, i_{2}, \ldots, i_{n}\}$ï¼ˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ™‚é–“ã®æ™‚ç³»åˆ—é †ï¼‰ã‚’æ¡ä»¶ã¨ã—ã¦ä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€ã‚¿ã‚¹ã‚¯ã¯å€™è£œã‚¢ã‚¤ãƒ†ãƒ  $C = \{i_{j}\}_{j=1}^{m}\$ ã‚’ãƒ©ãƒ³ã‚¯ä»˜ã‘ã—ã€é–¢å¿ƒã®ã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ãŒã‚ˆã‚Šé«˜ã„ä½ç½®ã«ãƒ©ãƒ³ã‚¯ä»˜ã‘ã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã§ã™ã€‚
In practice, the candidate items are usually retrieved by candidate generation models from the whole item set â„ (m â‰ª |â„|). 
å®Ÿéš›ã«ã¯ã€å€™è£œã‚¢ã‚¤ãƒ†ãƒ ã¯é€šå¸¸ã€å€™è£œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å…¨ã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆ $I (m << |I|)$ ã‹ã‚‰å–å¾—ã•ã‚Œã¾ã™ã€‚
Further, we assume that each item i is associated with a descriptive text táµ¢. 
ã•ã‚‰ã«ã€å„ã‚¢ã‚¤ãƒ†ãƒ  $i$ ã¯è¨˜è¿°ãƒ†ã‚­ã‚¹ãƒˆ $t_{i}$ ã«é–¢é€£ä»˜ã‘ã‚‰ã‚Œã¦ã„ã‚‹ã¨ä»®å®šã—ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### 2.2ã€€Ranking with LLMs Using Natural Language Instructionsã€€è‡ªç„¶è¨€èªæŒ‡ç¤ºã‚’ç”¨ã„ãŸLLMsã«ã‚ˆã‚‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°

We use LLMs as ranking models to solve the above-mentioned task in an instruction-following paradigm[64]. 
ç§ãŸã¡ã¯ã€æŒ‡ç¤ºã«å¾“ã†ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ [64]ã§ä¸Šè¨˜ã®ã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«LLMsã‚’ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚
Specifically, for each user, we first construct two natural language patterns that contain sequential interaction histories â„‹ (conditions) and retrieved candidate items ğ’ (candidates), respectively. 
**å…·ä½“çš„ã«ã¯ã€å„ãƒ¦ãƒ¼ã‚¶ã«ã¤ã„ã¦ã€ã¾ãšæ¡ä»¶ã¨ã—ã¦ã®é€æ¬¡çš„ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´ $H$ ã¨ã€å€™è£œã¨ã—ã¦ã®å–å¾—ã•ã‚ŒãŸå€™è£œã‚¢ã‚¤ãƒ†ãƒ  $C$ ã‚’å«ã‚€2ã¤ã®è‡ªç„¶è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™**ã€‚
Then these patterns are filled into a natural language template T as the final instruction. 
æ¬¡ã«ã€ã“ã‚Œã‚‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è‡ªç„¶è¨€èªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ $T$ ã«åŸ‹ã‚è¾¼ã¿ã€æœ€çµ‚çš„ãªæŒ‡ç¤ºã¨ã—ã¾ã™ã€‚
In this way, LLMs are expected to understand the instructions and output the ranking results as the instruction suggests. 
ã“ã®ã‚ˆã†ã«ã—ã¦ã€LLMsã¯æŒ‡ç¤ºã‚’ç†è§£ã—ã€æŒ‡ç¤ºãŒç¤ºã™é€šã‚Šã«ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚
The overall framework of the ranking approach by LLMs is depicted in Figure 1. 
LLMsã«ã‚ˆã‚‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å…¨ä½“çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€å›³1ã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚
Next, we describe the detailed instruction design in our approach. 
æ¬¡ã«ã€ç§ãŸã¡ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ãŠã‘ã‚‹è©³ç´°ãªæŒ‡ç¤ºè¨­è¨ˆã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚

![]()

#### Sequential historical interactions.  é€æ¬¡çš„ãªæ­´å²çš„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã€‚

To investigate whether LLMs can capture user preferences from historical user behaviors, we include sequential historical interactions â„‹ into the instructions as inputs of LLMs. 
LLMsãŒæ­´å²çš„ãªãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ã®å¥½ã¿ã‚’æ‰ãˆã‚‰ã‚Œã‚‹ã‹ã‚’èª¿æŸ»ã™ã‚‹ãŸã‚ã«ã€é€æ¬¡çš„ãªæ­´å²çš„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ $H$ ã‚’LLMsã®å…¥åŠ›ã¨ã—ã¦æŒ‡ç¤ºã«å«ã‚ã¾ã™ã€‚
To enable LLMs to be aware of the sequential nature of historical interactions, we propose three ways to construct the instructions: 
LLMsãŒæ­´å²çš„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã®é€æ¬¡çš„ãªæ€§è³ªã‚’èªè­˜ã§ãã‚‹ã‚ˆã†ã«ã€**æŒ‡ç¤ºã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®3ã¤ã®æ–¹æ³•**ã‚’ææ¡ˆã—ã¾ã™ã€‚

- Sequential prompting: Arrange the historical interactions in chronological order. 
   é€æ¬¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ï¼šæ­´å²çš„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ™‚ç³»åˆ—ã§é…ç½®ã—ã¾ã™ã€‚
   This way has also been used in prior studies[13]. 
   ã“ã®æ–¹æ³•ã¯ä»¥å‰ã®ç ”ç©¶[13]ã§ã‚‚ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
   For example, â€œIâ€™ve watched the following movies in the past in order: â€™0. Multiplicityâ€™, â€™1. Jurassic Parkâ€™,â€¦normal-â€¦\ldotsâ€¦â€. 
   ä¾‹ãˆã°ã€ã€Œéå»ã«æ¬¡ã®æ˜ ç”»ã‚’ã“ã®é †ç•ªã§è¦‹ã¾ã—ãŸï¼šâ€™0. Multiplicityâ€™, â€™1. Jurassic Parkâ€™,â€¦ã€ã€‚

- Recency-focused prompting: In addition to the sequential interaction records, we can add an additional sentence to emphasize the most recent interaction. 
   æœ€è¿‘é‡è¦–ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ï¼šé€æ¬¡çš„ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³è¨˜éŒ²ã«åŠ ãˆã¦ã€**æœ€ã‚‚æœ€è¿‘ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¼·èª¿ã™ã‚‹ãŸã‚ã®è¿½åŠ ã®æ–‡ã‚’åŠ ãˆã‚‹**ã“ã¨ãŒã§ãã¾ã™ã€‚
   For example, â€œIâ€™ve watched the following movies in the past in order: â€™0. Multiplicityâ€™, â€™1. Jurassic Parkâ€™,â€¦normal-â€¦\ldotsâ€¦.Note that my most recently watched movie is Dead Presidents.â€¦normal-â€¦\ldotsâ€¦â€. 
   ä¾‹ãˆã°ã€ã€Œéå»ã«æ¬¡ã®æ˜ ç”»ã‚’ã“ã®é †ç•ªã§è¦‹ã¾ã—ãŸï¼šâ€™0. Multiplicityâ€™, â€™1. Jurassic Parkâ€™,â€¦normal-â€¦\ldotsâ€¦ã€‚æœ€è¿‘è¦‹ãŸæ˜ ç”»ã¯Dead Presidentsã§ã™â€¦normal-â€¦\ldotsâ€¦ã€ã€‚

- In-context learning (ICL): ICL is a prominent prompting approach for LLMs to solve various tasks[79], where it includes demonstration examples in the prompt. 
  ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ï¼ˆICLï¼‰ï¼šICLã¯ã€LLMsãŒã•ã¾ã–ã¾ãªã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®é‡è¦ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ[79]ã§ã‚ã‚Šã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¾‹ã‚’å«ã¿ã¾ã™ã€‚
   For the personalized recommendation task, simply introducing examples of other users may introduce noises because users usually have different preferences. 
   ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸæ¨è–¦ã‚¿ã‚¹ã‚¯ã§ã¯ã€ä»–ã®ãƒ¦ãƒ¼ã‚¶ã®ä¾‹ã‚’å˜ã«å°å…¥ã™ã‚‹ã“ã¨ã¯ã€ãƒ¦ãƒ¼ã‚¶ãŒé€šå¸¸ç•°ãªã‚‹å¥½ã¿ã‚’æŒã¤ãŸã‚ã€ãƒã‚¤ã‚ºã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
   Instead, we introduce demonstration examples by augmenting the input interaction sequence itself. 
   ãã®ä»£ã‚ã‚Šã«ã€å…¥åŠ›ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹è‡ªä½“ã‚’æ‹¡å¼µã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¾‹ã‚’å°å…¥ã—ã¾ã™ã€‚
   We pair the prefix of the input interaction sequence and the corresponding successor as examples. 
   **å…¥åŠ›ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æ¥é ­è¾ã¨å¯¾å¿œã™ã‚‹å¾Œç¶šã‚’ä¾‹ã¨ã—ã¦ãƒšã‚¢ã«ã—ã¾ã™**ã€‚(ã“ã‚Œã¯è¦ã™ã‚‹ã«few-shot learningã£ã¦ã“ã¨ã‹...!:thinking:)
   For instance, â€œIf Iâ€™ve watched the following movies in the past in order: â€™0. Multiplicityâ€™, â€™1. Jurassic Parkâ€™,â€¦normal-â€¦\ldotsâ€¦, then you should recommend Dead Presidents to me and now that Iâ€™ve watched Dead Presidents, thenâ€¦normal-â€¦\ldotsâ€¦â€.  
   ä¾‹ãˆã°ã€ã€Œã‚‚ã—ç§ãŒéå»ã«æ¬¡ã®æ˜ ç”»ã‚’ã“ã®é †ç•ªã§è¦‹ãŸãªã‚‰ï¼šâ€™0. Multiplicityâ€™, â€™1. Jurassic Parkâ€™,...ã€ã‚ãªãŸã¯Dead Presidentsã‚’ç§ã«æ¨è–¦ã™ã¹ãã§ã€ä»Šç§ãŒDead Presidentsã‚’è¦‹ãŸãªã‚‰...ã€ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

#### Retrieved candidate items. å–å¾—ã•ã‚ŒãŸå€™è£œã‚¢ã‚¤ãƒ†ãƒ ã€‚

Typically, candidate items to be ranked are first retrieved by candidate generation models[10]. 
é€šå¸¸ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã•ã‚Œã‚‹å€™è£œã‚¢ã‚¤ãƒ†ãƒ ã¯ã€æœ€åˆã«å€™è£œç”Ÿæˆãƒ¢ãƒ‡ãƒ«[10]ã«ã‚ˆã£ã¦å–å¾—ã•ã‚Œã¾ã™ã€‚
In this work, we consider a relatively small pool for the candidates, and keep 20 candidate items (i.e., $m=20$) for ranking. 
æœ¬ç ”ç©¶ã§ã¯ã€å€™è£œã®ãŸã‚ã«æ¯”è¼ƒçš„å°ã•ãªãƒ—ãƒ¼ãƒ«ã‚’è€ƒæ…®ã—ã€**ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ãŸã‚ã«20ã®å€™è£œã‚¢ã‚¤ãƒ†ãƒ ï¼ˆã™ãªã‚ã¡ã€$m=20$ï¼‰ã‚’ä¿æŒ**ã—ã¾ã™ã€‚
To rank these candidates with LLMs, we arrange the candidate items ğ’ in a sequential manner. 
ã“ã‚Œã‚‰ã®å€™è£œã‚’LLMsã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ã€å€™è£œã‚¢ã‚¤ãƒ†ãƒ  $C$ ã‚’é€æ¬¡çš„ã«é…ç½®ã—ã¾ã™ã€‚
For example, â€œNow there are 20 candidate movies that I can watch next: â€™0. Sister Actâ€™, â€™1. Sunset Blvdâ€™,â€¦normal-â€¦\ldotsâ€¦â€. 
ä¾‹ãˆã°ã€ã€Œ**ä»Šã€ç§ãŒæ¬¡ã«è¦‹ã‚‹ã“ã¨ãŒã§ãã‚‹20ã®å€™è£œæ˜ ç”»ãŒã‚ã‚Šã¾ã™ï¼šâ€™0. Sister Actâ€™, â€™1. Sunset Blvdâ€™,...**ã€ã€‚
Note that, following the classic candidate generation approach[10], there is no specific order for candidate items. 
å¤å…¸çš„ãªå€™è£œç”Ÿæˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒ[10]ã«å¾“ã£ã¦ã€**å€™è£œã‚¢ã‚¤ãƒ†ãƒ ã«ã¯ç‰¹å®šã®é †åºã¯ãªã„ã“ã¨ã«æ³¨æ„**ã—ã¦ãã ã•ã„ã€‚
As a result, 
ãã®çµæœã€
We generate different orders for the candidate items in the prompts, which enables us to further examine whether the ranking results of LLMs are affected by the arrangement order of candidates, i.e., position bias, and how to alleviate position bias via bootstrapping. 
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®å€™è£œã‚¢ã‚¤ãƒ†ãƒ ã®ç•°ãªã‚‹é †åºã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã€LLMsã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœãŒå€™è£œã®é…ç½®é †åºã€ã™ãªã‚ã¡ä½ç½®ãƒã‚¤ã‚¢ã‚¹ã®å½±éŸ¿ã‚’å—ã‘ã‚‹ã‹ã©ã†ã‹ã€ã¾ãŸãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚’é€šã˜ã¦ä½ç½®ãƒã‚¤ã‚¢ã‚¹ã‚’è»½æ¸›ã™ã‚‹æ–¹æ³•ã‚’ã•ã‚‰ã«æ¤œè¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

#### Ranking with large language models. å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€‚

Existing studies show that LLMs can follow natural language instructions to solve diverse tasks in a zero-shot setting[64,79]. 
æ—¢å­˜ã®ç ”ç©¶ã¯ã€LLMsãŒã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆè¨­å®š[64,79]ã§å¤šæ§˜ãªã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«è‡ªç„¶è¨€èªã®æŒ‡ç¤ºã«å¾“ã†ã“ã¨ãŒã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
To rank using LLMs, we infill the patterns above into the instruction template T. 
LLMsã‚’ä½¿ç”¨ã—ã¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ã€ä¸Šè¨˜ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒ‡ç¤ºãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ $T$ ã«åŸ‹ã‚è¾¼ã¿ã¾ã™ã€‚
An example instruction template can be given as: â€œ[pattern that contains sequential historical interactions â„‹][pattern that contains retrieved candidate items ğ’]Please rank these movies by measuring the possibilities that I would like to watch next most, according to my watching history.â€. 
ä¾‹ã¨ã—ã¦ã€æŒ‡ç¤ºãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼šã€Œ[é€æ¬¡çš„ãªæ­´å²çš„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ $H$ ã‚’å«ã‚€ãƒ‘ã‚¿ãƒ¼ãƒ³][å–å¾—ã•ã‚ŒãŸå€™è£œã‚¢ã‚¤ãƒ†ãƒ  $C$ ã‚’å«ã‚€ãƒ‘ã‚¿ãƒ¼ãƒ³]ç§ã®è¦–è´å±¥æ­´ã«åŸºã¥ã„ã¦ã€æ¬¡ã«æœ€ã‚‚è¦‹ãŸã„æ˜ ç”»ã®å¯èƒ½æ€§ã‚’æ¸¬å®šã—ã¦ã€ã“ã‚Œã‚‰ã®æ˜ ç”»ã‚’ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã—ã¦ãã ã•ã„ã€‚ã€ã€‚

#### Parsing the output of LLMs. LLMsã®å‡ºåŠ›ã®è§£æã€‚

Note that the output of LLMs is still in natural language text, and we parse the output with heuristic text-matching methods and ground the recommendation results on the specified item set. 
LLMsã®å‡ºåŠ›ã¯ä¾ç„¶ã¨ã—ã¦è‡ªç„¶è¨€èªãƒ†ã‚­ã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã€ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ãªãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒãƒãƒ³ã‚°æ‰‹æ³•ã‚’ç”¨ã„ã¦å‡ºåŠ›ã‚’è§£æã—ã€æŒ‡å®šã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆã«åŸºã¥ã„ã¦æ¨è–¦çµæœã‚’ç¢ºå®šã—ã¾ã™ã€‚
In detail, we can directly perform efficient substring matching algorithms like KMP[35] between the LLM outputs and the text of candidate items. 
å…·ä½“çš„ã«ã¯ã€LLMã®å‡ºåŠ›ã¨å€™è£œã‚¢ã‚¤ãƒ†ãƒ ã®ãƒ†ã‚­ã‚¹ãƒˆã®é–“ã§ã€KMP[35]ã®ã‚ˆã†ãªåŠ¹ç‡çš„ãªéƒ¨åˆ†æ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç›´æ¥å®Ÿè¡Œã§ãã¾ã™ã€‚
We also found that LLMs occasionally generate items that are not present in the candidate set. 
**ã¾ãŸã€LLMsã¯æ™‚æŠ˜ã€å€™è£œã‚»ãƒƒãƒˆã«å­˜åœ¨ã—ãªã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã‚ã‚‹ã“ã¨ã‚‚ã‚ã‹ã‚Šã¾ã—ãŸã€‚**
For GPT-3.5, such deviations occur in a mere 3% of cases. 
GPT-3.5ã®å ´åˆã€ãã®ã‚ˆã†ãªé€¸è„±ã¯ã‚ãšã‹3%ã®ã‚±ãƒ¼ã‚¹ã§ç™ºç”Ÿã—ã¾ã™ã€‚
One can either reprocess the illegal cases or simply treat the out-of-candidate items as incorrect recommendations. 
ä¸æ­£ãªã‚±ãƒ¼ã‚¹ã‚’å†å‡¦ç†ã™ã‚‹ã‹ã€å˜ã«å€™è£œå¤–ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ä¸æ­£ç¢ºãªæ¨è–¦ã¨ã—ã¦æ‰±ã†ã“ã¨ãŒã§ãã¾ã™ã€‚

![table1]()

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## 3 Empirical Studies å®Ÿè¨¼ç ”ç©¶

#### Datasets.

The experiments are conducted on two widely-used public datasets for recommender systems: (1) the movie rating dataset MovieLens-1M [24] (in short, ML-1M) where user ratings are regarded as interactions, and (2) one category from the Amazon Review dataset [46] named Games where reviews are regarded as interactions. 
å®Ÿé¨“ã¯ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ãŸã‚ã®2ã¤ã®åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å®Ÿæ–½ã•ã‚Œã¾ã™ï¼š(1) ãƒ¦ãƒ¼ã‚¶ã®è©•ä¾¡ãŒã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã¨è¦‹ãªã•ã‚Œã‚‹æ˜ ç”»è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ MovieLens-1M [24]ï¼ˆç•¥ã—ã¦ ML-1Mï¼‰ã€ãŠã‚ˆã³ (2) ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã¨è¦‹ãªã•ã‚Œã‚‹ Amazon Review ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ [46] ã®ã€ŒGamesã€ã¨ã„ã†ã‚«ãƒ†ã‚´ãƒªã§ã™ã€‚
We filter out users and items with fewer than five interactions. 
5å›æœªæº€ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŒã¤ãƒ¦ãƒ¼ã‚¶ã¨ã‚¢ã‚¤ãƒ†ãƒ ã¯é™¤å¤–ã—ã¾ã™ã€‚
Then we sort the interactions of each user by timestamp, with the oldest interactions first, to construct the corresponding historical interaction sequences. 
æ¬¡ã«ã€å„ãƒ¦ãƒ¼ã‚¶ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆã—ã€æœ€ã‚‚å¤ã„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’æœ€åˆã«ã—ã¦ã€å¯¾å¿œã™ã‚‹æ­´å²çš„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
The movie/product titles are used as the descriptive text of an item. 
**æ˜ ç”»/è£½å“ã®ã‚¿ã‚¤ãƒˆãƒ«ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ ã®èª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä½¿ç”¨**ã•ã‚Œã¾ã™ã€‚
We use item titles in this study for two reasons: (1) to determine if LLMs can make recommendations based on their intrinsic world knowledge with minimal information provided, and (2) to conserve computational resources. 
ã“ã®ç ”ç©¶ã§ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ ã‚¿ã‚¤ãƒˆãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ç†ç”±ã¯2ã¤ã‚ã‚Šã¾ã™ï¼š(1) LLMãŒæœ€å°é™ã®æƒ…å ±æä¾›ã§å†…åœ¨çš„ãªä¸–ç•ŒçŸ¥è­˜ã«åŸºã¥ã„ã¦æ¨è–¦ã‚’è¡Œãˆã‚‹ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã™ã‚‹ãŸã‚ã€(2) è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’ç¯€ç´„ã™ã‚‹ãŸã‚ã§ã™ã€‚
Exploring how LLMs use more extensive textual features for recommendations will be the focus of our future work. 
LLMãŒæ¨è–¦ã®ãŸã‚ã«ã‚ˆã‚Šåºƒç¯„ãªãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´ã‚’ã©ã®ã‚ˆã†ã«ä½¿ç”¨ã™ã‚‹ã‹ã‚’æ¢ã‚‹ã“ã¨ãŒã€ç§ãŸã¡ã®ä»Šå¾Œã®ç ”ç©¶ã®ç„¦ç‚¹ã¨ãªã‚Šã¾ã™ã€‚

#### Evaluation and implementation details. 

Following existing works [33,30], we apply the leave-one-out strategy for evaluation. 
è©•ä¾¡ã¨å®Ÿè£…ã®è©³ç´°ã€‚æ—¢å­˜ã®ç ”ç©¶ [33,30] ã«å¾“ã„ã€**(ã‚ªãƒ•ãƒ©ã‚¤ãƒ³)è©•ä¾¡ã®ãŸã‚ã« leave-one-out æˆ¦ç•¥ã‚’é©ç”¨**ã—ã¾ã™ã€‚
For each historical interaction sequence, the last item is used as the ground-truth item in test set. 
å„æ­´å²çš„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã—ã¦ã€æœ€å¾Œã®ã‚¢ã‚¤ãƒ†ãƒ ãŒãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®çœŸã®ã‚¢ã‚¤ãƒ†ãƒ ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
The item before the last one is used in the validation set (used for training baseline methods). 
æœ€å¾Œã®ã‚¢ã‚¤ãƒ†ãƒ ã®å‰ã®ã‚¢ã‚¤ãƒ†ãƒ ã¯ã€æ¤œè¨¼ã‚»ãƒƒãƒˆï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ‰‹æ³•ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ï¼‰ã§ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
We adopt the widely used metric NDCG@K (in short, N@K) to evaluate the ranking results over the given $m$ candidates, where $K \leq m$. 
ç§ãŸã¡ã¯ã€ä¸ãˆã‚‰ã‚ŒãŸ $m$ ã®å€™è£œã«å¯¾ã™ã‚‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹æŒ‡æ¨™ NDCG@Kï¼ˆç•¥ã—ã¦ N@Kï¼‰ã‚’æ¡ç”¨ã—ã¾ã™ã€‚ã“ã“ã§ã€$K \leq m$ ã§ã™ã€‚
To ease the reproduction of this work, our experiments are conducted using a popular open-source recommendation library RecBole [78]. 
ã“ã®ç ”ç©¶ã®å†ç¾ã‚’å®¹æ˜“ã«ã™ã‚‹ãŸã‚ã«ã€ç§ãŸã¡ã®å®Ÿé¨“ã¯äººæ°—ã®ã‚ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹æ¨è–¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª RecBole [78] ã‚’ä½¿ç”¨ã—ã¦å®Ÿæ–½ã•ã‚Œã¾ã™ã€‚
The historical interaction sequences are truncated within a length of 50. 
æ­´å²çš„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¯ã€é•·ã•50ä»¥å†…ã«åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã™ã€‚
We evaluate LLM-based methods on all users in ML-1M dataset and randomly sampled 6,000 users for Games dataset by default. 
ç§ãŸã¡ã¯ã€ML-1M ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã™ã¹ã¦ã®ãƒ¦ãƒ¼ã‚¶ã«å¯¾ã—ã¦ LLM ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã‚’è©•ä¾¡ã—ã€Games ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãŸã‚ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ 6,000 ãƒ¦ãƒ¼ã‚¶ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚
Unless specified, the evaluated LLM is accessed by calling OpenAIâ€™s API gpt-3.5-turbo. 
ç‰¹ã«æŒ‡å®šãŒãªã„é™ã‚Šã€è©•ä¾¡ã•ã‚ŒãŸ LLM ã¯ OpenAI ã® API gpt-3.5-turbo ã‚’å‘¼ã³å‡ºã™ã“ã¨ã§ã‚¢ã‚¯ã‚»ã‚¹ã•ã‚Œã¾ã™ã€‚
The hyperparameter temperature of calling LLMs is set to 0.2. 
LLM ã‚’å‘¼ã³å‡ºã™éš›ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¸©åº¦ã¯ 0.2 ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚
All the reported results are the average of at least three repeat runs to reduce the effect of randomness. 
å ±å‘Šã•ã‚ŒãŸã™ã¹ã¦ã®çµæœã¯ã€ãƒ©ãƒ³ãƒ€ãƒ æ€§ã®å½±éŸ¿ã‚’æ¸›ã‚‰ã™ãŸã‚ã«ã€å°‘ãªãã¨ã‚‚ 3 å›ã®ç¹°ã‚Šè¿”ã—å®Ÿè¡Œã®å¹³å‡ã§ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### 3.1 LLMã¯é€£ç¶šã—ãŸæ­´å²çš„ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç†è§£ã§ãã‚‹ã‹ï¼Ÿ

In LLM-based methods, historical interactions are naturally arranged in an ordered sequence.  
LLMãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã§ã¯ã€æ­´å²çš„ãªç›¸äº’ä½œç”¨ã¯è‡ªç„¶ã«é †åºä»˜ã‘ã‚‰ã‚ŒãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«é…ç½®ã•ã‚Œã¾ã™ã€‚
By designing different configurations of â„‹, we aim to examine whether LLMs can leverage these historical user behaviors and perceive the sequential nature for making accurate recommendations.  
$H$ ã®ç•°ãªã‚‹æ§‹æˆ(å‰è¿°ã•ã‚Œã¦ãŸ3ãƒ‘ã‚¿ãƒ¼ãƒ³ã ã£ã‘?)ã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ã§ã€LLMãŒã“ã‚Œã‚‰ã®æ­´å²çš„ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã‚’æ´»ç”¨ã—ã€æ­£ç¢ºãªæ¨è–¦ã‚’è¡Œã†ãŸã‚ã«ãã®é †åºæ€§ã‚’èªè­˜ã§ãã‚‹ã‹ã©ã†ã‹ã‚’æ¤œè¨¼ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚
LLMs struggle to perceive the order of given historical user behaviors.  
**LLMã¯ä¸ãˆã‚‰ã‚ŒãŸæ­´å²çš„ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã®é †åºã‚’èªè­˜ã™ã‚‹ã®ã«è‹¦åŠ´**ã—ã¦ã„ã¾ã™ã€‚
In this section, we examine whether LLMs can understand prompts with ordered historical interactions and give personalized recommendations.  
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€LLMãŒé †åºä»˜ã‘ã‚‰ã‚ŒãŸæ­´å²çš„ç›¸äº’ä½œç”¨ã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç†è§£ã—ã€å€‹åˆ¥åŒ–ã•ã‚ŒãŸæ¨è–¦ã‚’è¡Œã†ã“ã¨ãŒã§ãã‚‹ã‹ã©ã†ã‹ã‚’æ¤œè¨ã—ã¾ã™ã€‚
The task is to rank a candidate set of 20 items, containing one ground-truth item and 19 randomly sampled negatives.  
**ã‚¿ã‚¹ã‚¯ã¯ã€1ã¤ã®çœŸã®ã‚¢ã‚¤ãƒ†ãƒ ã¨19ã®ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒã‚¬ãƒ†ã‚£ãƒ–ã‚’å«ã‚€20ã‚¢ã‚¤ãƒ†ãƒ ã®å€™è£œã‚»ãƒƒãƒˆã‚’ãƒ©ãƒ³ã‚¯ä»˜ã‘ã™ã‚‹ã“ã¨**ã§ã™ã€‚
By analyzing historical behaviors, items of interest should be ranked at a higher position.  
æ­´å²çš„è¡Œå‹•ã‚’åˆ†æã™ã‚‹ã“ã¨ã§ã€é–¢å¿ƒã®ã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã¯ã‚ˆã‚Šé«˜ã„ä½ç½®ã«ãƒ©ãƒ³ã‚¯ä»˜ã‘ã•ã‚Œã‚‹ã¹ãã§ã™ã€‚
We compare the ranking results of three LLM-based methods: 
ç§ãŸã¡ã¯ã€3ã¤ã®LLMãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã®ãƒ©ãƒ³ã‚¯ä»˜ã‘çµæœã‚’æ¯”è¼ƒã—ã¾ã™ï¼š
(a) Ours, which ranks as we have described in Section 2.2.  Historical user behaviors are encoded into prompts using the â€œsequential promptingâ€ strategy.  
(a) ç§ãŸã¡ã®æ‰‹æ³•ã§ã€ã“ã‚Œã¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³2.2ã§èª¬æ˜ã—ãŸã‚ˆã†ã«ãƒ©ãƒ³ã‚¯ä»˜ã‘ã•ã‚Œã¾ã™ã€‚æ­´å²çš„ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã¯ã€Œã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã€æˆ¦ç•¥ã‚’ä½¿ç”¨ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ã€‚
(b) Random Order, where the historical user behaviors will be randomly shuffled before being fed to the model, and  
(b) ãƒ©ãƒ³ãƒ€ãƒ ã‚ªãƒ¼ãƒ€ãƒ¼ã§ã¯ã€æ­´å²çš„ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ãŒãƒ¢ãƒ‡ãƒ«ã«ä¾›çµ¦ã•ã‚Œã‚‹å‰ã«ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã•ã‚Œã¾ã™ã€‚
(c) Fake History, where we replace all the items in original historical behaviors with randomly sampled items as fake historical behaviors.  
(c) ãƒ•ã‚§ã‚¤ã‚¯ãƒ’ã‚¹ãƒˆãƒªãƒ¼ã§ã¯ã€å…ƒã®æ­´å²çš„è¡Œå‹•ã®ã™ã¹ã¦ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã§ç½®ãæ›ãˆã€ãƒ•ã‚§ã‚¤ã‚¯ã®æ­´å²çš„è¡Œå‹•ã¨ã—ã¾ã™ã€‚
From Figure 2(a), we can see that Ours has better performance than variants with fake historical behaviors.  
å›³2(a)ã‹ã‚‰ã€ç§ãŸã¡ã®æ‰‹æ³•ã¯ãƒ•ã‚§ã‚¤ã‚¯ã®æ­´å²çš„è¡Œå‹•ã‚’æŒã¤ãƒãƒªã‚¢ãƒ³ãƒˆã‚ˆã‚Šã‚‚å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚
However, the performance of Ours and Random Order is similar, indicating that LLMs are not sensitive to the order of the given historical user interactions.  
ã—ã‹ã—ã€ç§ãŸã¡ã®æ‰‹æ³•ã¨ãƒ©ãƒ³ãƒ€ãƒ ã‚ªãƒ¼ãƒ€ãƒ¼ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ä¼¼ã¦ãŠã‚Šã€LLMãŒä¸ãˆã‚‰ã‚ŒãŸæ­´å²çš„ãƒ¦ãƒ¼ã‚¶ç›¸äº’ä½œç”¨ã®é †åºã«æ•æ„Ÿã§ãªã„ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

![figure2]()

Moreover, in Figure 2(b), we vary the number of latest historical user behaviors (|â„‹|) used for constructing the prompt from 5 to 50.  
ã•ã‚‰ã«ã€å›³2(b)ã§ã¯ã€**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹æœ€æ–°ã®æ­´å²çš„ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã®æ•°ï¼ˆ|â„‹|ï¼‰ã‚’5ã‹ã‚‰50ã«å¤‰åŒ–**ã•ã›ã¾ã™ã€‚
The results show that increasing the number of historical user behaviors does not improve, but rather negatively impacts the ranking performance.  
**çµæœã¯ã€æ­´å²çš„ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã®æ•°ã‚’å¢—ã‚„ã™ã“ã¨ãŒæ”¹å–„ã«ã¯ã¤ãªãŒã‚‰ãšã€ã‚€ã—ã‚ãƒ©ãƒ³ã‚¯ä»˜ã‘ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«æ‚ªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™**ã€‚
We speculate that this phenomenon is caused by the fact that LLMs have difficulty understanding the order, but consider all the historical behaviors equally.  
ç§ãŸã¡ã¯ã€ã“ã®ç¾è±¡ãŒLLMãŒé †åºã‚’ç†è§£ã™ã‚‹ã®ã«è‹¦åŠ´ã—ã€ã™ã¹ã¦ã®æ­´å²çš„è¡Œå‹•ã‚’åŒç­‰ã«è€ƒæ…®ã™ã‚‹ãŸã‚ã«èµ·ã“ã‚‹ã¨æ¨æ¸¬ã—ã¦ã„ã¾ã™ã€‚
Therefore too many historical user behaviors (e.g., |â„‹| = 50) may overwhelm LLMs and lead to a performance drop.  
ã—ãŸãŒã£ã¦ã€ã‚ã¾ã‚Šã«ã‚‚å¤šãã®æ­´å²çš„ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ï¼ˆä¾‹ï¼š|H| = 50ï¼‰ã¯LLMã‚’åœ§å€’ã—ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ä½ä¸‹ã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
In contrast, a relatively small |â„‹| enables LLMs to concentrate on the most recently interacted items, resulting in better recommendation performance.  
**å¯¾ç…§çš„ã«ã€æ¯”è¼ƒçš„å°ã•ãª|H|ã¯LLMãŒæœ€ã‚‚æœ€è¿‘ç›¸äº’ä½œç”¨ã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã«é›†ä¸­ã§ãã‚‹ã‚ˆã†ã«ã—ã€ã‚ˆã‚Šè‰¯ã„æ¨è–¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ã‚‚ãŸã‚‰ã—ã¾ã™**ã€‚


**Triggering LLMs to perceive the interaction order.**
ç›¸äº’ä½œç”¨ã®é †åºã‚’èªè­˜ã™ã‚‹ã‚ˆã†ã«LLMã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ã€‚
Based on the above observations, we find it difficult for LLMs to perceive the order in interaction histories by a default prompting strategy.  
ä¸Šè¨˜ã®è¦³å¯Ÿã«åŸºã¥ã„ã¦ã€ç§ãŸã¡ã¯LLMãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥ã«ã‚ˆã£ã¦ç›¸äº’ä½œç”¨ã®æ­´å²ã«ãŠã‘ã‚‹é †åºã‚’èªè­˜ã™ã‚‹ã®ãŒé›£ã—ã„ã“ã¨ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚
As a result, we aim to elicit the order-perceiving abilities of LLMs, by proposing two alternative prompting strategies and emphasizing the recently interacted items.  
ãã®çµæœã€ç§ãŸã¡ã¯LLMã®é †åºèªè­˜èƒ½åŠ›ã‚’å¼•ãå‡ºã™ã“ã¨ã‚’ç›®æŒ‡ã—ã€2ã¤ã®ä»£æ›¿ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥ã‚’ææ¡ˆã—ã€æœ€è¿‘ç›¸äº’ä½œç”¨ã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’å¼·èª¿ã—ã¾ã™ã€‚
Detailed descriptions of the proposed strategies have been given in Section 2.2.  
ææ¡ˆã•ã‚ŒãŸæˆ¦ç•¥ã®è©³ç´°ãªèª¬æ˜ã¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³2.2ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚
In Table 2, we can see that both recency-focused prompting and in-context learning can generally improve the ranking performance of LLMs, though the best strategy may vary on different datasets.  
è¡¨2ã‹ã‚‰ã€æœ€è¿‘é‡è¦–ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã¨ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ã®ä¸¡æ–¹ãŒä¸€èˆ¬çš„ã«LLMã®ãƒ©ãƒ³ã‚¯ä»˜ã‘ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ”¹å–„ã§ãã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ãŒã€æœ€è‰¯ã®æˆ¦ç•¥ã¯ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
The above results can be summarized as the following key observation:  
ä¸Šè¨˜ã®çµæœã¯ã€ä»¥ä¸‹ã®é‡è¦ãªè¦³å¯Ÿã¨ã—ã¦è¦ç´„ã§ãã¾ã™ï¼š

---

**Observation 1**. LLMs struggle to perceive the order of the given sequential interaction histories.  
è¦³å¯Ÿ1. LLMã¯ä¸ãˆã‚‰ã‚ŒãŸé€£ç¶šçš„ãªç›¸äº’ä½œç”¨ã®æ­´å²ã®é †åºã‚’èªè­˜ã™ã‚‹ã®ã«è‹¦åŠ´ã—ã¦ã„ã¾ã™ã€‚
By employing specifically designed promptings, LLMs can be triggered to perceive the order of historical user behaviors, leading to improved ranking performance.  
ç‰¹åˆ¥ã«è¨­è¨ˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€LLMã¯æ­´å²çš„ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã®é †åºã‚’èªè­˜ã™ã‚‹ã‚ˆã†ã«ä¿ƒã•ã‚Œã€ãƒ©ãƒ³ã‚¯ä»˜ã‘ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Šã—ã¾ã™ã€‚

![table2]()

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### 3.2 LLMã¯ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä¸­ã«ãƒã‚¤ã‚¢ã‚¹ã®å½±éŸ¿ã‚’å—ã‘ã‚‹ã‹ï¼Ÿ

The biases and debiasing methods in conventional recommender systems have been widely studied[5].  
å¾“æ¥ã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹ãƒã‚¤ã‚¢ã‚¹ã¨ãƒ‡ãƒã‚¤ã‚¢ã‚¹æ‰‹æ³•ã¯åºƒãç ”ç©¶ã•ã‚Œã¦ã„ã¾ã™[5]ã€‚
For LLM-based recommendation models, both the input and output are natural language texts and will inevitably introduce new biases.  
**LLMãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€å…¥åŠ›ã¨å‡ºåŠ›ã®ä¸¡æ–¹ãŒè‡ªç„¶è¨€èªãƒ†ã‚­ã‚¹ãƒˆã§ã‚ã‚Šã€å¿…ç„¶çš„ã«æ–°ãŸãªãƒã‚¤ã‚¢ã‚¹ã‚’å°å…¥ã—ã¾ã™**ã€‚
In this section, we discuss two kinds of biases that LLM-based recommendation models suffer from.  
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€**LLMãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ãƒ¢ãƒ‡ãƒ«ãŒç›´é¢ã™ã‚‹2ç¨®é¡ã®ãƒã‚¤ã‚¢ã‚¹**ã«ã¤ã„ã¦è­°è«–ã—ã¾ã™ã€‚
We also make discussions on how to alleviate these biases.  
ã¾ãŸã€ã“ã‚Œã‚‰ã®ãƒã‚¤ã‚¢ã‚¹ã‚’è»½æ¸›ã™ã‚‹æ–¹æ³•ã«ã¤ã„ã¦ã‚‚è­°è«–ã—ã¾ã™ã€‚

#### The order of candidates affects the ranking results of LLMs. å€™è£œã®é †åºã¯ã€LLMã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœã«å½±éŸ¿ã‚’ä¸ãˆã¾ã™ã€‚

For conventional ranking methods, the order of retrieved candidates usually will not affect the ranking results[33,28].  
å¾“æ¥ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ‰‹æ³•ã§ã¯ã€å–å¾—ã•ã‚ŒãŸå€™è£œã®é †åºã¯é€šå¸¸ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœã«å½±éŸ¿ã‚’ä¸ãˆã¾ã›ã‚“[33,28]ã€‚
However, for the LLM-based approach that is described in Section 2.2, the candidates are arranged in a sequential manner and infilled into a prompt.  
ã—ã‹ã—ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³2.2ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹LLMãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã¯ã€å€™è£œãŒé †æ¬¡é…ç½®ã•ã‚Œã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸ‹ã‚è¾¼ã¾ã‚Œã¾ã™ã€‚
It has been shown that LLMs are generally sensitive to the order of examples in the prompts for NLP tasks[80,44].  
**LLMã¯ã€NLPã‚¿ã‚¹ã‚¯ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®ä¾‹ã®é †åºã«ä¸€èˆ¬çš„ã«æ•æ„Ÿã§ã‚ã‚‹ã“ã¨**ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™[80,44]ã€‚
As a result, we also conduct experiments to examine whether the order of candidates affects the ranking performance of LLMs.  
ãã®çµæœã€å€™è£œã®é †åºãŒLLMã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ã©ã†ã‹ã‚’èª¿ã¹ã‚‹å®Ÿé¨“ã‚‚è¡Œã„ã¾ã™ã€‚
We follow the experimental settings adopted in Section 3.1.  
ç§ãŸã¡ã¯ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³3.1ã§æ¡ç”¨ã•ã‚ŒãŸå®Ÿé¨“è¨­å®šã«å¾“ã„ã¾ã™ã€‚
The only difference is that we control the order of these candidates in the prompts, by making the ground-truth items appear at a certain position.  
**å”¯ä¸€ã®é•ã„ã¯ã€ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ãŒç‰¹å®šã®ä½ç½®ã«ç¾ã‚Œã‚‹ã‚ˆã†ã«ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®ã“ã‚Œã‚‰ã®å€™è£œã®é †åºã‚’åˆ¶å¾¡ã™ã‚‹ã“ã¨**ã§ã™ã€‚(ã“ã‚Œã¯ã‚ãã¾ã§å®Ÿé¨“ã¨ã—ã¦ã£ã¦ã“ã¨ã‹...! å®Ÿé‹ç”¨ã§ã¯ground-truthã¯æœªçŸ¥ãªã®ã§ã“ã®æ‰‹æ³•ã¯å–ã‚Œãªã„:thinking:)
We vary the position of ground-truth items at {0,5,10,15,19} and present the results in Figure 3(a).  
ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ã®ä½ç½®ã‚’{0,5,10,15,19}ã§å¤‰åŒ–ã•ã›ã€çµæœã‚’å›³3(a)ã«ç¤ºã—ã¾ã™ã€‚
We can see that the performance varies when the ground-truth items appear at different positions.  
ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ãŒç•°ãªã‚‹ä½ç½®ã«ç¾ã‚Œã‚‹ã¨ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå¤‰åŒ–ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚
Especially, the ranking performance drops significantly when the ground-truth items appear at the last few positions.  
**ç‰¹ã«ã€ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ãŒæœ€å¾Œã®æ•°ä½ç½®ã«ç¾ã‚Œã‚‹ã¨ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ãŒå¤§å¹…ã«ä½ä¸‹ã—ã¾ã™ã€‚**
The results indicate that LLM-based rankers are affected by the order of candidates, i.e., position bias, which may not affect conventional recommendation models.  
çµæœã¯ã€LLMãƒ™ãƒ¼ã‚¹ã®ãƒ©ãƒ³ã‚«ãƒ¼ãŒå€™è£œã®é †åºã€ã™ãªã‚ã¡ä½ç½®ãƒã‚¤ã‚¢ã‚¹ã®å½±éŸ¿ã‚’å—ã‘ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ãŠã‚Šã€ã“ã‚Œã¯å¾“æ¥ã®æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã«ã¯å½±éŸ¿ã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
The order of candidates affects the ranking results of LLMs.  
å€™è£œã®é †åºã¯ã€LLMã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœã«å½±éŸ¿ã‚’ä¸ãˆã¾ã™ã€‚

#### Alleviating position bias by bootstrapping.  ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã«ã‚ˆã‚‹ä½ç½®ãƒã‚¤ã‚¢ã‚¹ã®è»½æ¸›ã€‚

A simple strategy to alleviate position bias is to bootstrap the ranking process.  
ä½ç½®ãƒã‚¤ã‚¢ã‚¹ã‚’è»½æ¸›ã™ã‚‹ãŸã‚ã®ç°¡å˜ãªæˆ¦ç•¥ã¯ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã‚’ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã™ã‚‹ã“ã¨ã§ã™ã€‚
We may rank the candidate set repeatedly for $B$ times, with candidates randomly shuffled at each round.  
å€™è£œã‚»ãƒƒãƒˆã‚’$B$å›ç¹°ã‚Šè¿”ã—ãƒ©ãƒ³ã‚¯ä»˜ã‘ã—ã€å„ãƒ©ã‚¦ãƒ³ãƒ‰ã§å€™è£œã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
In this way, one candidate may appear in different positions.  
ã“ã®æ–¹æ³•ã§ã¯ã€1ã¤ã®å€™è£œãŒç•°ãªã‚‹ä½ç½®ã«ç¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
We then merge the results of each round to derive the final ranking.  
æ¬¡ã«ã€å„ãƒ©ã‚¦ãƒ³ãƒ‰ã®çµæœã‚’çµ±åˆã—ã¦æœ€çµ‚çš„ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å°ãå‡ºã—ã¾ã™ã€‚
From Figure 3(b), we follow the setting in Section 3.1 and apply the bootstrapping strategy to Ours.  
å›³3(b)ã‹ã‚‰ã€ç§ãŸã¡ã¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³3.1ã®è¨­å®šã«å¾“ã„ã€ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—æˆ¦ç•¥ã‚’ç§ãŸã¡ã®ãƒ¢ãƒ‡ãƒ«ã«é©ç”¨ã—ã¾ã™ã€‚
Each candidate set will be ranked for 3 times.  
å„å€™è£œã‚»ãƒƒãƒˆã¯3å›ãƒ©ãƒ³ã‚¯ä»˜ã‘ã•ã‚Œã¾ã™ã€‚
We can see that bootstrapping improves the ranking performance on both datasets.  
ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ”ãƒ³ã‚°ãŒä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

#### Popularity degrees of candidates affect ranking results of LLMs. å€™è£œã®äººæ°—åº¦ã¯ã€LLMã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœã«å½±éŸ¿ã‚’ä¸ãˆã¾ã™ã€‚

For popular items, the associated text may also appear frequently in the pre-training corpora of LLMs.
**äººæ°—ã®ã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®å ´åˆã€é–¢é€£ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯LLMã®äº‹å‰å­¦ç¿’ã‚³ãƒ¼ãƒ‘ã‚¹ã«é »ç¹ã«ç¾ã‚Œã‚‹å¯èƒ½æ€§**ãŒã‚ã‚Šã¾ã™ã€‚
For example, a best-selling book would be widely discussed on the Web.  
ä¾‹ãˆã°ã€ãƒ™ã‚¹ãƒˆã‚»ãƒ©ãƒ¼ã®æœ¬ã¯Webä¸Šã§åºƒãè­°è«–ã•ã‚Œã‚‹ã§ã—ã‚‡ã†ã€‚
Thus, we aim to examine whether the ranking results are affected by the popularity of candidates.  
ã—ãŸãŒã£ã¦ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœãŒå€™è£œã®äººæ°—ã«ã‚ˆã£ã¦å½±éŸ¿ã‚’å—ã‘ã‚‹ã‹ã©ã†ã‹ã‚’èª¿ã¹ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚
However, it is difficult to directly measure the popularity of item text.  
ã—ã‹ã—ã€ã‚¢ã‚¤ãƒ†ãƒ ãƒ†ã‚­ã‚¹ãƒˆã®äººæ°—ã‚’ç›´æ¥æ¸¬å®šã™ã‚‹ã“ã¨ã¯é›£ã—ã„ã§ã™ã€‚
Here, we hypothesize that the text popularity can be indirectly measured by item frequency in one recommendation dataset.  
ã“ã“ã§ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã®äººæ°—ã¯1ã¤ã®æ¨è–¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã‘ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®é »åº¦ã«ã‚ˆã£ã¦é–“æ¥çš„ã«æ¸¬å®šã§ãã‚‹ã¨ä»®å®šã—ã¾ã™ã€‚
In Figure 3(c), we report the item popularity score (measured by the normalized item frequency of appearance in the training set) at each position of the ranked item lists.  
å›³3(c)ã§ã¯ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆã®å„ä½ç½®ã«ãŠã‘ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®äººæ°—ã‚¹ã‚³ã‚¢ï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã«ãŠã‘ã‚‹å‡ºç¾é »åº¦ã®æ­£è¦åŒ–ã«ã‚ˆã£ã¦æ¸¬å®šï¼‰ã‚’å ±å‘Šã—ã¾ã™ã€‚
We can see that popular items tend to be ranked at higher positions.  
**äººæ°—ã®ã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã¯ã€ã‚ˆã‚Šé«˜ã„ä½ç½®ã«ãƒ©ãƒ³ã‚¯ä»˜ã‘ã•ã‚Œã‚‹å‚¾å‘ãŒã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™**ã€‚

#### Making LLMs focus on historical interactions helps reduce popularity bias. LLMãŒæ­´å²çš„ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ã“ã¨ã§ã€äººæ°—ãƒã‚¤ã‚¢ã‚¹ã‚’è»½æ¸›ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚

We assume that if LLMs focus on historical interactions, they may give more personalized recommendations but not more popular ones.  
LLMãŒæ­´å²çš„ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹å ´åˆã€ã‚ˆã‚Šãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸæ¨è–¦ã‚’è¡Œã†ãŒã€ã‚ˆã‚Šäººæ°—ã®ã‚ã‚‹ã‚‚ã®ã§ã¯ãªã„ã¨ä»®å®šã—ã¾ã™ã€‚
From Figure 2(b), we know that LLMs make better use of historical interactions when using less historical interactions.  
å›³2(b)ã‹ã‚‰ã€LLMã¯å°‘ãªã„æ­´å²çš„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹éš›ã«ã€æ­´å²çš„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚ˆã‚Šè‰¯ãæ´»ç”¨ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚
From Figure 3(d), we compare the popularity scores of the best-ranked items varying the number of historical interactions.  
å›³3(d)ã‹ã‚‰ã€æ­´å²çš„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã®æ•°ã‚’å¤‰åŒ–ã•ã›ãŸæœ€ã‚‚é«˜ããƒ©ãƒ³ã‚¯ä»˜ã‘ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã®äººæ°—ã‚¹ã‚³ã‚¢ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚
It can be observed that as $|H|$ decreases, the popularity score decreases as well.  
$|H|$ãŒæ¸›å°‘ã™ã‚‹ã«ã¤ã‚Œã¦ã€äººæ°—ã‚¹ã‚³ã‚¢ã‚‚æ¸›å°‘ã™ã‚‹ã“ã¨ãŒè¦³å¯Ÿã§ãã¾ã™ã€‚
This suggests that one can reduce the effects of popularity bias when LLMs focus more on historical interactions.  
**ã“ã‚Œã¯ã€LLMãŒæ­´å²çš„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šç„¦ç‚¹ã‚’å½“ã¦ã‚‹ã¨ã€äººæ°—ãƒã‚¤ã‚¢ã‚¹ã®å½±éŸ¿ã‚’è»½æ¸›ã§ãã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚**
From the above experiments, we can conclude the following:  
ä¸Šè¨˜ã®å®Ÿé¨“ã‹ã‚‰ã€æ¬¡ã®ã“ã¨ã‚’çµè«–ã§ãã¾ã™ï¼š

---

**Observation 2.** LLMs suffer from position bias and popularity bias while ranking, which can be alleviated by bootstrapping or specially designed prompting strategies.  
è¦³å¯Ÿ2. LLMã¯ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä¸­ã«ä½ç½®ãƒã‚¤ã‚¢ã‚¹ã¨äººæ°—ãƒã‚¤ã‚¢ã‚¹ã®å½±éŸ¿ã‚’å—ã‘ã€ã“ã‚Œã¯ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ”ãƒ³ã‚°ã‚„ç‰¹åˆ¥ã«è¨­è¨ˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæˆ¦ç•¥ã«ã‚ˆã£ã¦è»½æ¸›ã§ãã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### 3.3 LLMã¯ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆè¨­å®šã§å€™è£œã‚’ã©ã‚Œã ã‘ã†ã¾ããƒ©ãƒ³ã‚¯ä»˜ã‘ã§ãã‚‹ã‹ï¼Ÿ

We further evaluate LLM-based methods on candidates with hard negatives that are retrieved by different strategies to further investigate what the ranking of LLMs depends on.
ç§ãŸã¡ã¯ã€LLMãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã‚’ã€ç•°ãªã‚‹æˆ¦ç•¥ã§å–å¾—ã•ã‚ŒãŸãƒãƒ¼ãƒ‰ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚’æŒã¤å€™è£œã«å¯¾ã—ã¦ã•ã‚‰ã«è©•ä¾¡ã—ã€LLMã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãŒä½•ã«ä¾å­˜ã—ã¦ã„ã‚‹ã®ã‹ã‚’èª¿æŸ»ã—ã¾ã™ã€‚
Then, we present the ranking performance of different methods on candidates retrieved by multiple candidate generation models to simulate a more practical and difficult setting.
æ¬¡ã«ã€ã‚ˆã‚Šå®Ÿè·µçš„ã§å›°é›£ãªè¨­å®šã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ãŸã‚ã«ã€è¤‡æ•°ã®å€™è£œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å–å¾—ã•ã‚ŒãŸå€™è£œã«å¯¾ã™ã‚‹ç•°ãªã‚‹æ‰‹æ³•ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ã‚’ç¤ºã—ã¾ã™ã€‚


#### LLMs have promising zero-shot ranking abilities.LLMã¯æœ‰æœ›ãªã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãƒ©ãƒ³ã‚­ãƒ³ã‚°èƒ½åŠ›ã‚’æŒã£ã¦ã„ã¾ã™ã€‚

In Table 2, we conduct experiments to compare the ranking abilities of LLM-based methods with existing methods.
è¡¨2ã§ã¯ã€LLMãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°èƒ½åŠ›ã‚’æ—¢å­˜ã®æ‰‹æ³•ã¨æ¯”è¼ƒã™ã‚‹å®Ÿé¨“ã‚’è¡Œã„ã¾ã™ã€‚
We follow the same setting in Section 3.1 where $|\mathcal{C}|=20$ and candidate items are randomly retrieved.
ç§ãŸã¡ã¯ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³3.1ã¨åŒã˜è¨­å®šã«å¾“ã„ã€$|\mathcal{C}|=20$ã§å€™è£œã‚¢ã‚¤ãƒ†ãƒ ãŒãƒ©ãƒ³ãƒ€ãƒ ã«å–å¾—ã•ã‚Œã¾ã™ã€‚
We include three conventional models that are trained on the training set, i.e., Pop (recommending according to item popularity), BPRMF[49], and SASRec[33].
ç§ãŸã¡ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã§è¨“ç·´ã•ã‚ŒãŸ3ã¤ã®å¾“æ¥ã®ãƒ¢ãƒ‡ãƒ«ã€ã™ãªã‚ã¡ã€ã‚¢ã‚¤ãƒ†ãƒ ã®äººæ°—ã«åŸºã¥ã„ã¦æ¨è–¦ã™ã‚‹Popã€BPRMF[49]ã€ãŠã‚ˆã³SASRec[33]ã‚’å«ã‚ã¾ã™ã€‚
We also evaluate three zero-shot recommendation methods that are not trained on the target datasets, including BM25[50] (rank according to the textual similarity between candidates and historical interactions), UniSRec[30], and VQ-Rec[29].
ã¾ãŸã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¨“ç·´ã•ã‚Œã¦ã„ãªã„3ã¤ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ¨è–¦æ‰‹æ³•ã€ã™ãªã‚ã¡ã€BM25[50]ï¼ˆå€™è£œã¨æ­´å²çš„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³é–“ã®ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼æ€§ã«åŸºã¥ã„ã¦ãƒ©ãƒ³ã‚¯ä»˜ã‘ï¼‰ã€UniSRec[30]ã€ãŠã‚ˆã³VQ-Rec[29]ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
For UniSRec and VQ-Rec, we use their publicly available pre-trained models.
UniSRecã¨VQ-Recã«ã¤ã„ã¦ã¯ã€å…¬é–‹ã•ã‚Œã¦ã„ã‚‹äº‹å‰è¨“ç·´ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
We do not include ZESRec[15] because there is no pre-trained model released.
ZESRec[15]ã¯ã€ãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸäº‹å‰è¨“ç·´ãƒ¢ãƒ‡ãƒ«ãŒãªã„ãŸã‚ã€å«ã‚ã¾ã›ã‚“ã€‚
In addition, we compare the zero-shot ranking performance of different LLMs in Table 3.
ã•ã‚‰ã«ã€è¡¨3ã§ã¯ã€ç•°ãªã‚‹LLMã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚
â€œRecency-Focusedâ€ prompting strategy is used for LLM-based rankers.
LLMãƒ™ãƒ¼ã‚¹ã®ãƒ©ãƒ³ã‚«ãƒ¼ã«ã¯ã€Œæœ€è¿‘é‡è¦–ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

From Table 2 and 3, we can see that LLMs with more parameters generally perform better.
**è¡¨2ã¨3ã‹ã‚‰ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¤šã„LLMã¯ä¸€èˆ¬çš„ã«ã‚ˆã‚Šè‰¯ã„æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚**
The best LLM-based methods outperform existing zero-shot recommendation methods by a large margin, showing promising zero-shot ranking abilities.
æœ€è‰¯ã®LLMãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã¯ã€æ—¢å­˜ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ¨è–¦æ‰‹æ³•ã‚’å¤§ããä¸Šå›ã‚Šã€æœ‰æœ›ãªã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãƒ©ãƒ³ã‚­ãƒ³ã‚°èƒ½åŠ›ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
We would highlight that it is difficult to conduct zero-shot recommendations on the ML-1M dataset, due to the difficulty in measuring the similarity between movies merely by the similarity of their titles.
**ML-1Mãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ¨è–¦ã‚’è¡Œã†ã“ã¨ã¯ã€æ˜ ç”»ã®ã‚¿ã‚¤ãƒˆãƒ«ã®é¡ä¼¼æ€§ã ã‘ã§æ˜ ç”»é–“ã®é¡ä¼¼æ€§ã‚’æ¸¬å®šã™ã‚‹ã“ã¨ãŒé›£ã—ã„ãŸã‚ã€å›°é›£ã§ã‚ã‚‹ã“ã¨ã‚’å¼·èª¿ã—ãŸã„ã¨æ€ã„ã¾ã™ã€‚**
However, LLMs can use their intrinsic knowledge to measure the similarity between movies and make recommendations.
**ã—ã‹ã—ã€LLMã¯ãã®å†…åœ¨çš„ãªçŸ¥è­˜ã‚’ä½¿ç”¨ã—ã¦æ˜ ç”»é–“ã®é¡ä¼¼æ€§ã‚’æ¸¬å®šã—ã€æ¨è–¦ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚**
We would emphasize that the goal of evaluating zero-shot recommendation methods is not to surpass conventional models.
ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ¨è–¦æ‰‹æ³•ã‚’è©•ä¾¡ã™ã‚‹ç›®çš„ã¯ã€å¾“æ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã“ã¨ã§ã¯ãªã„ã“ã¨ã‚’å¼·èª¿ã—ãŸã„ã¨æ€ã„ã¾ã™ã€‚
The goal is to demonstrate the strong recommendation capabilities of pre-trained base models, which can be further adapted and transferred to downstream scenarios.
ç›®æ¨™ã¯ã€äº‹å‰è¨“ç·´ã•ã‚ŒãŸãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å¼·åŠ›ãªæ¨è–¦èƒ½åŠ›ã‚’ç¤ºã™ã“ã¨ã§ã‚ã‚Šã€ã“ã‚Œã‚‰ã¯ã•ã‚‰ã«é©å¿œã•ã‚Œã€ä¸‹æµã®ã‚·ãƒŠãƒªã‚ªã«è»¢é€ã•ã‚Œã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

#### LLMs rank candidates based on item popularity, text features as well as user behaviors. LLMã¯ã‚¢ã‚¤ãƒ†ãƒ ã®äººæ°—ã€ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã«åŸºã¥ã„ã¦å€™è£œã‚’ãƒ©ãƒ³ã‚¯ä»˜ã‘ã—ã¾ã™ã€‚

To further investigate how LLMs rank the given candidates, we evaluate LLMs on candidates that are retrieved by different candidate generation methods.
LLMãŒä¸ãˆã‚‰ã‚ŒãŸå€™è£œã‚’ã©ã®ã‚ˆã†ã«ãƒ©ãƒ³ã‚¯ä»˜ã‘ã™ã‚‹ã‹ã‚’ã•ã‚‰ã«èª¿æŸ»ã™ã‚‹ãŸã‚ã«ã€ç•°ãªã‚‹å€™è£œç”Ÿæˆæ‰‹æ³•ã§å–å¾—ã•ã‚ŒãŸå€™è£œã«å¯¾ã—ã¦LLMã‚’è©•ä¾¡ã—ã¾ã™ã€‚
These candidates can be viewed as hard negatives for ground-truth items, which can be used to measure the ranking ability of LLMs for specific categories of items.
ã“ã‚Œã‚‰ã®å€™è£œã¯ã€çœŸã®ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã™ã‚‹ãƒãƒ¼ãƒ‰ãƒã‚¬ãƒ†ã‚£ãƒ–ã¨è¦‹ãªã™ã“ã¨ãŒã§ãã€ç‰¹å®šã®ã‚«ãƒ†ã‚´ãƒªã®ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã™ã‚‹LLMã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°èƒ½åŠ›ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã¾ã™ã€‚
We consider two categories of strategies to retrieve the candidates:
**å€™è£œã‚’å–å¾—ã™ã‚‹ãŸã‚ã®2ã¤ã®æˆ¦ç•¥ã‚«ãƒ†ã‚´ãƒª**ã‚’è€ƒæ…®ã—ã¾ã™ï¼š

(1) content-based methods like BM25[50] and BERT[14] retrieve candidates based on the text feature similarities, and
**(1) BM25[50]ã‚„BERT[14]ã®ã‚ˆã†ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´ã®é¡ä¼¼æ€§ã«åŸºã¥ã„ã¦å€™è£œã‚’å–å¾—ã—ã¾ã™**ã€‚(ä»Šã®ä¿ºãŸã¡ã˜ã‚ƒã‚“! :thinking:)
(2) interaction-based methods, including Pop, BPRMF[49], GRU4Rec[28], and SASRec[33], retrieve items using neural networks trained on user-item interactions.
(2) Popã€BPRMF[49]ã€GRU4Rec[28]ã€ãŠã‚ˆã³SASRec[33]ã‚’å«ã‚€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼-ã‚¢ã‚¤ãƒ†ãƒ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã§è¨“ç·´ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—ã—ã¾ã™ã€‚(ã“ã£ã¡ã¯CFã£ã¦ã„ã†ã‹id-onlyã®æ‰‹æ³•ã‹...!:thinking:)
Given candidates, we compare the ranking performance of the LLM-based model (Ours) and representative methods.
ä¸ãˆã‚‰ã‚ŒãŸå€™è£œã«å¯¾ã—ã¦ã€LLMãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆç§ãŸã¡ã®ãƒ¢ãƒ‡ãƒ«ï¼‰ã¨ä»£è¡¨çš„ãªæ‰‹æ³•ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚

From Figure 4, we can see that the ranking performance of the LLM-based method varies on different candidate sets and different datasets.
å›³4ã‹ã‚‰ã€LLMãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ã¯ã€ç•°ãªã‚‹å€™è£œã‚»ãƒƒãƒˆã‚„ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ç•°ãªã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚
(1) On ML-1M, LLM-based method cannot rank well on candidate sets that contain popular items (e.g., Pop and BPRMF), indicating the LLM-based method recommend items largely depend on item popularity on ML-1M dataset.
(1) ML-1Mã§ã¯ã€LLMãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã¯äººæ°—ã‚¢ã‚¤ãƒ†ãƒ ã‚’å«ã‚€å€™è£œã‚»ãƒƒãƒˆã§ã†ã¾ããƒ©ãƒ³ã‚¯ä»˜ã‘ã§ããšï¼ˆä¾‹ï¼šPopã‚„BPRMFï¼‰ã€**LLMãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ãŒML-1Mãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚¢ã‚¤ãƒ†ãƒ ã®äººæ°—ã«å¤§ããä¾å­˜ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚**
(2) On Games, we can observe that Ours has similar performance both on popular candidates and textual similar candidates, showing that item popularity and text features contribute similarly to the ranking of LLMs.
(2) Gamesã§ã¯ã€ç§ãŸã¡ã®ãƒ¢ãƒ‡ãƒ«ãŒäººæ°—ã®ã‚ã‚‹å€™è£œã¨ãƒ†ã‚­ã‚¹ãƒˆçš„ã«é¡ä¼¼ã—ãŸå€™è£œã®ä¸¡æ–¹ã§åŒæ§˜ã®æ€§èƒ½ã‚’ç¤ºã—ã¦ãŠã‚Šã€ã‚¢ã‚¤ãƒ†ãƒ ã®äººæ°—ã¨ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´ãŒLLMã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«åŒæ§˜ã«å¯„ä¸ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚
(3) On both two datasets, the performance of Ours is affected by hard negatives retrieved by interaction-based candidate generation models, but not as severe as those interaction-based rankers like SASRec.
(3) ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€ç§ãŸã¡ã®ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ã®å€™è£œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å–å¾—ã•ã‚ŒãŸãƒãƒ¼ãƒ‰ãƒã‚¬ãƒ†ã‚£ãƒ–ã®å½±éŸ¿ã‚’å—ã‘ã¾ã™ãŒã€SASRecã®ã‚ˆã†ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ã®ãƒ©ãƒ³ã‚«ãƒ¼ã»ã©æ·±åˆ»ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
The above results demonstrate that LLM-based methods not only consider one single aspect for ranking, but make use of item popularity, text features, and even user behaviors.
**ä¸Šè¨˜ã®çµæœã¯ã€LLMãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ãŒãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ãŸã‚ã«å˜ä¸€ã®å´é¢ã ã‘ã§ãªãã€ã‚¢ã‚¤ãƒ†ãƒ ã®äººæ°—ã€ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´ã€ã•ã‚‰ã«ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚**
On different datasets, the weights of these three aspects to affect the ranking performance may also vary.
ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã‚Œã‚‰3ã¤ã®å´é¢ã®é‡ã¿ã‚‚ç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

#### LLMs can effectively rank candidates retrieved by multiple candidate generation models. LLMã¯è¤‡æ•°ã®å€™è£œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å–å¾—ã•ã‚ŒãŸå€™è£œã‚’åŠ¹æœçš„ã«ãƒ©ãƒ³ã‚¯ä»˜ã‘ã§ãã¾ã™ã€‚

For real-world recommender systems, the items to be ranked are usually retrieved by multiple candidate generation models.
å®Ÿä¸–ç•Œã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€ãƒ©ãƒ³ã‚¯ä»˜ã‘ã•ã‚Œã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã¯é€šå¸¸ã€è¤‡æ•°ã®å€™è£œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å–å¾—ã•ã‚Œã¾ã™ã€‚
As a result, we also conduct experiments in a more practical and difficult setting.
ãã®ãŸã‚ã€ç§ãŸã¡ã¯ã‚ˆã‚Šå®Ÿè·µçš„ã§å›°é›£ãªè¨­å®šã§å®Ÿé¨“ã‚’è¡Œã„ã¾ã™ã€‚
We use the above-mentioned seven candidate generation models to retrieve items.
ä¸Šè¨˜ã®7ã¤ã®å€™è£œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—ã—ã¾ã™ã€‚
The top-3333 best items retrieved by each candidate generation model will be merged into a candidate set containing a total of 21212121 items.
**å„å€™è£œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å–å¾—ã•ã‚ŒãŸä¸Šä½3ã®ã‚¢ã‚¤ãƒ†ãƒ ã¯ã€åˆè¨ˆ21ã‚¢ã‚¤ãƒ†ãƒ ã‚’å«ã‚€å€™è£œã‚»ãƒƒãƒˆã«çµ±åˆã•ã‚Œã¾ã™**ã€‚
As a more practical setting, we do not complement the ground-truth item to each candidate set.
ã‚ˆã‚Šå®Ÿè·µçš„ãªè¨­å®šã¨ã—ã¦ã€å„å€™è£œã‚»ãƒƒãƒˆã«çœŸã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’è£œå®Œã—ã¾ã›ã‚“ã€‚
Note that the experiments here were conducted under the implicit preference setup, indicating that implicit positive instances (not explicitly labeled) may exist among the retrieved items.
ã“ã“ã§ã®å®Ÿé¨“ã¯ã€æš—é»™ã®å¥½ã¿è¨­å®šã®ä¸‹ã§è¡Œã‚ã‚ŒãŸã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã¯ã€å–å¾—ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã®ä¸­ã«æš—é»™ã®æ­£ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆæ˜ç¤ºçš„ã«ãƒ©ãƒ™ãƒ«ä»˜ã‘ã•ã‚Œã¦ã„ãªã„ï¼‰ãŒå­˜åœ¨ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
A more faithful evaluation might require a human study, which we intend to explore in our future work.
ã‚ˆã‚Šä¿¡é ¼æ€§ã®é«˜ã„è©•ä¾¡ã«ã¯äººé–“ã®ç ”ç©¶ãŒå¿…è¦ã«ãªã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€ã“ã‚Œã¯ç§ãŸã¡ã®ä»Šå¾Œã®ç ”ç©¶ã§æ¢æ±‚ã™ã‚‹äºˆå®šã§ã™ã€‚
For Ours, we summarize the experiences gained from Section 3.1 and 3.2.
ç§ãŸã¡ã®ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ã¯ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³3.1ã¨3.2ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸçµŒé¨“ã‚’è¦ç´„ã—ã¾ã™ã€‚
We use the recency-focused prompting strategy to encode $|\mathcal{H}|=5$ sequential historical interactions into prompts and use a bootstrapping strategy to repeatedly rank for 3333 rounds.
ç§ãŸã¡ã¯ã€æœ€è¿‘é‡è¦–ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥ã‚’ä½¿ç”¨ã—ã¦ã€$|\mathcal{H}|=5$ ã®é€£ç¶šã—ãŸæ­´å²çš„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—æˆ¦ç•¥ã‚’ä½¿ç”¨ã—ã¦3333ãƒ©ã‚¦ãƒ³ãƒ‰ã§ç¹°ã‚Šè¿”ã—ãƒ©ãƒ³ã‚¯ä»˜ã‘ã—ã¾ã™ã€‚

![table4]()

From Table 4, we can see that the LLM-based model (Ours) yields the second-best performance over the compared recommendation models on most metrics.
è¡¨4ã‹ã‚‰ã€LLMãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆç§ãŸã¡ã®ãƒ¢ãƒ‡ãƒ«ï¼‰ãŒã»ã¨ã‚“ã©ã®æŒ‡æ¨™ã§æ¯”è¼ƒã—ãŸæ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®ä¸­ã§2ç•ªç›®ã«è‰¯ã„æ€§èƒ½ã‚’ç™ºæ®ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚
The results show that LLM-based zero-shot ranker even beats the conventional recommendation model Pop and BPRMF that has been trained on the target datasets, further demonstrating the strong zero-shot ranking ability of LLMs.
çµæœã¯ã€LLMãƒ™ãƒ¼ã‚¹ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãƒ©ãƒ³ã‚«ãƒ¼ãŒã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¨“ç·´ã•ã‚ŒãŸå¾“æ¥ã®æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹Popã‚„BPRMFã‚’ä¸Šå›ã£ã¦ãŠã‚Šã€LLMã®å¼·åŠ›ãªã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãƒ©ãƒ³ã‚­ãƒ³ã‚°èƒ½åŠ›ã‚’ã•ã‚‰ã«ç¤ºã—ã¦ã„ã¾ã™ã€‚
We assume that LLMs can make use of their intrinsic world knowledge to rank the candidates comprehensively considering popularity, text features, and user behaviors.
ç§ãŸã¡ã¯ã€LLMãŒãã®å†…åœ¨çš„ãªä¸–ç•ŒçŸ¥è­˜ã‚’åˆ©ç”¨ã—ã¦ã€äººæ°—ã€ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã‚’è€ƒæ…®ã—ãªãŒã‚‰å€™è£œã‚’åŒ…æ‹¬çš„ã«ãƒ©ãƒ³ã‚¯ä»˜ã‘ã§ãã‚‹ã¨ä»®å®šã—ã¾ã™ã€‚
In comparison, existing models (as narrow experts) may lack the ability to rank items in a complicated setting.
æ¯”è¼ƒã™ã‚‹ã¨ã€æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆç‹­ã„å°‚é–€å®¶ã¨ã—ã¦ï¼‰ã¯ã€è¤‡é›‘ãªè¨­å®šã§ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ©ãƒ³ã‚¯ä»˜ã‘ã™ã‚‹èƒ½åŠ›ãŒæ¬ ã‘ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
The above findings can be summarized as:
ä¸Šè¨˜ã®ç™ºè¦‹ã¯æ¬¡ã®ã‚ˆã†ã«è¦ç´„ã§ãã¾ã™ï¼š

---

Observation 3. LLMs have promising zero-shot ranking abilities, especially on candidates retrieved by multiple candidate generation models with different practical strategies.
è¦³å¯Ÿ3. LLMã¯æœ‰æœ›ãªã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãƒ©ãƒ³ã‚­ãƒ³ã‚°èƒ½åŠ›ã‚’æŒã£ã¦ãŠã‚Šã€ç‰¹ã«ç•°ãªã‚‹å®Ÿè·µçš„æˆ¦ç•¥ã‚’æŒã¤è¤‡æ•°ã®å€™è£œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å–å¾—ã•ã‚ŒãŸå€™è£œã«å¯¾ã—ã¦ã§ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## 4Related Work é–¢é€£ç ”ç©¶

#### Transfer learning for recommender systems. ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®ãŸã‚ã®è»¢ç§»å­¦ç¿’ã€‚

As recommender systems are mostly trained on data collected from a single source, people have sought to transfer knowledge from other domains[71,85,45,86,76,83], markets[3,51], or platforms[4,19].  
ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã¯ä¸»ã«å˜ä¸€ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã•ã‚Œã‚‹ãŸã‚ã€äººã€…ã¯ä»–ã®ãƒ‰ãƒ¡ã‚¤ãƒ³[71,85,45,86,76,83]ã€å¸‚å ´[3,51]ã€ã¾ãŸã¯ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ [4,19]ã‹ã‚‰çŸ¥è­˜ã‚’è»¢é€ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ã€‚
Typical transfer learning methods for recommender systems rely on anchors, including shared users/items[45,84,69,70,7,8]or representations from a shared space[11,18,38].  
ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®å…¸å‹çš„ãªè»¢ç§»å­¦ç¿’æ‰‹æ³•ã¯ã€å…±æœ‰ãƒ¦ãƒ¼ã‚¶ãƒ¼/ã‚¢ã‚¤ãƒ†ãƒ [45,84,69,70,7,8]ã‚„å…±æœ‰ç©ºé–“ã‹ã‚‰ã®è¡¨ç¾[11,18,38]ã‚’å«ã‚€ã‚¢ãƒ³ã‚«ãƒ¼ã«ä¾å­˜ã—ã¦ã„ã¾ã™ã€‚
However, these anchors are usually sparse among different scenarios, making transferring difficult for recommendations[85].  
ã—ã‹ã—ã€ã“ã‚Œã‚‰ã®ã‚¢ãƒ³ã‚«ãƒ¼ã¯é€šå¸¸ã€ç•°ãªã‚‹ã‚·ãƒŠãƒªã‚ªé–“ã§ã¾ã°ã‚‰ã§ã‚ã‚Šã€æ¨è–¦ã®ãŸã‚ã®è»¢é€ã‚’å›°é›£ã«ã—ã¾ã™[85]ã€‚
More recently, there are studies aiming to transfer knowledge stored in language models by adapting them to recommendation tasks via tuning[1,21,12,53]or prompting[37,75,39].  
æœ€è¿‘ã§ã¯ã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°[1,21,12,53]ã‚„ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°[37,75,39]ã‚’é€šã˜ã¦ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã«ä¿å­˜ã•ã‚ŒãŸçŸ¥è­˜ã‚’æ¨è–¦ã‚¿ã‚¹ã‚¯ã«é©å¿œã•ã›ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ãŸç ”ç©¶ãŒã‚ã‚Šã¾ã™ã€‚
In this paper, we conduct zero-shot recommendation experiments to examine the potential to transfer knowledge from LLMs.  
æœ¬è«–æ–‡ã§ã¯ã€**LLMã‹ã‚‰çŸ¥è­˜ã‚’è»¢é€ã™ã‚‹å¯èƒ½æ€§ã‚’æ¤œè¨ã™ã‚‹ãŸã‚ã«ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ¨è–¦å®Ÿé¨“**ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚

#### Large language models for recommender systems. ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®ãŸã‚ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€‚

The design of recommendation models, especially sequential recommendation models, has been long inspired by the design of language models, from word2vec[2,22,25]to recent neural networks[28,33,82,54].  
æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã€ç‰¹ã«é€æ¬¡æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®è¨­è¨ˆã¯ã€word2vec[2,22,25]ã‹ã‚‰æœ€è¿‘ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯[28,33,82,54]ã«è‡³ã‚‹ã¾ã§ã€é•·ã„é–“è¨€èªãƒ¢ãƒ‡ãƒ«ã®è¨­è¨ˆã«è§¦ç™ºã•ã‚Œã¦ãã¾ã—ãŸã€‚
In recent years, with the development of pre-trained language models (PLMs)[14], people have tried to transfer knowledge stored in PLMs to recommendation models, by either representing items using their text features or representing behavior sequences in the format of natural language[21,58,42,16,68].  
**è¿‘å¹´ã€äº‹å‰å­¦ç¿’ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆPLMsï¼‰ã®ç™ºå±•[14]ã«ä¼´ã„ã€äººã€…ã¯PLMsã«ä¿å­˜ã•ã‚ŒãŸçŸ¥è­˜ã‚’æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã«è»¢é€ã—ã‚ˆã†ã¨è©¦ã¿ã¦ãŠã‚Šã€ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´ã‚’ä½¿ç”¨ã—ã¦è¡¨ç¾ã™ã‚‹ã‹ã€è¡Œå‹•ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’è‡ªç„¶è¨€èªã®å½¢å¼ã§è¡¨ç¾**ã—ã¦ã„ã¾ã™[21,58,42,16,68]ã€‚
(ãªã‚‹ã»ã©ã€‚semanticãªã‚¢ã‚¤ãƒ†ãƒ åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ã†ã“ã¨ã‚‚ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã‚’æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã«è»¢é€ã™ã‚‹æ–¹æ³•ã®1ã¤ã¨è¦‹åšã›ã‚‹ã®ã‹...!:thinking:)
Very recently, large language models (LLMs) have been shown superior language understanding and generation abilities[79,56,47,66,17,6,67].  
ã”ãæœ€è¿‘ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ãŒå„ªã‚ŒãŸè¨€èªç†è§£ã¨ç”Ÿæˆèƒ½åŠ›ã‚’ç¤ºã—ã¦ã„ã¾ã™[79,56,47,66,17,6,67]ã€‚
Studies have been made to make recommender systems more interactive by integrating LLMs along with conventional recommendation models[20,36,43,59,27,61,65,48]or fine-tuned with specially designed instructions[12,21,1,31,81].  
LLMsã‚’å¾“æ¥ã®æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã¨çµ±åˆã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚ˆã‚Šã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã«ã™ã‚‹ãŸã‚ã®ç ”ç©¶ãŒè¡Œã‚ã‚Œã¦ã„ã¾ã™[20,36,43,59,27,61,65,48]ã¾ãŸã¯ç‰¹åˆ¥ã«è¨­è¨ˆã•ã‚ŒãŸæŒ‡ç¤ºã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã¾ã™[12,21,1,31,81]ã€‚
There are also early explorations showing LLMs have zero-shot recommendation abilities[59,41,13,34,72,60,63,73].  
LLMsãŒã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ¨è–¦èƒ½åŠ›ã‚’æŒã¤ã“ã¨ã‚’ç¤ºã™åˆæœŸã®æ¢æ±‚ã‚‚ã‚ã‚Šã¾ã™[59,41,13,34,72,60,63,73]ã€‚
Despite being effective to some extent, few works have explored what determines the recommendation performance of LLMs.  
ã‚ã‚‹ç¨‹åº¦åŠ¹æœçš„ã§ã‚ã‚‹ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€LLMsã®æ¨è–¦æ€§èƒ½ã‚’æ±ºå®šã™ã‚‹è¦å› ã‚’æ¢æ±‚ã—ãŸç ”ç©¶ã¯ã»ã¨ã‚“ã©ã‚ã‚Šã¾ã›ã‚“ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## 5Conclusion çµè«–

In this work, we investigated the capacities of LLMs that act as the zero-shot ranking model for recommender systems. 
æœ¬ç ”ç©¶ã§ã¯ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹LLMã®èƒ½åŠ›ã‚’èª¿æŸ»ã—ã¾ã—ãŸã€‚
To rank with LLMs, we constructed natural language prompts that contain historical interactions, candidates, and instruction templates. 
LLMã‚’ä½¿ç”¨ã—ã¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¡Œã†ãŸã‚ã«ã€æ­´å²çš„ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã€å€™è£œã€ãŠã‚ˆã³æŒ‡ç¤ºãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å«ã‚€è‡ªç„¶è¨€èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚
We then propose several specially designed prompting strategies to trigger the ability of LLMs to perceive orders of sequential behaviors. 
æ¬¡ã«ã€LLMãŒé †æ¬¡è¡Œå‹•ã®é †åºã‚’èªè­˜ã™ã‚‹èƒ½åŠ›ã‚’å¼•ãå‡ºã™ãŸã‚ã«ã€ç‰¹åˆ¥ã«è¨­è¨ˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæˆ¦ç•¥ã‚’ã„ãã¤ã‹ææ¡ˆã—ã¾ã™ã€‚
We also introduce bootstrapping and prompting strategies to alleviate the position bias and popularity bias issues that LLM-based ranking models may suffer. 
ã¾ãŸã€LLMãƒ™ãƒ¼ã‚¹ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ãŒç›´é¢ã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ä½ç½®ãƒã‚¤ã‚¢ã‚¹ã¨äººæ°—ãƒã‚¤ã‚¢ã‚¹ã®å•é¡Œã‚’è»½æ¸›ã™ã‚‹ãŸã‚ã«ã€ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ”ãƒ³ã‚°ãŠã‚ˆã³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæˆ¦ç•¥ã‚’å°å…¥ã—ã¾ã™ã€‚
Extensive empirical studies indicate that LLMs have promising zero-shot ranking abilities. 
åºƒç¯„ãªå®Ÿè¨¼ç ”ç©¶ã¯ã€LLMãŒæœ‰æœ›ãªã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãƒ©ãƒ³ã‚­ãƒ³ã‚°èƒ½åŠ›ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
The empirical studies demonstrate the strong potential of transferring knowledge from LLMs as powerful recommendation models. 
å®Ÿè¨¼ç ”ç©¶ã¯ã€LLMã‹ã‚‰å¼·åŠ›ãªæ¨è–¦ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦çŸ¥è­˜ã‚’è»¢é€ã™ã‚‹å¼·ã„å¯èƒ½æ€§ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
We aim at shedding light on several promising directions to further improve the ranking abilities of LLMs, including 
ç§ãŸã¡ã¯ã€LLMã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°èƒ½åŠ›ã‚’ã•ã‚‰ã«å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®ã„ãã¤ã‹ã®æœ‰æœ›ãªæ–¹å‘æ€§ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚

(1) better perceiving the order of sequential historical interactions 
ï¼ˆ1ï¼‰é †æ¬¡ã®æ­´å²çš„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã®é †åºã‚’ã‚ˆã‚Šè‰¯ãèªè­˜ã™ã‚‹ã“ã¨

and (2) alleviating the position bias and popularity bias. 
ï¼ˆ2ï¼‰ä½ç½®ãƒã‚¤ã‚¢ã‚¹ã¨äººæ°—ãƒã‚¤ã‚¢ã‚¹ã‚’è»½æ¸›ã™ã‚‹ã“ã¨ã€‚

For future work, we consider developing technical approaches to solve the above-mentioned key challenges when deploying LLMs as recommendation models. 
ä»Šå¾Œã®ç ”ç©¶ã§ã¯ã€LLMã‚’æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦å±•é–‹ã™ã‚‹éš›ã«ã€ä¸Šè¨˜ã®é‡è¦ãªèª²é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®æŠ€è¡“çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’é–‹ç™ºã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ã„ã¾ã™ã€‚
We also would like to develop LLM-based recommendation models that can be efficiently tuned on downstream user behaviors for effective personalized recommendations. 
ã¾ãŸã€åŠ¹æœçš„ãªãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸæ¨è–¦ã®ãŸã‚ã«ã€ä¸‹æµã®ãƒ¦ãƒ¼ã‚¶è¡Œå‹•ã«å¯¾ã—ã¦åŠ¹ç‡çš„ã«èª¿æ•´ã§ãã‚‹LLMãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã‚’é–‹ç™ºã—ãŸã„ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## 6 Limitations åˆ¶é™äº‹é …

In most experiments in this paper, ChatGPT is used as the primary target LLM for evaluation. 
æœ¬è«–æ–‡ã®ã»ã¨ã‚“ã©ã®å®Ÿé¨“ã§ã¯ã€ChatGPTãŒè©•ä¾¡ã®ãŸã‚ã®ä¸»è¦ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆLLMã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
However, being a closed-source commercial service, ChatGPT might integrate additional techniques with its core large language model to improve performance. 
ã—ã‹ã—ã€ChatGPTã¯ã‚¯ãƒ­ãƒ¼ã‚ºãƒ‰ã‚½ãƒ¼ã‚¹ã®å•†ç”¨ã‚µãƒ¼ãƒ“ã‚¹ã§ã‚ã‚‹ãŸã‚ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã‚³ã‚¢ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«è¿½åŠ ã®æŠ€è¡“ã‚’çµ±åˆã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
While there are open-source LLMs available, such as LLaMA 2[57] and Mistral[32], they exhibit a notable performance disparity compared to ChatGPT (e.g., LLaMA-2-70B-Chat vs. ChatGPT in Table 3). 
LLaMA 2[57]ã‚„Mistral[32]ãªã©ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®LLMãŒåˆ©ç”¨å¯èƒ½ã§ã‚ã‚‹ä¸€æ–¹ã§ã€ã“ã‚Œã‚‰ã¯ChatGPTã¨æ¯”è¼ƒã—ã¦é¡•è‘—ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å·®ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼ˆä¾‹ï¼šè¡¨3ã®LLaMA-2-70B-Chatã¨ChatGPTï¼‰ã€‚
This gap makes it difficult to evaluate the emergent abilities of LLMs on the recommendation tasks using purely open-source models. 
ã“ã®ã‚®ãƒ£ãƒƒãƒ—ã¯ã€ç´”ç²‹ã«ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦æ¨è–¦ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹LLMã®æ–°ãŸãªèƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ã“ã¨ã‚’å›°é›£ã«ã—ã¾ã™ã€‚
In addition, we should note that the observations might be biased by specific prompts and datasets. 
ã•ã‚‰ã«ã€è¦³å¯Ÿçµæœã¯ç‰¹å®šã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã£ã¦ãƒã‚¤ã‚¢ã‚¹ãŒã‹ã‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã™ã¹ãã§ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->
