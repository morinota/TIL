## link

- https://ar5iv.labs.arxiv.org/html/2303.18223

## title

A Survey of Large Language Models

## abstract

Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable artificial intelligence (AI) algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various natural language processing (NLP) tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities (e.g., in-context learning) that are not present in small-scale language models (e.g., BERT). To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size (e.g., containing tens or hundreds of billions of parameters). Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. Considering this rapid technical progress, in this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions. This survey provides an up-to-date review of the literature on LLMs, which can be a useful resource for both researchers and engineers.

# Introduction

Language is a prominent ability of human beings for expression and communication, which develops in early childhood and evolves over a lifetime [1, 2]. Whereas for machines, they cannot naturally grasp the abilities of understanding and communicating in the form of human language, unless equipped with powerful artificial intelligence (AI) algorithms. To achieve this goal, it has been a longstanding research challenge that enables machines to read, write, and communicate like humans [3].

Technically, language modeling (LM) is one of the major approaches to advancing language intelligence of machines. In general, LM aims to model the generative likelihood of word sequences, so as to predict the probabilities of future (or missing) tokens. The research of LM has received extensive research attention in the literature, which can be roughly divided into four major development stages:

∙
Statistical language models (SLM). SLMs [4, 5, 6, 7] are developed based on statistical learning methods that rose in the 1990s. The basic idea is to build the word prediction model based on the Markov assumption, e.g., predicting the next word based on the most recent context. The SLMs with a fixed context length
�
are also called
�
-gram language models, e.g., bigram and trigram language models. SLMs have been widely applied to enhance task performance in information retrieval (IR) [8, 9] and natural language processing (NLP) [10, 11, 12]. However, they often suffer from the curse of dimensionality: it is difficult to accurately estimate high-order language models since an exponential number of transition probabilities need to be estimated. Thus, specially designed smoothing strategies such as back-off estimation [13] and Good–Turing estimation [14] have been introduced to alleviate the data sparsity problem.

∙
Neural language models (NLM). NLMs [15, 16, 17] characterize the probability of word sequences by neural networks, e.g., recurrent neural networks (RNNs). As a remarkable contribution, the work in [15] introduced the concept of distributed representation of words and modeled the context representation by aggregating the related distributed word vectors. By extending the idea of learning effective features for words or sentences, a general neural network approach was developed to build a unified solution for various NLP tasks [18]. Further, word2vec [19, 20] was proposed to build a simplified shallow neural network for learning distributed word representations, which were shown to be very effective across a variety of NLP tasks. These studies initiate the use of language models for representation learning (beyond word sequence modeling), having an important impact on the field of NLP.

∙
Pre-trained language models (PLM). As an early attempt, ELMo [21] was proposed to capture context-aware word representations by first pre-training a bidirectional LSTM (biLSTM) network (instead of learning fixed word representations) and fine-tuning the biLSTM network according to specific downstream tasks. Further, based on the highly parallelizable Transformer architecture [22] with self-attention mechanisms, BERT [23] was proposed by pre-training bidirectional language models with specially designed pre-training tasks on large-scale unlabeled corpora. These pre-trained context-aware word representations are very effective as general-purpose semantic features, which have largely raised the performance bar of NLP tasks. This work has inspired a large number of follow-up work, which sets the “pre-training and fine-tuning” learning paradigm. Following this paradigm, a great number of studies on PLMs have been developed, introducing either different architectures [24, 25] (e.g., GPT-2 [26] and BART [24]) or improved pre-training strategies [27, 28, 29]. In this paradigm, it often requires fine-tuning the PLM for adapting to different downstream tasks.

∙
Large language models (LLM). Researchers find that scaling PLM (e.g., scaling model size or data size) often leads to an improved model capacity on downstream tasks (i.e., following the scaling law [30]). A number of studies have explored the performance limit by training an ever larger PLM (e.g., the 175B-parameter GPT-3 and the 540B-parameter PaLM). Although scaling is mainly conducted in model size (with similar architectures and pre-training tasks), these large-sized PLMs display different behaviors from smaller PLMs (e.g., 330M-parameter BERT and 1.5B-parameter GPT-2) and show surprising abilities (called emergent abilities [31]) in solving a series of complex tasks. For example, GPT-3 can solve few-shot tasks through in-context learning, whereas GPT-2 cannot do well. Thus, the research community coins the term “large language models (LLM)”1 for these large-sized PLMs [32, 33, 34, 35]. A remarkable application of LLMs is ChatGPT2 that adapts the LLMs from the GPT series for dialogue, which presents an amazing conversation ability with humans.

In the existing literature, PLMs have been widely discussed and surveyed [36, 37, 38, 39], while LLMs are seldom reviewed in a systematic way. To motivate our survey, we first highlight three major differences between LLMs and PLMs. First, LLMs display some surprising emergent abilities that may not be observed in previous smaller PLMs. These abilities are key to the performance of language models on complex tasks, making AI algorithms unprecedently powerful and effective. Second, LLMs have revolutionized the way that humans develop and use AI algorithms. Unlike small PLMs, the major approach to accessing LLMs is through the prompting interface (e.g., GPT-4 API). Humans have to understand how LLMs work and format their tasks in a way that LLMs can follow. Third, the development of LLMs no longer draws a clear distinction between research and engineering. The training of LLMs requires extensive practical experiences in large-scale data processing and distributed parallel training. To develop capable LLMs, researchers have to solve complicated engineering issues, working with engineers or being engineers.

Nowadays, LLMs are posing a significant impact on the AI community, and the advent of ChatGPT and GPT-4 even leads to the rethinking of the possibilities of artificial general intelligence (AGI). OpenAI has published a technical article entitled “Planning for AGI and beyond”, which discussed the short-term and long-term plans to approach AGI [40], and a more recent paper has argued that GPT-4 might be considered as an early version of an AGI system [41]. The research areas of AI have been revolutionized by the rapid progress of LLMs. In the field of NLP, LLMs can serve as a general-purpose language task solver (to some extent), and the research paradigm has been shifting towards the use of LLMs. In the field of IR, traditional search engines are challenged by the new information seeking way through AI chatbots (i.e., ChatGPT), and New Bing3 presents an initial attempt that enhances the search results based on LLMs. In the field of CV, the researchers try to develop ChatGPT-like vision-language models that can better serve multimodal dialogues [42, 43, 44, 45], and GPT-4 [46] has supported multimodal input by integrating the visual information. This new wave of technology would potentially lead to a prosperous ecosystem of real-world applications based on LLMs. For instance, Microsoft 365 is being empowered by LLMs (i.e., Copilot) to automate office work, and ChatGPT enables the integration of useful plugins for solving complex tasks.

Despite the progress and impact, the underlying principles of LLMs are still not well explored. Firstly, it is still mysterious why emergent abilities occur in LLMs, instead of smaller PLMs. As a more general issue, there still lacks a deep, detailed investigation of the key factors that contribute to the abilities of LLMs. It is important to study when and how LLMs obtain such abilities [47]. Although there are some meaningful discussions about this problem [31, 47], more principled investigations are needed to uncover the “secrets“ of LLMs. Secondly, it is difficult to train capable LLMs for the research community. Due to the huge cost of model pre-training, it is difficult to carry out repetitive, ablating studies for the research community. Indeed, LLMs are mainly trained by industry, where many important training details (e.g., data collection and cleaning) are not revealed to the public. Thirdly, it is very challenging to align LLMs with human values or preferences. Despite the capacities, LLMs are also likely to produce toxic, fictitious, or harmful contents. It requires effective and efficient control approaches to eliminating the potential risk of the use of LLMs [46].

Faced with both opportunities and challenges, it needs more attention on the research and development of LLMs. In order to provide a basic understanding of LLMs, this survey provides a literature review of the recent advances in LLMs from four major aspects, including pre-training (how to pre-train a capable LLM), adaptation tuning (how to effectively tune pre-trained LLMs from the two perspectives of effectiveness and safety), utilization (how to use LLMs for solving various downstream tasks) and capability evaluation (how to evaluate the abilities of LLMs and existing empirical findings). We thoroughly comb the literature and summarize the key findings, techniques, and methods of LLMs. For this survey, we also create a GitHub project website4 by collecting the supporting resources for LLMs. We are also aware of several related review articles on PLMs or LLMs [38, 39, 48, 36, 49, 50, 32, 51, 52, 43, 53, 54]. These papers either discuss PLMs or some specific (or general) aspects of LLMs. Compared with them, we focus on the techniques and methods to develop and use LLMs and provide a relatively comprehensive reference to important aspects of LLMs.

The remainder of the survey is organized as follows: Section 2 introduces the background for the survey article, with the terminology, settings, resources, and organization outline, followed by the summarization of available resources for developing LLMs in Section 3. Sections 4, 5, 6, and 7 review and summarize the recent progress from the four aspects of pre-training, adaptation tuning, utilization, and capacity evaluation, respectively. Finally, we conclude this survey in Section 8 by summarizing the major findings and discuss the remaining issues for future work.

# Overview

In this section, we introduce the background of LLMs with key terminologies, abilities and techniques.

Background. Typically, large language models (LLMs) refer to language models that contain hundreds of billions (or more) of parameters5, which are trained on massive text data [32], such as GPT-3 [55], PaLM [56], Galactica [35], and LLaMA [57]. Specifically, LLMs are built upon the Transformer architecture [22], where multi-head attention layers are stacked in a very deep neural network. Existing LLMs mainly adopt similar model architectures (i.e., Transformer) and pre-training objectives (i.e., language modeling) as small language models. As the major difference, LLMs largely scale the model size, pre-training data, and total compute (orders of magnification). They can better understand the natural language and generate high-quality text based on the given context (i.e., prompts). Such a capacity improvement can be partially described by the scaling law, where the performance roughly follows a substantial increase with respect to the model size [30]. However, some abilities (e.g., in-context learning [55]) are unpredictable according to the scaling law, which can be observed only when the model size exceeds a certain level (as discussed below).

Emergent Abilities of LLMs. In the literature [31], emergent abilities of LLMs are formally defined as “the abilities that are not present in small models but arise in large models”, which is one of the most distinctive features that distinguish LLMs from previous PLMs. It also introduces a notable characteristic when emergent abilities occur: performance rises significantly above random when the scale reaches a certain level. By analogy, such an emergent pattern has close connections with the phenomenon of phase transition in physics [31, 58]. In principle, emergent abilities can be also defined in relation to some complex tasks [59, 31], while we are more concerned with general abilities that can be applied to solve multiple tasks. Here, we briefly introduce three representative emergent abilities for LLMs, described as follows.

∙
In-context learning. The in-context learning ability is formally introduced by GPT-3 [55]: assuming that the language model has been provided with a natural language instruction and/or several task demonstrations, it can generate the expected output for the test instances by completing the word sequence of input text, without requiring additional training or gradient updates6.

∙
Instruction following. By fine-tuning with a mixture of multi-task datasets formatted via natural language descriptions (i.e., instructions), LLMs are shown to perform well on unseen tasks that are also described in the form of instructions [61, 62, 28]. With this ability, instruction tuning enables LLMs to perform new tasks by understanding the task instructions without using explicit examples, which can largely improve the generalization ability.

∙
Step-by-step reasoning. For small language models, it is usually difficult to solve complex tasks that involve multiple reasoning steps, e.g., mathematical word problems. While, with the chain-of-thought reasoning strategy [33], LLMs can solve such tasks by utilizing the prompting mechanism that involves intermediate reasoning steps for deriving the final answer. This ability is speculated to be potentially obtained by training on code [47, 33].

Key Techniques for LLMs. It has been a long way that LLMs evolve into the current state: general and capable learners. In the development process, a number of important techniques are proposed, which largely improve the capacity of LLMs. Here, we briefly list several important techniques that (potentially) lead to the success of LLMs, as follows.

∙
Scaling. Scaling is the key factor to increase the model capacity of LLMs. As the initial attempt, GPT-3 firstly increases the model size to an extremely large scale of 175B parameters. Later on, PaLM further raises the parameter scale to a new record of 540B. As discussed before, a large model size is essential to emergent abilities. While, scaling is not only conducted on model size but also related to data size and total compute [34, 63]. A recent study [34] has discussed the optimal schedule among the three aspects of model size, data size, and total compute, given a fixed budget. Further, the quality of the pre-training data plays a key role in achieving good performance, so that data collection and cleaning strategies are very important to consider when scaling the pre-training corpora.

∙
Training. Due to the huge model size, it is very challenging to successfully train a capable LLM. Distributed training algorithms are needed to learn the network parameters of LLMs, in which various parallel strategies are often jointly utilized. To support distributed training, several optimization frameworks have been released to facilitate the implementation and deployment of parallel algorithms, such as DeepSpeed [64] and Megatron-LM [65]. Besides, optimization tricks are also important for training stability and model performance, e.g., restart with training loss spike [56] and mixed precision training [66]. More recently, GPT-4 [46] proposes to develop special infrastructure and optimization methods that reliably predict the performance of large models with much smaller models.

∙
Ability eliciting. After being pre-trained on large-scale corpora, LLMs are endowed with potential abilities as general task solvers. While, these abilities might not be explicitly exhibited when LLMs perform some specific tasks. As a major approach, it is useful to design suitable task instructions or specific in-context strategies to elicit such abilities. For instance, chain-of-thought prompting has been shown to be useful to solve complex reasoning tasks by including intermediate reasoning steps. Besides, we can further perform instruction tuning on LLMs with task descriptions in natural language, for improving the generalizability of LLMs on unseen tasks. While, these techniques mainly correspond to the emergent abilities of LLMs, which may not show the same effect on small language models.

∙
Alignment tuning. Since LLMs are trained to capture the data characteristics of pre-training corpora (including both high-quality and low-quality data), they are likely to generate toxic, biased, or even harmful content for humans. It is necessary to align LLMs with human values, e.g., helpful, honest, and harmless. For this purpose, InstructGPT [61] designs an effective tuning approach that enables LLMs to follow the expected instructions, which utilizes the technique of reinforcement learning with human feedback [67, 61]. It incorporates human in the training loop with elaborately designed labeling strategies. ChatGPT is indeed developed on a similar technique to InstructGPT, which shows a strong alignment capacity in producing high-quality, harmless responses, e.g., rejecting to answer insulting questions.

∙
Tools manipulation. In essence, LLMs are trained as text generators over massive plain text corpora, thus performing less well on the tasks that are not best formed or expressed in the text (e.g., numerical computation). Besides, their capacities are also limited to the pre-training data, e.g., the inability to capture up-to-date information. To tackle these issues, a recently proposed technique is to employ external tools to compensate for the deficiencies of LLMs [68, 69]. For example, LLMs can utilize the calculator for accurate computation [69] and employ search engines to retrieve unknown information [70]. More recently, ChatGPT has enabled the mechanism of using external plugins (existing or newly created apps)7, which are by analogy with the “eyes and ears” of LLMs. Such a mechanism can broadly expand the scope of capacities for LLMs.

Besides, many other factors (e.g., the upgrade of hardware) also contribute to the success of LLMs. While, we limit our discussion to the technical approaches and key findings for developing LLMs.

# Resources of LLMs

It is by no means an easy job to develop or reproduce LLMs, considering the challenging technical issues and huge demands of computation resources. A feasible way is to learn experiences from existing LLMs and reuse publicly available resources for incremental development or experimental study. In this section, we will mainly summarize the open-source model checkpoints or APIs, available corpora, and useful libraries for LLMs.

## Publicly Available Model Checkpoints or APIs

Given the huge cost of model pre-training, well-trained model checkpoints are critical to the study and development of LLMs for the research community. Since the parameter scale is a key factor to consider for using LLMs, we categorize these public models into two scale levels (i.e., tens of billions of parameters or hundreds of billions of parameters), which is useful for users to identify the suitable resources according to their resource budget. Besides, for inference, we can directly employ public APIs to perform our tasks, without running the model locally. In this section, we briefly summarize the publicly available checkpoints and APIs for LLMs.

Models with Tens of Billions of Parameters. Most open-source models of this category have a parameter scale ranging from 10B to 20B, except LLaMA [57] (containing 65B parameters in the largest version). Other models within this range include mT5 [72], T0 [28], GPT-NeoX-20B [75], CodeGen [76], UL2 [78], Flan-T5 [81], mT0 [82], and PanGu-
�
[73]. Among them, Flan-T5 (11B version) can serve as a premier model for research on instruction tuning, since it explores the instruction tuning from three aspects [81]: increasing the number of tasks, scaling the model size, and fine-tuning with chain-of-thought prompting data. Besides, CodeGen (11B version), as an autoregressive language model designed for generating code, can be considered as a good open-source candidate for exploring the code generation ability. It also introduces a new benchmark MTPB [76] specially for multi-turn program synthesis, which is composed by 115 expert-generated problems. To solve these problems, it requires LLMs to acquire sufficient programming knowledge (e.g., math, array operations, and algorithms). As for multilingual tasks, mT0 (13B version) might be a good candidate model, which has been fine-tuned on multilingual tasks with multilingual prompts. Furthermore, PanGu-
�
[73] shows good performance in Chinese downstream tasks in zero-shot or few-shot settings, which is developed based on the deep learning framework MindSpore [99]. Note that PanGu-
�
[73] holds multiple versions of models (up to 200B parameters), while the largest public version has 13B parameters. As a more recent release, LLaMA (65B version) [57], which contains approximately five times as many parameters as other models, has exhibited superior performance in tasks related to instruction following. Typically, pre-training models at this scale require hundreds or even thousands of GPUs or TPUs. For instance, GPT-NeoX-20B uses 12 supermicro servers, each equipped with 8 NVIDIA A100-SXM4-40GB GPUs, while LLaMA utilizes 2,048 A100-80G GPUs as reported in their original publications. To accurately estimate the computation resources needed, it is suggested to use the metrics measuring the number of involved computations such as FLOPS (i.e., FLoating point number Operations Per Second) [30].

Models with Hundreds of Billions of Parameters. For models in this category, only a handful of models have been publicly released. For example, OPT [79], OPT-IML [83], BLOOM [66], and BLOOMZ [82] have nearly the same number of parameters as GPT-3 (175B version), while GLM [80] and Galactica [35] have 130B and 120B parameters, respectively. Among them, OPT (175B version) has been specially motivated for open-source sharing, which aims to enable researchers to carry out reproducible research at scale. For research in cross-lingual generalization, BLOOM (176B version) and BLOOMZ (176B version) can be used as base models, due to the competence in multilingual language modeling tasks. Among these models, Galactica, GLM, and OPT-IML have been tuned with instructions, which might be good candidates for studying the effect of instruction tuning. Models of this scale typically require thousands of GPUs or TPUs to train. For instance, OPT (175B version) requires 992 A100-80GB GPUs, while GLM (130B version) uses a cluster of 96 NVIDIA DGX-A100 (8x40G) GPU nodes.

Public API of LLMs. Instead of directly using the model copies, APIs provide a more convenient way for common users to use LLMs, without the need of running the model locally. As a representative interface for using LLMs, the APIs for the GPT-series models [55, 87, 61, 46] have been widely used for both academia and industry8. OpenAI has provided seven major interfaces to the models in GPT-3 series: ada, babbage, curie, davinci (the most powerful version in GPT-3 series), text-ada-001, text-babbage-001, and text-curie-001. Among them, the first four interfaces can be further fine-tuned on the host server of OpenAI. In particular, babbage, curie, and davinci correspond to the GPT-3 (1B), GPT-3 (6.7B), and GPT-3 (175B) models, respectively [55]. Besides, there are also two APIs related to Codex [87], called code-cushman-001 (a powerful and multilingual version of the Codex (12B) [87]) and code-davinci-002. Further, GPT-3.5 series include one base model code-davinci-002 and three enhanced versions, namely text-davinci-002, text-davinci-003, and gpt-3.5-turbo-0301. It is worth noting that gpt-3.5-turbo-0301 is the interface to invoke ChatGPT. More recently, OpenAI has also released the corresponding APIs for GPT-4, including gpt-4, gpt-4-0314, gpt-4-32k, and gpt-4-32k-0314. Overall, the choice of API interfaces depends on the specific application scenarios and response requirements. The detailed usage can be found on their project websites9.

## COmmnly User Corpora

In contrast to earlier PLMs, LLMs which consist of a significantly larger number of parameters require a higher volume of training data that covers a broad range of content. For this need, there are increasingly more accessible training datasets that have been released for research. In this section, we will briefly summarize several widely used corpora for training LLMs. Based on their content types, we categorize these corpora into six groups: Books, CommonCrawl, Reddit links, Wikipedia, Code, and others.

Books. BookCorpus [100] is a commonly used dataset in previous small-scale models (e.g., GPT [110] and GPT-2 [26]), consisting of over 11,000 books covering a wide range of topics and genres (e.g., novels and biographies). Another large-scale book corpus is Project Gutenberg [101], consisting of over 70,000 literary books including novels, essays, poetry, drama, history, science, philosophy, and other types of works in the public domain. It is currently one of the largest open-source book collections, which is used in training of MT-NLG [90] and LLaMA [57]. As for Books1 [55] and Books2 [55] used in GPT-3 [55], they are much larger than BookCorpus but have been not publicly released so far.

CommonCrawl. CommonCrawl [111] is one of the largest open-source web crawling databases, containing a petabyte-scale data volume, which has been widely used as training data for existing LLMs. As the whole dataset is very large, existing studies mainly extract subsets of web pages from it within a specific period. However, due to the widespread existence of noisy and low-quality information in web data, it is necessary to perform data preprocessing before usage. Based on CommonCrawl, there are four filtered datasets that are commonly used in existing work: C4 [71], CC-Stories [102], CC-News [27], and RealNews [103]. The Colossal Clean Crawled Corpus (C4) includes five variants10, namely en (806G), en.noclean (6T), realnewslike (36G), webtextlike (17G), and multilingual (38T). The en version has been utilized for pre-training T5 [71], LaMDA [85], Gopher [59], and UL2 [78]. The multilingual C4, also called mC4, has been used in mT5 [72]. CC-Stories (31G) is composed of a subset of CommonCrawl data, in which the contents are made in a story-like way. While, the original source of CC-Stories is not available now, so a reproduction version, CC-Stories-R [112], has been included in Table II. Moreover, two news corpora extracted from CommonCrawl, i.e., REALNEWS (120G) and CC-News (76G), are also commonly used as the pre-training data.

Reddit Links. Reddit is a social media platform that enables users to submit links and text posts, which can be voted on by others through “upvotes” or “downvotes”. Highly upvoted posts are often considered useful, and can be utilized to create high-quality datasets. WebText [26] is a well-known corpus composed of highly upvoted links from Reddit, but it is not publicly available. As a surrogate, there is a readily accessible open-source alternative called OpenWebText [104]. Another corpus extracted from Reddit is PushShift.io [105], a real-time updated dataset that consists of historical data from Reddit since its creation day. Pushshift provides not only monthly data dumps but also useful utility tools to support users in searching, summarizing, and conducting preliminary investigations on the entire dataset. This makes it easy for users to collect and process Reddit data.

Wikipedia. Wikipedia [106] is an online encyclopedia containing a large volume of high-quality articles on diverse topics. Most of these articles are composed in an expository style of writing (with supporting references), covering a wide range of languages and fields. Typically, the English-only filtered versions of Wikipedia are widely used in most LLMs (e.g., GPT-3 [55], LaMDA [85], and LLaMA [57]). Wikipedia is available in multiple languages, so it can be used in multilingual settings.

Code. To collect code data, existing work mainly crawls open-source licensed codes from the Internet. Two major sources are public code repositories under open-source licenses (e.g., GitHub) and code-related question-answering platforms (e.g., StackOverflow). Google has publicly released the BigQuery dataset [107], which includes a substantial number of open-source licensed code snippets in various programming languages, serving as a representative code dataset. CodeGen has utilized BIGQUERY [76], a subset of the BigQuery dataset, for training the multilingual version of CodeGen (CodeGen-Multi).

Others. The Pile [108] is a large-scale, diverse, and open-source text dataset consisting of over 800GB of data from multiple sources, including books, websites, codes, scientific papers, and social media platforms. It is constructed from 22 diverse high-quality subsets. The Pile dataset is widely used in models with different parameter scales, such as GPT-J (6B) [113], CodeGen (16B) [76], and Megatron-Turing NLG (530B) [65]. Besides, ROOTS [109] is composed of various smaller datasets (totally 1.61 TB of text) and covers 59 different languages (containing natural languages and programming languages), which have been used for training BLOOM [66].

As we can see from Figure 2, LLMs no longer rely on a single corpus, but instead utilize multiple data sources for pre-training. Therefore, existing studies commonly mix several ready-made datasets (e.g., C4, OpenWebText, and the Pile), and then perform further processing to obtain the pre-training corpus. Besides, to train the LLMs that are adaptive to specific applications, it is also important to extract data from relevant sources (e.g., Wikipedia and BigQuery) for enriching the corresponding information in pre-training data. To have a quick reference of the data sources used in existing LLMs, we present the pre-training corpora of three representative LLMs:

∙
GPT-3 (175B) [55] was trained on a mixed dataset of 300B tokens, including CommonCrawl [111], WebText2 [55], Books1 [55], Books2 [55], and Wikipedia [106].

∙
PaLM (540B) [56] uses a pre-training dataset of 780B tokens, which is sourced from social media conversations, filtered webpages, books, Github, multilingual Wikipedia, and news.

∙
LLaMA [57] extracts training data from various sources, including CommonCrawl, C4 [71], Github, Wikipedia, books, ArXiv, and StackExchange. The training data size for LLaMA (6B) and LLaMA (13B) is 1.0T tokens, while 1.4T tokens are used for LLaMA (32B) and LLaMA (65B).

## Library Resource

In this part, we briefly introduce a series of available libraries for developing LLMs.

∙
Transformers [114] is an open-source Python library for building models using the Transformer architecture, which is developed and maintained by Hugging Face. It has a simple and user-friendly API, making it easy to use and customize various pre-trained models, as well as tools for dataset processing and evaluation. It is a powerful library with a large and active community of users and developers who regularly update and improve the models and algorithms.

∙
DeepSpeed [64] is a PyTorch-based deep learning optimization library developed by Microsoft, which has been used to train a number of LLMs, such as GPT-Neo [115] and BLOOM [66]. It provides various optimization techniques for distributed training, such as memory optimization (ZeRO technique), gradient checkpointing, and pipeline parallelism. Additionally, it provides the API for fine-tuning and evaluating these models.

∙
Megatron-LM [116] is a PyTorch-based deep learning library developed by NVIDIA for training large-scale language models. It also provides rich optimization techniques for distributed training, including model and data parallelism, mixed-precision training, FlashAttention, and gradient checkpointing. These optimization techniques can significantly improve the training efficiency and speed, enabling efficient distributed training across GPUs and machines.

∙
JAX [117] is a Python library for high-performance machine learning developed by Google Brain, allowing users to easily perform computations on arrays with hardware acceleration (GPU or TPU) support. It supports computation on various devices and also provides several convenient functions, such as just-in-time compilation acceleration and automatic batching.

∙
Colossal-AI [118] is a deep learning library developed by EleutherAI for training large-scale language models. It is built on top of JAX and supports optimization strategies for training such as mixed-precision training and parallelism. Recently, a ChatGPT-like model called ColossalChat [119] has been publicly released with two versions (7B and 13B), which are developed using Colossal-AI based on LLaMA [57].

∙
BMTrain [120] is an efficient library developed by OpenBMB for training models with large-scale parameters in a distributed manner, which emphasizes code simplicity, low resource, and high availability. BMTrain has already incorporated several common LLMs (e.g., Flan-T5 [81] and GLM [80]) into its ModelCenter, where developers can use these models directly.

∙
FastMoE [121] is a specialized training library for MoE (i.e., mixture-of-experts) models. It is developed on top of PyTorch, prioritizing both efficiency and user-friendliness in its design. FastMoE simplifies the process of transferring Transformer models to MoE models and supports both data parallelism and model parallelism during training.

Besides the above libraries, existing deep learning frameworks (e.g., PyTorch [122], TensorFlow [123], MXNet [124], PaddlePaddle [125], MindSpore [99], and OneFlow [126]) have also provided support for parallelism algorithms, which are commonly used for training large-scale models.

# Pre-training

Pre-training establishes the basis of the abilities of LLMs. By pre-training on large-scale corpora, LLMs can acquire essential language understanding and generation skills [55, 56]. In this process, the scale and quality of the pre-training corpus are critical for LLMs to attain powerful capabilities. Besides, to effectively pre-train LLMs, model architectures, acceleration methods, and optimization techniques need to be well designed. In what follows, we first discuss the data collection and processing in Section 4.1, then introduce the commonly used model architectures in Section 4.2, and finally present the training techniques to stably and efficiently optimize LLMs in Section 4.3.

## Data Collection

Compared with small-scale language models, LLMs have a stronger demand for high-quality data for model pre-training, and their model capacities largely rely on the pre-training corpus and how it has been preprocessed. In this part, we discuss the collection and processing of pre-training data, including data sources, preprocessing methods, and important analysis of how pre-training data affects the performance of LLMs.

### Data Source

To develop a capable LLM, it is key to collect a large amount of natural language corpus from various data sources. Existing LLMs mainly leverage a mixture of diverse public textual datasets as the pre-training corpus. Figure 2 shows the distribution of the sources of pre-training data for several existing LLMs.

The source of pre-training corpus can be broadly categorized into two types: general data and specialized data. General data, such as webpages, books, and conversational text, is utilized by most LLMs [56, 55, 79] due to its large, diverse, and accessible nature, which can enhance the language modeling and generalization abilities of LLMs. In light of the impressive generalization capabilities exhibited by LLMs, there are also studies that extend their pre-training corpus to more specialized datasets, such as multilingual data, scientific data, and code, endowing LLMs with specific task-solving capabilities [56, 35, 76]. In what follows, we describe these two types of pre-training data sources and their effects on LLMs. For a detailed introduction to the commonly used corpus, one can refer to Section 3.2.

General Data. As we can see in Figure 2, the vast majority of LLMs adopt general-purpose pre-training data, such as webpages, books, and conversational text, which provides rich text sources on a variety of topics. Next, we briefly summarize three important kinds of general data.

∙
Webpages. Owing to the proliferation of the Internet, various types of data have been created, which enables LLMs to gain diverse linguistic knowledge and enhance their generalization capabilities [26, 71]. For convenient use of these data resources, a large amount of data is crawled from the web in previous work, such as CommonCrawl [111]. However, the crawled web data tends to contain both high-quality text, such as Wikipedia and low-quality text, like spam mail, thus it is important to filter and process webpages for improving the data quality.

∙
Conversation text. Conversation data can enhance the conversational competence of LLMs [79] and potentially improve their performance on a range of question-answering tasks [56]. Researchers can utilize subsets of public conversation corpus (e.g., PushShift.io Reddit corpus) [127, 105] or collect conversation data from online social media. Since online conversational data often involves discussions among multiple participants, an effective processing way is to transform a conversation into a tree structure, where the utterance is linked to the one it responds to. In this way, the multi-party conversation tree can be divided into multiple sub-conversations, which can be collected in the pre-training corpus. Furthermore, a potential risk is that the excessive integration of dialogue data into LLMs may result in a side effect [79]: declarative instructions and direct interrogatives are erroneously perceived as the beginning of conversations, thus leading to a decline in the efficacy of the instructions.

∙
Books. Compared to other corpus, books provide an important source of formal long texts, which are potentially beneficial for LLMs to learn linguistic knowledge, model long-term dependency, and generate narrative and coherent texts. To obtain open-source book data, existing studies usually adopt the Books3 and Bookcorpus2 datasets, which are available in the Pile dataset [108].

Specialized Data. Specialized datasets are useful to improve the specific capabilities of LLMs on downstream tasks. Next, we introduce three kinds of specialized data.

∙
Multilingual text. Besides the text in the target language, integrating a multilingual corpus can enhance the multilingual abilities of language understanding and generation. For example, BLOOM [66] and PaLM [56] have curated multilingual data covering 46 and 122 languages, respectively, within their pre-training corpora. These models demonstrate impressive performance in multilingual tasks, such as translation, multilingual summarization, and multilingual question answering, and achieve comparable or superior performance to the state-of-the-art models that are fine-tuned on the corpus in the target language(s).

∙
Scientific text. The exploration of science by humans has been witnessed by the increasing growth of scientific publications. In order to enhance the understanding of scientific knowledge for LLMs [35, 128], it is useful to incorporate a scientific corpus for model pre-training [35, 128]. By pre-training on a vast amount of scientific text, LLMs can achieve impressive performance in scientific and reasoning tasks [129]. To construct the scientific corpus, existing efforts mainly collect arXiv papers, scientific textbooks, math webpages, and other related scientific resources. Due to the complex nature of data in scientific fields, such as mathematical symbols and protein sequences, specific tokenization and preprocessing techniques are usually required to transform these different formats of data into a unified form that can be processed by language models.

∙
Code. Program synthesis has been widely studied in the research community [130, 131, 132, 87, 133], especially the use of PLMs trained on code [115, 113]. However, it remains challenging for these PLMs (e.g., GPT-J [113]) to generate high-quality and accurate programs. Recent studies [87, 133] have found that training LLMs on a vast code corpus can lead to a substantial improvement in the quality of the synthesized programs. The generated programs can successfully pass expert-designed unit-test cases [87] or solve competitive programming questions [94]. In general, two types of code corpora are commonly used for pre-training LLMs. The first source is from programming question answering communities like Stack Exchange [134, 135]. The second source is from public software repositories such as GitHub [87, 133, 76], where code data (including comments and docstrings) are collected for utilization. Compared to natural language text, code is in the format of a programming language, corresponding to long-range dependencies and accurate execution logic [136]. A recent study [47] also speculates that training on code might be a source of complex reasoning abilities (e.g., chain-of-thought ability [33]). Besides, it has been shown that formatting reasoning tasks into code can help LLMs generate more accurate results [136, 137].

### Data Preprocessing

After collecting a large amount of text data, it is essential to preprocess it for constructing the pre-training corpus, especially removing noisy, redundant, irrelevant, and potentially toxic data [59, 56], which may largely affect the capacity and performance of LLMs. In this part, we review the detailed data preprocessing strategies to improve the quality of the collected data [59, 93, 66]. A typical pipeline of preprocessing the pre-training data for LLMs has been illustrated in Figure 3.

Quality Filtering. To remove low-quality data from the collected corpus, existing work generally adopts two approaches: (1) classifier-based, and (2) heuristic-based. The former approach trains a selection classifier based on high-quality texts and leverages it to identify and filter out low-quality data. Typically, these methods [55, 93, 56] train a binary classifier with well-curated data (e.g., Wikipedia pages) as positive instances and sample candidate data as negative instances, and predict the score that measures the quality of each data example. However, several studies [93, 59] also find that a classifier-based approach may result in the unintentional removal of high-quality texts in dialectal, colloquial, and sociolectal languages, which potentially leads to bias in the pre-training corpus and diminishes the corpus diversity. As the second approach, several studies, such as BLOOM [66] and Gopher [59], employ heuristic-based approaches to eliminate low-quality texts through a set of well-designed rules, which can be summarized as follows:

• Language filtering. If a LLM would be mainly used in the tasks of certain languages, the text in other languages can be filtered.
• Metric filtering. Evaluation metrics about the generated texts, e.g., perplexity, can be employed to detect and remove unnatural sentences.
• Statistic filtering. Statistical features of a corpus, e.g., the punctuation distribution, symbol-to-word ratio, and sentence length, can be utilized to measure the text quality and filter the low-quality data.
• Keyword filtering. Based on specific keyword set, the noisy or unuseful elements in the text, such as HTML tags, hyperlinks, boilerplates, and offensive words, can be identified and removed.
De-duplication. Existing work [138] has found that duplicate data in a corpus would reduce the diversity of language models, which may cause the training process unstable and thus affect the model performance. Therefore, it is necessary to de-duplicate the pre-training corpus. Specially, de-duplication can be performed at different granularities, including sentence-level, document-level, and dataset-level de-duplication. First, low-quality sentences that contain repeated words and phrases should be removed, as they may introduce repetitive patterns in language modeling [139]. At the document level, existing studies mostly rely on the overlap ratio of surface features (e.g., words and
�
-grams overlap) between documents to detect and remove duplicate documents containing similar contents [59, 57, 66, 140]. Furthermore, to avoid the dataset contamination problem, it is also crucial to prevent the overlap between the training and evaluation sets [56], by removing the possible duplicate texts from the training set. It has been shown that the three levels of de-duplication are useful to improve the training of LLMs [141, 56], which should be jointly used in practice.

Privacy Redaction. The majority of pre-training text data is obtained from web sources, including user-generated content involving sensitive or personal information, which may increase the risk of privacy breaches [142]. Thus, it is necessary to remove the personally identifiable information (PII) from the pre-training corpus. One direct and effective approach is to employ rule-based methods, such as keyword spotting, to detect and remove PII such as names, addresses, and phone numbers [109]. Furthermore, researchers also find that the vulnerability of LLMs under privacy attacks can be attributed to the presence of duplicate PII data in the pre-training corpus [143]. Therefore, de-duplication can also reduce privacy risks to some extent.

Tokenization. Tokenization is also a crucial step for data preprocessing. It aims to segment raw text into sequences of individual tokens, which are subsequently used as the inputs of LLMs. Although it is expedient to leverage an existing tokenizer (e.g., OPT [79] and GPT-3 [55] utilize the tokenizer of GPT-2 [26]), using a tokenizer specially designed for the pre-training corpus can be highly beneficial [66], especially for the corpus that consists of diverse domains, languages, and formats. Therefore, several recent LLMs train the customized tokenizers specially for the pre-training corpus with SentencePiece [144]. The byte-level Byte Pair Encoding (BPE) algorithm [145] is utilized to ensure that the information after tokenization is lossless [56, 59]. Notably, normalization techniques in BPE, such as NFKC [146], may even degrade the tokenization performance [34, 59, 66].

### Effect of Pre-training Data on LLMs

Unlike small-scale PLMs, it is usually infeasible to iterate the pre-training of LLMs multiple times, due to the huge demand for computational resources. Thus, it is particularly important to construct a well-prepared pre-training corpus before training a LLM. In this part, we discuss how the quality and distribution of the pre-training corpus potentially influence the performance of LLMs.

Mixture of Sources. As discussed before, pre-training data from different domains or scenarios has distinct linguistic characteristics or semantic knowledge. By pre-training on a mixture of text data from diverse sources, LLMs can acquire a broad scope of knowledge and may exhibit a strong generalization capacity. When mixing different sources, one needs to carefully set the distribution of pre-training data, since it is also likely to affect the performance of LLMs on downstream tasks [59]. Gopher [59] conducts the ablation experiment on data distribution to examine the impact of mixed sources on downstream tasks. Experimental results on the LAMBADA dataset [147] show that increasing the proportion of books data can improve the capacity of the model in capturing long-term dependencies from text, and increasing the proportion of the C4 dataset [71] leads to performance improvement on the C4 validation dataset [59]. While, as a side effect, training on excessive data about a certain domain would affect the generalization capability of LLMs on other domains [35, 59]. Therefore, it is suggested that researchers should carefully determine the proportion of data from different domains in the pre-training corpus, in order to develop LLMs that better meet their specific needs. The readers can refer to Figure 2 for a comparison of the data sources for different LLMs.

Amount of Pre-training Data. For pre-training an effective LLM, it is important to collect sufficient high-quality data that satisfies the data quantity demand of the LLM. Existing studies have found that with the increasing parameter scale in the LLM, more data is also required to train the model [57, 34]: a similar scaling law as model size is also observed in data size, with respect to model performance. Chinchilla [34] demonstrates that a number of existing LLMs suffer from sub-optimal training due to inadequate pre-training data. By conducting extensive experiments, it further shows that it is necessary to adopt equal scales of the model parameters and training tokens for a given compute budget. More recently, LLaMA [57] shows that with more data and longer training, smaller models can also achieve good performance. Therefore, it is suggested that researchers should pay more attention to the amount of high-quality data for adequately training the model, especially when scaling the model parameters.

Quality of Pre-training Data. Existing work has shown that pre-training on the low-quality corpus, such as noisy, toxic, and duplicate data, may hurt the performance of models [59, 140, 143, 138]. For developing a well-performing LLM, it is crucial to consider not only the quantity but also the quality of the collected training data. Recent studies, such as T5 [71], GLaM [93], and Gopher [59], have investigated the influence of data quality on the performance of downstream tasks. By comparing the performances of models trained on the filtered and unfiltered corpus, they reach the same conclusion that pre-training LMs on cleaned data can improve the model performance. More specifically, the duplication of data may result in the “double descent” (referring to the phenomenon of performance initially deteriorating and subsequently improving) [138, 148], or even overwhelm the training process [138]. Besides, it has been shown that duplicate data degrades the ability of LLMs to copy from the context, which might further affect the generalization capacity of LLMs using in-context learning [138]. Therefore, as suggested in existing work [59, 66, 56], it is essential to incorporate preprocessing methods on the pre-training corpus carefully (as illustrated in Section 4.1.2), to improve stability of the training process and avoid affecting the model performance.

## Architecture

In this section, we review the architecture design of LLMs, i.e., mainstream architecture, pre-training objective, and detailed configuration. Table III presents the model cards of several representative LLMs with public details.

### Mainstream Architectures

Due to the excellent parallelizability and capacity, the Transformer architecture [22] has become the de facto backbone to develop various LLMs, making it possible to scale language models to hundreds or thousands of billions of parameters. In general, the mainstream architectures of existing LLMs can be roughly categorized into three major types, namely encoder-decoder, casual decoder, and prefix decoder.

Encoder-decoder Architecture. The vanilla Transformer model is built on the encoder-decoder architecture [22], which consists of two stacks of Transformer blocks as the encoder and decoder, respectively. The encoder adopts stacked multi-head self-attention layers to encode the input sequence for generating its latent representations, while the decoder performs cross-attention on these representations and autoregressively generates the target sequence. Encoder-decoder PLMs (e.g., T5 [71] and BART [24]) have shown effectiveness on a variety of NLP tasks. So far, there are only a small number of LLMs that are built based on the encoder-decoder architecture, e.g., Flan-T5 [81]. We leave a detailed discussion about the architecture selection in Section 4.2.4.

Casual Decoder Architecture. The casual decoder architecture incorporates the uni-directional attention mask, to guarantee that each input token can only attend to the past tokens and itself. The input and output tokens are processed in the same fashion through the decoder. As representative language models of this architecture, the GPT-series models [110, 26, 55] are developed based on the casual-decoder architecture. In particular, GPT-3 [55] has successfully demonstrated the effectiveness of this architecture, also showing an amazing in-context learning capability of LLMs. Interestingly, GPT-1 [110] and GPT-2 [26] do not exhibit such superior abilities as those in GPT-3, and it seems that scaling plays an important role in increasing the model capacity of this model architecture. So far, the casual decoders have been widely adopted as the architecture of LLMs by various existing LLMs, such as OPT [79], BLOOM [66], and Gopher [59]. Note that both the casual decoder and prefix decoder discussed next belong to decoder-only architectures. While, when mentioning “decoder-only architecture”, it mainly refers to the casual decoder architecture in existing literature, unless specified.

Prefix Decoder Architecture. The prefix decoder architecture (a.k.a., non-casual decoder [149]) revises the masking mechanism of casual decoders, to enable performing bidirectional attention over the prefix tokens [150] and unidirectional attention only on generated tokens. In this way, like the encoder-decoder architecture, the prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens one by one, where the same parameters are shared during encoding and decoding. Instead of pre-training from scratch, a practical suggestion is to continually train casual decoders and then convert them into prefix decoders for accelerating convergence [29], e.g., U-PaLM [97] is derived from PaLM [56]. Existing representative LLMs based on prefix decoders include GLM-130B [80] and U-PaLM [97].

For the three types of architectures, we can also consider extending them via the mixture-of-experts (MoE) scaling, in which a subset of neural network weights for each input are sparsely activated, e.g., Switch Transformer [25] and GLaM [93]. It has been shown that substantial performance improvement can be observed by increasing either the number of experts or the total parameter size [151].

### Detailed Configuration

Since the launch of Transformer [22], various improvements have been proposed to enhance its training stability, performance, and computational efficiency. In this part, we will discuss the corresponding configurations for four major parts of the Transformer, including normalization, position embeddings, activation functions, attention, and bias.

Normalization. Training instability is a challenging issue for pre-training LLMs. To alleviate this problem, layer normalization (Layer Norm, LN) [152] is widely employed in Transformer architectures. The position of LN is vital to the performance of LLMs. While the initial Transformer [22] uses post-LN, most LLMs employ pre-LN for more stable training in spite of decreasing performance [153]. Based on pre-LN, Sandwich-LN [154] adds extra LN before the residual connections to avoid value explosion. However, it has been found that Sandwich-LN sometimes fails to stabilize the training of LLMs and may lead to the collapse of training [80]. Recently, several advanced normalization techniques have been proposed as alternatives to LN. In Gopher [59] and Chinchilla [34], RMS Norm [155] is employed due to its superiority in training speed and performance [156]. Compared with LN, DeepNorm [157] has shown a better capability to ensure the stability in training, which has been adopted by GLM-130B with post normalization. In addition, adding an extra LN after the embedding layer can also stabilize the training of LLMs. However, it tends to incur a significant performance drop [158], which has been removed in several recent LLMs [66].

Activation Functions. To obtain good performance, activation functions also need to be properly set in feed-forward networks. In existing LLMs, GeLU activations [159] are widely used. Besides, in the latest LLMs (e.g., PaLM and LaMDA), variants of GLU activation [160, 161] have also been utilized, especially the SwiGLU and GeGLU variants, which often achieve better performance in practice [156]. However, compared with GeLU, they require extra parameters (about 50%) in the feed-forward networks [158].

Position Embeddings. Since the self-attention modules in Transformer are permutation equivariant, position embeddings are employed to inject absolute or relative position information for modeling sequences. There are two variants of absolute position embeddings in the vanilla Transformer [22], i.e., sinusoids and learned position embeddings, where the latter is commonly employed in LLMs. Unlike absolute position embeddings, relative positional encodings generate embeddings according to the offsets between keys and queries [71], so it can perform well on sequences longer than those it has seen during training, i.e., extrapolation [162]. ALiBi [162] biases attention scores using a penalty based on the distance between keys and queries. Empirical results have shown that it has better zero-shot generalization with a stronger extrapolation capacity than other position embeddings [29]. Besides, by setting specific rotatory matrices based on the absolute position, the scores between keys and queries in RoPE [163] can be computed with relative position information, which is useful to model long sequences. As a result, RoPE has been widely adopted in several latest LLMs [56, 57, 80]

Attention and Bias. Beyond the full self-attention in the original Transformer [22], sparse attention with lower computation complexity is employed in GPT-3 (i.e., Factorized Attention [164, 55]). In order to effectively and efficiently model longer sequences, more attempts have been made by either introducing special attention patterns [165, 166] or considering GPU memory access (i.e., FlashAttention [167]). Besides, following the original Transformer, most LLMs keep the biases in each dense kernel and Layer Norm. However, in PaLM [56] and Galactica [35], biases are removed. It demonstrates that no biases can enhance training stability for LLMs [56].

To put all these discussions together, we summarize the suggestions from existing literature for detailed configuration. For stronger generalization and training stability, it is suggested to choose the pre RMS Norm for layer normalization, and SwiGLU or GeGLU as the activation function. While, Layer Norm may not be used immediately after embedding layers, which is likely to incur performance degradation. Besides, as for position embeddings, RoPE or ALiBi is a better choice since it performs better on long sequences.

### Pre-training Tasks

Pre-training plays a key role that encodes general knowledge from large-scale corpus into the massive model parameters. For training LLMs, there are two commonly used pre-training tasks, namely language modeling and denoising autoencoding. Language Modeling. The language modeling task (LM) is the most commonly used objective to pre-train decoder-only LLMs, e.g., GPT3 [55] and PaLM [56]. Given a sequence of tokens 𝐱 = { � 1 , … , � � } , the LM task aims to autoregressively predict the target tokens � � based on the preceding tokens � < � in a sequence. A general training objective is to maximize the following likelihood:

$$
\tag{1}
$$

Since most language tasks can be cast as the prediction problem based on the input, these decoder-only LLMs might be potentially advantageous to implicitly learn how to accomplish these tasks in a unified LM way. Some studies have also revealed that decoder-only LLMs can be naturally transferred to certain tasks by autoregressively predicting the next tokens [26, 55], without fine-tuning. An important variant of LM is the prefix language modeling task, which is designed for pre-training models with the prefix decoder architecture. The tokens within a randomly selected prefix would be not used in computing the loss of prefix language modeling. With the same amount of tokens seen during pre-training, prefix language modeling performs slightly worse than language modeling, since fewer tokens in the sequence are involved for model pre-training [29]. Denoising Autoencoding. Besides conventional LM, the denoising autoencoding task (DAE) has also been widely used to pre-train language models [24, 71]. The inputs 𝐱 \ 𝐱 ~ for DAE task are corrupted text with randomly replaced spans. Then, the language models are trained to recover the replaced tokens 𝐱 ~ . Formally, the training objective of DAE is denoted as follows:

$$
\tag{2}
$$

However, the DAE task seems to be more complicated in implementation than LM task. As a result, it has not been widely used to pre-train large language models. Existing LLMs that take DAE as pre-training objectives include T5 [71] and GLM-130B [80]. These models are mainly trained to recover the replaced spans in an autoregressive way.

### Summary and Discussion

The choice of architecture and pre-training tasks may incur different inductive biases for LLMs, which would lead to different model capacities. In this part, we summarize some important findings or discussions in the existing literature on this issue.

∙
By pre-training with the LM objective, it seems that casual decoder architecture can achieve a superior zero-shot and few-shot generalization capacity. Existing research has shown that without multi-task fine-tuning, the casual decoder has better zero-shot performance than other architectures [29]. The success of GPT-3 [55] has demonstrated that the large casual decoder model can be a good few-shot learner. In addition, instruction tuning and alignment tuning discussed in Section 5 have been proven to further enhance the capability of large causal decoder models [62, 61, 81].

∙
Scaling law has been widely observed in casual decoders. By scaling the model size, the dataset size, and the total computation, the performance of casual decoders can be substantially improved [30, 55]. Thus, it has become an important strategy to increase the model capacity of the casual decoder via scaling. However, more detailed investigation on encoder-decoder models is still lacking, and more efforts are needed to investigate the performance of encoder-decoder models at a large scale.

More research efforts about the discussions on architectures and pre-training objectives are in need to analyze how the choices of the architecture and pre-training tasks affect the capacity of LLMs, especially for encoder-decoder architectures. Besides the major architecture, the detailed configuration of LLM is also worth attention, which has been discussed in Section 4.2.2.

## Model Training
