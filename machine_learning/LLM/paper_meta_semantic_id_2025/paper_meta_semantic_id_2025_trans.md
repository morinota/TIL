refs: https://arxiv.org/html/2504.02137v1

# Enhancing Embedding Representation Stability in Recommendation Systems with Semantic ID
# ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDã‚’ç”¨ã„ãŸæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã®å®‰å®šæ€§å‘ä¸Š

## Abstract

The exponential growth of online content has posed significant challenges to ID-based models in industrial recommendation systems, ranging from extremely high cardinality and dynamically growing ID space, to highly skewed engagement distributions, to prediction instability as a result of natural id life cycles (e.g, the birth of new IDs and retirement of old IDs).  
ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æŒ‡æ•°é–¢æ•°çš„ãªæˆé•·ã¯ã€IDãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦é‡è¦ãªèª²é¡Œã‚’æèµ·ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã¯ã€**éå¸¸ã«é«˜ã„ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã‚„å‹•çš„ã«æˆé•·ã™ã‚‹IDç©ºé–“**ã€åã£ãŸã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆåˆ†å¸ƒã€è‡ªç„¶ãªIDãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ï¼ˆä¾‹ï¼šæ–°ã—ã„IDã®èª•ç”Ÿã‚„å¤ã„IDã®å¼•é€€ï¼‰ã«ã‚ˆã‚‹äºˆæ¸¬ã®ä¸å®‰å®šæ€§ãŒå«ã¾ã‚Œã¾ã™ã€‚
To address these issues, many systems rely on random hashing to handle the id space and control the corresponding model parameters (i.e embedding table). 
ã“ã‚Œã‚‰ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€å¤šãã®ã‚·ã‚¹ãƒ†ãƒ ã¯IDç©ºé–“ã‚’å‡¦ç†ã—ã€å¯¾å¿œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã™ãªã‚ã¡åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã«ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ã«ä¾å­˜ã—ã¦ã„ã¾ã™ã€‚
(ç‰¹å¾´é‡ã«åŸºã¥ãåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã¿ãŸã„ãªæ„å‘³??:thinking:)
However, this approach introduces data pollution from multiple ids sharing the same embedding, leading to degraded model performance and embedding representation instability.
ã—ã‹ã—ã€ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€åŒã˜åŸ‹ã‚è¾¼ã¿ã‚’å…±æœ‰ã™ã‚‹è¤‡æ•°ã®IDã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿æ±šæŸ“ã‚’å¼•ãèµ·ã“ã—ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ä½ä¸‹ã‚„åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã®ä¸å®‰å®šæ€§ã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

This paper examines these challenges and introduces Semantic ID prefix ngram, a novel token parameterization technique that significantly improves the performance of the original Semantic ID.  
æœ¬è«–æ–‡ã§ã¯ã€ã“ã‚Œã‚‰ã®èª²é¡Œã‚’æ¤œè¨ã—ã€å…ƒã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–æŠ€è¡“ã§ã‚ã‚‹Semantic ID prefix ngramã‚’ç´¹ä»‹ã—ã¾ã™ã€‚
Semantic ID prefix ngram creates semantically meaningful collisions by hierarchically clustering items based on their content embeddings, as opposed to random assignments.  
Semantic ID prefix ngramã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ãªå‰²ã‚Šå½“ã¦ã§ã¯ãªãã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åŸ‹ã‚è¾¼ã¿ã«åŸºã¥ã„ã¦ã‚¢ã‚¤ãƒ†ãƒ ã‚’éšå±¤çš„ã«ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€æ„å‘³çš„ã«æœ‰æ„ç¾©ãªè¡çªã‚’ç”Ÿæˆã—ã¾ã™ã€‚
Through extensive experimentation, we demonstrate that Semantic ID prefix ngram not only addresses embedding instability but also significantly improves tail id modeling, reduces overfitting, and mitigates representation shifts.  
åºƒç¯„ãªå®Ÿé¨“ã‚’é€šã˜ã¦ã€Semantic ID prefix ngramãŒåŸ‹ã‚è¾¼ã¿ã®ä¸å®‰å®šæ€§ã«å¯¾å‡¦ã™ã‚‹ã ã‘ã§ãªãã€ãƒ†ãƒ¼ãƒ«IDãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’å¤§å¹…ã«æ”¹å–„ã—ã€éå­¦ç¿’ã‚’æ¸›å°‘ã•ã›ã€è¡¨ç¾ã®ã‚·ãƒ•ãƒˆã‚’ç·©å’Œã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
We further highlight the advantages of Semantic ID prefix ngram in attention-based models that contextualize user histories, showing substantial performance improvements.  
ã•ã‚‰ã«ã€**ãƒ¦ãƒ¼ã‚¶ã®å±¥æ­´ã‚’æ–‡è„ˆåŒ–ã™ã‚‹attentionãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«**ã«ãŠã‘ã‚‹Semantic ID prefix ngramã®åˆ©ç‚¹ã‚’å¼·èª¿ã—ã€å®Ÿè³ªçš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å‘ä¸Šã‚’ç¤ºã—ã¾ã™ã€‚
We also report our experience of integrating Semantic ID into Meta production Ads Ranking system, leading to notable performance gains and enhanced prediction stability in live deployments.  
ã¾ãŸã€Semantic IDã‚’Metaã®è£½å“åºƒå‘Šãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã«çµ±åˆã—ãŸçµŒé¨“ã‚’å ±å‘Šã—ã€ãƒ©ã‚¤ãƒ–å±•é–‹ã«ãŠã‘ã‚‹é¡•è‘—ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã¨äºˆæ¸¬ã®å®‰å®šæ€§ã®å‘ä¸Šã‚’ã‚‚ãŸã‚‰ã—ã¾ã—ãŸã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## 1Introduction 1 ã¯ã˜ã‚ã«

Item recommendation can involve many signal-rich features, including categorical features corresponding to item IDs. 
ã‚¢ã‚¤ãƒ†ãƒ æ¨è–¦ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ IDã«å¯¾å¿œã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´ã‚’å«ã‚€ã€å¤šãã®ä¿¡å·è±Šå¯Œãªç‰¹å¾´ã‚’å«ã‚€ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚
The raw item IDs are usually mapped to embeddings, which are then further processed by deep learning-based model architectures such as the widely deployed Deep Learning Recommendation Model (DLRM)(Covington etal.,2016; Naumov etal.,2019). 
**ç”Ÿã®ã‚¢ã‚¤ãƒ†ãƒ IDã¯é€šå¸¸ã€åŸ‹ã‚è¾¼ã¿ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œ**(=entity embedding)ã€ãã®å¾Œã€åºƒãå±•é–‹ã•ã‚Œã¦ã„ã‚‹æ·±å±¤å­¦ç¿’æ¨è–¦ãƒ¢ãƒ‡ãƒ«ï¼ˆDLRMï¼‰ï¼ˆCovington etal.,2016; Naumov etal.,2019ï¼‰ãªã©ã®æ·±å±¤å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã£ã¦ã•ã‚‰ã«å‡¦ç†ã•ã‚Œã¾ã™ã€‚
However, in industry-scale online settings, several key data-related challenges have emerged in learning item embedding representations. 
ã—ã‹ã—ã€ç”£æ¥­è¦æ¨¡ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç’°å¢ƒã§ã¯ã€**ã‚¢ã‚¤ãƒ†ãƒ åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹éš›ã«ã„ãã¤ã‹ã®é‡è¦ãªãƒ‡ãƒ¼ã‚¿é–¢é€£ã®èª²é¡Œ**ãŒæµ®ä¸Šã—ã¦ã„ã¾ã™ã€‚
In particular, item cardinality, the huge number of total items; impression skew, the fact that only a few items comprise most user impressions or conversions(MilojeviÄ‡,2010); and ID drifting, or the majority of items entering and leaving the system within short time periods(Gama etal.,2014). 
ç‰¹ã«ã€ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ï¼ˆç·ã‚¢ã‚¤ãƒ†ãƒ æ•°ã®å·¨å¤§ã•ï¼‰ã€ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®åã‚Šï¼ˆã»ã¨ã‚“ã©ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã‚„ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å ã‚ã‚‹ã®ã¯ã”ãå°‘æ•°ã®ã‚¢ã‚¤ãƒ†ãƒ ã§ã‚ã‚‹äº‹å®Ÿï¼ˆMilojeviÄ‡,2010ï¼‰ï¼‰ã€ãŠã‚ˆã³IDã®æ¼‚æµï¼ˆçŸ­æœŸé–“ã«ã‚·ã‚¹ãƒ†ãƒ ã«å‡ºå…¥ã‚Šã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®å¤§å¤šæ•°ï¼‰ï¼ˆGama etal.,2014ï¼‰ã§ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

A popular and simple approach to learning embedding representations is random hashing, where raw item IDs are randomly hashed to share the same embedding(Zhang etal.,2020). 
åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã®ä¸€èˆ¬çš„ã§ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ã§ã‚ã‚Šã€ç”Ÿã®ã‚¢ã‚¤ãƒ†ãƒ IDãŒãƒ©ãƒ³ãƒ€ãƒ ã«ãƒãƒƒã‚·ãƒ¥ã•ã‚Œã¦åŒã˜åŸ‹ã‚è¾¼ã¿ã‚’å…±æœ‰ã—ã¾ã™ï¼ˆZhang etal.,2020ï¼‰ã€‚
Hashing is used due to the large item cardinality and system constraints on embedding table sizes. 
ãƒãƒƒã‚·ãƒ³ã‚°ã¯ã€å¤§ããªã‚¢ã‚¤ãƒ†ãƒ ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã¨åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºã«å¯¾ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã®åˆ¶ç´„ã®ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
However, random hashing and ID drifting together lead to undesirable embedding representation instability when the model is trained over long time periods. 
ã—ã‹ã—ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ã¨IDã®æ¼‚æµãŒçµ„ã¿åˆã‚ã•ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ãŒé•·æœŸé–“ã«ã‚ãŸã£ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã‚‹éš›ã«æœ›ã¾ã—ããªã„åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã®ä¸å®‰å®šæ€§ã‚’å¼•ãèµ·ã“ã—ã¾ã™ã€‚
This is due to the nature of random hash collisions, which result in contradictory gradient updates to the embedding weights. 
ã“ã‚Œã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ¥ã®è¡çªã®æ€§è³ªã«ã‚ˆã‚‹ã‚‚ã®ã§ã€åŸ‹ã‚è¾¼ã¿é‡ã¿ã¸ã®çŸ›ç›¾ã—ãŸå‹¾é…æ›´æ–°ã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚
Further, as the items in the system change over time from ID drifting, the learning from old items is lost and the embedding weights for new items are essentially random. 
ã•ã‚‰ã«ã€ã‚·ã‚¹ãƒ†ãƒ å†…ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒIDã®æ¼‚æµã«ã‚ˆã‚Šæ™‚é–“ã¨ã¨ã‚‚ã«å¤‰åŒ–ã™ã‚‹ã«ã¤ã‚Œã¦ã€å¤ã„ã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰ã®å­¦ç¿’ãŒå¤±ã‚ã‚Œã€æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ã®åŸ‹ã‚è¾¼ã¿é‡ã¿ã¯æœ¬è³ªçš„ã«ãƒ©ãƒ³ãƒ€ãƒ ã«ãªã‚Šã¾ã™ã€‚
This approach is ill-suited for items with few impressions, which are the majority of items due to impression skew. 
ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®åã‚Šã«ã‚ˆã‚Šå¤§å¤šæ•°ã‚’å ã‚ã‚‹ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®å°‘ãªã„ã‚¢ã‚¤ãƒ†ãƒ ã«ã¯ä¸é©åˆ‡ã§ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ã¯ã¾ã ã‚ˆãã‚ã‹ã£ã¦ãªã„ã‘ã© -->

To mitigate these drawbacks, a stable ID space is needed. 
**ã“ã‚Œã‚‰ã®æ¬ ç‚¹ã‚’è»½æ¸›ã™ã‚‹ãŸã‚ã«ã¯ã€å®‰å®šã—ãŸIDç©ºé–“ãŒå¿…è¦**ã§ã™ã€‚
A stable ID space ideally ensures that a learned embedding representation has a stable meaning as the model learns from more data. 
ç†æƒ³çš„ãªå®‰å®šã—ãŸIDç©ºé–“ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã™ã‚‹ã«ã¤ã‚Œã¦ã€å­¦ç¿’ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿è¡¨ç¾ãŒå®‰å®šã—ãŸæ„å‘³ã‚’æŒã¤ã“ã¨ã‚’ä¿è¨¼ã—ã¾ã™ã€‚
In this work, we investigate a recently proposed item representation approach called Semantic ID(Singh etal.,2023; Rajput etal.,2024) as a candidate for a stable ID space. 
**æœ¬ç ”ç©¶ã§ã¯ã€å®‰å®šã—ãŸIDç©ºé–“ã®å€™è£œã¨ã—ã¦ã€Semantic IDï¼ˆSingh etal.,2023; Rajput etal.,2024ï¼‰ã¨å‘¼ã°ã‚Œã‚‹æœ€è¿‘ææ¡ˆã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ è¡¨ç¾ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’èª¿æŸ»**ã—ã¾ã™ã€‚
Semantic ID derives item IDs based on hierarchical clusters learned from the semantic similarity of items as given by their text, image, or video content. 
Semantic IDã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€ã¾ãŸã¯ãƒ“ãƒ‡ã‚ªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ã‚ˆã£ã¦ä¸ãˆã‚‰ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã®æ„å‘³çš„é¡ä¼¼æ€§ã‹ã‚‰å­¦ç¿’ã•ã‚ŒãŸéšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ã«åŸºã¥ã„ã¦ã‚¢ã‚¤ãƒ†ãƒ IDã‚’å°å‡ºã—ã¾ã™ã€‚
A given itemâ€™s Semantic ID is then mapped to embedding representations via a parameterization scheme. 
ç‰¹å®šã®ã‚¢ã‚¤ãƒ†ãƒ ã®Semantic IDã¯ã€**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã‚¹ã‚­ãƒ¼ãƒ ã‚’ä»‹ã—ã¦åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã«ãƒãƒƒãƒ”ãƒ³ã‚°**ã•ã‚Œã¾ã™ã€‚(æ„å‘³ã‚ˆãã‚ã‹ã£ã¦ãªã„...!:thinking:)
Importantly, the ID space of Semantic ID is fixed a priori and has semantic meaning â€“ meaning that it can address embedding representation instability. 
é‡è¦ãªã“ã¨ã«ã€Semantic IDã®IDç©ºé–“ã¯äº‹å‰ã«å›ºå®šã•ã‚Œã¦ãŠã‚Šã€æ„å‘³çš„ãªæ„å‘³ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã¤ã¾ã‚Šã€åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã®ä¸å®‰å®šæ€§ã«å¯¾å‡¦ã§ãã‚‹ã¨ã„ã†ã“ã¨ã§ã™ã€‚(?)
However, one challenge in using Semantic ID in recommendation modeling is defining a mapping from its cluster assignments to the embedding table. 
ã—ã‹ã—ã€æ¨è–¦ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ãŠã‘ã‚‹Semantic IDã®ä½¿ç”¨ã«ãŠã‘ã‚‹ä¸€ã¤ã®èª²é¡Œã¯ã€ãã®ã‚¯ãƒ©ã‚¹ã‚¿å‰²ã‚Šå½“ã¦ã‹ã‚‰åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å®šç¾©ã™ã‚‹ã“ã¨ã§ã™ã€‚

The main contributions of this paper are: 
æœ¬è«–æ–‡ã®ä¸»ãªè²¢çŒ®ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š

- Using experiments on a simplified version of Metaâ€™s production ads ranking model, we deepen the empirical understanding of how Semantic ID improves embedding representation stability. 
Metaã®è£½å“åºƒå‘Šãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®ç°¡ç•¥åŒ–ã•ã‚ŒãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«é–¢ã™ã‚‹å®Ÿé¨“ã‚’ä½¿ç”¨ã—ã¦ã€**Semantic IDãŒåŸ‹ã‚è¾¼ã¿è¡¨ç¾ã®å®‰å®šæ€§ã‚’ã©ã®ã‚ˆã†ã«æ”¹å–„ã™ã‚‹ã‹ã«ã¤ã„ã¦ã®çµŒé¨“çš„ç†è§£**ã‚’æ·±ã‚ã¾ã™ã€‚
We further propose Semantic ID prefix-ngram, a novel token parameterization technique on top of Semantic ID that brings significant performance gains compared to the original Semantic ID introduced in(Singh etal.,2023). 
ã•ã‚‰ã«ã€Semantic IDã®ä¸Šã«æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–æŠ€è¡“ã§ã‚ã‚‹Semantic ID prefix-ngramã‚’ææ¡ˆã—ã€ï¼ˆSingh etal.,2023ï¼‰ã§å°å…¥ã•ã‚ŒãŸå…ƒã®Semantic IDã¨æ¯”è¼ƒã—ã¦é‡è¦ãªæ€§èƒ½å‘ä¸Šã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚

- We characterize the item data distribution in terms of the number of items (item cardinality), the fact that most items have few impressions (impression skew), and the short item lifetime in the system (ID drifting) and explain their connection with embedding representation stability. 
ã‚¢ã‚¤ãƒ†ãƒ ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‚’ã€ã‚¢ã‚¤ãƒ†ãƒ ã®æ•°ï¼ˆã‚¢ã‚¤ãƒ†ãƒ ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ï¼‰ã€ã»ã¨ã‚“ã©ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒå°‘ãªã„ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã‚’æŒã¤äº‹å®Ÿï¼ˆã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®åã‚Šï¼‰ã€ãŠã‚ˆã³ã‚·ã‚¹ãƒ†ãƒ å†…ã®çŸ­ã„ã‚¢ã‚¤ãƒ†ãƒ ãƒ©ã‚¤ãƒ•ã‚¿ã‚¤ãƒ ï¼ˆIDæ¼‚æµï¼‰ã®è¦³ç‚¹ã‹ã‚‰ç‰¹å¾´ä»˜ã‘ã€ãã‚Œã‚‰ã®åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã®å®‰å®šæ€§ã¨ã®é–¢ä¿‚ã‚’èª¬æ˜ã—ã¾ã™ã€‚

- We describe the productionization of Semantic ID prefix-ngram into both sparse and sequential features in Metaâ€™s production system. 
Semantic ID prefix-ngramã‚’Metaã®è£½å“ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹ã‚¹ãƒ‘ãƒ¼ã‚¹ãŠã‚ˆã³é€æ¬¡ç‰¹å¾´ã®ä¸¡æ–¹ã«æœ¬ç•ªæ´»ç”¨(productionization)ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚
We show that adding these features brings online performance gains and improves online prediction stability. 
ã“ã‚Œã‚‰ã®ç‰¹å¾´ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å‘ä¸ŠãŒå¾—ã‚‰ã‚Œã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã®å®‰å®šæ€§ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

In offline experiments on Metaâ€™s ads ranking data, we show that Semantic ID improves generalization and is less sensitive to distribution shift compared to random hashing. 
Metaã®åºƒå‘Šãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å®Ÿé¨“ã§ã¯ã€Semantic IDãŒä¸€èˆ¬åŒ–ã‚’æ”¹å–„ã—ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ã¨æ¯”è¼ƒã—ã¦åˆ†å¸ƒã®ã‚·ãƒ•ãƒˆã«å¯¾ã—ã¦æ•æ„Ÿã§ãªã„ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
Confirming our hypothesis on impression skew, we find that most gains from Semantic ID come from the long tail of the item distribution. 
ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®åã‚Šã«é–¢ã™ã‚‹ç§ãŸã¡ã®ä»®èª¬ã‚’ç¢ºèªã—ã€Semantic IDã‹ã‚‰ã®ã»ã¨ã‚“ã©ã®åˆ©ç›ŠãŒã‚¢ã‚¤ãƒ†ãƒ åˆ†å¸ƒã®ãƒ­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ«ã‹ã‚‰æ¥ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚
We show that by incorporating hierarchical cluster information, the proposed prefix-ngram is crucial to Semantic IDâ€™s effectiveness. 
éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿æƒ…å ±ã‚’å–ã‚Šå…¥ã‚Œã‚‹ã“ã¨ã§ã€ææ¡ˆã•ã‚ŒãŸprefix-ngramãŒSemantic IDã®åŠ¹æœã«ã¨ã£ã¦é‡è¦ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
We also demonstrate that semantic similarity translates to prediction similarity in both online and offline settings (Section 6.3 and Section 7.4). 
ã¾ãŸã€æ„å‘³çš„é¡ä¼¼æ€§ãŒã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãŠã‚ˆã³ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã®ä¸¡æ–¹ã®è¨­å®šã«ãŠã„ã¦äºˆæ¸¬ã®é¡ä¼¼æ€§ã«å¤‰æ›ã•ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³6.3ãŠã‚ˆã³ã‚»ã‚¯ã‚·ãƒ§ãƒ³7.4ï¼‰ã€‚
Further, Semantic ID results in outsized gains when incorporated in contextualizing models of usersâ€™ item interaction history. 
ã•ã‚‰ã«ã€**Semantic IDã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´ã‚’æ–‡è„ˆåŒ–ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã«çµ„ã¿è¾¼ã¾ã‚Œã‚‹ã¨ã€éå¤§ãªåˆ©ç›Šã‚’ã‚‚ãŸã‚‰ã—ã¾ã™**ã€‚

In an online setting, we describe the implementation of Semantic ID prefix-ngram features in Metaâ€™s production ads recommendation system, where they serve as the top sparse features by feature importance and result in 0.15% online performance gain. 
ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç’°å¢ƒã§ã¯ã€Metaã®è£½å“åºƒå‘Šæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹Semantic ID prefix-ngramç‰¹å¾´ã®å®Ÿè£…ã‚’èª¬æ˜ã—ã¾ã™ã€‚ã“ã“ã§ã¯ã€ç‰¹å¾´ã®é‡è¦æ€§ã«ã‚ˆã£ã¦ãƒˆãƒƒãƒ—ã‚¹ãƒ‘ãƒ¼ã‚¹ç‰¹å¾´ã¨ã—ã¦æ©Ÿèƒ½ã—ã€0.15%ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚
Finally, we find that incorporating Semantic ID features significantly reduces the modelâ€™s prediction variance for the same item. 
æœ€å¾Œã«ã€Semantic IDç‰¹å¾´ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€åŒã˜ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬åˆ†æ•£ãŒå¤§å¹…ã«æ¸›å°‘ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚
This is crucial to ensure advertisersâ€™ trust in Metaâ€™s recommendation system and to improve stability of the final item ranking. 
ã“ã‚Œã¯ã€åºƒå‘Šä¸»ãŒMetaã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚’ä¿¡é ¼ã™ã‚‹ã“ã¨ã‚’ä¿è¨¼ã—ã€æœ€çµ‚çš„ãªã‚¢ã‚¤ãƒ†ãƒ ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®å®‰å®šæ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«é‡è¦ã§ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

The remaining sections are organized as follows: 
æ®‹ã‚Šã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ï¼š
Section 2 explains related work. 
ã‚»ã‚¯ã‚·ãƒ§ãƒ³2ã§ã¯é–¢é€£ç ”ç©¶ã‚’èª¬æ˜ã—ã¾ã™ã€‚
Section 3 provides an overview of the ranking model. 
ã‚»ã‚¯ã‚·ãƒ§ãƒ³3ã§ã¯ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®æ¦‚è¦ã‚’æä¾›ã—ã¾ã™ã€‚
Section 4 introduces Semantic ID and token parameterization. 
ã‚»ã‚¯ã‚·ãƒ§ãƒ³4ã§ã¯Semantic IDã¨ãƒˆãƒ¼ã‚¯ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚
Section 5 explains the three item impression distribution challenges. 
ã‚»ã‚¯ã‚·ãƒ§ãƒ³5ã§ã¯3ã¤ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³åˆ†å¸ƒã®èª²é¡Œã‚’èª¬æ˜ã—ã¾ã™ã€‚
Section 6 describes the offline experiments. 
ã‚»ã‚¯ã‚·ãƒ§ãƒ³6ã§ã¯ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å®Ÿé¨“ã‚’èª¬æ˜ã—ã¾ã™ã€‚
Section 7 describes the productionization of Semantic ID at Meta and the online experiments. 
ã‚»ã‚¯ã‚·ãƒ§ãƒ³7ã§ã¯Metaã«ãŠã‘ã‚‹Semantic IDã®ç”Ÿç”£åŒ–ã¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å®Ÿé¨“ã‚’èª¬æ˜ã—ã¾ã™ã€‚
Section 8 concludes. 
ã‚»ã‚¯ã‚·ãƒ§ãƒ³8ã§çµè«–ã‚’è¿°ã¹ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## 2Related Work 2é–¢é€£ç ”ç©¶

#### Item representations in recommendation æ¨è–¦ã«ãŠã‘ã‚‹ã‚¢ã‚¤ãƒ†ãƒ è¡¨ç¾

Many modern deep learning recommendation models use trained embeddings to represent categorical (â€œsparseâ€) features(Covington etal.,2016; Naumov etal.,2019; Naumov,2019). 
**å¤šãã®ç¾ä»£ã®æ·±å±¤å­¦ç¿’æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã¯ã€è¨“ç·´ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã—ã¦ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ï¼ˆã€Œã‚¹ãƒ‘ãƒ¼ã‚¹ã€ï¼‰ç‰¹å¾´ã‚’è¡¨ç¾ã—ã¾ã™**ï¼ˆCovington etal.,2016; Naumov etal.,2019; Naumov,2019ï¼‰ã€‚(=ã“ã‚ŒãŒã„ã‚ã‚†ã‚‹entity embedding!:thinking:)
A simple solution to high item cardinality is to use random hashing(Weinberger etal.,2009), but random hash collisions can be undesirable. 
é«˜ã„ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã«å¯¾ã™ã‚‹ç°¡å˜ãªè§£æ±ºç­–ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ï¼ˆWeinberger etal.,2009ï¼‰ãŒã€ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ¥ã®è¡çªã¯æœ›ã¾ã—ããªã„å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
One option is to modify the hashing procedure. 
1ã¤ã®é¸æŠè‚¢ã¯ã€ãƒãƒƒã‚·ãƒ³ã‚°æ‰‹é †ã‚’ä¿®æ­£ã™ã‚‹ã“ã¨ã§ã™ã€‚
Under this category, collision-free hashing(Liu etal.,2022)introduces individual embeddings for each item by dynamically free the memory of embeddings for retired items.  
ã“ã®ã‚«ãƒ†ã‚´ãƒªã®ä¸‹ã§ã¯ã€collision-free hashingï¼ˆLiu etal.,2022ï¼‰ãŒã€**é€€å½¹ã‚¢ã‚¤ãƒ†ãƒ ã®åŸ‹ã‚è¾¼ã¿ã®ãƒ¡ãƒ¢ãƒªã‚’å‹•çš„ã«è§£æ”¾ã™ã‚‹ã“ã¨**ã«ã‚ˆã£ã¦ã€å„ã‚¢ã‚¤ãƒ†ãƒ ã®å€‹åˆ¥ã®åŸ‹ã‚è¾¼ã¿ã‚’å°å…¥ã—ã¾ã™ã€‚(ã†ã‚“ã€ç‰¹ã«ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã ã¨retired itemsã‚ã‚‹ã‚ˆãªã...:thinking:)
Double hashing(Zhang etal.,2020)utilizes two independent hash functions to reduce memory usage, but still has random collision. 
Double hashingï¼ˆZhang etal.,2020ï¼‰ã¯ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã«2ã¤ã®ç‹¬ç«‹ã—ãŸãƒãƒƒã‚·ãƒ¥é–¢æ•°ã‚’åˆ©ç”¨ã—ã¾ã™ãŒã€ä¾ç„¶ã¨ã—ã¦ãƒ©ãƒ³ãƒ€ãƒ ãªè¡çªãŒã‚ã‚Šã¾ã™ã€‚
Learning to hash methods(Wang etal.,2017)focus on similarity preserving by training ML-based hash functions. 
Learning to hashæ‰‹æ³•ï¼ˆWang etal.,2017ï¼‰ã¯ã€MLãƒ™ãƒ¼ã‚¹ã®ãƒãƒƒã‚·ãƒ¥é–¢æ•°ã‚’è¨“ç·´ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦é¡ä¼¼æ€§ã‚’ä¿æŒã™ã‚‹ã“ã¨ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚
There have also been works that address impression skew through contrastive learning or clustering(Yao etal.,2021; Chang etal.,2024); we view these as complementary approaches. 
å¯¾ç…§å­¦ç¿’ã‚„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’é€šã˜ã¦å°è±¡ã®åã‚Šã«å¯¾å‡¦ã™ã‚‹ç ”ç©¶ã‚‚ã‚ã‚Šã¾ã™ï¼ˆYao etal.,2021; Chang etal.,2024ï¼‰ï¼›ç§ãŸã¡ã¯ã“ã‚Œã‚‰ã‚’è£œå®Œçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨è¦‹ãªã—ã¦ã„ã¾ã™ã€‚
We take a holistic approach of designing a stable ID space, to minimize the need for hashing and to address embedding representation shifting directly. 
ç§ãŸã¡ã¯ã€ãƒãƒƒã‚·ãƒ³ã‚°ã®å¿…è¦æ€§ã‚’æœ€å°é™ã«æŠ‘ãˆã€åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã®ã‚·ãƒ•ãƒˆã«ç›´æ¥å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€å®‰å®šã—ãŸIDç©ºé–“ã‚’è¨­è¨ˆã™ã‚‹åŒ…æ‹¬çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å–ã‚Šã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

#### Stable embedding representation å®‰å®šåŸ‹ã‚è¾¼ã¿è¡¨ç¾

Stable ID is inspired by tokenization approaches in NLP, which learn a fixed vocabulary of tokens to represent text in language modeling(Sennrich,2015; Kudo,2018; Devlin,2018). 
Stable IDã¯ã€NLPã«ãŠã‘ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«è§¦ç™ºã•ã‚Œã¦ãŠã‚Šã€è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ãŠã„ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¾ã™ã‚‹ãŸã‚ã®å›ºå®šã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®èªå½™ã‚’å­¦ç¿’ã—ã¾ã™(Sennrich,2015; Kudo,2018; Devlin,2018)ã€‚
In designing a tokenization scheme for item recommendation, Hou et al. (2023) proposes to vector-quantize the embeddings learned from an item content understanding model; 
ã‚¢ã‚¤ãƒ†ãƒ æ¨è–¦ã®ãŸã‚ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚¹ã‚­ãƒ¼ãƒ ã‚’è¨­è¨ˆã™ã‚‹ã«ã‚ãŸã‚Šã€Hou et al. (2023)ã¯ã‚¢ã‚¤ãƒ†ãƒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç†è§£ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å­¦ç¿’ã—ãŸåŸ‹ã‚è¾¼ã¿ã‚’ãƒ™ã‚¯ãƒˆãƒ«é‡å­åŒ–ã™ã‚‹ã“ã¨ã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚
Qu et al. (2024) introduce a masked vector-quantizer to transfer the learned representations from collaborative filtering models to a generative recommender. 
Qu et al. (2024)ã¯ã€å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç”Ÿæˆçš„ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã¸ã®å­¦ç¿’ã—ãŸè¡¨ç¾ã‚’è»¢é€ã™ã‚‹ãŸã‚ã«ã€ãƒã‚¹ã‚¯ä»˜ããƒ™ã‚¯ãƒˆãƒ«é‡å­åŒ–å™¨ã‚’å°å…¥ã—ã¦ã„ã¾ã™ã€‚
Semantic ID is introduced concurrently in (Singh et al., 2023; Rajput et al., 2024), which is based on (Hou et al., 2023) and uses an RQ-VAE for quantization, showing its benefits in generalization performance and sequential recommendation, respectively. 
Semantic IDã¯(Singh et al., 2023; Rajput et al., 2024)ã§åŒæ™‚ã«å°å…¥ã•ã‚Œã¦ãŠã‚Šã€(Hou et al., 2023)ã«åŸºã¥ã„ã¦ãŠã‚Šã€é‡å­åŒ–ã®ãŸã‚ã«RQ-VAEã‚’ä½¿ç”¨ã—ã€ä¸€èˆ¬åŒ–æ€§èƒ½ã¨é€æ¬¡æ¨è–¦ã«ãŠã‘ã‚‹ãã®åˆ©ç‚¹ã‚’ãã‚Œãã‚Œç¤ºã—ã¦ã„ã¾ã™ã€‚
In this work, we adapt Semantic ID as our stable ID method and analyze its effectiveness in addressing the three challenges in online item recommendation. 
æœ¬ç ”ç©¶ã§ã¯ã€Semantic IDã‚’ç§ãŸã¡ã®å®‰å®šIDæ‰‹æ³•ã¨ã—ã¦é©å¿œã•ã›ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¢ã‚¤ãƒ†ãƒ æ¨è–¦ã«ãŠã‘ã‚‹3ã¤ã®èª²é¡Œã«å¯¾å‡¦ã™ã‚‹åŠ¹æœã‚’åˆ†æã—ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## 3Ranking Model Overview 3 ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®æ¦‚è¦

The recommendation problem is posed as a classification task, where a data point is the user- and item-side features associated with an item impression or conversion and a binary label indicating whether or not the user interacted or converted for that item. 
æ¨è–¦å•é¡Œã¯åˆ†é¡ã‚¿ã‚¹ã‚¯ã¨ã—ã¦å®šå¼åŒ–ã•ã‚Œã€ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã¯ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã¾ãŸã¯ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«é–¢é€£ã™ã‚‹ãƒ¦ãƒ¼ã‚¶å´ãŠã‚ˆã³ã‚¢ã‚¤ãƒ†ãƒ å´ã®ç‰¹å¾´ã¨ã€ãã®ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã—ã¦ãƒ¦ãƒ¼ã‚¶ãŒç›¸äº’ä½œç”¨ã—ãŸã‹ã©ã†ã‹ã‚’ç¤ºã™ãƒã‚¤ãƒŠãƒªãƒ©ãƒ™ãƒ«ã§æ§‹æˆã•ã‚Œã¾ã™ã€‚(ã†ã‚“ã†ã‚“ã€å…¸å‹çš„ãªbandit feedbackã®æ§‹é€ ã :thinking:)
We now give a brief overview of the ranking model architecture.
ã“ã“ã§ã¯ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã¤ã„ã¦ç°¡å˜ã«æ¦‚èª¬ã—ã¾ã™ã€‚

### 3.1 Model ãƒ¢ãƒ‡ãƒ«

(DLRMã£ã¦ä¸»è¦ãªæ–¹æ³•è«–ãªã®ã‹ãªã€‚åˆã‚ã¦èã„ãŸ...!:thinking:)

The recommendation system follows a deep neural architecture based on the DLRM (Covington et al., 2016; Naumov et al., 2019). 
æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¯ã€DLRMï¼ˆCovington et al., 2016; Naumov et al., 2019ï¼‰ã«åŸºã¥ãæ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«å¾“ã„ã¾ã™ã€‚
The model consists of three stacked sections. 
ãƒ¢ãƒ‡ãƒ«ã¯ã€3ã¤ã®ã‚¹ã‚¿ãƒƒã‚¯ã•ã‚ŒãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚
First is the information aggregation section, where the sparse (i.e., categorical), dense, and user history-based features are processed independently. 
æœ€åˆã¯æƒ…å ±é›†ç´„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã€ã“ã“ã§ã¯ã‚¹ãƒ‘ãƒ¼ã‚¹ï¼ˆã™ãªã‚ã¡ã€ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ï¼‰ã€ãƒ‡ãƒ³ã‚¹ã€ãŠã‚ˆã³ãƒ¦ãƒ¼ã‚¶å±¥æ­´ã«åŸºã¥ãç‰¹å¾´ãŒç‹¬ç«‹ã—ã¦å‡¦ç†ã•ã‚Œã¾ã™ã€‚
The output of each of these modules is a list of embedding vectors. 
ã“ã‚Œã‚‰ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å‡ºåŠ›ã¯ã€åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆã§ã™ã€‚
Second, these are concatenated into a single list which goes through the interaction layer, where dot products (or higher order interactions) are taken between all pairs of vectors. 
æ¬¡ã«ã€ã“ã‚Œã‚‰ã¯1ã¤ã®ãƒªã‚¹ãƒˆã«é€£çµã•ã‚Œã€ç›¸äº’ä½œç”¨å±¤ã‚’é€šéã—ã¾ã™ã€‚ã“ã“ã§ã¯ã€ã™ã¹ã¦ã®ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒšã‚¢é–“ã§ãƒ‰ãƒƒãƒˆç©ï¼ˆã¾ãŸã¯é«˜æ¬¡ã®ç›¸äº’ä½œç”¨ï¼‰ãŒè¨ˆç®—ã•ã‚Œã¾ã™ã€‚
Third, the output of the interaction layer is transformed via an MLP to produce the logit score and a sigmoid is taken to output a probability. 
ç¬¬ä¸‰ã«ã€ç›¸äº’ä½œç”¨å±¤ã®å‡ºåŠ›ã¯MLPã‚’ä»‹ã—ã¦å¤‰æ›ã•ã‚Œã€ãƒ­ã‚¸ãƒƒãƒˆã‚¹ã‚³ã‚¢ãŒç”Ÿæˆã•ã‚Œã€ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰ãŒå–ã‚‰ã‚Œã¦ç¢ºç‡ãŒå‡ºåŠ›ã•ã‚Œã¾ã™ã€‚
The model is trained using cross-entropy loss. 
ãƒ¢ãƒ‡ãƒ«ã¯ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã‚’ä½¿ç”¨ã—ã¦è¨“ç·´ã•ã‚Œã¾ã™ã€‚(=ã¤ã¾ã‚Š2å€¤åˆ†é¡ã‚¿ã‚¹ã‚¯ãŒä»£ç†å­¦ç¿’å•é¡Œãªã®ã‹ãª...!:thinking:)
In the remainder of the paper we focus on the information aggregation section of the model. 
è«–æ–‡ã®æ®‹ã‚Šã®éƒ¨åˆ†ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±é›†ç´„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

#### Embedding module åŸ‹ã‚è¾¼ã¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

Let $I$ be the total number of raw IDs in the system and let $[1..N]$ denote the integers from $1$ to $N$.
$I$ã‚’ã‚·ã‚¹ãƒ†ãƒ å†…ã®ç”Ÿã®IDã®ç·æ•°ã¨ã—ã€$[1..N]$ã‚’$1$ã‹ã‚‰$N$ã¾ã§ã®æ•´æ•°ã¨ã—ã¾ã™ã€‚

The embedding table is a matrix $\mathbf{E} \in \mathbb{R}^{H \times d_{m}}$, where $d_{m}$ is the embedding dimension and $H$ is the number of embeddings.
åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã¯è¡Œåˆ— $\mathbf{E} \in \mathbb{R}^{H \times d_{m}}$ ã§ã‚ã‚Šã€$d_{m}$ ã¯åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã€$H$ ã¯åŸ‹ã‚è¾¼ã¿ã®æ•°ã§ã™ã€‚
Let $f=(f_{1},\dots,f_{G}):[1..I]\to[1..H]^{G}$ be an embedding lookup function that maps a raw ID to $G$ embedding table row indices.
$f=(f_{1},\dots,f_{G}):[1..I]\to[1..H]^{G}$ã‚’ã€ç”Ÿã®IDã‚’$G$å€‹ã®åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹åŸ‹ã‚è¾¼ã¿ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—é–¢æ•°ã¨ã—ã¾ã™ã€‚
Then for each raw ID $x \in [1..I]$, the sparse module looks up embedding rows $\mathbf{e}_{f_{1}}(x),\dots,\mathbf{e}_{f_{G}}(x)$ and produces a single output embedding via sum-pooling, $\mathbf{e}_{f}(x):=\sum_{i=1}^{G}\mathbf{e}_{f_{i}}(x)$.
æ¬¡ã«ã€å„ç”Ÿã®ID $x \in [1..I]$ã«ã¤ã„ã¦ã€ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯åŸ‹ã‚è¾¼ã¿è¡Œ$\mathbf{e}_{f_{1}}(x),\dots,\mathbf{e}_{f_{G}}(x)$ã‚’å‚ç…§ã—ã€åˆè¨ˆãƒ—ãƒ¼ãƒªãƒ³ã‚°ã‚’ä»‹ã—ã¦å˜ä¸€ã®å‡ºåŠ›åŸ‹ã‚è¾¼ã¿ $\mathbf{e}_{f}(x):=\sum_{i=1}^{G}\mathbf{e}_{f_{i}}(x)$ ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

$$
\mathbf{e}_{f}(x):=\sum_{i=1}^{G}\mathbf{e}_{f_{i}}(x)
$$

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

#### Sparse module ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

A sparse feature is a set $\mathbf{x}:=\{x_{1},\dots,x_{n}\}$ of raw IDs. 
ã‚¹ãƒ‘ãƒ¼ã‚¹ç‰¹å¾´ã¯ã€ç”Ÿã®IDã®é›†åˆ $\mathbf{x}:=\{x_{1},\dots,x_{n}\}$ ã§ã™ã€‚
For instance, this could be a set of $n$ product category IDs a given item belongs to. 
ä¾‹ãˆã°ã€ã“ã‚Œã¯**ç‰¹å®šã®ã‚¢ã‚¤ãƒ†ãƒ ãŒå±ã™ã‚‹ $n$ å€‹ã®è£½å“ã‚«ãƒ†ã‚´ãƒªIDã®é›†åˆ**ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
We usually produce a single embedding $\mathbf{e}_{f}(\mathbf{x})$ by sum-pooling embeddings $\mathbf{e}_{f}(x_{i})$ for constituent raw IDs. 
ç§ãŸã¡ã¯é€šå¸¸ã€æ§‹æˆã™ã‚‹ç”Ÿã®IDã®åŸ‹ã‚è¾¼ã¿ $\mathbf{e}_{f}(x_{i})$ ã‚’**åˆè¨ˆãƒ—ãƒ¼ãƒªãƒ³ã‚°**ã—ã¦ã€å˜ä¸€ã®åŸ‹ã‚è¾¼ã¿ $\mathbf{e}_{f}(\mathbf{x})$ ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

#### User history module ãƒ¦ãƒ¼ã‚¶å±¥æ­´ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

We model a userâ€™s item interaction history as a sequence of sparse features ğ±u:=(ğ±1u,â€¦,ğ±Tu) assigns superscript ğ±ğ‘¢ superscript subscript ğ±1ğ‘¢â€¦ superscript subscript ğ±ğ‘‡ğ‘¢ 
ç§ãŸã¡ã¯ã€ãƒ¦ãƒ¼ã‚¶ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´ã‚’ã‚¹ãƒ‘ãƒ¼ã‚¹ç‰¹å¾´ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨ã—ã¦ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ $\mathbf{x}^{u}:=(\mathbf{x}_{1}^{u},\dots,\mathbf{x}_{T}^{u})$ã€‚
and the corresponding interaction timestamps. 
ãŠã‚ˆã³å¯¾å¿œã™ã‚‹ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã€‚
When working with these features, there are system constraints due to the number of items and the sequence length $T$. 
ã“ã‚Œã‚‰ã®ç‰¹å¾´ã‚’æ‰±ã†éš›ã«ã¯ã€**ã‚¢ã‚¤ãƒ†ãƒ ã®æ•°ã¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é•·ã• $T$ ã«ã‚ˆã‚‹ã‚·ã‚¹ãƒ†ãƒ åˆ¶ç´„**ãŒã‚ã‚Šã¾ã™ã€‚
We include item interaction history for up to three months, which brings the item cardinality for the model to process to over one billion. 
ç§ãŸã¡ã¯ã€**æœ€å¤§ä¸‰ãƒ¶æœˆã®ã‚¢ã‚¤ãƒ†ãƒ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´ã‚’å«ã‚ã¦ãŠã‚Šã€ãƒ¢ãƒ‡ãƒ«ãŒå‡¦ç†ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã¯10å„„ã‚’è¶…ãˆã¾ã™**ã€‚
It is important for the user history module to contextualize the sequence of features before they are further processed downstream. 
ãƒ¦ãƒ¼ã‚¶å±¥æ­´ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã€ã“ã‚Œã‚‰ã®ç‰¹å¾´ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æ–‡è„ˆåŒ–ã™ã‚‹ã“ã¨ã¯ã€ã•ã‚‰ãªã‚‹ä¸‹æµå‡¦ç†ã®å‰ã«é‡è¦ã§ã™ã€‚
We describe the architecture below. 
ä»¥ä¸‹ã«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’èª¬æ˜ã—ã¾ã™ã€‚

First, we use the sparse module to embed each sparse feature ğ±iusuperscriptsubscriptğ±ğ‘–ğ‘¢ 
ã¾ãšã€ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦å„ã‚¹ãƒ‘ãƒ¼ã‚¹ç‰¹å¾´ $\mathbf{x}_{i}^{u}$ ã‚’åŸ‹ã‚è¾¼ã¿ã¾ã™ã€‚
and obtain a learned timestamp embedding; the sum is ğfu(ğ±iu)superscriptsubscriptğğ‘“ğ‘¢superscriptsubscriptğ±ğ‘–ğ‘¢ 
å­¦ç¿’ã•ã‚ŒãŸã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—ã—ã€ãã®åˆè¨ˆã¯ $\mathbf{e}_{f}^{u}(\mathbf{x}_{i}^{u})$ ã§ã™ã€‚
Let ğ—=[ğfu(x1u);â€¦;ğfu(xTu)]âŠºâˆˆâ„TÃ—dm 
$\mathbf{X}=\left[\mathbf{e}_{f}^{u}(x_{1}^{u});\dots;\mathbf{e}_{f}^{u}(x_{T}^{u})\right]^{\intercal}\in\mathbb{R}^{T\times d_{m}}$ ã¨ã—ã¾ã™ã€‚
denote the resulting encoding. 
ã“ã‚Œã¯çµæœã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¤ºã—ã¾ã™ã€‚
We then contextualize this sequence of embeddings via an aggregation module. 
æ¬¡ã«ã€**é›†ç´„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä»‹ã—ã¦ã“ã®åŸ‹ã‚è¾¼ã¿ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æ–‡è„ˆåŒ–**ã—ã¾ã™ã€‚
We use one of the following three aggregation module architectures: Bypass, Transformer, and Pooled Multihead Attention (PMA), which are defined in Appendix A. 
æ¬¡ã®3ã¤ã®é›†ç´„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ã„ãšã‚Œã‹ã‚’ä½¿ç”¨ã—ã¾ã™ï¼šãƒã‚¤ãƒ‘ã‚¹ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã€ãŠã‚ˆã³ãƒ—ãƒ¼ãƒ«ã•ã‚ŒãŸãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆPMAï¼‰ã€‚ã“ã‚Œã‚‰ã¯ä»˜éŒ²Aã§å®šç¾©ã•ã‚Œã¦ã„ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### 3.2Metrics ãƒ¡ãƒˆãƒªã‚¯ã‚¹

#### Normalized Entropy æ­£è¦åŒ–ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼

We measure model performance by normalized entropy (NE), defined as the model cross-entropy divided by the cross-entropy from predicting the data mean frequency of positive labels. 
ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯ã€æ­£è¦åŒ–ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆNEï¼‰ã«ã‚ˆã£ã¦æ¸¬å®šã•ã‚Œã€ã“ã‚Œã¯ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’ã€æ­£ã®ãƒ©ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å¹³å‡é »åº¦ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã«ã‚ˆã‚‹ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã§å‰²ã£ãŸã‚‚ã®ã¨ã—ã¦å®šç¾©ã•ã‚Œã¾ã™ã€‚
The NE equation is
NEã®æ–¹ç¨‹å¼ã¯

$$
NE = \frac{-\frac{1}{N} \sum_{i=1}^{N} y_{i} \log(p_{i})}{-\frac{1}{N} \sum_{i=1}^{N} y_{i} \log(p)}
$$


where $N$ is the number of training examples, $y_{i} \in \{0,1\}$ is the label for example $i$, $p_{i}$ is the model prediction for example $i$, and $p = \frac{\sum_{i=1}^{N} y_{i}}{N}$. 
ã“ã“ã§ã€$N$ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¾‹ã®æ•°ã€$y_{i} \in \{0,1\}$ã¯ä¾‹$i$ã®ãƒ©ãƒ™ãƒ«ã€$p_{i}$ã¯ä¾‹$i$ã®ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã€$p = \frac{\sum_{i=1}^{N} y_{i}}{N}$ã§ã™ã€‚
Lower is better.
å€¤ãŒä½ã„ã»ã©è‰¯ã„ã§ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## 4Semantic ID and Parameterizations

The primary motivation for Semantic ID is to design an efficient clustering schema to represent items that allows knowledge sharing between items with shared semantics. 
Semantic IDã®ä¸»ãªå‹•æ©Ÿã¯ã€**å…±æœ‰ã•ã‚ŒãŸæ„å‘³ã‚’æŒã¤ã‚¢ã‚¤ãƒ†ãƒ é–“ã§çŸ¥è­˜ã‚’å…±æœ‰ã§ãã‚‹ã‚ˆã†ã«ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¡¨ç¾ã™ã‚‹åŠ¹ç‡çš„ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¹ã‚­ãƒ¼ãƒã‚’è¨­è¨ˆã™ã‚‹ã“ã¨**ã§ã™ã€‚
Intuitively, if we have hundreds of ads about pizza that different users clicked on, we would want an example involving one of the ads to be informed by the other adsâ€™ representations. 
**ç›´æ„Ÿçš„ã«è¨€ãˆã°ã€ç•°ãªã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¯ãƒªãƒƒã‚¯ã—ãŸãƒ”ã‚¶ã«é–¢ã™ã‚‹æ•°ç™¾ã®åºƒå‘ŠãŒã‚ã‚‹å ´åˆã€ç§ãŸã¡ã¯ãã®åºƒå‘Šã®ä¸€ã¤ã«é–¢ã™ã‚‹ä¾‹ãŒä»–ã®åºƒå‘Šã®è¡¨ç¾ã‹ã‚‰æƒ…å ±ã‚’å¾—ã‚‹ã“ã¨**ã‚’æœ›ã‚€ã§ã—ã‚‡ã†ã€‚
We craft the design of Semantic ID to potentially address the data-related challenges of item cardinality, impression skew, and ID drifting described in Section 5. 
ç§ãŸã¡ã¯ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³5ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã€ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®åã‚Šã€IDã®æ¼‚æµã¨ã„ã†ãƒ‡ãƒ¼ã‚¿é–¢é€£ã®èª²é¡Œã«å¯¾å‡¦ã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹Semantic IDã®è¨­è¨ˆã‚’è€ƒæ¡ˆã—ã¾ã™ã€‚
Compared to embedding representations based on random clusters, semantics-based representations will likely be more stable over time. 
**ãƒ©ãƒ³ãƒ€ãƒ ã‚¯ãƒ©ã‚¹ã‚¿ã«åŸºã¥ãåŸ‹ã‚è¾¼ã¿è¡¨ç¾ã¨æ¯”è¼ƒã—ã¦ã€æ„å‘³ã«åŸºã¥ãè¡¨ç¾ã¯æ™‚é–“ã¨ã¨ã‚‚ã«ã‚ˆã‚Šå®‰å®šã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™**ã€‚
Semantics-based clustering will also allow tail items to learn from more training examples. 
æ„å‘³ã«åŸºã¥ãã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¯ã€ãƒ†ãƒ¼ãƒ«ã‚¢ã‚¤ãƒ†ãƒ ãŒã‚ˆã‚Šå¤šãã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¾‹ã‹ã‚‰å­¦ã¶ã“ã¨ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚
The learning from items that have left the system can also be utilized, and embedding weights for new items do not have to be learned from scratch. 
**ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰é›¢ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰ã®å­¦ç¿’ã‚‚æ´»ç”¨ã§ãã€æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ã®åŸ‹ã‚è¾¼ã¿é‡ã¿ã¯ã‚¼ãƒ­ã‹ã‚‰å­¦ç¿’ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚** (ãã‚Œã“ãç”¨é€”ã¨ã—ã¦ã¯two-towerã§ã‚‚æº€ãŸã›ã‚‹æ„Ÿã˜ãªã®ã‹ãª...!:thinking:)
We investigate these hypotheses empirically in Section 6. 
ç§ãŸã¡ã¯ã€ã“ã‚Œã‚‰ã®ä»®èª¬ã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³6ã§å®Ÿè¨¼çš„ã«èª¿æŸ»ã—ã¾ã™ã€‚

First, we give an overview of Semantic ID in Section 4.1. 
ã¾ãšã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.1ã§Semantic IDã®æ¦‚è¦ã‚’èª¬æ˜ã—ã¾ã™ã€‚
We then describe token parameterization in Section 4.2. 
æ¬¡ã«ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.2ã§ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚
This step is crucial to incorporate Semantic ID into the recommendation model. 
ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã€Semantic IDã‚’æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã«çµ„ã¿è¾¼ã‚€ãŸã‚ã«é‡è¦ã§ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### 4.1 æ¦‚è¦

Semantic IDs are learned for items in two stages: first, apply a content understanding model to the itemsâ€™ text, image, or video to produce dense content embeddings. 
Semantic IDã¯ã€ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã—ã¦2æ®µéšã§å­¦ç¿’ã•ã‚Œã¾ã™ã€‚ã¾ãšã€ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€ã¾ãŸã¯å‹•ç”»ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç†è§£ãƒ¢ãƒ‡ãƒ«ã‚’é©ç”¨ã—ã¦ã€**å¯†ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„åŸ‹ã‚è¾¼ã¿**ã‚’ç”Ÿæˆã—ã¾ã™ã€‚ 
Then, train an RQ-VAE(Zeghidour etal.,2021)on the content embeddings to obtain a vector quantization for each item, which is represented as a sequence of coarse-to-fine discrete codes called the itemâ€™s Semantic ID.
æ¬¡ã«ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åŸ‹ã‚è¾¼ã¿ã«å¯¾ã—ã¦RQ-VAEï¼ˆZeghidour et al., 2021ï¼‰ã‚’è¨“ç·´ã—ã€å„ã‚¢ã‚¤ãƒ†ãƒ ã®**ãƒ™ã‚¯ãƒˆãƒ«é‡å­åŒ–**ã‚’å–å¾—ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ã‚¢ã‚¤ãƒ†ãƒ ã®Semantic IDã¨å‘¼ã°ã‚Œã‚‹ç²—ã‹ã‚‰ç´°ã¸ã®é›¢æ•£ã‚³ãƒ¼ãƒ‰ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨ã—ã¦è¡¨ã•ã‚Œã¾ã™ã€‚

(ã‚ã€ãƒ„ã‚¤ãƒ¼ãƒˆã§è¦‹ãŸã®ã¯ã€ãªã‚“ã§å¯†ãªåŸ‹ã‚è¾¼ã¿ã®ã¾ã¾ã§ã¯ãƒ€ãƒ¡ã§ã€ã‚ã–ã‚ã–é›¢æ•£åŒ–ã™ã‚‹ã‚“ã ã‚ã†? ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ä½•? ã£ã¦ã„ã†è©±ã ã£ãŸãª...!:thinking:)

![]()

Figure 1: The RQVAE model with L=3.
å›³1ï¼šL=3ã®RQ-VAEãƒ¢ãƒ‡ãƒ«ã€‚

Let $L$ be the number of layers (i.e., length of the sequence) and $K$ be the codebook size (i.e., number of clusters at each layer). 
$L$ ã‚’å±¤ã®æ•°ï¼ˆã™ãªã‚ã¡ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é•·ã•ï¼‰ã¨ã—ã€$K$ ã‚’ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã®ã‚µã‚¤ã‚ºï¼ˆã™ãªã‚ã¡ã€å„å±¤ã®ã‚¯ãƒ©ã‚¹ã‚¿æ•°, 1~Kã®ã‚¯ãƒ©ã‚¹ã‚¿ã«å„ã‚¢ã‚¤ãƒ†ãƒ ãŒæ‰€å±ã™ã‚‹ã£ã¦ã“ã¨ã‹...!:thinking:ï¼‰ã¨ã—ã¾ã™ã€‚
RQ-VAE consists of an encoder that maps the content embedding $\mathbf{x} \in \mathbb{R}^{D}$ to a continuous latent representation, $\mathbf{z} \in \mathbb{R}^{D^{\prime}}$, a residual quantizer that quantizes $\mathbf{z}$ into a series of discrete codes $\mathbf{c}:=(c_{1},\dots,c_{L}) \in K^{L}$, and a decoder that reconstructs $\mathbf{x}$ from $\mathbf{c}$. 
RQ-VAEã¯ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åŸ‹ã‚è¾¼ã¿$\mathbf{x} \in \mathbb{R}^{D}$ã‚’é€£ç¶šçš„ãªæ½œåœ¨è¡¨ç¾$\mathbf{z} \in \mathbb{R}^{D^{\prime}}$ ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã€$\mathbf{z}$ã‚’ä¸€é€£ã®é›¢æ•£ã‚³ãƒ¼ãƒ‰$\mathbf{c}:=(c_{1},\dots,c_{L}) \in K^{L}$ã«é‡å­åŒ–ã™ã‚‹æ®‹å·®é‡å­åŒ–å™¨ã€ãŠã‚ˆã³$\mathbf{c}$ã‹ã‚‰$\mathbf{x}$ã‚’å†æ§‹ç¯‰ã™ã‚‹ãƒ‡ã‚³ãƒ¼ãƒ€ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚
This is done by associating each layer $l$ with a codebook which is a set of $K$ vectors $\{\mathbf{v}^{l}_{k}\}_{k=1}^{K}$. 
ã“ã‚Œã¯ã€å„å±¤ $l$ ã‚’ $K$ å€‹ã®ãƒ™ã‚¯ãƒˆãƒ«é›†åˆ $\{\mathbf{v}^{l}_{k}\}_{k=1}^{K}$ ã«é–¢é€£ä»˜ã‘ã‚‹ã“ã¨ã«ã‚ˆã£ã¦è¡Œã‚ã‚Œã¾ã™ã€‚
The sequence of discrete codes is hierarchical: $c_{l}$ corresponds to the codebook vector $\mathbf{v}^{l}_{c_{l}}$ that approximates $\mathbf{r}_{l}$, the remaining residual from $\mathbf{z}$ after recursively applying the codebook vectors from layers $(l-1)$ to 1, i.e.,
é›¢æ•£ã‚³ãƒ¼ãƒ‰ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¯éšå±¤çš„ã§ã™: $c_{l}$ ã¯ã€$\mathbf{r}_{l}$ ã‚’è¿‘ä¼¼ã™ã‚‹ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ãƒ™ã‚¯ãƒˆãƒ« $\mathbf{v}^{l}_{c_{l}}$ ã«å¯¾å¿œã—ã€ã“ã‚Œã¯ $\mathbf{z}$ ã‹ã‚‰ã®æ®‹ã‚Šã®æ®‹å·®ã§ã‚ã‚Šã€å±¤ $(l-1)$ ã‹ã‚‰1ã¾ã§ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ãƒ™ã‚¯ãƒˆãƒ«ã‚’å†å¸°çš„ã«é©ç”¨ã—ãŸå¾Œã®ã‚‚ã®ã§ã™ã€‚

$$
r_{l} := z - \sum_{i=1}^{l-1} v^{i}_{c_{i}},
c_{l} := \arg\min_{c} \| v^{l}_{c} - r_{l} \|_{2}.
\tag{2}
$$

In Section 4.2, we provide more intuition on the nature of RQ-VAEâ€™s hierarchical clustering and how it informs the choice of token parameterization.
ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.2ã§ã¯ã€**RQ-VAEã®éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®æ€§è³ª**ã¨ã€ãã‚ŒãŒãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã®é¸æŠã«ã©ã®ã‚ˆã†ã«å½±éŸ¿ã™ã‚‹ã‹ã«ã¤ã„ã¦ã€ã‚ˆã‚Šç›´æ„Ÿçš„ãªèª¬æ˜ã‚’æä¾›ã—ã¾ã™ã€‚

The RQ-VAE is trained using two loss terms, a reconstruction loss and a loss that encourages the residuals and codebook vectors to be close to each other, 
RQ-VAEã¯ã€å†æ§‹ç¯‰æå¤±ã¨æ®‹å·®ã¨ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ãƒ™ã‚¯ãƒˆãƒ«ãŒäº’ã„ã«è¿‘ããªã‚‹ã‚ˆã†ã«ä¿ƒã™æå¤±ã®2ã¤ã®æå¤±é …ã‚’ä½¿ç”¨ã—ã¦è¨“ç·´ã•ã‚Œã¾ã™ã€‚

$$
L_{RQ-VAE} = ||x - dec(c)||^2 + 
$$

where $\text{dec}(\mathbf{c})$ is the result of applying the decoder to the codes $\mathbf{c}$, $\text{sg}(\cdot)$ corresponds to the stop-gradient operator, and $\beta$ is a hyperparameter we set to 0.5 in the experiments. 
ã“ã“ã§ã€$\text{dec}(\mathbf{c})$ã¯ã€ã‚³ãƒ¼ãƒ‰$\mathbf{c}$ã«ãƒ‡ã‚³ãƒ¼ãƒ€ã‚’é©ç”¨ã—ãŸçµæœã§ã‚ã‚Šã€$\text{sg}(\cdot)$ã¯ã‚¹ãƒˆãƒƒãƒ—ã‚°ãƒ©ãƒ‡ã‚¤ãƒ³ãƒˆæ¼”ç®—å­ã«å¯¾å¿œã—ã€$\beta$ã¯å®Ÿé¨“ã§0.5ã«è¨­å®šã—ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™ã€‚

A Semantic ID is defined as the sequence of discrete codes $(c_{1},\dots,c_{L})$ produced by the encoder and residual quantizer.
Semantic IDã¯ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¨æ®‹å·®é‡å­åŒ–å™¨ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸé›¢æ•£ã‚³ãƒ¼ãƒ‰ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹$(c_{1},\dots,c_{L})$ã¨ã—ã¦å®šç¾©ã•ã‚Œã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### 4.2 Token Parameterization ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–

In our experiments, we use the same codebook size for each level, resulting in $K^{L}$ total clusters. 
ç§ãŸã¡ã®å®Ÿé¨“ã§ã¯ã€å„ãƒ¬ãƒ™ãƒ«ã«å¯¾ã—ã¦åŒã˜ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’ä½¿ç”¨ã—ã€$K^{L}$ã®åˆè¨ˆã‚¯ãƒ©ã‚¹ã‚¿ã‚’å¾—ã¾ã—ãŸã€‚
An important feature of RQ-VAE is that it produces hierarchical clusters. 
RQ-VAEã®é‡è¦ãªç‰¹å¾´ã¯ã€éšå±¤çš„ãªã‚¯ãƒ©ã‚¹ã‚¿ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚

Assuming $L=3$ for simplicity, a raw item ID is mapped to a sequence $(c_{1},c_{2},c_{3})$. 
ç°¡å˜ã®ãŸã‚ã«$L=3$ã¨ä»®å®šã™ã‚‹ã¨ã€ç”Ÿã®ã‚¢ã‚¤ãƒ†ãƒ IDã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹$(c_{1},c_{2},c_{3})$ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã¾ã™ã€‚

The precision of vector quantization increases as one moves from the first token $c_{1}$ to the deeper token $c_{2}$, and finally $c_{3}$. 
ãƒ™ã‚¯ãƒˆãƒ«é‡å­åŒ–ã®ç²¾åº¦ã¯ã€æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³$c_{1}$ã‹ã‚‰ã‚ˆã‚Šæ·±ã„ãƒˆãƒ¼ã‚¯ãƒ³$c_{2}$ã€ãã—ã¦æœ€çµ‚çš„ã«$c_{3}$ã«ç§»å‹•ã™ã‚‹ã«ã¤ã‚Œã¦å‘ä¸Šã—ã¾ã™ã€‚

The first token $c_{1}$ represents the coarsest bucket: e.g., all ads related to food. 
æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³$c_{1}$ã¯æœ€ã‚‚ç²—ã„ãƒã‚±ãƒƒãƒˆã‚’è¡¨ã—ã¾ã™ï¼šä¾‹ãˆã°ã€é£Ÿã¹ç‰©ã«é–¢é€£ã™ã‚‹ã™ã¹ã¦ã®åºƒå‘Šã§ã™ã€‚

The second token $c_{2}$ refines this information, e.g., $(c_{1},c_{2})$ may represent all ads related to pizza. 
2ç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³$c_{2}$ã¯ã“ã®æƒ…å ±ã‚’æ´—ç·´ã•ã›ã¾ã™ã€‚ä¾‹ãˆã°ã€$(c_{1},c_{2})$ã¯ãƒ”ã‚¶ã«é–¢é€£ã™ã‚‹ã™ã¹ã¦ã®åºƒå‘Šã‚’è¡¨ã™ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚

The last token $c_{3}$ further refines this information, e.g., $(c_{1},c_{2},c_{3})$ may represent all ads related to pizza and written in a specific language such as English. 
æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³$c_{3}$ã¯ã“ã®æƒ…å ±ã‚’ã•ã‚‰ã«æ´—ç·´ã•ã›ã¾ã™ã€‚ä¾‹ãˆã°ã€$(c_{1},c_{2},c_{3})$ã¯ãƒ”ã‚¶ã«é–¢é€£ã—ã€è‹±èªãªã©ã®ç‰¹å®šã®è¨€èªã§æ›¸ã‹ã‚ŒãŸã™ã¹ã¦ã®åºƒå‘Šã‚’è¡¨ã™ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚

Due to this, we can control the amount and the structure of the information that the recommendation model receives from Semantic ID. 
ã“ã‚Œã«ã‚ˆã‚Šã€æ¨è–¦ãƒ¢ãƒ‡ãƒ«ãŒSemantic IDã‹ã‚‰å—ã‘å–ã‚‹æƒ…å ±ã®é‡ã¨æ§‹é€ ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚

Notably, supplying the most fine-grained information (all possible $(c_{1},c_{2},\dots,c_{L})$ tuples) is often not feasible due to high cardinality of the possible combinations. 
ç‰¹ã«ã€æœ€ã‚‚è©³ç´°ãªæƒ…å ±ï¼ˆã™ã¹ã¦ã®å¯èƒ½ãª$(c_{1},c_{2},\dots,c_{L})$ã‚¿ãƒ—ãƒ«ï¼‰ã‚’æä¾›ã™ã‚‹ã“ã¨ã¯ã€å¯èƒ½ãªçµ„ã¿åˆã‚ã›ã®é«˜ã„åŸºæ•°ã®ãŸã‚ã«ã—ã°ã—ã°å®Ÿç¾ä¸å¯èƒ½ã§ã™ã€‚

Hence, a tradeoff exists between the cardinality of the token parameterization and the amount of information the model receives from Semantic ID. 
ã—ãŸãŒã£ã¦ã€ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã®åŸºæ•°ã¨ãƒ¢ãƒ‡ãƒ«ãŒSemantic IDã‹ã‚‰å—ã‘å–ã‚‹æƒ…å ±ã®é‡ã¨ã®é–“ã«ã¯ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒå­˜åœ¨ã—ã¾ã™ã€‚

![]()
table1: token parameterization techniques.

Lets $(x):[1..I] \to K^{L}$ be the Semantic ID lookup that maps raw IDs to Semantic IDs learned by RQ-VAE. 
$L$ã‚’Semantic IDã®ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã—ã€ç”Ÿã®IDã‚’RQ-VAEã«ã‚ˆã£ã¦å­¦ç¿’ã•ã‚ŒãŸSemantic IDã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¾ã™ã€‚
Keeping the hierarchical nature of the tokens in mind, we must specify a token parameterization that maps a Semantic ID to embedding table rows, $p(\mathbf{c};H):K^{L} \to [1..H]^{G}$. 
ãƒˆãƒ¼ã‚¯ãƒ³ã®éšå±¤çš„ãªæ€§è³ªã‚’è€ƒæ…®ã—ã€Semantic IDã‚’åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡Œã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

Table 1 defines several possible parameterizations. 
è¡¨1ã¯ã€ã„ãã¤ã‹ã®å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã‚’å®šç¾©ã—ã¦ã„ã¾ã™ã€‚

When the Semantic ID cardinality is larger than the embedding size, a modulo hash function is applied. 
Semantic IDã®åŸºæ•°ãŒåŸ‹ã‚è¾¼ã¿ã‚µã‚¤ã‚ºã‚ˆã‚Šå¤§ãã„å ´åˆã€ãƒ¢ã‚¸ãƒ¥ãƒ­ãƒãƒƒã‚·ãƒ¥é–¢æ•°ãŒé©ç”¨ã•ã‚Œã¾ã™ã€‚

When there are multiple IDs, a shifting factor is added to avoid the collision between different positions. 
è¤‡æ•°ã®IDãŒã‚ã‚‹å ´åˆã€ç•°ãªã‚‹ä½ç½®é–“ã®è¡çªã‚’é¿ã‘ã‚‹ãŸã‚ã«ã‚·ãƒ•ãƒˆãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãŒè¿½åŠ ã•ã‚Œã¾ã™ã€‚

Among all the parameterization techniques, only Prefix-ngram consists of all possible tuples from different granularity. 
ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–æŠ€è¡“ã®ä¸­ã§ã€Prefix-ngramã®ã¿ãŒç•°ãªã‚‹ç²’åº¦ã‹ã‚‰ã®ã™ã¹ã¦ã®å¯èƒ½ãªã‚¿ãƒ—ãƒ«ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚

Table 2: 
è¡¨2ï¼š

NE performance for different tokenization parameterizations 
ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã«å¯¾ã™ã‚‹NEãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

Table 2 summarizes the model performance across different token parameterizations. 
è¡¨2ã¯ã€ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã«ãŠã‘ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è¦ç´„ã—ã¦ã„ã¾ã™ã€‚

We draw the following conclusions: 
ç§ãŸã¡ã¯ä»¥ä¸‹ã®çµè«–ã‚’å°ãã¾ã™ï¼š

i) Prefix-ngram is the best parameterization. 
i) Prefix-ngramãŒæœ€è‰¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã§ã™ã€‚

This indicates that incorporating the hierarchical nature of the clustering in the embedding table mapping is necessary for effectiveness, as it allows for knowledge sharing among more items than a flat mapping; 
ã“ã‚Œã¯ã€åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã«ãŠã„ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®éšå±¤çš„ãªæ€§è³ªã‚’å–ã‚Šå…¥ã‚Œã‚‹ã“ã¨ãŒåŠ¹æœçš„ã§ã‚ã‚‹ãŸã‚ã«å¿…è¦ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã€ãƒ•ãƒ©ãƒƒãƒˆãƒãƒƒãƒ”ãƒ³ã‚°ã‚ˆã‚Šã‚‚å¤šãã®ã‚¢ã‚¤ãƒ†ãƒ é–“ã§ã®çŸ¥è­˜å…±æœ‰ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚

ii) Increasing the depth of Prefix-ngram improves NE performance; 
ii) Prefix-ngramã®æ·±ã•ã‚’å¢—ã™ã“ã¨ã§NEãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Šã—ã¾ã™ã€‚

iii) increasing the RQ-VAE cardinality improves NE performance. 
iii) RQ-VAEã®åŸºæ•°ã‚’å¢—ã‚„ã™ã“ã¨ã§NEãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Šã—ã¾ã™ã€‚

<!-- å¾Œã§èª­ã‚€! -->

## 5Item Impression Distribution Issues 5ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³åˆ†å¸ƒã®å•é¡Œ

In this section, we discuss the data distribution aspects that present challenges for recommendation modeling in Meta ads ranking and how we address them with the use of Semantic ID. 
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€Metaåºƒå‘Šãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«ãŠã‘ã‚‹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹èª²é¡Œã‚’æç¤ºã™ã‚‹ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®å´é¢ã¨ã€Semantic IDã‚’ä½¿ç”¨ã—ã¦ãã‚Œã‚‰ã«ã©ã®ã‚ˆã†ã«å¯¾å‡¦ã™ã‚‹ã‹ã«ã¤ã„ã¦è­°è«–ã—ã¾ã™ã€‚

#### Item cardinality

For certain features, such as the target item, the number of distinct items $I$ considered by the model can be much larger than a feasible embedding table size $H$ in the sparse module. 
ç‰¹å®šã®ç‰¹å¾´ã€ä¾‹ãˆã°ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ãŒè€ƒæ…®ã™ã‚‹ç•°ãªã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®æ•° $I$ (=ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£)ã¯ã€ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ãŠã‘ã‚‹å®Ÿè¡Œå¯èƒ½ãªåŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚º $H$ ã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«å¤§ãããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
In such cases the mapping function $f(\mathbf{x})$ introduces collisions: two or more raw IDs will map to the same row. 
**ã“ã®ã‚ˆã†ãªå ´åˆã€ãƒãƒƒãƒ”ãƒ³ã‚°é–¢æ•° $f(\mathbf{x})$ ã¯è¡çªã‚’å¼•ãèµ·ã“ã—ã¾ã™ï¼š2ã¤ä»¥ä¸Šã®ç”Ÿã®IDãŒåŒã˜è¡Œã«ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã¾ã™**ã€‚
The mapping function $f(\mathbf{x})$ is often chosen to be a simple hash. 
ãƒãƒƒãƒ”ãƒ³ã‚°é–¢æ•° $f(\mathbf{x})$ ã¯ã€ã—ã°ã—ã°å˜ç´”ãªãƒãƒƒã‚·ãƒ¥ã¨ã—ã¦é¸æŠã•ã‚Œã¾ã™ã€‚
Since the initial raw IDs are randomly generated at the time of item creation, the resulting collisions will essentially be random. 
åˆæœŸã®ç”Ÿã®IDã¯ã‚¢ã‚¤ãƒ†ãƒ ä½œæˆæ™‚ã«ãƒ©ãƒ³ãƒ€ãƒ ã«ç”Ÿæˆã•ã‚Œã‚‹ãŸã‚ã€çµæœã¨ã—ã¦ç”Ÿã˜ã‚‹è¡çªã¯æœ¬è³ªçš„ã«ãƒ©ãƒ³ãƒ€ãƒ ã«ãªã‚Šã¾ã™ã€‚
Such random collisions can negatively affect the resulting representation quality of embeddings and serve as an obstacle to effective knowledge sharing across items. 
ã“ã®ã‚ˆã†ãªãƒ©ãƒ³ãƒ€ãƒ ãªè¡çªã¯ã€åŸ‹ã‚è¾¼ã¿ã®çµæœã¨ã—ã¦ã®è¡¨ç¾å“è³ªã«æ‚ªå½±éŸ¿ã‚’åŠã¼ã—ã€ã‚¢ã‚¤ãƒ†ãƒ é–“ã®åŠ¹æœçš„ãªçŸ¥è­˜å…±æœ‰ã®éšœå®³ã¨ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

#### Impression skew ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®åã‚Š

Figure 2: Figure 2:Impression Skew â€“ cumulative impressions as a function of the share of items considered. As items are sorted by the impression count, one sees that the majority of impressions comes from a fraction of most popular items.
å›³2: ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®åã‚Š â€“ è€ƒæ…®ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã®å‰²åˆã«å¯¾ã™ã‚‹ç´¯ç©ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã€‚ã‚¢ã‚¤ãƒ†ãƒ ãŒã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ã§ã‚½ãƒ¼ãƒˆã•ã‚Œã‚‹ã¨ã€ã»ã¨ã‚“ã©ã®ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ãŒæœ€ã‚‚äººæ°—ã®ã‚ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®ä¸€éƒ¨ã‹ã‚‰æ¥ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

For the target item feature, the item distribution in the training data is highly skewed.
ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¢ã‚¤ãƒ†ãƒ ã®ç‰¹å¾´ã«é–¢ã—ã¦ã€**ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿å†…ã®ã‚¢ã‚¤ãƒ†ãƒ åˆ†å¸ƒã¯éå¸¸ã«åã£ã¦ã„ã¾ã™**ã€‚
Figure 2 shows that in our system, a small percentage of items dominates the item impression distribution: 
å›³2ã¯ã€ç§ãŸã¡ã®ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã„ã¦ã€å°‘æ•°ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒã‚¢ã‚¤ãƒ†ãƒ ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³åˆ†å¸ƒã‚’æ”¯é…ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
when sorting the items by popularity, the top 0.1% â€œheadâ€ items have 25% of all item impressions, 
ã‚¢ã‚¤ãƒ†ãƒ ã‚’äººæ°—é †ã«ä¸¦ã¹ã‚‹ã¨ã€ä¸Šä½0.1%ã®ã€Œãƒ˜ãƒƒãƒ‰ã€ã‚¢ã‚¤ãƒ†ãƒ ãŒå…¨ã‚¢ã‚¤ãƒ†ãƒ ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®25%ã‚’å ã‚ã€
the next 5.5% â€œtorsoâ€ items have 50% of cumulative impressions, 
æ¬¡ã®5.5%ã®ã€Œãƒˆãƒ«ã‚½ã€ã‚¢ã‚¤ãƒ†ãƒ ãŒç´¯ç©ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®50%ã‚’å ã‚ã€
while the remaining 94.4% â€œtailâ€ items account for the remaining 25% of impressions.
æ®‹ã‚Šã®94.4%ã®ã€Œãƒ†ã‚¤ãƒ«ã€ã‚¢ã‚¤ãƒ†ãƒ ãŒæ®‹ã‚Šã®25%ã®ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã‚’å ã‚ã¦ã„ã¾ã™ã€‚

As tail items have few training examples, it can be challenging to learn embedding representations $\mathbf{e}(\mathbf{x})$ that generalize well.
**ãƒ†ã‚¤ãƒ«ã‚¢ã‚¤ãƒ†ãƒ ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¾‹ãŒå°‘ãªã„ãŸã‚ã€ä¸€èˆ¬åŒ–ã®è‰¯ã„åŸ‹ã‚è¾¼ã¿è¡¨ç¾ $\mathbf{e}(\mathbf{x})$ ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã¯å›°é›£**ã§ã™ã€‚
(ã†ãƒ¼ã‚“ã€ã˜ã‚ƒã‚ç‰¹ã«lifetimeãŒçŸ­ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã«ãŠã„ã¦ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ ã®entity embeddingã‚’å­¦ç¿’ã™ã‚‹ã®ã£ã¦ã‚ã‚“ã¾ã‚Šä¾¡å€¤ç™ºæ®ã—ãªã•ãã†ã ãª...!!:thinking:)
Random hashing doesnâ€™t allow the head and torso items to effectively share knowledge with semantically similar tail items since the assignment of several items to a single embedding is random.
ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ã¯ã€è¤‡æ•°ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’å˜ä¸€ã®åŸ‹ã‚è¾¼ã¿ã«ãƒ©ãƒ³ãƒ€ãƒ ã«å‰²ã‚Šå½“ã¦ã‚‹ãŸã‚ã€ãƒ˜ãƒƒãƒ‰ã‚¢ã‚¤ãƒ†ãƒ ã¨ãƒˆãƒ«ã‚½ã‚¢ã‚¤ãƒ†ãƒ ãŒæ„å‘³çš„ã«é¡ä¼¼ã—ãŸãƒ†ã‚¤ãƒ«ã‚¢ã‚¤ãƒ†ãƒ ã¨åŠ¹æœçš„ã«çŸ¥è­˜ã‚’å…±æœ‰ã™ã‚‹ã“ã¨ã‚’è¨±ã—ã¾ã›ã‚“ã€‚
(ã§ã‚‚ã‚¢ã‚¤ãƒ†ãƒ ç‰¹å¾´é‡ã‚’ä½¿ãˆã°çŸ¥è­˜ã‚’å…±æœ‰ã§ãã‚‹ã‚ˆã­ã€ã£ã¦è©±ã‹ãªã€‚ã˜ã‚ƒã‚ã‚„ã£ã±ã‚Štwo-towerã¯ç‰¹å¾´é‡ã¨ã—ã¦ã®richãªitem embeddingã‚’ä½œã‚‹ã¨ã—ã¦ã‚‚ä¾¡å€¤ãŒã‚ã‚Šãã†...!!:thinking:)

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

#### ID drifting IDã®æ¼‚æµ

![]()

Figure 3:ID Drift â€“ share of items that remain active in the initial corpus as a function of time. Half of the original corpus exits the system after 6 days. An equal number of new items enters the system, creating a severe item distribution drift.
å›³3: IDã®æ¼‚æµ â€“ æ™‚é–“ã®é–¢æ•°ã¨ã—ã¦ã€æœ€åˆã®ã‚³ãƒ¼ãƒ‘ã‚¹ã§ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã¾ã¾ã§ã„ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®å‰²åˆã€‚å…ƒã®ã‚³ãƒ¼ãƒ‘ã‚¹ã®åŠåˆ†ã¯6æ—¥å¾Œã«ã‚·ã‚¹ãƒ†ãƒ ã‚’é€€å‡ºã—ã¾ã™ã€‚åŒæ•°ã®æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ãŒã‚·ã‚¹ãƒ†ãƒ ã«å…¥ã‚Šã€æ·±åˆ»ãªã‚¢ã‚¤ãƒ†ãƒ åˆ†å¸ƒã®å¤‰åŒ–ã‚’å¼•ãèµ·ã“ã—ã¾ã™ã€‚

The existing item ID space is highly dynamic with large numbers of old items retiring (Figure3) and new items entering the system. 
æ—¢å­˜ã®ã‚¢ã‚¤ãƒ†ãƒ IDç©ºé–“ã¯éå¸¸ã«å‹•çš„ã§ã€å¤šãã®å¤ã„ã‚¢ã‚¤ãƒ†ãƒ ãŒé€€å½¹ã—ï¼ˆå›³3ï¼‰ã€æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ãŒã‚·ã‚¹ãƒ†ãƒ ã«å…¥ã£ã¦ãã¾ã™ã€‚ (ãã‚Œã“ããƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã ã¨é¡•è‘—ã ã‚ˆã­...!:thinking:)
We call this item distribution shift in our system â€œraw ID drifting.â€ 
ã“ã®ã‚¢ã‚¤ãƒ†ãƒ åˆ†å¸ƒã®å¤‰åŒ–ã‚’ç§ãŸã¡ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€Œ**ç”Ÿã®IDæ¼‚æµï¼ˆraw ID driftingï¼‰**ã€ã¨å‘¼ã³ã¾ã™ã€‚ (ãã†ã„ã†å‘¼ã³æ–¹ã‚’ã™ã‚‹ã®ã‹...!:thinking:)
The raw ID drifting phenomenon stems from the nature of online recommendation systems, where new ads are created on a daily basis and most ads have a relatively short lifespan. 
ç”Ÿã®IDæ¼‚æµç¾è±¡ã¯ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹æ€§ã«èµ·å› ã—ã¦ãŠã‚Šã€æ–°ã—ã„åºƒå‘ŠãŒæ—¥ã€…ä½œæˆã•ã‚Œã€ã»ã¨ã‚“ã©ã®åºƒå‘Šã¯æ¯”è¼ƒçš„çŸ­ã„å¯¿å‘½ã‚’æŒã£ã¦ã„ã¾ã™ã€‚(ãªã‚‹ã»ã©ã€‚åºƒå‘Šæ¨è–¦ã‚‚ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ã¨è¿‘ã„æ€§è³ªã‚’æŒã£ã¦ã‚‹ã‚“ã ãª...!:thinking:)

As a byproduct, a recommendation model based on random hashing experiences severe embedding representation drift over time: a given embedding $\mathbf{e}$ represents different items over time as items enter and exit the system. 
ãã®å‰¯ç”£ç‰©ã¨ã—ã¦ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ã«åŸºã¥ãæ¨è–¦ãƒ¢ãƒ‡ãƒ«ã¯ã€æ™‚é–“ã®çµŒéã¨ã¨ã‚‚ã«æ·±åˆ»ãªåŸ‹ã‚è¾¼ã¿è¡¨ç¾ã®æ¼‚æµã‚’çµŒé¨“ã—ã¾ã™ï¼šç‰¹å®šã®åŸ‹ã‚è¾¼ã¿ $\mathbf{e}$ ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ ãŒã‚·ã‚¹ãƒ†ãƒ ã«å…¥ã£ãŸã‚Šå‡ºãŸã‚Šã™ã‚‹ã«ã¤ã‚Œã¦ã€æ™‚é–“ã¨ã¨ã‚‚ã«ç•°ãªã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¡¨ã—ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

#### Item representation with Semantic ID ã‚¢ã‚¤ãƒ†ãƒ è¡¨ç¾ã¨ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ID

We hypothesize that switching from raw IDs to Semantic IDs can effectively address the issues above.  
ç§ãŸã¡ã¯ã€ç”Ÿã®IDã‹ã‚‰ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDã«åˆ‡ã‚Šæ›¿ãˆã‚‹ã“ã¨ã§ã€ä¸Šè¨˜ã®å•é¡Œã«åŠ¹æœçš„ã«å¯¾å‡¦ã§ãã‚‹ã¨ä»®å®šã—ã¾ã™ã€‚(semanticæƒ…å ±ã‚’åæ˜ ã•ã›ãŸidã£ã¦ã“ã¨?? two-towerã®å‡ºåŠ›ã¨ä½•ãŒé•ã†ã‚“ã ã‚??:thinking:)

When an advertiser introduces a new ad $\mathbf{x}$ to the system and retires the previous one $\mathbf{y}$, the fine-grained content details of the new ad may be different from the retired one, but the broad semantic category of the product usually remains the same.  
åºƒå‘Šä¸»ãŒæ–°ã—ã„åºƒå‘Š $\mathbf{x}$ ã‚’ã‚·ã‚¹ãƒ†ãƒ ã«å°å…¥ã—ã€ä»¥å‰ã®åºƒå‘Š $\mathbf{y}$ ã‚’å¼•é€€ã•ã›ã‚‹ã¨ãã€æ–°ã—ã„åºƒå‘Šã®è©³ç´°ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å¼•é€€ã—ãŸåºƒå‘Šã¨ã¯ç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ãŒã€è£½å“ã®åºƒç¯„ãªã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚«ãƒ†ã‚´ãƒªã¯é€šå¸¸åŒã˜ã¾ã¾ã§ã™ã€‚
(ãªã‚‹ã»ã©ã€‚åŒã˜åºƒå‘Šä¸»ãŒåŒã˜å•†å“ã‚’å£²ã‚‹ãŸã‚ã«æ–°ã—ã„åºƒå‘Šã‚’å‡ºã™å ´åˆã€ç´°ã‹ã„å†…å®¹ã¯å¤‰ã‚ã£ã¦ã‚‚ã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãªã‚«ãƒ†ã‚´ãƒªã¯å¤‰ã‚ã‚‰ãªã„ã‚ˆã­ã€ã£ã¦ã“ã¨ã‹...!:thinking:)
Therefore, the new and retired adsâ€™ Semantic IDs will match (or at least share a prefix).  
ã—ãŸãŒã£ã¦ã€æ–°ã—ã„åºƒå‘Šã¨å¼•é€€ã—ãŸåºƒå‘Šã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDã¯ä¸€è‡´ã™ã‚‹ã‹ï¼ˆå°‘ãªãã¨ã‚‚æ¥é ­è¾ã‚’å…±æœ‰ã—ã¾ã™ï¼‰ã€‚
Hence, the item impression distribution in Semantic ID space exhibits much less drift compared to the raw ID space as long as the broad semantic categories remain temporally stable.  
ã—ãŸãŒã£ã¦ã€**åºƒç¯„ãªã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚«ãƒ†ã‚´ãƒªãŒæ™‚é–“çš„ã«å®‰å®šã—ã¦ã„ã‚‹é™ã‚Šã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDç©ºé–“ã«ãŠã‘ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³åˆ†å¸ƒã¯ã€ç”Ÿã®IDç©ºé–“ã¨æ¯”è¼ƒã—ã¦ã¯ã‚‹ã‹ã«å°‘ãªã„ãƒ‰ãƒªãƒ•ãƒˆã‚’ç¤ºã—ã¾ã™**ã€‚

Similarly, if a tail item $\mathbf{x}$ has similar content to a head or torso item $\mathbf{y}$, their Semantic IDs will match (or at least share a prefix).  
åŒæ§˜ã«ã€ãƒ†ãƒ¼ãƒ«ã‚¢ã‚¤ãƒ†ãƒ  $\mathbf{x}$ ãŒãƒ˜ãƒƒãƒ‰ã¾ãŸã¯ãƒˆãƒ«ã‚½ãƒ¼ã‚¢ã‚¤ãƒ†ãƒ  $\mathbf{y}$ ã¨é¡ä¼¼ã®å†…å®¹ã‚’æŒã¤å ´åˆã€å½¼ã‚‰ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDã¯ä¸€è‡´ã™ã‚‹ã‹ï¼ˆå°‘ãªãã¨ã‚‚æ¥é ­è¾ã‚’å…±æœ‰ã—ã¾ã™ï¼‰ã€‚
The resulting item impression distribution in Semantic ID space exhibits less skew compared to the raw ID space (see Appendix B).  
ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDç©ºé–“ã«ãŠã‘ã‚‹çµæœã®ã‚¢ã‚¤ãƒ†ãƒ ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³åˆ†å¸ƒã¯ã€ç”Ÿã®IDç©ºé–“ã¨æ¯”è¼ƒã—ã¦æ­ªã¿ãŒå°‘ãªããªã‚Šã¾ã™ï¼ˆä»˜éŒ²Bã‚’å‚ç…§ï¼‰ã€‚

In both of the cases above, the embeddings $\mathbf{e(x)}$ and $\mathbf{e(y)}$ will be equal (or similar if the Semantic IDs only share a prefix).  
ä¸Šè¨˜ã®ä¸¡æ–¹ã®ã‚±ãƒ¼ã‚¹ã§ã¯ã€åŸ‹ã‚è¾¼ã¿ $\mathbf{e(x)}$ ã¨ $\mathbf{e(y)}$ ã¯ç­‰ã—ããªã‚Šã¾ã™ï¼ˆã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDãŒæ¥é ­è¾ã®ã¿ã‚’å…±æœ‰ã™ã‚‹å ´åˆã¯é¡ä¼¼ã—ã¾ã™ï¼‰ã€‚
(ã“ã“ã§xã¨yã¯ã€åŒã˜ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚«ãƒ†ã‚´ãƒªã«å±ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã‚’æŒ‡ã—ã¦ã„ã‚‹æ„Ÿã˜...!:thinking:)
This is a way for the model to transfer knowledge from item $\mathbf{y}$ with many training examples to item $\mathbf{x}$.
ã“ã‚Œã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒå¤šãã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¾‹ã‚’æŒã¤ã‚¢ã‚¤ãƒ†ãƒ  $\mathbf{y}$ ã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ  $\mathbf{x}$ ã¸çŸ¥è­˜ã‚’è»¢é€ã™ã‚‹æ–¹æ³•ã§ã™ã€‚(è¦ã™ã‚‹ã«ã‚¢ã‚¤ãƒ†ãƒ ç‰¹å¾´é‡ã‚’ä½¿ã£ãŸencoderã¨å½¹å‰²ã¯è¿‘ãã†ã ã‚ˆãªã...:thinking:)
Summarizing, temporal stability of semantic concepts results in stability of Semantic ID encoding, which mitigates embedding representation instability for the model.  
è¦ç´„ã™ã‚‹ã¨ã€**ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¦‚å¿µã®æ™‚é–“çš„å®‰å®šæ€§ã¯ã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å®‰å®šæ€§ã‚’ã‚‚ãŸã‚‰ã—ã€ãƒ¢ãƒ‡ãƒ«ã®åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã®ä¸å®‰å®šæ€§ã‚’è»½æ¸›**ã—ã¾ã™ã€‚(ã“ã®æ–‡ç« ã¯ã„ã„ãª! è¦ã™ã‚‹ã«semanticãªç‰¹å¾´é‡ã¯æ™‚é–“çš„ã«å®‰å®šã—ã¦ã„ã‚‹ã‹ã‚‰ã€å­¦ç¿’æ™‚ã«ä¾¡å€¤ã‚’ç™ºæ®ã—ã‚„ã™ã„embeddingã«ãªã‚‹ã‚ˆã­ã€ã£ã¦ã“ã¨ã ã‚ˆãª...!:thinking:)


Table 3:  Performance of three item representation approaches over various item segments.  
è¡¨3:ã•ã¾ã–ã¾ãªã‚¢ã‚¤ãƒ†ãƒ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«ãŠã‘ã‚‹3ã¤ã®ã‚¢ã‚¤ãƒ†ãƒ è¡¨ç¾ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ€§èƒ½ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## 6Offline Experiments 6ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å®Ÿé¨“

To investigate our hypotheses about the advantages of Semantic ID over baseline item representation approaches, we conduct a suite of offline experiments. 
Semantic IDã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¢ã‚¤ãƒ†ãƒ è¡¨ç¾ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«å¯¾ã™ã‚‹åˆ©ç‚¹ã«é–¢ã™ã‚‹ä»®èª¬ã‚’èª¿æŸ»ã™ã‚‹ãŸã‚ã«ã€ä¸€é€£ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å®Ÿé¨“ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚

We use a simplified version of Metaâ€™s production ads ranking model, keeping all the dense features and the userâ€™s item interaction history, but including only the target item in the sparse module (and removing $O(100)$ other sparse features). 
Metaã®ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³åºƒå‘Šãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®ç°¡ç•¥åŒ–ç‰ˆã‚’ä½¿ç”¨ã—ã€ã™ã¹ã¦ã®å¯†ãªç‰¹å¾´ã¨ãƒ¦ãƒ¼ã‚¶ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´ã‚’ä¿æŒã—ã¾ã™ãŒã€ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ã¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¢ã‚¤ãƒ†ãƒ ã®ã¿ã‚’å«ã‚ï¼ˆ$O(100)$ã®ä»–ã®ã‚¹ãƒ‘ãƒ¼ã‚¹ç‰¹å¾´ã‚’å‰Šé™¤ã—ã¾ã™ï¼‰ã€‚
We train on production user interaction data from a four-day time period, processing the training data sequentially and training for a single epoch. 
**4æ—¥é–“ã®æœŸé–“ã«ã‚ãŸã‚‹ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ¦ãƒ¼ã‚¶ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’é€æ¬¡å‡¦ç†ã—ã€1ã‚¨ãƒãƒƒã‚¯ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã¾ã™ã€‚**
We evaluate the model on the first six hours of the next day of data. 
**æ¬¡ã®æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®6æ™‚é–“ã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã¾ã™ã€‚**

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### 6.1 ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

In Section 5, we outlined data-related challenges and opportunities in designing a good embedding lookup function $f(\mathbf{x})$ for embedding-based item representations. 
ç¬¬5ç¯€ã§ã¯ã€åŸ‹ã‚è¾¼ã¿ãƒ™ãƒ¼ã‚¹ã®ã‚¢ã‚¤ãƒ†ãƒ è¡¨ç¾ã®ãŸã‚ã®è‰¯ã„åŸ‹ã‚è¾¼ã¿ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—é–¢æ•° $f(\mathbf{x})$ ã‚’è¨­è¨ˆã™ã‚‹éš›ã®ãƒ‡ãƒ¼ã‚¿é–¢é€£ã®èª²é¡Œã¨æ©Ÿä¼šã«ã¤ã„ã¦æ¦‚èª¬ã—ã¾ã—ãŸã€‚
We describe two baseline approaches, individual embeddings (IE) and random hashing (RH), and compare those to Semantic ID (SemID). 
å€‹åˆ¥åŸ‹ã‚è¾¼ã¿ï¼ˆIEï¼‰ã¨ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ¥ï¼ˆRHï¼‰ã®2ã¤ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’èª¬æ˜ã—ã€ãã‚Œã‚‰ã‚’Semantic IDï¼ˆSemIDï¼‰ã¨æ¯”è¼ƒã—ã¾ã™ã€‚
(å‰è€…IEãŒentity embeddingã‹ãª!:thinking:)

#### Individual embeddings å€‹åˆ¥åŸ‹ã‚è¾¼ã¿

Each raw ID gets its own embedding table row, I=Hğ¼ğ»I=Hitalic_I = italic_HandfIE(x):=xassignsubscriptğ‘“IEğ‘¥ğ‘¥f_{\text{IE}}(x):=xitalic_f start_POSTSUBSCRIPT IE end_POSTSUBSCRIPT ( italic_x ) := italic_x. 
å„ç”Ÿã®IDã¯ç‹¬è‡ªã®åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡Œã‚’æŒã¡ã€$I=H$ã€$f_{\text{IE}}(x):=x$ ã§ã™ã€‚ 
(ã†ã‚“ã€‚æ™®é€šã®entity embeddingã£ã½ã„ãª!:thinking:)
While unrealistic in production scenarios due to system constraints, we consider this model for illustration purposes. 
ã“ã‚Œã¯**ã‚·ã‚¹ãƒ†ãƒ ã®åˆ¶ç´„ã«ã‚ˆã‚Šå®Ÿéš›ã®é‹ç”¨ã‚·ãƒŠãƒªã‚ªã§ã¯éç¾å®Ÿçš„**ã§ã™ãŒã€èª¬æ˜ã®ãŸã‚ã«ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’è€ƒæ…®ã—ã¾ã™ã€‚ 
During evaluation, IDs that are not seen during training are mapped to a randomly initialized untrained embedding. 
**è©•ä¾¡ä¸­ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«è¦‹ã‚‰ã‚Œãªã‹ã£ãŸIDã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã•ã‚ŒãŸæœªå­¦ç¿’ã®åŸ‹ã‚è¾¼ã¿ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã¾ã™**ã€‚(ã†ã‚“ã†ã‚“ã€‚å¤šåˆ†unknownãƒˆãƒ¼ã‚¯ãƒ³çš„ãªã‚„ã¤!:thinking:)

#### Random hashing ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°

In the case where $I \approx a \cdot H$ for some $a > 1$, we can randomly hash raw IDs to embedding table rows, 
$I \approx a \cdot H$ã®ã¨ãã€$a > 1$ã®ãŸã‚ã«ã€ç§ãŸã¡ã¯**ç”Ÿã®IDã‚’åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡Œã«ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒãƒƒã‚·ãƒ¥ã™ã‚‹**ã“ã¨ãŒã§ãã¾ã™ã€‚(i.e. ã‚¢ã‚¤ãƒ†ãƒ ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ãŒåŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºã‚ˆã‚Šã‚‚å¤§ãã„å ´åˆ!:thinking:)

$$
f_{\text{RH}}(x) := h(x)
$$

where $h(x): [1..I] \to [1..H]$ is one of the standard hash functions such as modulo hash. 
ã“ã“ã§ã€$h(x): [1..I] \to [1..H]$ ã¯ã€ãƒ¢ã‚¸ãƒ¥ãƒ­ãƒãƒƒã‚·ãƒ¥ãªã©ã®æ¨™æº–çš„ãªãƒãƒƒã‚·ãƒ¥é–¢æ•°ã®1ã¤ã§ã™ã€‚

This creates random collisions with an average collision factor of $a$.
ã“ã‚Œã«ã‚ˆã‚Šã€å¹³å‡è¡çªå› å­ $a$ ã‚’æŒã¤ãƒ©ãƒ³ãƒ€ãƒ ãªè¡çªãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚
(ã¤ã¾ã‚Šã€åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã®1è¡Œã«ã¤ãå¹³å‡ã—ã¦aå€‹ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã‚‹ã€ã£ã¦è©±...!:thinking:)

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

#### Semantic ID ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ID

The itemsâ€™ content embeddings are obtained from a multimodal image and text foundation model. 
**ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åŸ‹ã‚è¾¼ã¿ã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å–å¾—ã•ã‚Œã¾ã™ã€‚** (i.e. äº‹å‰å­¦ç¿’ãšã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸsemanticåŸ‹ã‚è¾¼ã¿...!:thinking:)
The foundation model is pre-trained on a large training set of items using image and text alignment objectives (Radford et al., 2021). 
ã“ã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã¯ã€ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®æ•´åˆæ€§ç›®æ¨™ã‚’ä½¿ç”¨ã—ã¦ã€å¤§è¦æ¨¡ãªã‚¢ã‚¤ãƒ†ãƒ ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã§äº‹å‰å­¦ç¿’ã•ã‚Œã¦ã„ã¾ã™ï¼ˆRadford et al., 2021ï¼‰ã€‚
The RQ-VAE is then trained on the content embeddings of all target items from the past three months, with $L=3$ and $K=2048$. 
æ¬¡ã«ã€RQ-VAEã¯éå»3ã‹æœˆã®ã™ã¹ã¦ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¢ã‚¤ãƒ†ãƒ ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åŸ‹ã‚è¾¼ã¿ã«å¯¾ã—ã¦è¨“ç·´ã•ã‚Œã€$L=3$ ãŠã‚ˆã³ $K=2048$ ã¨ã—ã¾ã™ã€‚
We use the prefix $f_{\text{SemId}}=p\circ s$ from Section 4.2. 
ç§ãŸã¡ã¯ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.2ã‹ã‚‰ã®æ¥é ­è¾ $f_{\text{SemId}}=p\circ s$ ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
(ã“ã‚ŒãŒå¿…è¦ãªã®ã‹ã‚ã‹ã£ã¦ãªã„ã‚“ã ã‚ˆãªãã€‚semanticãªcontent embeddingã ã‘ã§ã¯ãƒ€ãƒ¡ãªã®ã‹:thinking:)

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

The analyses in Section 6.2 and Section 6.3 focus on the target item sparse feature. 
ã‚»ã‚¯ã‚·ãƒ§ãƒ³6.2ãŠã‚ˆã³ã‚»ã‚¯ã‚·ãƒ§ãƒ³6.3ã®åˆ†æã¯ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¢ã‚¤ãƒ†ãƒ ã®ã‚¹ãƒ‘ãƒ¼ã‚¹ç‰¹å¾´ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚
We train three versions of the recommendation model using the above three embedding lookup functions. 
ä¸Šè¨˜ã®3ã¤ã®åŸ‹ã‚è¾¼ã¿ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ã€æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®3ã¤ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’è¨“ç·´ã—ã¾ã™ã€‚
The size of the item embedding table is equal to the total number of items for IE and set to a smaller size for RH and SemID, with the average collision factor of 3. 
ã‚¢ã‚¤ãƒ†ãƒ åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚µã‚¤ã‚ºã¯ã€IEã®ã‚¢ã‚¤ãƒ†ãƒ ã®å ´åˆã¯ç·æ•°ã¨ç­‰ã—ãã€RHãŠã‚ˆã³SemIDã®ãŸã‚ã«å°ã•ã„ã‚µã‚¤ã‚ºã«è¨­å®šã•ã‚Œã€å¹³å‡è¡çªä¿‚æ•°ã¯3ã§ã™ã€‚
(ã£ã¦ã“ã¨ã¯ã€RHã¨SemIDã¯ã‚¢ã‚¤ãƒ†ãƒ ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã®1/3ã®ã‚µã‚¤ã‚ºã®åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ã†ã£ã¦ã“ã¨ã‹...!:thinking:)
The user history features are mapped using random hashing. 
ãƒ¦ãƒ¼ã‚¶å±¥æ­´ã®ç‰¹å¾´ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã¾ã™ã€‚(ã“ã‚Œã€ã©ã†ã„ã†æ„å‘³?? ãƒ¦ãƒ¼ã‚¶idã‚’ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ¥ã—ã¦embeddingã«å¤‰æ›ã—ã¦ã‚‹ã£ã¦è©±??:thinking:)

In Section 6.4, we use Semantic ID for the user history features and study its effect on aggregation module architectures. 
ã‚»ã‚¯ã‚·ãƒ§ãƒ³6.4ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶å±¥æ­´ã®ç‰¹å¾´ã«ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDã‚’ä½¿ç”¨ã—ã€é›†ç´„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¸ã®å½±éŸ¿ã‚’ç ”ç©¶ã—ã¾ã™ã€‚
The item interaction history sequence length is fixed to $O(100)$. 
ã‚¢ã‚¤ãƒ†ãƒ ã®ç›¸äº’ä½œç”¨å±¥æ­´ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã¯ $O(100)$ ã«å›ºå®šã•ã‚Œã¦ã„ã¾ã™ã€‚
We pad or truncate the user history to fit the desired length. 
ãƒ¦ãƒ¼ã‚¶å±¥æ­´ã‚’å¸Œæœ›ã®é•·ã•ã«åˆã‚ã›ã‚‹ãŸã‚ã«ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¾ãŸã¯åˆ‡ã‚Šè©°ã‚ã‚’è¡Œã„ã¾ã™ã€‚
(ã†ã‚“ã†ã‚“ã€‚ã“ã“ã¯æ™®é€šã ...!:thinking:)

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### 6.2 ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ

(æ³¨æ„: è«–æ–‡ä¸­ã®"NE"ã¯è©•ä¾¡æŒ‡æ¨™ã®Normalized Entropyã®ã“ã¨!:thinking:)

(ã“ã“ã®åˆ†ææ–¹æ³•ã¨ã‹å‚è€ƒã«ãªã‚Šãã†...!:thinking:)

To understand the effect of impression skew on each approach, we segment the data based on the itemâ€™s number of impressions in the training period. 
å„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«å¯¾ã™ã‚‹ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®åã‚Šã®å½±éŸ¿ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æœŸé–“ä¸­ã®ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ã—ã¾ã™ã€‚
We sort all items by impression count. 
ã™ã¹ã¦ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ã§ã‚½ãƒ¼ãƒˆã—ã¾ã™ã€‚
As before, we segment items into head, torso, and tail items according to whether they generate 25%, 75%, or 100% of the cumulative impression count in this sorted order. 
å‰å›ã¨åŒæ§˜ã«ã€**ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸé †åºã§ç´¯ç©ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ã®25%ã€75%ã€ã¾ãŸã¯100%ã‚’ç”Ÿæˆã™ã‚‹ã‹ã©ã†ã‹ã«å¿œã˜ã¦ã€ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ˜ãƒƒãƒ‰ã€ãƒˆãƒ«ã‚½ãƒ¼ã€ãƒ†ãƒ¼ãƒ«ã‚¢ã‚¤ãƒ†ãƒ ã«ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–**ã—ã¾ã™ã€‚
Due to the impression skew, the percentage of items that are head, torso, or tail items are 0.1%, 5.5%, or 94.4%, respectively. 
ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®åã‚Šã«ã‚ˆã‚Šã€ãƒ˜ãƒƒãƒ‰ã€ãƒˆãƒ«ã‚½ãƒ¼ã€ã¾ãŸã¯ãƒ†ãƒ¼ãƒ«ã‚¢ã‚¤ãƒ†ãƒ ã®å‰²åˆã¯ãã‚Œãã‚Œ0.1%ã€5.5%ã€ã¾ãŸã¯94.4%ã§ã™ã€‚
We also evaluate on the segment of new items that only appear in the evaluation period and were not seen during training. 
**ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«è¦‹ã‚‰ã‚Œãªã‹ã£ãŸè©•ä¾¡æœŸé–“ä¸­ã«ã®ã¿ç¾ã‚Œã‚‹æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ**ã§ã‚‚è©•ä¾¡ã‚’è¡Œã„ã¾ã™ã€‚
The performance of the three item representation approaches is shown in Table 3(a). 
3ã¤ã®ã‚¢ã‚¤ãƒ†ãƒ è¡¨ç¾ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ã€è¡¨3(a)ã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚

![]()
table3: hoge

Compared to the baselines, Semantic ID improves generalization for tail items, is NE neutral for head items, and slightly beneficial for torso items. 
**ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒã—ã¦ã€Semantic IDã¯ãƒ†ãƒ¼ãƒ«ã‚¢ã‚¤ãƒ†ãƒ ã®ä¸€èˆ¬åŒ–ã‚’æ”¹å–„ã—ã€ãƒ˜ãƒƒãƒ‰ã‚¢ã‚¤ãƒ†ãƒ ã«ã¯NEä¸­ç«‹ã§ã€ãƒˆãƒ«ã‚½ãƒ¼ã‚¢ã‚¤ãƒ†ãƒ ã«ã¯ã‚ãšã‹ã«æœ‰ç›Š**ã§ã™ã€‚
As this is also relative to the individual embeddings approach, Semantic ID is not simply better at clustering than random hashing, but we find that the target item feature benefits from semantics-based knowledge sharing. 
ã“ã‚Œã¯å€‹åˆ¥ã®åŸ‹ã‚è¾¼ã¿ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«å¯¾ã—ã¦ã‚‚ç›¸å¯¾çš„ã§ã‚ã‚Šã€Semantic IDã¯ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ã‚ˆã‚Šã‚‚å˜ã«ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãŒå„ªã‚Œã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªãã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¢ã‚¤ãƒ†ãƒ ã®ç‰¹å¾´(=id embedding)ãŒ**æ„å‘³ã«åŸºã¥ãçŸ¥è­˜å…±æœ‰ã‹ã‚‰åˆ©ç›Šã‚’å¾—ã¦ã„ã‚‹**ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚
(çŸ¥è­˜å…±æœ‰=knowledge sharingã£ã¦ã„ã†è¡¨ç¾ã‚’ã€ä»Šå¾Œèª¬æ˜ã™ã‚‹ã¨ãã«ä½¿ã£ã¦ã„ããŸã„...!:thinking:)

Specifically, the knowledge sharing occurs through the shared embedding weights, which receive updates for semantically-related items. 
å…·ä½“çš„ã«ã¯ã€çŸ¥è­˜å…±æœ‰ã¯ã€æ„å‘³çš„ã«é–¢é€£ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®æ›´æ–°ã‚’å—ã‘å–ã‚‹å…±æœ‰åŸ‹ã‚è¾¼ã¿é‡ã¿ã‚’é€šã˜ã¦è¡Œã‚ã‚Œã¾ã™ã€‚
Knowledge sharing benefits are largest on the new items segment, where SemID achieves large gains over both RH and IE (âˆ’0.41% and âˆ’0.33%, respectively). 
**çŸ¥è­˜å…±æœ‰ã®åˆ©ç›Šã¯æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã§æœ€å¤§ã§ã‚ã‚Šã€SemIDã¯RHãŠã‚ˆã³IEã«å¯¾ã—ã¦å¤§ããªåˆ©ç›Šã‚’é”æˆã—ã¾ã™ï¼ˆãã‚Œãã‚Œâˆ’0.41%ãŠã‚ˆã³âˆ’0.33%ï¼‰ã€‚** (ã“ã‚Œã¯æ„Ÿè¦šé€šã‚Šã¨ã„ã†ã‹æ„å›³é€šã‚Šã®åŠ¹æœã ã‚ˆã­...!:thinking:)
New items use pre-trained weights from semantically similar items seen during training for prediction, rather than using a non-relevant weight (RH) or an untrained weight (IE). 
æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«è¦‹ã‚‰ã‚ŒãŸæ„å‘³çš„ã«é¡ä¼¼ã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰ã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸé‡ã¿ã‚’äºˆæ¸¬ã«ä½¿ç”¨ã—ã€ç„¡é–¢ä¿‚ãªé‡ã¿ï¼ˆRHï¼‰ã‚„æœªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®é‡ã¿ï¼ˆIEï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚(ã†ã‚“ã†ã‚“...!:thinking:)

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

To measure the effect of embedding representation drifting on model performance, we evaluate the trained models back on the training data but on two different temporal segments. 
åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã®æ¼‚æµãŒãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã«ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã¾ã™ãŒã€2ã¤ã®ç•°ãªã‚‹æ™‚é–“çš„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã§è©•ä¾¡ã—ã¾ã™ã€‚
We take NE on 42-48 hours prior to the end of the training epoch and subtract NE on the last six hours of training. 
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒãƒƒã‚¯ã®çµ‚äº†42-48æ™‚é–“å‰ã®NEã‚’å–å¾—ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®æœ€å¾Œã®6æ™‚é–“ã®NEã‚’å¼•ãã¾ã™ã€‚
A smaller value indicates that the embedding representation shift caused by ID drifting affects model fit less. 
å°ã•ã„å€¤ã¯ã€IDã®æ¼‚æµã«ã‚ˆã£ã¦å¼•ãèµ·ã“ã•ã‚Œã‚‹åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã®ã‚·ãƒ•ãƒˆãŒãƒ¢ãƒ‡ãƒ«ã®é©åˆã«ä¸ãˆã‚‹å½±éŸ¿ãŒå°‘ãªã„ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
This is because our model is trained time-sequentially for one epoch, so the resulting model learns to fit the latest training time period at the end of training. 
ã“ã‚Œã¯ã€ç§ãŸã¡ã®ãƒ¢ãƒ‡ãƒ«ãŒ1ã‚¨ãƒãƒƒã‚¯ã®é–“ã«æ™‚é–“çš„ã«é †æ¬¡ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã‚‹ãŸã‚ã€çµæœã¨ã—ã¦å¾—ã‚‰ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®æœ€å¾Œã«æœ€æ–°ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“å¸¯ã«é©åˆã™ã‚‹ã“ã¨ã‚’å­¦ç¿’ã™ã‚‹ã‹ã‚‰ã§ã™ã€‚
The results are in Table 3(b). 
çµæœã¯è¡¨3(b)ã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚

![]()
table3(b)

Individual embeddings approach has a smaller performance gap compared with random hashing. 
**å€‹åˆ¥ã®åŸ‹ã‚è¾¼ã¿ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ã¨æ¯”è¼ƒã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ã‚®ãƒ£ãƒƒãƒ—ãŒå°ã•ã„**ã§ã™ã€‚
This highlights that random hashing suffers from ID drifting â€“ the embedding representations lose capability to represent older items over time as the weights are updated using new item examples. 
ã“ã‚Œã¯ã€**ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ãŒIDã®æ¼‚æµã«è‹¦ã—ã‚“ã§ã„ã‚‹ã“ã¨ã‚’å¼·èª¿ã—ã¦ã„ã¾ã™**ã€‚åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã¯ã€é‡ã¿ãŒæ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ã®ä¾‹ã‚’ä½¿ç”¨ã—ã¦æ›´æ–°ã•ã‚Œã‚‹ã«ã¤ã‚Œã¦ã€å¤ã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¡¨ç¾ã™ã‚‹èƒ½åŠ›ã‚’å¤±ã„ã¾ã™ã€‚
In contrast, Semantic ID matches the performance of individual embeddings, indicating that its learned representations are more stable over time. 
å¯¾ç…§çš„ã«ã€Semantic IDã¯å€‹åˆ¥ã®åŸ‹ã‚è¾¼ã¿ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«åŒ¹æ•µã—ã€ãã®å­¦ç¿’ã•ã‚ŒãŸè¡¨ç¾ãŒæ™‚é–“ã¨ã¨ã‚‚ã«ã‚ˆã‚Šå®‰å®šã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚(ã‚ã€ã§ã‚‚"åŒ¹æ•µ"ãã‚‰ã„ãªã®ã‹...!æ–°è¦ã‚¢ã‚¤ãƒ†ãƒ ã¸ã®å¯¾å¿œã¨ã¯ã¾ãŸã‚„ã‚„é•ã†æ€§èƒ½è©•ä¾¡ã®è©±ã ã‹ã‚‰ã‹ãª:thinking:)

We conjecture that this improved representation stability also allows the model to generalize better over much longer training durations, where ID drifting becomes even more pronounced. 
**ã“ã®æ”¹å–„ã•ã‚ŒãŸè¡¨ç¾ã®å®‰å®šæ€§ãŒã€IDã®æ¼‚æµãŒã•ã‚‰ã«é¡•è‘—ã«ãªã‚‹é•·æœŸé–“ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦ãƒ¢ãƒ‡ãƒ«ãŒã‚ˆã‚Šè‰¯ãä¸€èˆ¬åŒ–ã§ãã‚‹ã“ã¨ã‚’å¯èƒ½ã«ã™ã‚‹**ã¨æ¨æ¸¬ã—ã¾ã™ã€‚(ãªã‚‹ã»ã©ID driftã¯é•·æœŸé–“ã®å­¦ç¿’ã§å•é¡Œã«ãªã‚‹ã®ã‹...!æœŸé–“ã”ã¨ã«ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ãŒè¦³æ¸¬ã•ã‚Œã‚‹idãŒç•°ãªã‚‹ã‹ã‚‰...!:thinking:)
We train the RH and SemID models over a 20-day period and compare them to corresponding models trained only for the last four days of the period. 
**RHãŠã‚ˆã³SemIDãƒ¢ãƒ‡ãƒ«ã‚’20æ—¥é–“ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€æœŸé–“ã®æœ€å¾Œã®4æ—¥é–“ã®ã¿ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸå¯¾å¿œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒ**ã—ã¾ã™ã€‚
The results in Table 4 show that compared to random hashing, the performance of Semantic ID scales better with training data over a longer period. 
è¡¨4ã®çµæœã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ã¨æ¯”è¼ƒã—ã¦ã€Semantic IDã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒã‚ˆã‚Šé•·ã„æœŸé–“ã«ã‚ãŸã£ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚ˆã‚Šè‰¯ãã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

Table 4: NE improvement from training for 20 days of data instead of 4 days. 
è¡¨4: 4æ—¥é–“ã§ã¯ãªã20æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸéš›ã®NEã®æ”¹å–„ã€‚
(å­¦ç¿’æœŸé–“ã‚’éå»ã«ä¼¸ã°ã—ãŸæ–¹ãŒæ€§èƒ½ã¯åŸºæœ¬ä¸‹ãŒã£ã¡ã‚ƒã£ã¦ã‚‹ã®ã‹...!:thinking:)

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### 6.3 ã‚¢ã‚¤ãƒ†ãƒ è¡¨ç¾ç©ºé–“

To gain a better understanding of the item embedding representations, we extract the learned embedding weights from each trained model. 
ã‚¢ã‚¤ãƒ†ãƒ åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã‚’ã‚ˆã‚Šã‚ˆãç†è§£ã™ã‚‹ãŸã‚ã«ã€å„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å­¦ç¿’ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿é‡ã¿ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
One can view random hashing and Semantic ID as two different ways to partition the raw item ID corpus. 
ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ã¨Semantic IDã¯ã€ç”Ÿã®ã‚¢ã‚¤ãƒ†ãƒ IDã‚³ãƒ¼ãƒ‘ã‚¹ã‚’åˆ†å‰²ã™ã‚‹2ã¤ã®ç•°ãªã‚‹æ–¹æ³•ã¨è¦‹ãªã™ã“ã¨ãŒã§ãã¾ã™ã€‚
We wish to see whether the semantics-based partition produced by Semantic ID is better suited for the recommendation problem than the random partition produced by random hashing. 
ç§ãŸã¡ã¯ã€**Semantic IDã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸæ„å‘³ã«åŸºã¥ãåˆ†å‰²ãŒã€ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸãƒ©ãƒ³ãƒ€ãƒ ãªåˆ†å‰²ã‚ˆã‚Šã‚‚æ¨è–¦å•é¡Œã«é©ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã—ãŸã„**ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚(æ™®é€šã«è€ƒãˆã‚‹ã¨çµ¶å¯¾è‰¯ã•ãã†ã ã‘ã©...!:thinking:)

When several items are assigned to the same partition, they get mapped to the same embedding by the embedding lookup module. 
è¤‡æ•°ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒåŒã˜ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã‚‹ã¨ã€ãã‚Œã‚‰ã¯åŸ‹ã‚è¾¼ã¿ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ã‚ˆã£ã¦åŒã˜åŸ‹ã‚è¾¼ã¿ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã¾ã™ã€‚
We view this embedding vector as a summary of the per-item embeddings learned by the individual embeddings model. 
ç§ãŸã¡ã¯ã€ã“ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã€å€‹ã€…ã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å­¦ç¿’ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã”ã¨ã®åŸ‹ã‚è¾¼ã¿ã®è¦ç´„ã¨è¦‹ãªã—ã¾ã™ã€‚
While we fit the individual embeddings model for illustration purposes in this paper, IE is impractical in industry-level settings. 
ã“ã®è«–æ–‡ã§ã¯èª¬æ˜ã®ãŸã‚ã«å€‹ã€…ã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’é©åˆã•ã›ã¾ã™ãŒã€**IEã¯ç”£æ¥­ãƒ¬ãƒ™ãƒ«ã®è¨­å®šã§ã¯å®Ÿç”¨çš„ã§ã¯ã‚ã‚Šã¾ã›ã‚“**ã€‚
A partition with lower intra-partition embedding variance and higher inter-partition distances can be viewed as a more effective summary of individual embeddings. 
**ã‚ˆã‚Šä½ã„å†…éƒ¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿åˆ†æ•£ã¨ã‚ˆã‚Šé«˜ã„å¤–éƒ¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è·é›¢ã‚’æŒã¤ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã¯ã€å€‹ã€…ã®åŸ‹ã‚è¾¼ã¿ã®ã‚ˆã‚ŠåŠ¹æœçš„ãªè¦ç´„ã¨è¦‹ãªã™ã“ã¨ãŒã§ãã¾ã™**ã€‚(ãªã‚‹ã»ã©ã€‚ã“ã®è¦³ç‚¹ã‹ã‚‰åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã®è‰¯ã—æ‚ªã—ã‚’è©•ä¾¡ã™ã‚‹ã®ã‹...!:thinking:)
We compute these metrics for the RH and SemID partitions of the embeddings learned by the IE model. 
ç§ãŸã¡ã¯ã€IEãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å­¦ç¿’ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿ã®RHãŠã‚ˆã³SemIDãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«å¯¾ã—ã¦ã“ã‚Œã‚‰ã®æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã¾ã™ã€‚

We set the collision factor to 5555 for this experiment. 
ã“ã®å®Ÿé¨“ã§ã¯ã€è¡çªä¿‚æ•°ã‚’5555ã«è¨­å®šã—ã¾ã—ãŸã€‚(=ã¤ã¾ã‚Šã€åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã®1è¡Œã«å¹³å‡ã—ã¦5555å€‹ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã‚‹ã£ã¦ã“ã¨ã‹...!:thinking:)
As a result, the resulting clusters for RH and SemID partitions contain 5555 items on average. 
ãã®çµæœã€RHãŠã‚ˆã³SemIDãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¯å¹³å‡ã—ã¦5555ã‚¢ã‚¤ãƒ†ãƒ ã‚’å«ã¿ã¾ã™ã€‚(ã†ã‚“ã†ã‚“...!:thinking:)
However, since Semantic ID is the latent codes learned by an RQ-VAE model, the resulting cluster sizes are highly variable. 
ãŸã ã—ã€Semantic IDã¯RQ-VAEãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å­¦ç¿’ã•ã‚ŒãŸæ½œåœ¨ã‚³ãƒ¼ãƒ‰ã§ã‚ã‚‹ãŸã‚ã€çµæœã¨ã—ã¦å¾—ã‚‰ã‚Œã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µã‚¤ã‚ºã¯éå¸¸ã«å¤‰å‹•ã—ã¾ã™ã€‚
We compute metrics for two groups of Semantic ID clusters, small clusters with 4-10 items each and the top 1,000 clusters, where each cluster contains thousands of items. 
ç§ãŸã¡ã¯ã€4ã€œ10ã‚¢ã‚¤ãƒ†ãƒ ã‚’å«ã‚€å°ã•ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¨ã€å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒæ•°åƒã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’å«ã‚€ä¸Šä½1,000ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã®Semantic IDã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«å¯¾ã—ã¦æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
Table 5 contains the average variances and average pairwise distances, with standard deviations in parentheses. 
è¡¨5ã«ã¯ã€å¹³å‡åˆ†æ•£ã¨å¹³å‡ãƒšã‚¢ãƒ¯ã‚¤ã‚ºè·é›¢ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€æ¨™æº–åå·®ã¯æ‹¬å¼§å†…ã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚
The metrics are averaged across the embedding dimensions to produce single scalars for comparison. 
ã“ã‚Œã‚‰ã®æŒ‡æ¨™ã¯ã€æ¯”è¼ƒã®ãŸã‚ã«åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒå…¨ä½“ã§å¹³å‡åŒ–ã•ã‚Œã€å˜ä¸€ã®ã‚¹ã‚«ãƒ©ãƒ¼å€¤ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚

![]()
Table 5: Intra- and inter-cluster variances and pairwise distances for random hashing and SemID-based partitions. 
è¡¨5ï¼šãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ãŠã‚ˆã³SemIDãƒ™ãƒ¼ã‚¹ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®å†…éƒ¨ãŠã‚ˆã³å¤–éƒ¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®åˆ†æ•£ã¨ãƒšã‚¢ãƒ¯ã‚¤ã‚ºè·é›¢ã€‚

We observe that the Semantic ID partitions produce clusters with lower intra-cluster variance compared to random hashing. 
ç§ãŸã¡ã¯ã€**Semantic IDãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãŒãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ã¨æ¯”è¼ƒã—ã¦ã€ã‚ˆã‚Šä½ã„å†…éƒ¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ•£ã‚’æŒã¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨**ã‚’è¦³å¯Ÿã—ã¾ã™ã€‚
However, the resulting pairwise distances send a mixed signal. 
ã—ã‹ã—ã€çµæœã¨ã—ã¦å¾—ã‚‰ã‚Œã‚‹ãƒšã‚¢ãƒ¯ã‚¤ã‚ºè·é›¢ã¯æ··åˆä¿¡å·ã‚’é€ã‚Šã¾ã™ã€‚
We hypothesize that the low pairwise distances between the top 1,000 clusters is because RQ-VAE places multiple centroids into the regions with highest data density to minimize the model loss. 
ç§ãŸã¡ã¯ã€ä¸Šä½1,000ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é–“ã®ä½ã„ãƒšã‚¢ãƒ¯ã‚¤ã‚ºè·é›¢ã¯ã€RQ-VAEãŒãƒ¢ãƒ‡ãƒ«æå¤±ã‚’æœ€å°åŒ–ã™ã‚‹ãŸã‚ã«ã€ãƒ‡ãƒ¼ã‚¿å¯†åº¦ãŒæœ€ã‚‚é«˜ã„é ˜åŸŸã«è¤‡æ•°ã®ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚’é…ç½®ã™ã‚‹ãŸã‚ã§ã‚ã‚‹ã¨ä»®å®šã—ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### 6.4 ãƒ¦ãƒ¼ã‚¶å±¥æ­´ãƒ¢ãƒ‡ãƒªãƒ³ã‚°

In this section, we explore the effect of Semantic ID on user history modeling. 
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€**Semantic IDãŒãƒ¦ãƒ¼ã‚¶å±¥æ­´ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ä¸ãˆã‚‹å½±éŸ¿**ã‚’æ¢ã‚Šã¾ã™ã€‚
(ãƒ¦ãƒ¼ã‚¶å±¥æ­´ = ã‚¢ã‚¤ãƒ†ãƒ idã®sequence! :thinking:)
One role of the module is to contextualize and summarize the user history. 
ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å½¹å‰²ã®ä¸€ã¤ã¯ã€ãƒ¦ãƒ¼ã‚¶å±¥æ­´ã‚’æ–‡è„ˆåŒ–ã—è¦ç´„ã™ã‚‹ã“ã¨ã§ã™ã€‚

We find that using Semantic ID and a contextualizing attention-based aggregation module (PMA or Transformer) brings outsized gains compared to a baseline that does not contextualize the sequence (Bypass). 
ç§ãŸã¡ã¯ã€**Semantic IDã¨æ–‡è„ˆåŒ–ã•ã‚ŒãŸæ³¨æ„ãƒ™ãƒ¼ã‚¹ã®é›†ç´„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆPMAã¾ãŸã¯Transformerï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æ–‡è„ˆåŒ–ã—ãªã„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆBypassï¼‰ã¨æ¯”è¼ƒã—ã¦å¤§ããªåˆ©ç›ŠãŒå¾—ã‚‰ã‚Œã‚‹ã“ã¨**ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚

These results are summarized in Table 6. 
ã“ã‚Œã‚‰ã®çµæœã¯è¡¨6ã«ã¾ã¨ã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚


![]()
Table 6: Performance for three aggregation modules. Baseline: model with RH for each module. Semantic ID brings larger gains to the contextualizing modules.
è¡¨6ï¼š 3ç¨®ã®æ–‡è„ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€‚ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼šå„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«RHã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDã¯æ–‡è„ˆåŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ã‚ˆã‚Šå¤§ããªåˆ©ç›Šã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚
ï¼ˆæ–‡è„ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®é•ã„ã«ã‚ˆã‚‰ãšã€semantic idã®æ–¹ãŒæ€§èƒ½ãŒè‰¯ã‹ã£ãŸã¿ãŸã„!:thinking:ï¼‰

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! ä»¥ä¸‹ã®ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å†…å®¹ã¯ã¾ã ã‚ˆãã‚ã‹ã£ã¦ãªã„! -->

To understand how using Semantic ID changes the learned attention patterns in PMA and Transformer aggregation modules, we compute four metrics on the attention scores on a random subset of 1,000 evaluation examples.
Semantic IDã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§PMAãŠã‚ˆã³Transformeré›†ç´„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ãŠã‘ã‚‹å­¦ç¿’ã•ã‚ŒãŸæ³¨æ„ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã©ã®ã‚ˆã†ã«å¤‰åŒ–ã™ã‚‹ã‹ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€1,000ã®è©•ä¾¡ä¾‹ã®ãƒ©ãƒ³ãƒ€ãƒ ãªã‚µãƒ–ã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹æ³¨æ„ã‚¹ã‚³ã‚¢ã®4ã¤ã®ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’è¨ˆç®—ã—ã¾ã™ã€‚

Let $\mathbf{A} \in \mathbb{R}^{T \times S}$ be the attention score matrix, where $T$ is the target sequence length and $S$ is the source sequence length. 
$\mathbf{A} \in \mathbb{R}^{T \times S}$ã‚’æ³¨æ„ã‚¹ã‚³ã‚¢è¡Œåˆ—ã¨ã—ã€$T$ã¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é•·ã•ã€$S$ã¯ã‚½ãƒ¼ã‚¹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é•·ã•ã¨ã—ã¾ã™ã€‚
We have $T = S$ for Transformer and Bypass, and $T = 32$ for PMA. 
TransformerãŠã‚ˆã³Bypassã®å ´åˆã€$T = S$ã§ã‚ã‚Šã€PMAã®å ´åˆã¯$T = 32$ã§ã™ã€‚
Each row $\mathbf{a}_{i,:}$ of $\mathbf{A}$ represents a probability distribution over the source tokens. 
$\mathbf{A}$ã®å„è¡Œ$\mathbf{a}_{i,:}$ã¯ã‚½ãƒ¼ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã™ã‚‹ç¢ºç‡åˆ†å¸ƒã‚’è¡¨ã—ã¾ã™ã€‚
The metrics we consider are defined as follows. 
ç§ãŸã¡ãŒè€ƒæ…®ã™ã‚‹ãƒ¡ãƒˆãƒªãƒƒã‚¯ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã¾ã™ã€‚

First source token attention: 
æœ€åˆã®ã‚½ãƒ¼ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã®æ³¨æ„ï¼š

Padding token attention: 
ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®æ³¨æ„ï¼š

Entropy: 
ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼š

Token self-attention: 
ãƒˆãƒ¼ã‚¯ãƒ³è‡ªå·±æ³¨æ„ï¼š

![]()
Table 7: Attention score-based evaluation metrics for random hashing and SemID-based models for the user history item interaction features. 
è¡¨7ï¼šãƒ¦ãƒ¼ã‚¶å±¥æ­´ã‚¢ã‚¤ãƒ†ãƒ ç›¸äº’ä½œç”¨æ©Ÿèƒ½ã«å¯¾ã™ã‚‹ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ãŠã‚ˆã³SemIDãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã®æ³¨æ„ã‚¹ã‚³ã‚¢ã«åŸºã¥ãè©•ä¾¡ãƒ¡ãƒˆãƒªãƒƒã‚¯ã€‚

From the metric readings in Table 7, we see that models trained with Semantic ID have lower entropy, token self-attention, and padding token attention, and higher attention score on the first source token in the sequence. 
è¡¨7ã®ãƒ¡ãƒˆãƒªãƒƒã‚¯ã®èª­ã¿å–ã‚Šã‹ã‚‰ã€Semantic IDã§è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€ãƒˆãƒ¼ã‚¯ãƒ³è‡ªå·±æ³¨æ„ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®æ³¨æ„ãŒä½ãã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®æœ€åˆã®ã‚½ãƒ¼ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã™ã‚‹æ³¨æ„ã‚¹ã‚³ã‚¢ãŒé«˜ã„ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚
This means that Semantic ID-based models place more weight on higher-signal tokens (i.e., the first and most recent item in the sequence, rather than earlier and potentially stale tokens or padding tokens), have attention score distributions that are less diffuse over the entire sequence (i.e., lower entropy), and for Transformer, place higher weights on other tokens rather than self-attending. 
ã“ã‚Œã¯ã€Semantic IDãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ãŒé«˜ä¿¡å·ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆã™ãªã‚ã¡ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®æœ€åˆã®ã‚¢ã‚¤ãƒ†ãƒ ãŠã‚ˆã³æœ€ã‚‚æœ€è¿‘ã®ã‚¢ã‚¤ãƒ†ãƒ ï¼‰ã«ã‚ˆã‚Šå¤šãã®é‡ã¿ã‚’ç½®ãã€å…¨ä½“ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã™ã‚‹æ³¨æ„ã‚¹ã‚³ã‚¢åˆ†å¸ƒãŒã‚ˆã‚Šåˆ†æ•£ã—ã¦ã„ãªã„ï¼ˆã™ãªã‚ã¡ã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒä½ã„ï¼‰ã“ã¨ã‚’æ„å‘³ã—ã€Transformerã®å ´åˆã¯è‡ªå·±æ³¨æ„ã§ã¯ãªãä»–ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ã‚ˆã‚Šé«˜ã„é‡ã¿ã‚’ç½®ãã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
These metrics are promising signals that Semantic ID item representations are more stable and meaningful than their random hashing-based counterparts in user history modeling. 
**ã“ã‚Œã‚‰ã®ãƒ¡ãƒˆãƒªãƒƒã‚¯ã¯ã€Semantic IDã‚¢ã‚¤ãƒ†ãƒ è¡¨ç¾ãŒãƒ¦ãƒ¼ã‚¶å±¥æ­´ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ãŠã„ã¦ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ãƒ™ãƒ¼ã‚¹ã®å¯¾å¿œç‰©ã‚ˆã‚Šã‚‚ã‚ˆã‚Šå®‰å®šã—ã¦ãŠã‚Šã€æ„å‘³ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã™æœ‰æœ›ãªä¿¡å·**ã§ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## 7Productionization 7ç”Ÿç”£åŒ–

The Semantic ID features have been productionized in Meta Ads Recommendation System for more than a year. 
Semantic IDç‰¹å¾´é‡ã¯ã€Meta Ads Recommendation Systemã§1å¹´ä»¥ä¸Šã«ã‚ãŸã‚Šæœ¬ç•ªæ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
They serve as top sparse features in the existing ads ranking models according to feature importance studies. 
**ãã‚Œã‚‰ã¯ã€ç‰¹å¾´é‡è¦åº¦ã®ç ”ç©¶ã«åŸºã¥ã„ã¦ã€æ—¢å­˜ã®åºƒå‘Šãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹ä¸»è¦ãªã‚¹ãƒ‘ãƒ¼ã‚¹ç‰¹å¾´ã¨ã—ã¦æ©Ÿèƒ½ã—ã¦ã„ã¾ã™ã€‚**
In this section, we provide an overview of the online serving pipeline and key implementation details. 
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚µãƒ¼ãƒ“ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ä¸»è¦ãªå®Ÿè£…ã®è©³ç´°ã«ã¤ã„ã¦æ¦‚èª¬ã—ã¾ã™ã€‚

### 7.1 ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ RQ-VAE ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

The RQ-VAE models are trained on Content Understanding (CU) models for ads ranking at Meta. 
RQ-VAEãƒ¢ãƒ‡ãƒ«ã¯ã€**Metaã®åºƒå‘Šãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ãŸã‚ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç†è§£ï¼ˆCUï¼‰ãƒ¢ãƒ‡ãƒ«**ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¾ã™ã€‚
The CU models are pre-trained on the public CC100 dataset Conneau (2020) and then fine-tuned on internal ads datasets. 
CUãƒ¢ãƒ‡ãƒ«ã¯ã€**å…¬é–‹ã•ã‚ŒãŸCC100ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆConneau, 2020ï¼‰ã§äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã€ãã®å¾Œã€å†…éƒ¨åºƒå‘Šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**ã•ã‚Œã¾ã™ã€‚(ãªã‚‹ã»ã©...! ã“ã†ã„ã†æ‰‹ã‚‚ã‚ã‚‹ã®ã‹...! CC100ã¯ãŸã è¨€èªãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã ã£ãŸã€‚ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªç¤¾ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã™ã‚‹å‰ã«MINDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§äº‹å‰å­¦ç¿’ã™ã‚‹ã€ã¿ãŸã„ãªæ‰‹ã‚‚ã‚ã‚‹ã®ã‹ãª...??:thinking:)
We sample ad IDs and their corresponding content embeddings from the past three monthsâ€™ data and train the RQ-VAE model offline. 
ç§ãŸã¡ã¯ã€éå»3ã‹æœˆã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åºƒå‘ŠIDã¨ãã‚Œã«å¯¾å¿œã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åŸ‹ã‚è¾¼ã¿ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€RQ-VAEãƒ¢ãƒ‡ãƒ«ã‚’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚
For production models, we train RQ-VAEs with $L=6$ and $K=2048$, and Semantic ID follows the design of prefix-5gram from Section 4.2 with $H=O(50M)$. 
ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€$L=6$ãŠã‚ˆã³$K=2048$ã§RQ-VAEã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€Semantic IDã¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.2ã®prefix-5gramã®è¨­è¨ˆã«å¾“ã„ã€$H=O(50M)$ã¨ãªã‚Šã¾ã™ã€‚
After training, we use a frozen RQ-VAE checkpoint for online serving. 
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚µãƒ¼ãƒ“ã‚¹ã®ãŸã‚ã«ãƒ•ãƒ­ãƒ¼ã‚ºãƒ³RQ-VAEãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### 7.2 Online Semantic ID Serving System 7.2 ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 

![]()
Figure 4: å›³4:Semantic ID serving pipeline. ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDã‚µãƒ¼ãƒ“ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚

Figure 4 shows the online serving pipeline of real-time Semantic ID features. 
å›³4ã¯ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDæ©Ÿèƒ½ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚µãƒ¼ãƒ“ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
At ad creation time, we process the ad content information and provide it to the CU models. 
**åºƒå‘Šä½œæˆæ™‚ã«ã€åºƒå‘Šã‚³ãƒ³ãƒ†ãƒ³ãƒ„æƒ…å ±ã‚’å‡¦ç†ã—ã€ãã‚Œã‚’CUãƒ¢ãƒ‡ãƒ«ã«æä¾›**ã—ã¾ã™ã€‚(ã†ã‚“ã†ã‚“ã€streamlingã‚‚ã—ãã¯ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã§å‡¦ç†ã™ã‚‹æ„Ÿã˜ã‹ãª...!:thinking:)
The output CU embeddings are then passed through the RQ-VAE model which computes the Semantic ID signal for each raw ID. 
å‡ºåŠ›ã•ã‚ŒãŸCUåŸ‹ã‚è¾¼ã¿ã¯ã€RQ-VAEãƒ¢ãƒ‡ãƒ«ã‚’é€šéã—ã€å„ç”ŸIDã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDä¿¡å·(=ã“ã“ã§ã¯idãªã®ã‹...!:thinking:)ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
The signal is then stored in the Entity Data Store. 
ãã®ä¿¡å·ã¯ã€ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚(=feature storeçš„ãªã‚„ã¤!:thinking:)
At the feature generation stage, the target item raw ID and user engagement raw ID histories are enriched with the Semantic ID signal from the Entity Data Store to produce semantic features. 
ç‰¹å¾´(=idã®embedding)ç”Ÿæˆæ®µéšã§ã¯ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¢ã‚¤ãƒ†ãƒ ã®ç”ŸIDã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã®ç”ŸIDå±¥æ­´ãŒã€ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢ã‹ã‚‰ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDä¿¡å·ã§å¼·åŒ–ã•ã‚Œã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ç‰¹å¾´(=ã“ã“ã§idã‹ã‚‰embeddingã«ãªã‚‹!:thinking:)ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚
When serving requests arrive, the precomputed features are fetched and passed to the downstream ranking models. 
**ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒåˆ°ç€ã™ã‚‹ã¨ã€äº‹å‰è¨ˆç®—ã•ã‚ŒãŸç‰¹å¾´ãŒå–å¾—ã•ã‚Œã€ä¸‹æµã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã•ã‚Œã¾ã™**ã€‚(ã†ã‚“ã†ã‚“ã€‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–æ™‚ã«ã¯äº‹å‰è¨ˆç®—ã•ã‚ŒãŸembeddingã‚’å–å¾—ã™ã‚‹ã ã‘!:thinking:)

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->


### 7.3 ç”Ÿç”£æ€§èƒ½ã®å‘ä¸Š

We created six sparse and one sequential feature from different content embedding sources, including text, image, and video, and report the NE gain on the flagship Meta ads ranking model in Table 8. 
ç§ãŸã¡ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€å‹•ç”»ã‚’å«ã‚€ã•ã¾ã–ã¾ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„åŸ‹ã‚è¾¼ã¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰6ã¤ã®ã‚¹ãƒ‘ãƒ¼ã‚¹ç‰¹å¾´ã¨1ã¤ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ç‰¹å¾´ã‚’ä½œæˆã—ã€è¡¨8ã®ãƒ•ãƒ©ãƒƒã‚°ã‚·ãƒƒãƒ—Metaåºƒå‘Šãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹NEã®å‘ä¸Šã‚’å ±å‘Šã—ã¾ã™ã€‚
In Meta ads ranking, an offline NE gain larger than 0.02% is considered significant. 
**Metaåºƒå‘Šãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§ã¯ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã®NEã®å‘ä¸ŠãŒ0.02%ã‚’è¶…ãˆã‚‹ã¨significant**ã¨è¦‹ãªã•ã‚Œã¾ã™ã€‚
Overall, across multiple ads ranking models, incorporating the Semantic ID features have yielded a 0.15% gain in performance on our top-line online metric. 
å…¨ä½“ã¨ã—ã¦ã€è¤‡æ•°ã®åºƒå‘Šãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€Semantic IDç‰¹å¾´ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€ç§ãŸã¡ã®ä¸»è¦ãªã‚ªãƒ³ãƒ©ã‚¤ãƒ³æŒ‡æ¨™ã§0.15%ã®æ€§èƒ½å‘ä¸ŠãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚
As the Meta Ads recommender serves billions of users and has been one of the most optimized models in the company, a 0.15% online performance gain is considered significant. 
**Meta Adsãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã¯æ•°åå„„ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã—ã€ä¼šç¤¾ã®ä¸­ã§æœ€ã‚‚æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®1ã¤ã§ã‚ã‚‹ãŸã‚ã€0.15%ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ€§èƒ½å‘ä¸Šã¯significantã¨è¦‹ãªã•ã‚Œã¾ã™ã€‚**(ä¾¡å€¤ã®ã‚ã‚‹åŠ¹æœé‡ã£ã¦ã“ã¨ã­...!:thinking:)

![]()
Table 8: NE improvement from incorporating Semantic ID features in the flagship Meta ads ranking model. 
è¡¨8ï¼šãƒ•ãƒ©ãƒƒã‚°ã‚·ãƒƒãƒ—Metaåºƒå‘Šãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹Semantic IDç‰¹å¾´ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã«ã‚ˆã‚‹NEã®æ”¹å–„ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### 7.4 Semanic and Prediction Similarity 7.4 ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãŠã‚ˆã³äºˆæ¸¬é¡ä¼¼æ€§

Intuitively, one may think that if two items are semantically similar, their user engagement patterns will also be similar. 
ç›´æ„Ÿçš„ã«ã¯ã€2ã¤ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã«é¡ä¼¼ã—ã¦ã„ã‚‹å ´åˆã€ãã‚Œã‚‰ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚é¡ä¼¼ã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã—ã‹ã—ã€**ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ã‚„èªè­˜ã¯ã‚ˆã‚Šå¾®å¦™ã§ã‚ã‚Šã€ã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹ã«é–¢ã—ã¦äºˆæ¸¬å¯èƒ½ã«é€£ç¶šã—ã¦ã„ã‚‹ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“**ã€‚(=è¦ã™ã‚‹ã«ã€ãƒ¦ãƒ¼ã‚¶ã®å—œå¥½ã£ã¦å†…å®¹ãƒ™ãƒ¼ã‚¹ã ã‘ã˜ã‚ƒãªã„ã‚ˆã­ã€ã¿ãŸã„ãªè©±??:thinking:)
For robust delivery performance with Semantic ID, we must ensure a degree of continuity (or correlation) of the ranking modelâ€™s behaviour with respect to the semantic similarity relation between the items in our system. 
Semantic IDã‚’ç”¨ã„ãŸå …ç‰¢ãªé…ä¿¡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«ã¯ã€ã‚·ã‚¹ãƒ†ãƒ å†…ã®ã‚¢ã‚¤ãƒ†ãƒ é–“ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é¡ä¼¼æ€§é–¢ä¿‚ã«é–¢ã—ã¦ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®æŒ™å‹•ã®é€£ç¶šæ€§ï¼ˆã¾ãŸã¯ç›¸é–¢ï¼‰ã‚’ç¢ºä¿ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

To measure this correlation, we conduct an online A/B test where we select a set $S$ of items that are recommended to a user by our system. 
ã“ã®ç›¸é–¢ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã«ã€ç§ãŸã¡ã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³A/Bãƒ†ã‚¹ãƒˆã‚’å®Ÿæ–½ã—ã€ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã£ã¦ãƒ¦ãƒ¼ã‚¶ã«æ¨è–¦ã•ã‚Œã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚»ãƒƒãƒˆ$S$ã‚’é¸æŠã—ã¾ã™ã€‚
For a given user, with 50% probability, we mutate the set $S$ to $S'$ by randomly swapping an item in $S$ with a different item with the same prefix from Semantic ID. 
**ç‰¹å®šã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦ã€50%ã®ç¢ºç‡ã§ã€ã‚»ãƒƒãƒˆ$S$ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’Semantic IDã®åŒã˜ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’æŒã¤åˆ¥ã®ã‚¢ã‚¤ãƒ†ãƒ ã¨ãƒ©ãƒ³ãƒ€ãƒ ã«å…¥ã‚Œæ›¿ãˆã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€ã‚»ãƒƒãƒˆ$S$ã‚’$S'$ã«å¤‰ç•°ã•ã›ã¾ã™**ã€‚
This operation results in a ...
ã“ã®æ“ä½œçµæœã¯...

$$
\text{Click Loss Rate} = \frac{\text{CTR on S'} - \text{CTR on S}}{\text{CTR on S}}
\tag{3}
$$

The click loss rate decrease from using deeper prefixes from Semantic ID is summarized in Figure 5. 
Semantic IDã‹ã‚‰ã®ã‚ˆã‚Šæ·±ã„ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã«ã‚ˆã‚‹ã‚¯ãƒªãƒƒã‚¯æå¤±ç‡ã®æ¸›å°‘ã¯ã€å›³5ã«ã¾ã¨ã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

Figure 5: Click Loss Rate reduction from Semantic ID. 
å›³5ï¼šSemantic IDã‹ã‚‰ã®ã‚¯ãƒªãƒƒã‚¯æå¤±ç‡ã®æ¸›å°‘ã€‚

Since Semantic ID partitions the item corpus based on item semantics, we conclude that prediction similarity is correlated to semantic similarity. 
Semantic IDãŒã‚¢ã‚¤ãƒ†ãƒ ã®ã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹ã«åŸºã¥ã„ã¦ã‚¢ã‚¤ãƒ†ãƒ ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’åˆ†å‰²ã™ã‚‹ãŸã‚ã€**äºˆæ¸¬é¡ä¼¼æ€§ã¯ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é¡ä¼¼æ€§ã¨ç›¸é–¢ã—ã¦ã„ã‚‹**ã¨çµè«–ä»˜ã‘ã¾ã™ã€‚
This supports the representation space analysis result in Section 6.3. 
ã“ã‚Œã¯ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³6.3ã®è¡¨ç¾ç©ºé–“åˆ†æçµæœã‚’æ”¯æŒã—ã¾ã™ã€‚
Moreover, the hierarchy of codes in Semantic ID effectively captures the finer-grained details of an itemâ€™s semantics: deeper prefixes monotonically reduce the click loss rate. 
ã•ã‚‰ã«ã€**Semantic IDã®ã‚³ãƒ¼ãƒ‰ã®éšå±¤ã¯ã€ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹ã®ã‚ˆã‚Šç´°ã‹ã„è©³ç´°ã‚’åŠ¹æœçš„ã«æ‰ãˆã¦ã„ã¾ã™ï¼šã‚ˆã‚Šæ·±ã„ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã¯ã‚¯ãƒªãƒƒã‚¯æå¤±ç‡ã‚’å˜èª¿ã«æ¸›å°‘ã•ã›ã¾ã™**ã€‚

(å˜ã«ä¸€æ‹¬ã§åŸ‹ã‚è¾¼ã¿ã«å¤‰æ›ã™ã‚‹ã‚ˆã‚Šã‚‚ã€ã“ã†ã‚„ã£ã¦æ˜ç¤ºçš„ã«éšå±¤çš„ã«ã—ãŸæ–¹ãŒã€ã„ã„æ„Ÿã˜ã«ã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹ã‚’æ‰ãˆã‚‰ã‚ŒãŸã‚Šã™ã‚‹ã®ã‹ãª...!:thinking:)

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

### 7.5A/A Variance 7.5 A/Aåˆ†æ•£

Yet another disadvantage of a ranking model based on random hashing is inherent model prediction variance, resulting in downstream ad delivery variance. 
ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ã«åŸºã¥ããƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®ã‚‚ã†ä¸€ã¤ã®æ¬ ç‚¹ã¯ã€å›ºæœ‰ã®ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬åˆ†æ•£ã§ã‚ã‚Šã€ã“ã‚ŒãŒä¸‹æµã®åºƒå‘Šé…ä¿¡ã®åˆ†æ•£ã‚’å¼•ãèµ·ã“ã—ã¾ã™ã€‚
Specifically, one can create a copy of an item with a different raw item ID. 
å…·ä½“çš„ã«ã¯ã€ç•°ãªã‚‹ç”Ÿã®ã‚¢ã‚¤ãƒ†ãƒ IDã‚’æŒã¤ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
Then, both the original and item copy enter the recommendation system. 
ãã®å¾Œã€å…ƒã®ã‚¢ã‚¤ãƒ†ãƒ ã¨ãã®ã‚³ãƒ”ãƒ¼ã®ä¸¡æ–¹ãŒæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«å…¥ã‚Šã¾ã™ã€‚
Because the embeddings will be different for the original and item copy after hashing, the model predictions and delivery system behavior will also differ. 
ãƒãƒƒã‚·ãƒ³ã‚°å¾Œã€å…ƒã®ã‚¢ã‚¤ãƒ†ãƒ ã¨ãã®ã‚³ãƒ”ãƒ¼ã®åŸ‹ã‚è¾¼ã¿ãŒç•°ãªã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã¨é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œã‚‚ç•°ãªã‚Šã¾ã™ã€‚
We call this phenomenon the A/A variance, where â€œA/Aâ€ signifies that we consider an exact copy of the original item. 
ç§ãŸã¡ã¯ã“ã®ç¾è±¡ã‚’A/Aåˆ†æ•£ã¨å‘¼ã³ã¾ã™ã€‚ã€ŒA/Aã€ã¯ã€å…ƒã®ã‚¢ã‚¤ãƒ†ãƒ ã®æ­£ç¢ºãªã‚³ãƒ”ãƒ¼ã‚’è€ƒæ…®ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
A related type of a ranking model variance is the A/Aâ€™ variance where one instead considers minor item differences instead of exact copies, such as two images with the same object in the foreground, but with backgrounds of different colors. 
ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®åˆ†æ•£ã®é–¢é€£ã™ã‚‹ã‚¿ã‚¤ãƒ—ã¯A/Aâ€™åˆ†æ•£ã§ã‚ã‚Šã€æ­£ç¢ºãªã‚³ãƒ”ãƒ¼ã®ä»£ã‚ã‚Šã«ã€å‰æ™¯ã«åŒã˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒã‚ã‚‹ãŒèƒŒæ™¯ã®è‰²ãŒç•°ãªã‚‹2ã¤ã®ç”»åƒã®ã‚ˆã†ãªã€ã‚ãšã‹ãªã‚¢ã‚¤ãƒ†ãƒ ã®é•ã„ã‚’è€ƒæ…®ã—ã¾ã™ã€‚
This variance is undesirable since it reduces the robustness of the downstream ad ranking orders and the systemâ€™s ability to accurately target correct audiences. 
ã“ã®åˆ†æ•£ã¯æœ›ã¾ã—ããªãã€ä¸‹æµã®åºƒå‘Šãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®é †åºã®å …ç‰¢æ€§ã‚’ä½ä¸‹ã•ã›ã€ã‚·ã‚¹ãƒ†ãƒ ãŒæ­£ã—ã„ã‚ªãƒ¼ãƒ‡ã‚£ã‚¨ãƒ³ã‚¹ã‚’æ­£ç¢ºã«ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã™ã‚‹èƒ½åŠ›ã‚’æ¸›å°‘ã•ã›ã¾ã™ã€‚
Semantic ID helps reduce the A/A variance by eliminating the randomness caused by random hashing â€“ exact copies or very similar items will often have the same $k$-prefix Semantic ID. 
Semantic IDã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ã«ã‚ˆã£ã¦å¼•ãèµ·ã“ã•ã‚Œã‚‹ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’æ’é™¤ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€A/Aåˆ†æ•£ã‚’æ¸›å°‘ã•ã›ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚æ­£ç¢ºãªã‚³ãƒ”ãƒ¼ã‚„éå¸¸ã«ä¼¼ãŸã‚¢ã‚¤ãƒ†ãƒ ã¯ã€ã—ã°ã—ã°åŒã˜$k$-ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®Semantic IDã‚’æŒã¡ã¾ã™ã€‚

A/A variance A/Aåˆ†æ•£

A related type of a ranking model variance is the A/Aâ€™ variance where one instead considers minor item differences instead of exact copies, such as two images with the same object in the foreground, but with backgrounds of different colors. 
ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®åˆ†æ•£ã®é–¢é€£ã™ã‚‹ã‚¿ã‚¤ãƒ—ã¯A/Aâ€™åˆ†æ•£ã§ã‚ã‚Šã€æ­£ç¢ºãªã‚³ãƒ”ãƒ¼ã®ä»£ã‚ã‚Šã«ã€å‰æ™¯ã«åŒã˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒã‚ã‚‹ãŒèƒŒæ™¯ã®è‰²ãŒç•°ãªã‚‹2ã¤ã®ç”»åƒã®ã‚ˆã†ãªã€ã‚ãšã‹ãªã‚¢ã‚¤ãƒ†ãƒ ã®é•ã„ã‚’è€ƒæ…®ã—ã¾ã™ã€‚

We set up an online shadow ads experiment where we measure the relative A/A prediction difference (AAR) for a given model. 
ç§ãŸã¡ã¯ã€ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹ç›¸å¯¾çš„ãªA/Aäºˆæ¸¬å·®ï¼ˆAARï¼‰ã‚’æ¸¬å®šã™ã‚‹ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚·ãƒ£ãƒ‰ãƒ¼åºƒå‘Šå®Ÿé¨“ã‚’è¨­å®šã—ã¾ã—ãŸã€‚

For an A/A pair $(a_{1}, a_{2})$, 
A/Aãƒšã‚¢ $(a_{1}, a_{2})$ ã«å¯¾ã—ã¦ã€

where $p(a_{i})$ is the ranking model prediction for item $a_{i}$. 
ã“ã“ã§ã€$p(a_{i})$ ã¯ã‚¢ã‚¤ãƒ†ãƒ  $a_{i}$ ã«å¯¾ã™ã‚‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã§ã™ã€‚

The production model with six Semantic ID sparse features achieves a $43\%$ reduction in average AAR compared to the same model without the six features. 
6ã¤ã®Semantic IDã‚¹ãƒ‘ãƒ¼ã‚¹ç‰¹å¾´ã‚’æŒã¤ç”Ÿç”£ãƒ¢ãƒ‡ãƒ«ã¯ã€6ã¤ã®ç‰¹å¾´ãŒãªã„åŒã˜ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦ã€å¹³å‡AARã‚’$43\%$å‰Šæ¸›ã—ã¾ã™ã€‚
We believe that the majority of AAR reduction is from the tail items, as studied in Section 6.2. 
ç§ãŸã¡ã¯ã€AARå‰Šæ¸›ã®å¤§éƒ¨åˆ†ãŒã‚»ã‚¯ã‚·ãƒ§ãƒ³6.2ã§ç ”ç©¶ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ«ã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰æ¥ã¦ã„ã‚‹ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚

<!-- ã“ã®ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ã¡ã‚ƒã‚“ã¨èª­ã‚“ã§ãªã„ãŒä¸€æ—¦OK! -->

## 8Conclusion çµè«–

We show how Semantic ID can be used to create a stable ID space for item representation and propose Semantic ID prefix-ngram, which significantly improves Semantic IDâ€™s performance in ranking models. 
ç§ãŸã¡ã¯ã€Semantic IDãŒã‚¢ã‚¤ãƒ†ãƒ è¡¨ç¾ã®ãŸã‚ã®å®‰å®šã—ãŸIDç©ºé–“ã‚’ä½œæˆã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã€Semantic ID prefix-ngramã‚’ææ¡ˆã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹Semantic IDã®æ€§èƒ½ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã¾ã™ã€‚
In offline experiments, we study trained ranking models and find that under Semantic ID, the harmful effects of embedding representation instability are mitigated compared to random hashing and individual embeddings baselines. 
ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å®Ÿé¨“ã§ã¯ã€è¨“ç·´ã•ã‚ŒãŸãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’ç ”ç©¶ã—ã€Semantic IDã®ä¸‹ã§ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒã‚·ãƒ³ã‚°ã‚„å€‹åˆ¥ã®åŸ‹ã‚è¾¼ã¿ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒã—ã¦ã€åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã®ä¸å®‰å®šæ€§ã®æœ‰å®³ãªå½±éŸ¿ãŒè»½æ¸›ã•ã‚Œã‚‹ã“ã¨ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚
We detail the successful productionization of Semantic ID features in Metaâ€™s ads recommendation system, and show that the online production system obtains significant performance gains as well as reduced downstream ad delivery variance. 
ç§ãŸã¡ã¯ã€Metaã®åºƒå‘Šæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹Semantic IDæ©Ÿèƒ½ã®æˆåŠŸã—ãŸè£½å“åŒ–ã®è©³ç´°ã‚’èª¬æ˜ã—ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è£½å“ã‚·ã‚¹ãƒ†ãƒ ãŒé‡è¦ãªæ€§èƒ½å‘ä¸Šã¨ä¸‹æµã®åºƒå‘Šé…ä¿¡ã®ã°ã‚‰ã¤ãã®æ¸›å°‘ã‚’é”æˆã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## Appendix A Aggregation Module Architectures ä»˜éŒ²A é›†ç´„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### Bypass. ãƒã‚¤ãƒ‘ã‚¹ã€‚

Apply a linear weight matrix $\mathbf{W} \in \mathbb{R}^{d_{m} \times d_{m}}$ to each embedding separately,  
å„åŸ‹ã‚è¾¼ã¿ã«å¯¾ã—ã¦ç·šå½¢é‡ã¿è¡Œåˆ— $\mathbf{W} \in \mathbb{R}^{d_{m} \times d_{m}}$ ã‚’é©ç”¨ã—ã¾ã™ã€‚

$$
Bypass(X) := XW
\tag{5}
$$


### Transformer (Vaswani, 2017). 

Apply a Transformer layer to the embedding sequence. The attention submodule is defined as  
åŸ‹ã‚è¾¼ã¿ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«Transformerå±¤ã‚’é©ç”¨ã—ã¾ã™ã€‚æ³¨æ„ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã¾ã™ã€‚

$$
Attention(X) := \text{softmax}\left(\frac{XW^{Q}(XW^{K})^{T}}{\sqrt{d_{m}}}\right)XW^{V}
\tag{6}
$$

where $\mathbf{W}^{Q}, \mathbf{W}^{K}, \mathbf{W}^{V} \in \mathbb{R}^{d_{m} \times d_{a}}$ are the query, key, and value weight matrices, respectively, and $d_{a}$ is the query/key/value vector dimension. The full Transformer module is given by  
ã“ã“ã§ã€$\mathbf{W}^{Q}, \mathbf{W}^{K}, \mathbf{W}^{V} \in \mathbb{R}^{d_{m} \times d_{a}}$ ã¯ãã‚Œãã‚Œã‚¯ã‚¨ãƒªã€ã‚­ãƒ¼ã€ãƒãƒªãƒ¥ãƒ¼ã®é‡ã¿è¡Œåˆ—ã§ã‚ã‚Šã€$d_{a}$ ã¯ã‚¯ã‚¨ãƒª/ã‚­ãƒ¼/ãƒãƒªãƒ¥ãƒ¼ã®ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒã§ã™ã€‚å®Œå…¨ãªTransformerãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯æ¬¡ã®ã‚ˆã†ã«ä¸ãˆã‚‰ã‚Œã¾ã™ã€‚

$$
X^{(1)} = Attention(LayerNorm(X)) + X
\tag{7}
$$

$$
X^{(2)} = MLP(LayerNorm(X^{(1)})) + X^{(1)}
\tag{8}
$$

where LayerNorm and MLP designate the standard layer norm and position-wise MLP layers. We add standard positional embeddings to the encoding before applying Transformer or PMA modules.  
ã“ã“ã§ã€LayerNormã¨MLPã¯æ¨™æº–ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒãƒ«ãƒ ã¨ä½ç½®ã”ã¨ã®MLPå±¤ã‚’æŒ‡å®šã—ã¾ã™ã€‚Transformerã¾ãŸã¯PMAãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é©ç”¨ã™ã‚‹å‰ã«ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«æ¨™æº–çš„ãªä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’è¿½åŠ ã—ã¾ã™ã€‚

### Pooled Multihead Attention (PMA) (Lee et al., 2019). 

Apply a Transformer layer to the embedding sequence, but replace the attention query vectors with $d_{s}$ learnable weight vectors. The PMA attention submodule is defined as  
åŸ‹ã‚è¾¼ã¿ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«Transformerå±¤ã‚’é©ç”¨ã—ã¾ã™ãŒã€æ³¨æ„ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ« ($d_{m}$) ã‚’$d_{s}$ã®å­¦ç¿’å¯èƒ½ãªé‡ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«ç½®ãæ›ãˆã¾ã™ã€‚PMAæ³¨æ„ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã¾ã™ã€‚

$$
PMAttention(X) := \text{softmax}\left(\frac{S(XW^{K})^{T}}{\sqrt{d_{m}}}\right)XW^{V}
\tag{9}
$$

where $\mathbf{S} \in \mathbb{R}^{d_{s} \times d_{a}}$ is comprised of $d_{s}$ learnable query vectors, or seeds. In our experiments, $d_{s} = 32$.  
ã“ã“ã§ã€$\mathbf{S} \in \mathbb{R}^{d_{s} \times d_{a}}$ ã¯$d_{s}$ã®å­¦ç¿’å¯èƒ½ãªã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«ã€ã¾ãŸã¯ã‚·ãƒ¼ãƒ‰ã‹ã‚‰æ§‹æˆã•ã‚Œã¾ã™ã€‚ç§ãŸã¡ã®å®Ÿé¨“ã§ã¯ã€$d_{s} = 32$ã§ã™ã€‚

The PMA module is formed using the same equations as for the Transformer module (Equations 7 and 8), except that PMAttention is used in place of Attention.  
PMAãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€Transformerãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨åŒã˜æ–¹ç¨‹å¼ï¼ˆæ–¹ç¨‹å¼7ãŠã‚ˆã³8ï¼‰ã‚’ä½¿ç”¨ã—ã¦å½¢æˆã•ã‚Œã¾ã™ãŒã€Attentionã®ä»£ã‚ã‚Šã«PMAttentionãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->

## Appendix B ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDã®ã‚¯ãƒªãƒƒã‚¯åˆ†å¸ƒ

Appendix B  
å›³6:  
Figure 6  
The 30-day click distribution in raw ID and Semantic ID spaces.  
30æ—¥é–“ã®ç”ŸIDãŠã‚ˆã³ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDç©ºé–“ã«ãŠã‘ã‚‹ã‚¯ãƒªãƒƒã‚¯åˆ†å¸ƒã€‚  
The click distribution in Semantic ID space (Figure 6) clearly exhibits less skew compared to the click distribution in the raw ID space.  
ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDç©ºé–“ã«ãŠã‘ã‚‹ã‚¯ãƒªãƒƒã‚¯åˆ†å¸ƒï¼ˆå›³6ï¼‰ã¯ã€ç”ŸIDç©ºé–“ã«ãŠã‘ã‚‹ã‚¯ãƒªãƒƒã‚¯åˆ†å¸ƒã¨æ¯”è¼ƒã—ã¦æ˜ã‚‰ã‹ã«åã‚ŠãŒå°‘ãªã„ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚  
Note that while Figure 2 shows the cumulative distribution of impressions, Figure 6 shows the marginal distribution of clicks.  
å›³2ãŒã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®ç´¯ç©åˆ†å¸ƒã‚’ç¤ºã™ã®ã«å¯¾ã—ã€å›³6ã¯ã‚¯ãƒªãƒƒã‚¯ã®å‘¨è¾ºåˆ†å¸ƒã‚’ç¤ºã—ã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚  



##### Report Github Issue GitHubã®å•é¡Œå ±å‘Š

LATE
LATE

A
A

E
E

xml
xml



## Instructions for reporting errors ã‚¨ãƒ©ãƒ¼å ±å‘Šã®æ‰‹é †

We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. 
ç§ãŸã¡ã¯è«–æ–‡ã®HTMLãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æ”¹å–„ã—ç¶šã‘ã¦ãŠã‚Šã€ã‚ãªãŸã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã¨ãƒ¢ãƒã‚¤ãƒ«ã‚µãƒãƒ¼ãƒˆã®å‘ä¸Šã«å½¹ç«‹ã¡ã¾ã™ã€‚ 
To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:
HTMLã®ã‚¨ãƒ©ãƒ¼ã‚’å ±å‘Šã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®æ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

- Click the "Report Issue" button.
- "Report Issue"ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™ã€‚
- Open a report feedback form via keyboard, use "Ctrl + ?".
- ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦å ±å‘Šãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ•ã‚©ãƒ¼ãƒ ã‚’é–‹ãã«ã¯ã€ã€ŒCtrl + ?ã€ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
- Make a text selection and click the "Report Issue for Selection" button near your cursor.
- ãƒ†ã‚­ã‚¹ãƒˆã‚’é¸æŠã—ã€ã‚«ãƒ¼ã‚½ãƒ«ã®è¿‘ãã«ã‚ã‚‹ã€Œé¸æŠã®å•é¡Œã‚’å ±å‘Šã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™ã€‚
- You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã‚¢ã‚¯ã‚»ã‚·ãƒ–ãƒ«ãªå ±å‘Šãƒªãƒ³ã‚¯ã‚’ã‚ªãƒ³/ã‚ªãƒ•ã™ã‚‹ã«ã¯ã€Alt+Yã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

Our team has already identified the following issues. 
ç§ãŸã¡ã®ãƒãƒ¼ãƒ ã¯ã™ã§ã«ä»¥ä¸‹ã®å•é¡Œã‚’ç‰¹å®šã—ã¦ã„ã¾ã™ã€‚ 
We appreciate your time reviewing and reporting rendering errors we may not have found yet. 
ç§ãŸã¡ã¯ã€ã¾ã è¦‹ã¤ã‘ã¦ã„ãªã„å¯èƒ½æ€§ã®ã‚ã‚‹ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—å ±å‘Šã™ã‚‹ãŸã‚ã«ã‚ãªãŸãŒè²»ã‚„ã™æ™‚é–“ã«æ„Ÿè¬ã—ã¾ã™ã€‚ 
Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. 
ã‚ãªãŸã®åŠªåŠ›ã¯ã€ã™ã¹ã¦ã®èª­è€…ã®ãŸã‚ã«HTMLãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æ”¹å–„ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚ãªãœãªã‚‰ã€éšœå®³ã¯ç ”ç©¶ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã®éšœå£ã§ã‚ã£ã¦ã¯ãªã‚‰ãªã„ã‹ã‚‰ã§ã™ã€‚ 
Thank you for your continued support in championing open access for all.
ã™ã¹ã¦ã®äººã«ã‚ªãƒ¼ãƒ—ãƒ³ã‚¢ã‚¯ã‚»ã‚¹ã‚’æ¨é€²ã™ã‚‹ãŸã‚ã®ç¶™ç¶šçš„ãªã‚µãƒãƒ¼ãƒˆã«æ„Ÿè¬ã—ã¾ã™ã€‚

Have a free development cycle? Help support accessibility at arXiv! 
é–‹ç™ºã‚µã‚¤ã‚¯ãƒ«ã«ä½™è£•ãŒã‚ã‚Šã¾ã™ã‹ï¼ŸarXivã§ã®ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ãã ã•ã„ï¼ 
Our collaborators at LaTeXML maintain a list of packages that need conversion, and welcome developer contributions.
ç§ãŸã¡ã®LaTeXMLã®å”åŠ›è€…ã¯ã€å¤‰æ›ãŒå¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã‚’ç¶­æŒã—ã¦ãŠã‚Šã€é–‹ç™ºè€…ã®è²¢çŒ®ã‚’æ­“è¿ã—ã¾ã™ã€‚
