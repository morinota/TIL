## これは何?

- n週連続 推薦システム系論文読んだシリーズの43回目です。
  - 前回はこちら: [Operational(=本番システム上で実際に価値を発揮する?)なFeature Storeに必要な5つの最低条件を読んだメモ](https://qiita.com/morinota/items/9e5eb673be3abfc24296)
- 論文「Carousel Personalization in Music Streaming Apps with Contextual Bandits」を読んだメモです。

## ざっくりどんな内容?

- 音楽ストリーミングアプリ（Deezer）における **カルーセル（スワイプ可能な推薦リスト）のコンテンツパーソナライズ機能**を、文脈付きマルチアームバンディット（Contextual Multi-Armed Bandit: CMAB）問題とみなして改善アプローチを提案・実験してる論文。
  - より詳細には、**Multi-plays CMAB** (1ラウンドで複数のアームが選ばれる) のバンディット問題として定式化してる。
- 提案アプローチについては、**ユーザ行動にカスケード性(i.e. カスケードモデル)を仮定**した上で、以下の２通りのbandit手法を提案・検証してる。
  - セミパーソナライズ戦略（クラスタリングを活用した、context-free bandit）
  - フルパーソナライズ戦略（ユーザ特徴量 & アイテム特徴量を使ったcontextual bandit）
- ちなみに、「ユーザ行動のカスケード性」「カスケードモデル」とは??
  - ざっくりした理解では「**ユーザは手前側に表示されたアイテムから吟味していくはず...!**」みたいなユーザ行動の仮定を使うよ、ってこと:thinking:
  - なお、本手法でのカスケードモデルの仮定は、**banditのモデル構造には特に影響を与えず、学習に使うbandit feedbackデータの作り方で考慮してるみたい!** 
    - (ex. カルーセルの後ろの方に表示されたアイテムは、タップされなかったとしても「見られてない」可能性があるのでnegative sampleとして扱わない、とか!:thinking:)
- また、実験用の提案手法の実装と、半合成データを使ったオフライン評価用の環境を公開してるみたい(https://github.com/deezer/carousel_bandits)。

## 背景 & 研究の目的

- レコメンドの実応用の動きについて
  - ユーザにパーソナライズされたコンテンツを推薦することは、**ニュース、動画、音楽配信などのメディア系サービスにおいて重要**。
    - 良いレコメンダーがあると、ユーザはコンテンツ迷子にならずに済み、お気に入りのコンテンツを楽しめるし、新しい作品との出会いもある。結果として、**UX & エンゲージメントが向上**する!
  - そのため、**研究成果を実サービスに活かそうとする動き**が活発に。
- 実サービスでの採用例: カルーセル(Carousel)
  - 特に音楽ストリーミングサービスでは、**カルーセルUI（スワイプで切り替えられる推薦リスト）が主流!**
  - カルーセルUIで何を表示するかが重要! なぜなら...
    - カルーセルには表示枠(=スロット数)に制約。　表示スロット数 << コンテンツ数。
    - ユーザごとに好みも異なる。
- カルーセルUIの技術的な課題:
  - 見られないスロットもある(スワイプしないと見えない)。
- この論文の貢献:
  - カルーセルUIの推薦を、文脈付きマルチアームバンディット (Contextual Multi-Armed Bandit: CMAB) 問題として定式化。
    - 1ラウンド内で複数のアームが選択される **Multiple Plays MAB** に対応。
    - ユーザ行動のカスケード性を考慮したオンライン学習方法を提案。
  - 実サービスにてオンライン評価して実験。
  - 実験用データ & シミュレーション環境をオープンソースで公開。

## 提案手法: contextual multi-armed bandit * カスケードモデル

### まずMultiple Playsのバンディットって??

- カルーセルUIにおける推薦は、**Multiple Plays (複数プレイ)**のMAB(多腕バンディット)問題と言える。
- Multiple Plays MABとは??
  - **1ラウンド内で $L$ 個のアームを選んで、報酬を受け取る**という問題設定。
    - (i.e. ランキングタスクがMultiple Plays MABってこと...??:thinking:)
  - その一方で、通常のSingle Play MABは1ラウンドで1つのアームのみを選択して報酬を受け取る問題設定。
- Multiple Plays MABをざっくり定式化すると...(contextは一旦無視ver.)
  - 表記
    - アーム(アイテム): $K$ 個のアーム。
    - ラウンド: $t = 1, 2, \ldots, T$。
    - 選択可能なアームの集合: $S_t \subseteq \{1, 2, \ldots, K\}$。
    - 報酬: 各選択したアーム $i \in S_t$ に対して、報酬 $r_{i}$ はベルヌーイ分布に従うと仮定 (論文内ではbinaryの報酬を想定してるみたい:thinking:)
    - 報酬のベルヌーイ分布のパラメータを $p_{i}$ とする (=これはつまり、アーム $i$ が選ばれたときの報酬期待値 $E_{p_{i}(r)[r]}$ と同じ意味...!:thinking:)
  - 目標:
    - トップ $L$ 個のアームを選択することで、報酬を最大化する。
    - つまり、最も報酬期待値の高い $L$ 個のアーム集合 $\delta^*(L)$ を特定したい。
  - 方策の性能指標: 累積後悔(Cumulative Regret)
    - $Reg(T) = \sum_{t=1}^{T} (\sum_{i \in \delta^*(L)} p_{i} - \sum_{i \in S_t} p_{i})$
    - もし常に最適なアームを選べていたら、後悔はゼロ。
    - でも探索をしないと良いアームを見つけられないので、最初のうちはある程度のregretが発生する。
    - 良いアルゴリズムは、このregretをできるだけ小さくすることが目標。

### Multiple Plays MABをカルーセル推薦に適用する

- カルーセルUIにおける推薦の定式化:
  - 表記
    - K個のアイテム(アーム) (ex. プレイリスト、アルバム)
    - N人のユーザ。
    - L個のスロット(カルーセルの表示枠)にアイテムを配置する (L << K)
    - 報酬関数 $r_{ui}$ について。ストリームすれば1、しなければ0。
  - 目標:
    - 各ユーザ $u$ に対して、ストリーム確率が最も高い $L$ 個のアイテムを選びたい。つまり以下。
      - $S_t = \argmax_{S \subseteq \{1, 2, \ldots, K\}, |S| = L} \sum_{i \in S} p_{ui}$
      - (要するに $p_{ui}$ は、ユーザ $u$ にアイテム $i$ を見せた時にストリームされるかの期待値 $E_{p(r|u,i)}[r]$ ってこと...!:thinking:)
- 好みが違うからユーザごとに異なる推薦をする必要がある。じゃあどうする??
  - **最も単純な戦略は、「N個分のMABを独立に動かす」**ことである。だがこれは厳しい!
    - 学習コストが膨大になる。非現実的。
    - **推定すべきパラメータの数が K * N個になる**。多すぎる。ログデータを十分に貯めるのに何年かかるのか...!:thinking:
  - 本論文では**2つの戦略を提案**してる。
    - クラスタリングを活用したセミパーソナライズ戦略。
    - 文脈つきバンディットを活用したフルパーソナライズ戦略。

- ちなみに...個人的に実運用で気になったポイント!
  - プラットフォームの技術的制約により、カルーセルUI内のアイテムは定期的に更新される(i.e. **リアルタイム推論ではなくバッチ推論で運用する想定**...!!:thinking:)

### 提案1: クラスタリングを活用したセミパーソナライズ戦略

- 似たユーザをクラスタ化して、グループごとに学習。
- ユーザごとではなく、クラスタごとに最適な推薦を行う。
- 推定すべきパラメータ数がK*Qになる。Qはクラスタ数。(Q << N)
- 基本アイデア
  - 全てのユーザを個別に扱うと計算コストが大きすぎる
  - 似たユーザをクラスタ化して、一緒に扱う!
  - ex.「ロック好き」「ポップ好き」などのクラスタを作れる。
- 定式化
  - hogehoge
- 1クラスタごとにMABを1つ動かせばOK!

### 提案2: 文脈つきバンディットを活用したフルパーソナライズ戦略

- 各ユーザの特徴ベクトル $x_{u}$ を使って回帰モデルを学習。
- 推定すべきパラメータ数は K * D になる。Dは特徴ベクトルの次元数。
  - まあこれは線形回帰モデルを、アイテムごとに用意する考え前提だな...!!:thinking: 
  - 実際にニュース推薦に適用する場合には、ユーザ間だけでなくアイテム間でもパラメータを共有するべきのはず。

定式化

- ユーザ特徴量ベクトル: $x_u$
- アイテムiに対するストリーム確率:
  - $p_{ui} = \sigma(x_u^T \theta_{i})$
  - アイテム i 毎にパラメータを推定する想定。

### カスケードモデルの話

- (=ユーザが上から順にアイテムを見ていく、という行動を仮定するモデル、という認識...!:thinking:)
- 現実的な問題点:
  - 通常のMABの設定では、すべての選択したアイテム（L個）について報酬（0 or 1）が得られる という前提。
  - でも実際のカルーセルでは、**すべてのアイテムがユーザーに見られるわけじゃない**。
  - ex. 
    - ユーザがアプリを開いたときに最初に見えるのは、 $L_{init}$個のアイテムだけ。
    - **ユーザはスワイプしないと追加のアイテムを見れない。でも実際にどこまでスワイプしたかを正確に計測するのは難しい...!**
- もしこの問題を無視してしまうと...
  - 「クリックされなかったアイテムは全部 0 の報酬」とする。
  - でもユーザが「見てすらいない」可能性。
  - 結果として、クリック率（display-to-stream確率）が過小評価される
- この問題に対応するための仮定が、カスケードモデル
  - 基本の考え方:
    - 「クリックされたアイテムの位置までのアイテムは、ユーザーが見たとみなす」
      - 書籍「反実仮想機械学習」におけるカスケードモデルは、「**あるランキングのk番目のポジションで発生する期待報酬は、そのポジションよりも上位に提示されたアイテムのみに依存して決まる**」みたいな考え方として説明されてた...!! 一般化されてる感...!:thinking:
- カスケード報酬モデル:
  - ユーザの行動に応じて、どこまでのアイテムが「みられたか」を決定する。
  - 1. 何もストリームしなかった場合。
    - 最初の $L_{init}$ 個のアイテムは「見た」と判断する。
    - それ以降のアイテムは「見てない」と判断する。
  - 2. 2番目のアイテムをストリームした場合。
    - 1,2番目のアイテムは「見た」と判断する。
    - 3番目以降のアイテムは「見てない」と判断する。
  - 3. 2番目と6番目のアイテムをストリームした場合。
    - 1~6番目のアイテムは「見た」と判断する。
    - 7番目以降のアイテムは「見てない」と判断する。
- これにより何が解決できるのか??
  - 見られてないアイテムを学習の対象から外せる。
  - 見られたけどクリックされてないアイテムは「0の報酬」としてカウントできる。
  - 結果として、学習がより現実のユーザ行動に即したものになる。

### 遅延フィードバック (Delayed Feedback)の話

- リアルタイムで報酬を反映するのは難しい！(i.e. 要するに**MABモデルのパラメータ更新の話**...!:thinking:)
- 代わりに、1日の終わりなど「バッチ処理」でフィードバックを受け取る。
- 実験では、この遅延FBが学習にどのような影響を与えるかも検証。
- なぜ遅延FBなの??
  - 1. **技術的制約**: 大規模プラットフォームでは、**毎回リアルタイム更新は非効率**。
  - 2. 現実のシステムと一致させるため:
    - 多くのアプリでは「その場でFBを取る」よりも「後でまとめてログを処理」する。
- 悪影響は??
  - データが即時に反映されないので、学習が遅れる可能性。
  - **でも実際のシステムの制約を考慮すると、現実的**。(まあそうだよなぁ...:thinking:)

## 実験方法

- 2つの実験
  - オフライン評価
  - オンライン評価
- 実験で明らかにしたい質問たち
  - どの推薦アルゴリズムが最もユーザーのストリーム（視聴）を増やすか？
  - カルーセル型UI特有のバイアス（表示されるカード数の制約）を考慮できるか？
  - オフライン実験とオンラインA/Bテストの結果は整合するか？
- 実験データについて
  - 対象: 音楽アプリのカルーセルUIで推薦されるプレイリスト
  - プレイリスト数: K = 862 
    - 自社の専門キュレーターによって作成されたプレイリスト。(=あ、めちゃめちゃドメイン知識要求されるクリエイティブな作業だ...!!)
    - 音楽ジャンル、文化的要素、ムードに基づいたもの。
  - カルーセルの仕様
    - 最大12個のスロット (L=12)
    - 最初に表示されるカード数 (L_init=3)
    - ユーザがスワイプすることで追加のプレイリストを閲覧可能。
  - データ更新頻度: 毎日 (これはたぶん、観測された報酬によってモデルパラメータを更新する頻度の話! 前述されてた「遅延フィードバック」のこと! たぶんbanditだから事前学習なしの状態でモデルを本番に出してそうだから、学習頻度が高くないといけないんだろうな...!:thinking:)

### オフライン評価の方法

- 半合成データのシミュレーション環境を用意して、各バンディットアルゴリズムの性能のオフライン評価を行ってた。
  - ユーザ数は974,960人。
  - ユーザごとの特徴量ベクトル $x_u$ (次元数 D = 97)
    - 行列分解により、特徴量ベクトルを生成。
    - その後、バイアス項を追加(=たぶん正規分布とかに従う乱数を追加してそう!)
  - ユーザのクラスタリング (Q = 100)
    - 特徴量ベクトルに対して k-meansを適用し、類似したユーザをクラスタリング。
  - **シミュレーションなので、乱数を生成するために、擬似的に真の値を用意しておく**。
    - (この場合は具体的には、各(ユーザ, プレイリスト)ペアの、ストリーミングするか否かの確率分布の期待値を用意しておく)
    - 各(ユーザ, プレイリスト)ペアに対して、**ground-truthのストリーミング期待値 $p_{ui}$ を生成しておく。**(これは真の値!シミュレーションなので!)
    - 具体的には $p_{ui} = \sigma(x_u^T \theta_{i})$ 
    - 各アイテムの真のパラメータ $\theta_{i}$ は、2020年1月の観測データを元に推定させた値を使う。(たぶん)
  - 擬似的な真のストリーミング期待値 $p_{ui}$ を用意した後の、シミュレーションの流れ:
    - 各ラウンドで、ランダムに20000人のユーザを選択し、任意のアルゴリズムが12個のプレイリストを推薦。
    - 観測される報酬は、カスケードモデルに基づいてシミュレーション。
      - i.e. おそらく手前のアイテムから $p_{ui}$ に基づいてコイントスしていく感じ...?? それで1つでもクリックされたらそれ以降のアイテムは見ない??
      - この論文の前半にMulti-plays MABって話があったけど、これはどうシミュレーションしてるんだろ?? 通常のSingle Play MABだったら上記のシミュレーション方法でいいんだろうけど...! まあこのあたりはシミュレーション用の実装コードを見たらわかりそう...!:thinking:
    - 各ラウンドが終了するたびに、任意のアルゴリズムはFBに基づいてパラメータを更新。
    - 各ラウンドが終了するたびに、任意のアルゴリズムの累積後悔(Cumulative Regret)を計算。

### 比較するアルゴリズムたち

- random: 一様ランダムにプレイリストを推薦 (ベースライン)
- epsilon-greedy-seg: (セミパーソナライズ) epsilon-greedyによるバンディット。
- etc-seg: (セミパーソナライズ)最初にn回ランダム探索し、その後常にトップL個のアームを選択する。
- kl-ucb-seg: (セミパーソナライズ) UCBアルゴリズム。
- ts-seg: (セミパーソナライズ) Thompson Samplingアルゴリズム。
- ts-lin: (完全パーソナライズ) ロジスティック回帰モデルを使ったThompson Sampling。(contexual bandit)

デフォルトでは、すべてのアルゴリズムのパラメータ更新にカスケードモデルを採用する。
ちなみに追加実験として、モデルパラメータの更新にて、カスケードモデルを仮定するかnaiveな方法をとるかの比較も行ったみたい。カスケードモデルを採用しなかった場合の結果は `*-no-cascade` というsuffixがついてる。

## 実験結果

### オフライン実験

#### セミパーソナライズ vs フルパーソナライズの比較

- kl-ucb-seg は 全体を通して学習が遅い（収束に時間がかかる）
- ts-lin-pessimistic / ts-lin-naive は 数ラウンドで安定
- 悲観的ポリシー (pessimistic policies) はナイーブなものより優秀
  - これは要するに、カスケードモデルを採用したパラメータ更新の方が、naiveな方法よりもいい感じに学習できてたってことか...!:thinking:
- セミパーソナライズの優位性
  - 複数のセミパーソナライズポリシーが完全パーソナライズより優れた結果を出した
  - ts-seg-pessimistic は 最初の25ラウンドで全ての手法を上回った
- 計算コスト
  - 推定すべきモデルパラメータの数: K × Q（セミパーソナライズ） vs. K × D（完全パーソナライズ）
  - **クラスタリングが適切なら、セミパーソナライズは大規模アプリに適した選択肢**。

### オンライン実験(A/Bテスト)

- 実験のスケジュール
  - 2020年2月に Deezer の本番環境で A/B テストを実施
  - ユーザーごとの 12 推薦プレイリストを毎日更新
- **実際のユーザ行動は カスケードモデルの仮定よりも複雑な可能性あり。それでも、提案手法は 有意な改善をもたらした**

### まとめ
