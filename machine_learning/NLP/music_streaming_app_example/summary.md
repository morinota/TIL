## これは何?

- 論文「Carousel Personalization in Music Streaming Apps with Contextual Bandits」を読んだメモです。

## ざっくりどんな内容?

- 音楽ストリーミングアプリ（Deezer）における **カルーセル（スワイプ可能な推薦リスト）** のパーソナライズを、**文脈付きマルチアームバンディット（Contextual Multi-Armed Bandit: CMAB）** を用いて最適化する方法を提案してる。
- 論文の中身
  - カルーセル推薦をバンディット問題として定式化
  - **カスケードモデルの導入**（ユーザーは全てのアイテムを見ているわけじゃないことを考慮）
  - ユーザーのクラスタリング or 文脈ベクトルを使った推薦手法の比較
  - Deezerのデータを用いた大規模実験 & A/Bテスト
  - オープンソース環境 & 実験用データを公開

## 背景 & 研究の目的

- 音楽ストリーミングサービス では、ホーム画面にカルーセル（スワイプで切り替えられる推薦リスト） を配置するのが一般的。
  - いわゆる横カルーセルってやつか...!!
- ただし、カルーセルには 表示枠（スロット数） に制約があり、全てのコンテンツを載せられない。よって何を表示するかが重要！

## 提案手法: contextual multi-armed bandit * カスケードモデル

### Multiple Plays MAB

- 今回の事例は、**Multiple Plays (複数プレイ)のMAM(多腕バンディット)**問題。
  - 通常のSingle Play MABでは、1ラウンドで1つのアームしか選べない。
  - 一方で、Multiple Plays MABでは、1ラウンドで $L$ 個のアームを選ぶ。
    - (i.e. ランキングタスクがMultiple Plays MABってこと...??:thinking:)
- Multiple Plays MABの定式化
  - アーム(アイテム): $K$ 個のアーム。
  - ラウンド: $t = 1, 2, \ldots, T$。
  - 選択するアームの集合: $S_t \subseteq \{1, 2, \ldots, K\}$。
  - 報酬: 各選択したアーム $i \in S_t$ に対して、報酬 $r_{i}$ はベルヌーイ分布に従うと仮定。
    - (あ、じゃあbinaryの報酬関数のケースね...!:thinking:)
    - ベルヌーイ分布のパラメータ(i.e. 成功確率, 報酬期待値)を $p_{i}$ とする。
  - ゴール
    - トップ $L$ 個のアームを選択することで、報酬を最大化する。
    - つまり、もっとも報酬期待値の高い $L$ 個のアームの集合 $\delta^*(L)$ を特定したい。
  - 方策の性能指標: 累積後悔(Cumulative Regret)
    - どれだけ損をしたかという指標。
    - $Reg(T) = \sum_{t=1}^{T} (\sum_{i \in \delta^*(L)} p_{i} - \sum_{i \in S_t} p_{i})$
    - もし常に最適なアームを選べていたら、後悔はゼロ。
    - でも探索をしないと良いアームを見つけられないので、最初のうちはある程度の後悔が発生する。
    - 良いアルゴリズムは、この後悔をできるだけ小さくすることが目標。

### カルーセル推薦のためのMultiple Plays MAB

- カルーセル推薦の特徴
  - K個のアイテム(アーム)がある (ex. プレイリスト、アルバム)
  - N人のユーザがいる。
  - L個のスロット(カルーセルの表示枠)にアイテムを配置する (L << K)
  - つまり、「全アイテムの中からL個を選んで表示し、どのアイテムがクリックされるか?」を最適化するのが目的。
  - ポイント
    - ユーザごとに異なる推薦をする必要がある (好みが違うから)
    - カルーセルのアイテムは定期的に更新される (リアルタイムじゃなく、一定の間隔で)
- 目標:
  - 報酬関数 $r_{ui}$ について。クリックすれば1、しなければ0。
  - 各ユーザ $u$ に対して、ストリーム確率が最も高い $L$ 個のアイテムを選びたい。つまり以下。
    - $S_t = \argmax_{S \subseteq \{1, 2, \ldots, K\}, |S| = L} \sum_{i \in S} p_{ui}$
- **最も単純な方法 (N個のMABを独立に動かす)はだめ!**
  - 学習コストが膨大になる。非現実的。
  - 推定するパラメータの数が K * N個になる。多すぎる。ログデータを十分に貯めるのに何年かかるのか...!:thinking:
- じゃあどうするか?? 2つの戦略を提案。
  - クラスタリングを活用したセミパーソナライズ戦略。
  - 文脈つきバンディットを活用したフルパーソナライズ戦略。

### 案1: クラスタリングを活用したセミパーソナライズ戦略

- 似たユーザをクラスタ化して、グループごとに学習。
- ユーザごとではなく、クラスタごとに最適な推薦を行う。
- 推定すべきパラメータ数がK*Qになる。Qはクラスタ数。(Q << N)
- 基本アイデア
  - 全てのユーザを個別に扱うと計算コストが大きすぎる
  - 似たユーザをクラスタ化して、一緒に扱う!
  - ex.「ロック好き」「ポップ好き」などのクラスタを作れる。
- 定式化
  - hogehoge
- 1クラスタごとにMABを1つ動かせばOK!

### 案2: 文脈つきバンディットを活用したフルパーソナライズ戦略

- 各ユーザの特徴ベクトル $x_{u}$ を使って回帰モデルを学習。
- 推定すべきパラメータ数は K * D になる。Dは特徴ベクトルの次元数。
  - まあこれは線形回帰モデルを、アイテムごとに用意する考え前提だな...!!:thinking: 
  - 実際にニュース推薦に適用する場合には、ユーザ間だけでなくアイテム間でもパラメータを共有するべきのはず。

定式化

- ユーザ特徴量ベクトル: $x_u$
- アイテムiに対するストリーム確率:
  - $p_{ui} = \sigma(x_u^T \theta_{i})$
  - アイテム i 毎にパラメータを推定する想定。

### カスケードモデルの話

- (=ユーザが上から順にアイテムを見ていく、という行動を仮定するモデル、という認識...!:thinking:)
- 現実的な問題点:
  - 通常のMABの設定では、すべての選択したアイテム（L個）について報酬（0 or 1）が得られる という前提。
  - でも実際のカルーセルでは、**すべてのアイテムがユーザーに見られるわけじゃない**。
  - ex. 
    - ユーザがアプリを開いたときに最初に見えるのは、 $L_{limit}$個のアイテムだけ。
    - **ユーザはスワイプしないと追加のアイテムを見れない。でも実際にどこまでスワイプしたかを正確に計測するのは難しい...!**
- もしこの問題を無視してしまうと...
  - 「クリックされなかったアイテムは全部 0 の報酬」とする。
  - でもユーザが「見てすらいない」可能性。
  - 結果として、クリック率（display-to-stream確率）が過小評価される
- この問題に対応するための仮定が、カスケードモデル
  - 基本の考え方:
    - 「クリックされたアイテムの位置までのアイテムは、ユーザーが見たとみなす」
      - 書籍「反実仮想機械学習」におけるカスケードモデルは、「**あるランキングのk番目のポジションで発生する期待報酬は、そのポジションよりも上位に提示されたアイテムのみに依存して決まる**」みたいな考え方として説明されてた...!! 一般化されてる感...!:thinking:
- カスケード報酬モデル:
  - ユーザの行動に応じて、どこまでのアイテムが「みられたか」を決定する。
  - 1. 何もストリームしなかった場合。
    - 最初の $L_{init}$ 個のアイテムは「見た」と判断する。
    - それ以降のアイテムは「見てない」と判断する。
  - 2. 2番目のアイテムをストリームした場合。
    - 1,2番目のアイテムは「見た」と判断する。
    - 3番目以降のアイテムは「見てない」と判断する。
  - 3. 2番目と6番目のアイテムをストリームした場合。
    - 1~6番目のアイテムは「見た」と判断する。
    - 7番目以降のアイテムは「見てない」と判断する。
- これにより何が解決できるのか??
  - 見られてないアイテムを学習の対象から外せる。
  - 見られたけどクリックされてないアイテムは「0の報酬」としてカウントできる。
  - 結果として、学習がより現実のユーザ行動に即したものになる。

### 遅延フィードバック (Delayed Feedback)の話

- リアルタイムで報酬を反映するのは難しい！(i.e. 要するに**MABモデルのパラメータ更新の話**...!:thinking:)
- 代わりに、1日の終わりなど「バッチ処理」でフィードバックを受け取る。
- 実験では、この遅延FBが学習にどのような影響を与えるかも検証。
- なぜ遅延FBなの??
  - 1. **技術的制約**: 大規模プラットフォームでは、**毎回リアルタイム更新は非効率**。
  - 2. 現実のシステムと一致させるため:
    - 多くのアプリでは「その場でFBを取る」よりも「後でまとめてログを処理」する。
- 悪影響は??
  - データが即時に反映されないので、学習が遅れる可能性。
  - **でも実際のシステムの制約を考慮すると、現実的**。(まあそうだよなぁ...:thinking:)

## 実験 & 結果

### オフライン実験

### オンライン実験(A/Bテスト)

### まとめ
