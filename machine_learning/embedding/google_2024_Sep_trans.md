## refs å¯©åˆ¤

<https://medium.com/google-cloud/embeddings-how-to-select-the-right-one-135032315709>
<https://medium.com/google-cloud/embeddings-how-to-select-the-right-one-135032315709>

# Embeddings: How to select the right one? ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚° æ­£ã—ã„ã‚‚ã®ã‚’é¸ã¶ã«ã¯ï¼Ÿ

Notes from my reading in quest to answer questions like:
ä»¥ä¸‹ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã«èª­æ›¸ã—ãŸãƒ¡ãƒ¢:

How do I choose the right embedding model for a task?
ã‚¿ã‚¹ã‚¯ã«é©ã—ãŸã‚¨ãƒ³ãƒ™ãƒ‡ãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã‚’é¸ã¶ã«ã¯ï¼Ÿ

Will the same embedding model work for all my tasks?
åŒã˜ã‚¨ãƒ³ãƒ™ãƒ‡ãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ãŒã€ã™ã¹ã¦ã®ã‚¿ã‚¹ã‚¯ã«é©ç”¨ã•ã‚Œã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ

How can I evaluate an embedding model for a given task?
ä¸ãˆã‚‰ã‚ŒãŸã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ã«ã¯ï¼Ÿ

How do I detect bias in embedding model?
åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ã‚¢ã‚¹ã‚’æ¤œå‡ºã™ã‚‹ã«ã¯ï¼Ÿ

Is the model with higher number of dimensions always the best choice?
æ¬¡å…ƒæ•°ã®å¤šã„ãƒ¢ãƒ‡ãƒ«ãŒå¸¸ã«æœ€è‰¯ã®é¸æŠãªã®ã‹ï¼Ÿ

## What are Embeddings? ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã¨ã¯ï¼Ÿ

Imagine trying to explain the taste of an apple to someone whoâ€™s never had one.
ãƒªãƒ³ã‚´ã‚’é£Ÿã¹ãŸã“ã¨ã®ãªã„äººã«ãƒªãƒ³ã‚´ã®å‘³ã‚’èª¬æ˜ã—ã‚ˆã†ã¨ã™ã‚‹ã®ã‚’æƒ³åƒã—ã¦ã¿ã¦ã»ã—ã„ã€‚
You could use words like â€œsweet,â€ â€œcrunchy,â€ and â€œjuicy,â€ but itâ€™s hard to truly capture the experience.
ç”˜ã„ã€ã€ã€Œæ­¯ã”ãŸãˆãŒã‚ã‚‹ã€ã€ã€Œã‚¸ãƒ¥ãƒ¼ã‚·ãƒ¼ã€ã¨ã„ã£ãŸè¨€è‘‰ã‚’ä½¿ã†ã“ã¨ã¯ã§ãã¦ã‚‚ã€ãã®çµŒé¨“ã‚’æœ¬å½“ã«è¡¨ç¾ã™ã‚‹ã®ã¯é›£ã—ã„ã€‚
Embeddings are like giving computers a taste of language, helping them understand the meaning and connections between words, even if they donâ€™t â€œexperienceâ€ them like we do.
ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã«è¨€èªã‚’ä½“é¨“ã•ã›ã‚‹ã‚ˆã†ãªã‚‚ã®ã§ã€ç§ãŸã¡ã®ã‚ˆã†ã« ã€Œä½“é¨“ ã€ã™ã‚‹ã“ã¨ã¯ã§ããªãã¦ã‚‚ã€è¨€è‘‰ã®æ„å‘³ã‚„ã¤ãªãŒã‚Šã‚’ç†è§£ã™ã‚‹ã®ã«å½¹ç«‹ã¤ã€‚

Computers may be intelligent at crunching numbers, but they cannot understand text directly.
ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã¯æ•°å­—ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã«ã¯é•·ã‘ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ãŒã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ç›´æ¥ç†è§£ã™ã‚‹ã“ã¨ã¯ã§ããªã„ã€‚
Words, text and documents need to be converted into numbers for making computers understand them.
ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«ç†è§£ã•ã›ã‚‹ãŸã‚ã«ã¯ã€è¨€è‘‰ã€ãƒ†ã‚­ã‚¹ãƒˆã€æ–‡æ›¸ã‚’æ•°å­—ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

Essentially, embeddings turn words and phrases into special codes that computers can understand.
**åŸºæœ¬çš„ã«ã€ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã¯å˜èªã‚„ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒç†è§£ã§ãã‚‹ç‰¹åˆ¥ãªã‚³ãƒ¼ãƒ‰ã«å¤‰ãˆã‚‹ã€‚** (åŸºæœ¬çš„ã«ã¯NLPåˆ†é‡ã®embeddingã®è©±ã‚’ã—ã¦ã‚‹ã£ã½ã„...!sparseã‹denceã‹ã¨ã„ã†è©±ã§ã¯ãªã•ãã†:thinking:)
Similar words get codes that are close to each other, while different words get codes that are far apart.
ä¼¼ãŸã‚ˆã†ãªå˜èªã«ã¯è¿‘ã„ã‚³ãƒ¼ãƒ‰ãŒã¤ãã€é•ã†å˜èªã«ã¯é ã„ã‚³ãƒ¼ãƒ‰ãŒã¤ãã€‚
This allows computers to â€œseeâ€ how words relate, like understanding that â€œhappyâ€ and â€œjoyfulâ€ are similar, while â€œhappyâ€ and â€œsadâ€ are opposites.
ä¾‹ãˆã°ã€ã€Œå¬‰ã—ã„ã€ã¨ã€Œæ¥½ã—ã„ã€ã¯ä¼¼ã¦ã„ã‚‹ãŒã€ã€Œæ¥½ã—ã„ã€ã¨ã€Œæ‚²ã—ã„ã€ã¯æ­£åå¯¾ã§ã‚ã‚‹ã€‚
Embeddings could be created for words, sentences or documents as well.
ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã¯ã€å˜èªã€æ–‡ã€æ–‡æ›¸ã«å¯¾ã—ã¦ã‚‚ä½œæˆã§ãã‚‹ã€‚

## Why are embeddings becoming important? ãªãœåŸ‹ã‚è¾¼ã¿ãŒé‡è¦ã«ãªã‚‹ã®ã‹ï¼Ÿ

Embeddings are becoming increasingly important because they help computers handle the massive amounts of text we create every day.
**ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã¯ã€ç§ãŸã¡ãŒæ¯æ—¥ä½œæˆã™ã‚‹è†¨å¤§ãªé‡ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒå‡¦ç†ã™ã‚‹ã®ã«å½¹ç«‹ã¤ãŸã‚ã€ã¾ã™ã¾ã™é‡è¦ã«ãªã£ã¦ãã¦ã„ã‚‹**ã€‚(ã†ã‚“ã†ã‚“)
Theyâ€™re like super-efficient translators, allowing computers to quickly process and understand language, leading to better search results, more accurate translations, and even smarter chatbots.
ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ãŒè¨€èªã‚’ç´ æ—©ãå‡¦ç†ã—ç†è§£ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šè‰¯ã„æ¤œç´¢çµæœã€ã‚ˆã‚Šæ­£ç¢ºãªç¿»è¨³ã€ãã—ã¦ã•ã‚‰ã«è³¢ã„ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã«ã¤ãªãŒã‚‹ã€‚

Think of it like teaching a computer to read between the lines.
ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã«è¡Œé–“ã‚’èª­ã‚€ã“ã¨ã‚’æ•™ãˆã‚‹ã‚ˆã†ãªã‚‚ã®ã ã¨æ€ã£ã¦ãã ã•ã„ã€‚
Instead of just looking for specific words, embeddings help computers grasp the overall meaning of a sentence or document, leading to more relevant and helpful information.
embeddingsã¯ã€ç‰¹å®šã®å˜èªã ã‘ã§ãªãã€æ–‡ã‚„æ–‡æ›¸ã®å…¨ä½“çš„ãªæ„å‘³ã‚’æŠŠæ¡ã™ã‚‹ã®ã‚’åŠ©ã‘ã€ã‚ˆã‚Šé–¢é€£æ€§ã®é«˜ã„æƒ…å ±ã‚’æä¾›ã™ã‚‹ã€‚

In short, embeddings are like giving computers a secret decoder ring for human language.
è¦ã™ã‚‹ã«ã€embeddingsã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«äººé–“ã®è¨€èªã®ãŸã‚ã®ç§˜å¯†ã®ãƒ‡ã‚³ãƒ¼ãƒ€ãƒªãƒ³ã‚°ã‚’æä¾›ã™ã‚‹ã‚ˆã†ãªã‚‚ã®ã ã€‚
Theyâ€™re helping bridge the gap between how we communicate and how computers understand, making technology more intuitive and useful for everyone.
ç§ãŸã¡ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ–¹æ³•ã¨ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ãŒç†è§£ã™ã‚‹æ–¹æ³•ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’åŸ‹ã‚ã€ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã‚’ã‚ˆã‚Šç›´æ„Ÿçš„ã§èª°ã«ã¨ã£ã¦ã‚‚ä¾¿åˆ©ãªã‚‚ã®ã«ã—ã¦ã„ã‚‹ã€‚

## Design Considerations for choosing Embeddings ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã‚’é¸æŠã™ã‚‹éš›ã®è¨­è¨ˆä¸Šã®è€ƒæ…®ç‚¹

### Dimensionality æ¬¡å…ƒæ•°

Dimensionality refers to the number of features of the text represented by the embeddings.
æ¬¡å…ƒæ€§ã¨ã¯ã€åŸ‹ã‚è¾¼ã¿ã«ã‚ˆã£ã¦è¡¨ç¾ã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ç‰¹å¾´ã®æ•°ã‚’æŒ‡ã™ã€‚
This is the number of numerical values used to represent each word/phrase/document in a vector space.
ã“ã‚Œã¯ã€ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã§å„å˜èª/ãƒ•ãƒ¬ãƒ¼ã‚º/æ–‡æ›¸ã‚’è¡¨ç¾ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹æ•°å€¤ã®æ•°ã§ã‚ã‚‹ã€‚
Imagine each word as a point in a multi-dimensional space, where the number of dimensions determines the quality and complexity of this representation.
å„å˜èªã‚’å¤šæ¬¡å…ƒç©ºé–“ã®ç‚¹ã¨æƒ³åƒã—ã€æ¬¡å…ƒæ•°ã«ã‚ˆã£ã¦è¡¨ç¾ã®è³ªã¨è¤‡é›‘ã•ãŒæ±ºã¾ã‚‹ã€‚

Quality of Embeddings: Higher dimensionality of embeddings provides more features to capture subtle nuances and relationships between words.
**ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã®è³ª**ï¼š ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã®æ¬¡å…ƒã‚’é«˜ãã™ã‚‹ã“ã¨ã§ã€å˜èªé–“ã®å¾®å¦™ãªãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ã‚„é–¢ä¿‚æ€§ã‚’æ‰ãˆã‚‹ãŸã‚ã®ç‰¹å¾´ã‚’å¢—ã‚„ã™ã“ã¨ãŒã§ãã‚‹ã€‚
This can lead to improved performance in tasks like machine translation, sentiment analysis, and question answering, where context and precision are important.
ã“ã‚Œã«ã‚ˆã‚Šã€æ©Ÿæ¢°ç¿»è¨³ã€æ„Ÿæƒ…åˆ†æã€è³ªå•å¿œç­”ãªã©ã€æ–‡è„ˆã¨ç²¾åº¦ãŒé‡è¦ãªã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
Whereas lower dimensionality might not fully capture the semantic richness of words but can be sufficient for simpler tasks like word similarity or document clustering.
ä¸€æ–¹ã€æ¬¡å…ƒæ•°ãŒä½ã„ã¨å˜èªã®æ„å‘³çš„ãªè±Šã‹ã•ã‚’ååˆ†ã«æ‰ãˆã‚‰ã‚Œãªã„ã‹ã‚‚ã—ã‚Œãªã„ãŒã€å˜èªã®é¡ä¼¼æ€§ã‚„æ–‡æ›¸ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®ã‚ˆã†ãªå˜ç´”ãªã‚¿ã‚¹ã‚¯ã«ã¯ååˆ†ã§ã‚ã‚‹ã€‚

**Latency**: Higher dimensional embeddings results in larger embedding vectors, requiring more memory and computational resources for storage and processing.
é…å»¶ï¼š **é«˜æ¬¡å…ƒã®åŸ‹ã‚è¾¼ã¿ã¯åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒå¤§ãããªã‚Šã€ä¿å­˜ã¨å‡¦ç†ã«å¤šãã®ãƒ¡ãƒ¢ãƒªã¨è¨ˆç®—è³‡æºã‚’å¿…è¦ã¨ã™ã‚‹**ã€‚(ãã‚Šã‚ƒãã†:thinking:)
This can lead to increased latency, especially in real-time applications or on resource-constrained devices.
ã“ã‚Œã¯ã€ç‰¹ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ»ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ãƒªã‚½ãƒ¼ã‚¹ã«åˆ¶ç´„ã®ã‚ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã§ã¯ã€å¾…ã¡æ™‚é–“ã®å¢—åŠ ã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
Lower dimensional embeddings can enable faster processing and reduced latency
ä½æ¬¡å…ƒã®ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã¯ã€ã‚ˆã‚Šé«˜é€Ÿãªå‡¦ç†ã¨å¾…ã¡æ™‚é–“ã®çŸ­ç¸®ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚

There are newer embedding models that can handle long context and variable dimensions
é•·ã„æ–‡è„ˆã‚„å¯å¤‰æ¬¡å…ƒ(OpenAIã®ã‚„ã¤ã¨ã‹??é€”ä¸­ã§åˆ‡ã£ã¦ã‚‚OKã¿ãŸã„ãª)ã‚’æ‰±ãˆã‚‹ã€ã‚ˆã‚Šæ–°ã—ã„ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ãƒ»ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹ã€‚

- **Long Context Embeddings** that can encode longer sequences
ã‚ˆã‚Šé•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã§ãã‚‹é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿
- **Variable Dimension Embeddings** that can produce a latent representation of an arbitrary number of tokens
ä»»æ„ã®æ•°ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®æ½œåœ¨è¡¨ç¾ã‚’ç”Ÿæˆã§ãã‚‹å¯å¤‰æ¬¡å…ƒåŸ‹ã‚è¾¼ã¿

<!-- ã“ã“ã¾ã§èª­ã‚“ã  -->

### Sparsity ã‚¹ãƒ‘ãƒ¼ã‚¹

Sparsity refers to the proportion of zero values within the embedding vectors.
ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã¨ã¯ã€åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å†…ã®ã‚¼ãƒ­å€¤ã®å‰²åˆã‚’æŒ‡ã™ã€‚

Sparse Embeddings contain a large number of zero values, with only a few non-zero elements.
ã‚¹ãƒ‘ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿ã«ã¯å¤šæ•°ã®ã‚¼ãƒ­å€¤ãŒå«ã¾ã‚Œã€ã‚¼ãƒ­ä»¥å¤–ã®è¦ç´ ã¯ã‚ãšã‹ã§ã‚ã‚‹ã€‚
(ã‚ã€åŸ‹ã‚è¾¼ã¿ã®å®šç¾©ã¨ã—ã¦denceãªãƒ™ã‚¯ãƒˆãƒ«ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ã¨æ€ã£ã¦ã‚‹ã‘ã©ã€sparceã§ã‚‚ã„ã„ã®ã‹ã€‚æµçŸ³ã«one-hotãƒ™ã‚¯ãƒˆãƒ«ã¯åŸ‹ã‚è¾¼ã¿ã¨ã¯è¨€ã‚ãªã„ã®ã‹ãª:thinking:)
They are often associated with high-dimensional representations where many dimensions are irrelevant for the given word.
ã“ã‚Œã‚‰ã¯ã€å¤šãã®æ¬¡å…ƒãŒä¸ãˆã‚‰ã‚ŒãŸå˜èªã«ã¨ã£ã¦ç„¡é–¢ä¿‚ã§ã‚ã‚‹å ´åˆã«é«˜æ¬¡å…ƒè¡¨ç¾ã¨é–¢é€£ã—ã¦ã„ã‚‹ã€‚
SPLADE v2 model provides highly sparse representations and competitive results.
SPLADE v2ãƒ¢ãƒ‡ãƒ«ã¯ã€éå¸¸ã«ã‚¹ãƒ‘ãƒ¼ã‚¹ãªè¡¨ç¾ã¨ç«¶äº‰åŠ›ã®ã‚ã‚‹çµæœã‚’æä¾›ã—ã¾ã™ã€‚
Dense Embeddings have non-zero values in most or all dimensions, capturing a richer and more continuous representation of word meanings
å¯†ãªåŸ‹ã‚è¾¼ã¿ã¯ã€ã»ã¨ã‚“ã©ã®æ¬¡å…ƒã¾ãŸã¯ã™ã¹ã¦ã®æ¬¡å…ƒã§ã‚¼ãƒ­ä»¥å¤–ã®å€¤ã‚’æŒã¡ã€å˜èªã®æ„å‘³ã‚’ã‚ˆã‚Šè±Šã‹ã§é€£ç¶šçš„ã«è¡¨ç¾ã™ã‚‹ã€‚

- Quality of Embeddings: Sparse embeddings might compromise on the semantic information due to the limited number of non-zero values.
ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã®å“è³ªï¼š ç–ãªåŸ‹ã‚è¾¼ã¿ã§ã¯ã€éã‚¼ãƒ­å€¤ã®æ•°ãŒé™ã‚‰ã‚Œã¦ã„ã‚‹ãŸã‚ã€æ„å‘³æƒ…å ±ãŒæãªã‚ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
Dense embeddings capture more semantic information leading to improved accuracy in tasks like machine translation, sentiment analysis, and natural language inference.
å¯†ãªåŸ‹ã‚è¾¼ã¿ã¯ã€æ©Ÿæ¢°ç¿»è¨³ã€æ„Ÿæƒ…åˆ†æã€è‡ªç„¶è¨€èªæ¨è«–ã®ã‚ˆã†ãªã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€ç²¾åº¦ã®å‘ä¸Šã«ã¤ãªãŒã‚‹ã‚ˆã‚Šå¤šãã®æ„å‘³æƒ…å ±ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹ã€‚

- Computational Efficiency: Sparse embeddings allows for optimized storage and computation leading to faster processing.
è¨ˆç®—åŠ¹ç‡ï¼š ã‚¹ãƒ‘ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿ã«ã‚ˆã‚Šã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã¨è¨ˆç®—ãŒæœ€é©åŒ–ã•ã‚Œã€å‡¦ç†ãŒé«˜é€ŸåŒ–ã•ã‚Œã‚‹ã€‚
Dense embeddings would require more storage and computational resources that could impact the performance especially in real time applications.
é«˜å¯†åº¦ã®åŸ‹ã‚è¾¼ã¿ã¯ã€ã‚ˆã‚Šå¤šãã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã¨è¨ˆç®—è³‡æºã‚’å¿…è¦ã¨ã—ã€ç‰¹ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

Dimensionality reduction techniques like Principal Component Analysis (PCA) or Autoencoders can be used to transform high-dimensional dense embeddings into a lower dimensional space.
ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ã‚„ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®ã‚ˆã†ãªæ¬¡å…ƒå‰Šæ¸›æŠ€è¡“ã¯ã€é«˜æ¬¡å…ƒã®å¯†ãªåŸ‹ã‚è¾¼ã¿ã‚’ä½æ¬¡å…ƒç©ºé–“ã«å¤‰æ›ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

### Embedding Algorithms åŸ‹ã‚è¾¼ã¿ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

- **Traditional Algorithms**: Text embedding models like Word2Vec, Glove create static embeddings where each word or phrase has fixed representation regardless of the context.
å¾“æ¥ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  Word2Vecã‚„Gloveã®ã‚ˆã†ãªãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã¯ã€**æ–‡è„ˆã«é–¢ä¿‚ãªãå„å˜èªã‚„ãƒ•ãƒ¬ãƒ¼ã‚ºãŒå›ºå®šã•ã‚ŒãŸè¡¨ç¾ã‚’æŒã¤é™çš„åŸ‹ã‚è¾¼ã¿**ã‚’ä½œæˆã—ã¾ã™ã€‚

- **Contextual Algorithms**: With Transformers, context is included via self-attention making the recent embedding models better at more nuanced understanding of language though increasing computational complexity.
æ–‡è„ˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼š Transformersã§ã¯ã€self-attentionã‚’ä»‹ã—ã¦æ–‡è„ˆãŒå«ã¾ã‚Œã€æœ€è¿‘ã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã¯ã€è¨ˆç®—ã®è¤‡é›‘ã•ãŒå¢—ã™ã‚‚ã®ã®ã€è¨€èªã®å¾®å¦™ãªç†è§£ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚
Embeddings created using contextual algorithms can handle polysemy (words with multiple meanings) and homonymy (words with same spelling but different meanings) much better
æ–‡è„ˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¦ä½œæˆã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿ã¯ã€å¤šç¾©æ€§ï¼ˆè¤‡æ•°ã®æ„å‘³ã‚’æŒã¤å˜èªï¼‰ã‚„åŒéŸ³ç•°ç¾©èªï¼ˆåŒã˜ã‚¹ãƒšãƒ«ã‚’æŒã¤ãŒç•°ãªã‚‹æ„å‘³ã‚’æŒã¤å˜èªï¼‰ã‚’ã¯ã‚‹ã‹ã«ã†ã¾ãå‡¦ç†ã§ãã‚‹ã€‚

### Interpretability è§£é‡ˆå¯èƒ½æ€§

Implementing interpretability in embeddings involves employing techniques to understand and explain the relationships captured within the embedding space.
åŸ‹ã‚è¾¼ã¿ã«è§£é‡ˆå¯èƒ½æ€§ã‚’å®Ÿè£…ã™ã‚‹ã«ã¯ã€åŸ‹ã‚è¾¼ã¿ç©ºé–“å†…ã§æ‰ãˆã‚‰ã‚ŒãŸé–¢ä¿‚ã‚’ç†è§£ã—ã€èª¬æ˜ã™ã‚‹æŠ€è¡“ã‚’æ¡ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

#### Visualization of Embeddings ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã®å¯è¦–åŒ–

**[Embedding Projector](https://projector.tensorflow.org/)**: A tool that interactively visualizes embeddings in 3D, enabling exploration of nearest neighbors, clusters, and semantic relationships.
åŸ‹ã‚è¾¼ã¿ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿ãƒ¼ï¼š åŸ‹ã‚è¾¼ã¿ã‚’3Dã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã«å¯è¦–åŒ–ã—ã€æœ€è¿‘å‚æ¢ç´¢ã€ã‚¯ãƒ©ã‚¹ã‚¿æ¢ç´¢ã€æ„å‘³é–¢ä¿‚æ¢ç´¢ã‚’å¯èƒ½ã«ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã€‚
This tool uses different dimensionality reduction techniques like PCA, T-SNE, UMAP to visualize the embeddings as a 2D or 3D visualization
ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ã€PCAã€T-SNEã€UMAPã®ã‚ˆã†ãªæ§˜ã€…ãªæ¬¡å…ƒå‰Šæ¸›æŠ€è¡“ã‚’ä½¿ç”¨ã—ã¦ã€åŸ‹ã‚è¾¼ã¿ã‚’2Dã¾ãŸã¯3Dã§å¯è¦–åŒ–ã—ã¾ã™ã€‚
(ãŠã€T-SNEã ãƒ¼...!!:thinking:)

[Custom Code for visualizin](https://github.com/GoogleCloudPlatform/generative-ai/blob/68729b9c28fde5fd25147b74a05d65fff4b16800/embeddings/embedding-similarity-visualization.ipynb#L4)g the embeddings can also be leveraged
ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã‚’è¦–è¦šåŒ–ã™ã‚‹ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ¼ãƒ‰ã‚‚æ´»ç”¨ã§ãã¾ã™ã€‚

#### Visualizing Attention Mechanisms attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®å¯è¦–åŒ–

Attention in neural networks: Attention mechanisms highlight the most relevant parts of input data when making predictions, providing insights into which words or features are important for a specific task.
ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã‘ã‚‹attentionï¼š Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯ã€äºˆæ¸¬ã‚’è¡Œã†éš›ã«å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„éƒ¨åˆ†ã‚’å¼·èª¿ã—ã€ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã«é‡è¦ãªå˜èªã‚„ç‰¹å¾´ã‚’ç¤ºã™æ´å¯Ÿã‚’æä¾›ã™ã‚‹ã€‚
Visualizing attention mechanisms using tools like BERTViz can reveal the attention patterns, showing which words or regions of the input contribute most to the output.
**BERTVizã®ã‚ˆã†ãªãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’å¯è¦–åŒ–ã™ã‚‹ã“ã¨ã§ã€attentionãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ˜ã‚‰ã‹ã«ã—ã€å…¥åŠ›ã®ã©ã®å˜èªã‚„é ˜åŸŸãŒå‡ºåŠ›ã«æœ€ã‚‚è²¢çŒ®ã—ã¦ã„ã‚‹ã‹ã‚’ç¤ºã™ã“ã¨ãŒã§ãã‚‹ã€‚**

### Bias and Fairness ãƒã‚¤ã‚¢ã‚¹ã¨å…¬å¹³æ€§

Embeddings are often obtained from training on large pre-existing datasets, and are susceptible to biases due to unfair representations in the original datasets.
ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã¯ã€å¤šãã®å ´åˆã€æ—¢å­˜ã®å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹å­¦ç¿’ã‹ã‚‰å¾—ã‚‰ã‚Œã€å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã‘ã‚‹ä¸å…¬æ­£ãªè¡¨ç¾ã«ã‚ˆã‚‹ãƒã‚¤ã‚¢ã‚¹ã®å½±éŸ¿ã‚’å—ã‘ã‚„ã™ã„ã€‚
Some of the techniques available Word Embedding Fairness Evaluation (WEFE) is an open source library for measuring and mitigating bias in word embedding models.
[Word Embedding Fairness Evaluation (WEFE)](https://www.ijcai.org/proceedings/2020/0060.pdf)ã¯ã€å˜èªåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ã‚¢ã‚¹ã‚’æ¸¬å®šã—ç·©å’Œã™ã‚‹ãŸã‚ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚

WEFE also offers API, Github Repository and documentation to implement the framework.
ã¾ãŸã€WEFEã¯ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’å®Ÿè£…ã™ã‚‹ãŸã‚ã®APIã€Githubãƒªãƒã‚¸ãƒˆãƒªã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚æä¾›ã—ã¦ã„ã‚‹ã€‚

The framework includes different metrics like Word Embedding Association Test (WEAT), Relative Norm Distance (RND), Mean Average Cosine Similarity (MAC), Embedding Coherence Test (ECT)
ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã¯ã€Word Embedding Association Test (WEAT)ã€Relative Norm Distance (RND)ã€Mean Average Cosine Similarity (MAC)ã€Embedding Coherence Test (ECT)ãªã©ã®ã•ã¾ã–ã¾ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

WEFE also includes a mitigation framework and Debias APIs to overcome Bias
WEFEã«ã¯ã€ãƒã‚¤ã‚¢ã‚¹ã‚’å…‹æœã™ã‚‹ãŸã‚ã®ç·©å’Œãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨ãƒ‡ãƒ“ã‚¢ã‚¹APIã‚‚å«ã¾ã‚Œã¦ã„ã‚‹ã€‚

### Latency ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼

There are different ways of setting up Embedding models â€” accessing via HuggingFace, Langchain (uses HuggingFace integration) or use the embedding APIs from various providers.
HuggingFaceã€Langchainï¼ˆHuggingFaceã®çµ±åˆã‚’ä½¿ç”¨ï¼‰ã‚’ä»‹ã—ã¦ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã‹ã€ã¾ãŸã¯æ§˜ã€…ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ã‹ã‚‰ã®åŸ‹ã‚è¾¼ã¿APIã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

Custom embedding models hosted can also be accessed via an endpoint.
ãƒ›ã‚¹ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚«ã‚¹ã‚¿ãƒ åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä»‹ã—ã¦ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚
Network latency can be reduced in this case, however the infrastructure scaling and updating the embedding models would have to be managed.
ã“ã®å ´åˆã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å¾…ã¡æ™‚é–“ã¯çŸ­ç¸®ã§ãã‚‹ãŒã€ã‚¤ãƒ³ãƒ•ãƒ©ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¨åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°ã‚’ç®¡ç†ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚

Embedding APIs from different providers simplified the task of obtaining word embeddings and allowed developers to focus on building applications.
ç•°ãªã‚‹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‹ã‚‰ã®APIã‚’åŸ‹ã‚è¾¼ã‚€ã“ã¨ã§ã€å˜èªã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—ã™ã‚‹ä½œæ¥­ãŒç°¡ç´ åŒ–ã•ã‚Œã€é–‹ç™ºè€…ã¯ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æ§‹ç¯‰ã«é›†ä¸­ã§ãã‚‹ã€‚
They make it easy for the developers to access state of the art NLP technologies.
**ã“ã‚Œã«ã‚ˆã‚Šã€é–‹ç™ºè€…ã¯æœ€å…ˆç«¯ã®NLPæŠ€è¡“ã«ç°¡å˜ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã€‚**(ã“ã‚Œã¯æ­£ç›´ä¾¿åˆ©ã ã‚ˆãªã...è‡ªåˆ†ã§Pytorchã¨ã‹ã§NNã®æ§‹é€ ã‚’å®šç¾©ã—ã¦å­¦ç¿’æ¸ˆã¿é‡ã¿ã ã‘loadã™ã‚‹ã€ã¿ãŸã„ãªã“ã¨ãŒä¸è¦ã ã—:thinking:)
However, if latency, privacy and security are critical, embedding models can also be hosted privately.
**ã—ã‹ã—ã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãŒé‡è¦ãªå ´åˆã¯ã€åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã§ãƒ›ã‚¹ãƒˆã™ã‚‹ã“ã¨ã‚‚ã§ãã‚‹**ã€‚
Embedding APIs can be accessed directly from the provider like Google Cloud text embeddings API.
åŸ‹ã‚è¾¼ã¿APIã¯ã€Google Cloudã®ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿APIã®ã‚ˆã†ã«ã€ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‹ã‚‰ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã€‚
Another alternative is to use LangChain provided integrations with various model providers that allow you to use embeddings with LangChain.
ã‚‚ã†ä¸€ã¤ã®é¸æŠè‚¢ã¯ã€LangChainãŒæä¾›ã™ã‚‹æ§˜ã€…ãªãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ã¨ã®ã‚¤ãƒ³ãƒ†ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½¿ã†ã“ã¨ã§ã™ã€‚

It is important to understand the rate and quota limitations when using the Embedding APIs in your solution.
ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã§åŸ‹ã‚è¾¼ã¿APIã‚’ä½¿ç”¨ã™ã‚‹éš›ã«ã€ãƒ¬ãƒ¼ãƒˆã¨ã‚¯ã‚©ãƒ¼ã‚¿ã®åˆ¶é™ã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚

Embedding APIs from different providers also could vary in latency.
ç•°ãªã‚‹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®APIã‚’åŸ‹ã‚è¾¼ã‚€ã“ã¨ã§ã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ãŒç•°ãªã‚‹å¯èƒ½æ€§ã‚‚ã‚ã‚‹ã€‚
Choice of APIs can be evaluated for latency.
APIã®é¸æŠã¯ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã§è©•ä¾¡ã§ãã‚‹ã€‚
The blog by getzep.com provides a comparison of latency across different APIs.
getzep.comã®ãƒ–ãƒ­ã‚°ã§ã¯ã€ç•°ãªã‚‹APIé–“ã®å¾…ã¡æ™‚é–“ã®æ¯”è¼ƒã‚’æä¾›ã—ã¦ã„ã‚‹ã€‚

### Cost ã‚³ã‚¹ãƒˆ

Cloud based embedding models offer flexibility and scalability, but can incur ongoing costs.
ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ™ãƒ¼ã‚¹ã®çµ„ã¿è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã¯ã€æŸ”è»Ÿæ€§ã¨æ‹¡å¼µæ€§ã‚’æä¾›ã™ã‚‹ãŒã€ç¶™ç¶šçš„ãªã‚³ã‚¹ãƒˆãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
On-premise hosted models require upfront investment in compute but can be more cost-effective in the long run for high volume proprietary models.
ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹ã®ãƒ›ã‚¹ãƒˆå‹ãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆã¸ã®å…ˆè¡ŒæŠ•è³‡ãŒå¿…è¦ã ãŒã€å¤§å®¹é‡ã®ãƒ—ãƒ­ãƒ—ãƒ©ã‚¤ã‚¨ã‚¿ãƒªãƒ»ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€é•·æœŸçš„ã«ã¯ã‚³ã‚¹ãƒˆåŠ¹ç‡ãŒé«˜ããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

Many high quality open source models are available, which might reduce cost.
å¤šãã®é«˜å“è³ªãªã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã§ã€ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã§ãã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚
However, due diligence need to be done to assess the fitment to the use case.
ã—ã‹ã—ã€ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã¸ã®é©åˆæ€§ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã¯ã€due diligence(é©åˆ‡ãªæ³¨æ„)ãŒå¿…è¦ã§ã™ã€‚

For custom models, strategies like quantization and distillation to reduce the model size and batching inference requests can be used to optimize cost
ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹ãŸã‚ã®é‡å­åŒ–ã‚„è’¸ç•™ã®ã‚ˆã†ãªæˆ¦ç•¥ã‚„ã€æ¨è«–è¦æ±‚ã‚’ãƒãƒƒãƒå‡¦ç†ã™ã‚‹ã“ã¨ã§ã€ã‚³ã‚¹ãƒˆã‚’æœ€é©åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

Google Cloud offers different options and cost structures like multimodal embeddings for text, image, video.
Google Cloudã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€å‹•ç”»ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸ‹ã‚è¾¼ã¿ãªã©ã€ã•ã¾ã–ã¾ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ã‚³ã‚¹ãƒˆæ§‹é€ ã‚’æä¾›ã—ã¦ã„ã‚‹ã€‚
Pricing for embeddings for text also varies for online and batch requests.
ã¾ãŸã€ãƒ†ã‚­ã‚¹ãƒˆç”¨ã®ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã®ä¾¡æ ¼ã‚‚ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã¨ãƒãƒƒãƒã§ç•°ãªã‚Šã¾ã™ã€‚

### Ability to Fine Tune fine-tuningã®èƒ½åŠ›

Fine tuning the model on datasets specific to the use case can enhance the performance of the pre-trained model if it is not trained on data relevant to the domain or for the specific needs.
ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«ç‰¹åŒ–ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€äº‹å‰ã«è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

Google Cloud provides options to fine tune the text and multilingual embedding models with user specific data.
Google Cloudã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã¨å¤šè¨€èªåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼å›ºæœ‰ã®ãƒ‡ãƒ¼ã‚¿ã§å¾®èª¿æ•´ã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æä¾›ã—ã¾ã™ã€‚
The tuning is executed using Vertex AI Pipelines
ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯Vertex AI Pipelinesã‚’ä½¿ç”¨ã—ã¦å®Ÿè¡Œã•ã‚Œã‚‹ã€‚

### Selection of Embedding Models for your use case ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«å¿œã˜ãŸåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ

The constant evolution of models makes it challenging to identify the right embedding model for a given task and the nature of the dataset used for the use case.
ãƒ¢ãƒ‡ãƒ«ã®çµ¶ãˆé–“ãªã„é€²åŒ–ã¯ã€ä¸ãˆã‚‰ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚„ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ€§è³ªã«é©ã—ãŸåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®šã™ã‚‹ã“ã¨ã‚’å›°é›£ã«ã—ã¦ã„ã‚‹ã€‚
Massive Text Embedding Benchmark (MTEB) aims to provide clarity on how different models perform on a variety of embedding tasks on different datasets.
[Massive Text Embedding Benchmark (MTEB)](https://arxiv.org/pdf/2210.07316)ã¯ã€ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸Šã®æ§˜ã€…ãªåŸ‹ã‚è¾¼ã¿ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ãŒã©ã®ã‚ˆã†ã«å‹•ä½œã™ã‚‹ã‹ã‚’æ˜ç¢ºã«ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã‚‹ã€‚

MTEB code is available open-source enabling evaluation of any embedding model on different tasks and datasets in less than 10 lines of code.
MTEBã®ã‚³ãƒ¼ãƒ‰ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§å…¬é–‹ã•ã‚Œã¦ãŠã‚Šã€10è¡Œä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã§ã€æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ãŒå¯èƒ½ã§ã™ã€‚

Python code to use MTEB for a custom model
ã‚«ã‚¹ã‚¿ãƒ ãƒ»ãƒ¢ãƒ‡ãƒ«ã«MTEBã‚’ä½¿ç”¨ã™ã‚‹Pythonã‚³ãƒ¼ãƒ‰

```python
import mteb
# load a model from the hub (or for a custom implementation see https://github.com/embeddings-benchmark/mteb/blob/main/docs/reproducible_workflow.md)
model = mteb.get_model("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
tasks = mteb.get_tasks(â€¦) # get specific tasks
# or
from mteb.benchmarks import MTEB_MAIN_EN
tasks = MTEB_MAIN_EN # or use a specific benchmark
evaluation = mteb.MTEB(tasks=tasks)
evaluation.run(model, output_folder="results")
```

MTEB Leaderboard provides insights for selection of the right embedding model based on:
MTEBãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ã¯ã€é©åˆ‡ãªã‚¨ãƒ³ãƒ™ãƒ‡ãƒƒãƒ‰ãƒ»ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’æä¾›ã™ã‚‹ï¼š

- Embedding Model performance based on task and task specific metrics relevant for the use case ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«é–¢é€£ã™ã‚‹ã‚¿ã‚¹ã‚¯ãŠã‚ˆã³ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«åŸºã¥ãã€åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

- Embedding Model performance on different datasets similar to the datasets and languages relevant to the use case ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«é–¢é€£ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨è¨€èªã«é¡ä¼¼ã—ãŸç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã‘ã‚‹åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

MTEB evaluates models on datasets of varying text lengths which are grouped into three categories:
MTEBã¯ã€3ã¤ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«åˆ†é¡ã•ã‚ŒãŸæ§˜ã€…ãªãƒ†ã‚­ã‚¹ãƒˆé•·ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ï¼š

Sentence to Sentence (S2S) â€” a sentence is compared to another sentence
ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ãƒ»ãƒˆã‚¥ãƒ»ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ï¼ˆS2Sï¼‰ - æ–‡ã‚’åˆ¥ã®æ–‡ã¨æ¯”è¼ƒã™ã‚‹ã€‚

Paragraph to Paragraph (P2P) â€” A paragraph is compared with another paragraph
æ®µè½å¯¾æ®µè½ï¼ˆP2Pï¼‰ - æ®µè½ã‚’åˆ¥ã®æ®µè½ã¨æ¯”è¼ƒã™ã‚‹ã€‚

Sentence to Paragraph (S2P) â€” a single sentence is input query, but used to retrieve long documents consisting of multiple sentences.
Sentence to Paragraph (S2P) - å˜æ–‡ãŒå…¥åŠ›ã‚¯ã‚¨ãƒªãƒ¼ã§ã‚ã‚‹ãŒã€è¤‡æ•°ã®æ–‡ã‹ã‚‰ãªã‚‹é•·ã„æ–‡æ›¸ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ã€‚

MTEB leaderboard consists of 199 datasets (as displayed on the leaderboard).
MTEBãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ã¯199ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ï¼ˆãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ã«è¡¨ç¤ºï¼‰ã€‚
These include 10 multilingual datasets for some of the tasks.
ã“ã‚Œã‚‰ã«ã¯ã€ã„ãã¤ã‹ã®ã‚¿ã‚¹ã‚¯ã®ãŸã‚ã®10å€‹ã®å¤šè¨€èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå«ã¾ã‚Œã‚‹ã€‚
Some of the datasets have similarities with each other.
ã„ãã¤ã‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯äº’ã„ã«é¡ä¼¼ã—ã¦ã„ã‚‹ã€‚

Performance of the embedding models on custom datasets or datasets similar to the existing datasets can be evaluated based on the metrics for the given task.
ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¾ãŸã¯æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é¡ä¼¼ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯ã€ä¸ãˆã‚‰ã‚ŒãŸã‚¿ã‚¹ã‚¯ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«åŸºã¥ã„ã¦è©•ä¾¡ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

## References å‚è€ƒæ–‡çŒ®

Embeddings | Machine Learning | Google for Developers
 Machine Learning

On the Dimensionality of Embedding
ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã®æ¬¡å…ƒæ€§ã«ã¤ã„ã¦

SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval
SPLADE v2ï¼š æƒ…å ±æ¤œç´¢ã®ãŸã‚ã®ç–ãªèªå½™ã¨å±•é–‹ãƒ¢ãƒ‡ãƒ«

Embedding projector
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿ãƒ¼ã®åŸ‹ã‚è¾¼ã¿

BertVIZ
ãƒãƒ¼ãƒˆãƒ“ã‚º

WEFE: The Word Embeddings Fairness Evaluation Framework
WEFE å˜èªåŸ‹ã‚è¾¼ã¿å…¬æ­£è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

the WEFE documentation!
WEFEã®æ–‡æ›¸

Mitigation Framework â€” WEFE 0.4.1 documentation
è»½æ¸›ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ - WEFE 0.4.1 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

Text embeddings API | Generative AI on Vertex AI | Google Cloud
 Generative AI on Vertex AI

Embedding models | ğŸ¦œï¸ğŸ”— LangChain
 ğŸ¦œï¸ğŸ”— LangChain

longembed: extending embedding models for long context retrieval
longembedï¼š é•·ã„æ–‡è„ˆæ¤œç´¢ã®ãŸã‚ã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®æ‹¡å¼µ

[2305.09967] Variable Length Embeddings
[2305.09967] å¯å¤‰é•·ã®åŸ‹ã‚è¾¼ã¿

Get multimodal embeddings | Generative AI on Vertex AI | Google Cloud
 Generative AI on Vertex AI

Massive Text Embedding Benchmark
å¤§è¦æ¨¡ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

A Survey of Embedding Models (and why you should look beyond OpenAI)
ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ãƒ»ãƒ¢ãƒ‡ãƒ«ã®èª¿æŸ»ï¼ˆãã—ã¦ã€OpenAIä»¥å¤–ã«ã‚‚ç›®ã‚’å‘ã‘ã‚‹ã¹ãç†ç”±ï¼‰

Tune text embeddings | Generative AI on Vertex AI | Google Cloud
 Generative AI on Vertex AI

<!-- ã“ã“ã¾ã§èª­ã‚“ã ! -->
